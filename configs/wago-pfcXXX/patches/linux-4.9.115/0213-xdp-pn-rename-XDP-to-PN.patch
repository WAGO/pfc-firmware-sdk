From 81b9a8ba333fc88ac39ec21e5799c61a3c784740 Mon Sep 17 00:00:00 2001
From: Andreas Schmidt <andreas.schmidt@wago.com>
Date: Wed, 4 Jul 2018 15:42:43 +0200
Subject: [PATCH 213/261] xdp/pn: rename XDP to PN

Rename all xdp to pn and xsk to pnsk. Also rename
local xs variables to ps.

Signed-off-by: Andreas Schmidt <andreas.schmidt@wago.com>
Signed-off-by: Jan Sondhauss <jan.sondhauss@wago.com>
---
 include/linux/bpf.h                 |  26 +--
 include/linux/bpf_types.h           |   6 +-
 include/linux/filter.h              |  10 +-
 include/linux/netdevice.h           |  34 ++--
 include/linux/socket.h              |   6 +-
 include/net/pn.h                    |  82 ++++-----
 include/net/pn_sock.h               |  45 ++---
 include/uapi/linux/bpf.h            |  22 +--
 include/uapi/linux/if_link.h        |  14 +-
 include/uapi/linux/if_pn.h          |  60 +++---
 kernel/bpf/Makefile                 |   2 +-
 kernel/bpf/verifier.c               |  10 +-
 net/Kconfig                         |   2 +-
 net/Makefile                        |   2 +-
 net/core/Makefile                   |   2 +-
 net/core/dev.c                      |  18 +-
 net/core/filter.c                   |  68 +++----
 net/core/pn.c                       | 140 +++++++-------
 net/core/rtnetlink.c                |  60 +++---
 net/core/sock.c                     |   6 +-
 net/pn/Kconfig                      |   8 +-
 net/pn/Makefile                     |   2 +-
 net/pn/pn_umem.c                    |  54 +++---
 net/pn/pn_umem.h                    |  38 ++--
 net/pn/pn_umem_props.h              |  10 +-
 net/pn/pnsk.c                       | 358 ++++++++++++++++++------------------
 net/pn/pnsk_queue.c                 |  28 +--
 net/pn/pnsk_queue.h                 | 100 +++++-----
 samples/bpf/pnsock.h                |   6 +-
 samples/bpf/pnsock_user.c           | 296 ++++++++++++++---------------
 security/selinux/hooks.c            |   4 +-
 security/selinux/include/classmap.h |   2 +-
 tools/include/uapi/linux/bpf.h      |  21 ++-
 33 files changed, 772 insertions(+), 770 deletions(-)

diff --git a/include/linux/bpf.h b/include/linux/bpf.h
index 9d125f1..bf103d6 100644
--- a/include/linux/bpf.h
+++ b/include/linux/bpf.h
@@ -325,29 +325,29 @@ static inline struct bpf_prog *bpf_prog_inc(struct bpf_prog *prog)
 }
 #endif /* CONFIG_BPF_SYSCALL */
 
-#if defined(CONFIG_XDP_SOCKETS)
-struct xdp_buff;
-struct xdp_sock;
-struct xdp_sock *__xsk_map_lookup_elem(struct bpf_map *map, u32 key);
-int __xsk_map_redirect(struct bpf_map *map, struct xdp_buff *xdp,
-		       struct xdp_sock *xs);
-void __xsk_map_flush(struct bpf_map *map);
+#if defined(CONFIG_PN_SOCKETS)
+struct pn_buff;
+struct pn_sock;
+struct pn_sock *__pnsk_map_lookup_elem(struct bpf_map *map, u32 key);
+int __pnsk_map_redirect(struct bpf_map *map, struct pn_buff *pn,
+		       struct pn_sock *xs);
+void __pnsk_map_flush(struct bpf_map *map);
 #else
-struct xdp_buff;
-struct xdp_sock;
-static inline struct xdp_sock *__xsk_map_lookup_elem(struct bpf_map *map,
+struct pn_buff;
+struct pn_sock;
+static inline struct pn_sock *__pnsk_map_lookup_elem(struct bpf_map *map,
 						     u32 key)
 {
 	return NULL;
 }
 
-static inline int __xsk_map_redirect(struct bpf_map *map, struct xdp_buff *xdp,
-				     struct xdp_sock *xs)
+static inline int __pnsk_map_redirect(struct bpf_map *map, struct pn_buff *pn,
+				     struct pn_sock *xs)
 {
 	return -EOPNOTSUPP;
 }
 
-static inline void __xsk_map_flush(struct bpf_map *map)
+static inline void __pnsk_map_flush(struct bpf_map *map)
 {
 }
 #endif
diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index d7df1b32..142560b 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -5,7 +5,7 @@
 BPF_PROG_TYPE(BPF_PROG_TYPE_SOCKET_FILTER, sk_filter)
 BPF_PROG_TYPE(BPF_PROG_TYPE_SCHED_CLS, tc_cls_act)
 BPF_PROG_TYPE(BPF_PROG_TYPE_SCHED_ACT, tc_cls_act)
-BPF_PROG_TYPE(BPF_PROG_TYPE_XDP, xdp)
+BPF_PROG_TYPE(BPF_PROG_TYPE_PN, pn)
 BPF_PROG_TYPE(BPF_PROG_TYPE_CGROUP_SKB, cg_skb)
 BPF_PROG_TYPE(BPF_PROG_TYPE_CGROUP_SOCK, cg_sock)
 BPF_PROG_TYPE(BPF_PROG_TYPE_CGROUP_SOCK_ADDR, cg_sock_addr)
@@ -49,7 +49,7 @@ BPF_MAP_TYPE(BPF_MAP_TYPE_DEVMAP, dev_map_ops)
 BPF_MAP_TYPE(BPF_MAP_TYPE_SOCKMAP, sock_map_ops)
 #endif
 BPF_MAP_TYPE(BPF_MAP_TYPE_CPUMAP, cpu_map_ops)
-#if defined(CONFIG_XDP_SOCKETS)
-BPF_MAP_TYPE(BPF_MAP_TYPE_XSKMAP, xsk_map_ops)
+#if defined(CONFIG_PN_SOCKETS)
+BPF_MAP_TYPE(BPF_MAP_TYPE_PNSKMAP, pnsk_map_ops)
 #endif
 #endif
diff --git a/include/linux/filter.h b/include/linux/filter.h
index bee7b40..9982b50 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -26,7 +26,7 @@ struct sk_buff;
 struct sock;
 struct seccomp_data;
 struct bpf_prog_aux;
-struct xdp_buff;
+struct pn_buff;
 
 /* ArgX, context and stack frame pointer register positions. Note,
  * Arg1, Arg2, Arg3, etc are used as argument mappings of function
@@ -494,13 +494,13 @@ static inline u32 bpf_prog_run_clear_cb(const struct bpf_prog *prog,
 	return BPF_PROG_RUN(prog, skb);
 }
 
-static inline u32 bpf_prog_run_xdp(const struct bpf_prog *prog,
-				   struct xdp_buff *xdp)
+static inline u32 bpf_prog_run_pn(const struct bpf_prog *prog,
+				   struct pn_buff *pn)
 {
 	u32 ret;
 
 	rcu_read_lock();
-	ret = BPF_PROG_RUN(prog, (void *)xdp);
+	ret = BPF_PROG_RUN(prog, (void *)pn);
 	rcu_read_unlock();
 
 	return ret;
@@ -590,7 +590,7 @@ bool bpf_helper_changes_skb_data(void *func);
 
 struct bpf_prog *bpf_patch_insn_single(struct bpf_prog *prog, u32 off,
 				       const struct bpf_insn *patch, u32 len);
-void bpf_warn_invalid_xdp_action(u32 act);
+void bpf_warn_invalid_pn_action(u32 act);
 
 #ifdef CONFIG_BPF_JIT
 extern int bpf_jit_enable;
diff --git a/include/linux/netdevice.h b/include/linux/netdevice.h
index 77bc34b..a0dfad0 100644
--- a/include/linux/netdevice.h
+++ b/include/linux/netdevice.h
@@ -42,7 +42,7 @@
 #include <linux/ethtool.h>
 #include <net/net_namespace.h>
 #include <net/dsa.h>
-#include <net/xdp.h>
+#include <net/pn.h>
 #ifdef CONFIG_DCB
 #include <net/dcbnl.h>
 #endif
@@ -818,30 +818,30 @@ struct tc_to_netdev {
 	};
 };
 
-/* These structures hold the attributes of xdp state that are being passed
- * to the netdevice through the xdp op.
+/* These structures hold the attributes of pn state that are being passed
+ * to the netdevice through the pn op.
  */
-enum xdp_netdev_command {
+enum pn_netdev_command {
 	/* Set or clear a bpf program used in the earliest stages of packet
-	 * rx. The prog will have been loaded as BPF_PROG_TYPE_XDP. The callee
+	 * rx. The prog will have been loaded as BPF_PROG_TYPE_PN. The callee
 	 * is responsible for calling bpf_prog_put on any old progs that are
 	 * stored. In case of error, the callee need not release the new prog
 	 * reference, but on success it takes ownership and must bpf_prog_put
 	 * when it is no longer used.
 	 */
-	XDP_SETUP_PROG,
+	PN_SETUP_PROG,
 	/* Check if a bpf program is set on the device.  The callee should
 	 * return true if a program is currently attached and running.
 	 */
-	XDP_QUERY_PROG,
+	PN_QUERY_PROG,
 };
 
-struct netdev_xdp {
-	enum xdp_netdev_command command;
+struct netdev_pn {
+	enum pn_netdev_command command;
 	union {
-		/* XDP_SETUP_PROG */
+		/* PN_SETUP_PROG */
 		struct bpf_prog *prog;
-		/* XDP_QUERY_PROG */
+		/* PN_QUERY_PROG */
 		bool prog_attached;
 	};
 };
@@ -1143,9 +1143,9 @@ struct netdev_xdp {
  *	appropriate rx headroom value allows avoiding skb head copy on
  *	forward. Setting a negative value resets the rx headroom to the
  *	default value.
- * int (*ndo_xdp)(struct net_device *dev, struct netdev_xdp *xdp);
- *	This function is used to set or query state related to XDP on the
- *	netdevice. See definition of enum xdp_netdev_command for details.
+ * int (*ndo_pn)(struct net_device *dev, struct netdev_pn *pn);
+ *	This function is used to set or query state related to PN on the
+ *	netdevice. See definition of enum pn_netdev_command for details.
  *
  */
 struct net_device_ops {
@@ -1335,8 +1335,8 @@ struct net_device_ops {
 						       struct sk_buff *skb);
 	void			(*ndo_set_rx_headroom)(struct net_device *dev,
 						       int needed_headroom);
-	int			(*ndo_xdp)(struct net_device *dev,
-					   struct netdev_xdp *xdp);
+	int			(*ndo_pn)(struct net_device *dev,
+					   struct netdev_pn *pn);
 };
 
 /**
@@ -3413,7 +3413,7 @@ int dev_get_phys_port_id(struct net_device *dev,
 int dev_get_phys_port_name(struct net_device *dev,
 			   char *name, size_t len);
 int dev_change_proto_down(struct net_device *dev, bool proto_down);
-int dev_change_xdp_fd(struct net_device *dev, int fd);
+int dev_change_pn_fd(struct net_device *dev, int fd);
 struct sk_buff *validate_xmit_skb_list(struct sk_buff *skb, struct net_device *dev);
 struct sk_buff *dev_hard_start_xmit(struct sk_buff *skb, struct net_device *dev,
 				    struct netdev_queue *txq, int *ret);
diff --git a/include/linux/socket.h b/include/linux/socket.h
index c05adad..fd53510 100644
--- a/include/linux/socket.h
+++ b/include/linux/socket.h
@@ -202,7 +202,7 @@ struct ucred {
 #define AF_VSOCK	40	/* vSockets			*/
 #define AF_KCM		41	/* Kernel Connection Multiplexor*/
 #define AF_QIPCRTR	42	/* Qualcomm IPC Router          */
-#define AF_XDP		43  /* XDP socket*/
+#define AF_PN		43	/* PROFINET socket*/
 
 #define AF_MAX		44	/* For now.. */
 
@@ -252,7 +252,7 @@ struct ucred {
 #define PF_VSOCK	AF_VSOCK
 #define PF_KCM		AF_KCM
 #define PF_QIPCRTR	AF_QIPCRTR
-#define PF_XDP		AF_XDP
+#define PF_PN		AF_PN
 #define PF_MAX		AF_MAX
 
 /* Maximum queue length specifiable by listen.  */
@@ -331,7 +331,7 @@ struct ucred {
 #define SOL_ALG		279
 #define SOL_NFC		280
 #define SOL_KCM		281
-#define SOL_XDP		282
+#define SOL_PN		282
 
 /* IPX options */
 #define IPX_TYPE	1
diff --git a/include/net/pn.h b/include/net/pn.h
index 1ad52a3..0607a09 100644
--- a/include/net/pn.h
+++ b/include/net/pn.h
@@ -1,4 +1,4 @@
-/* include/net/xdp.h
+/* include/net/pn.h
  *
  * Copyright (c) 2017 Jesper Dangaard Brouer, Red Hat Inc.
  * Released under terms in GPL version 2.  See COPYING.
@@ -9,12 +9,12 @@
 /**
  * DOC: XDP RX-queue information
  *
- * The XDP RX-queue info (xdp_rxq_info) is associated with the driver
+ * The XDP RX-queue info (pn_rxq_info) is associated with the driver
  * level RX-ring queues.  It is information that is specific to how
  * the driver have configured a given RX-ring queue.
  *
- * Each xdp_buff frame received in the driver carry a (pointer)
- * reference to this xdp_rxq_info structure.  This provides the XDP
+ * Each pn_buff frame received in the driver carry a (pointer)
+ * reference to this pn_rxq_info structure.  This provides the XDP
  * data-path read-access to RX-info for both kernel and bpf-side
  * (limited subset).
  *
@@ -33,103 +33,103 @@
  * also mandatory during RX-ring setup.
  */
 
-enum xdp_mem_type {
+enum pn_mem_type {
 	MEM_TYPE_PAGE_SHARED = 0, /* Split-page refcnt based model */
 	MEM_TYPE_PAGE_ORDER0,     /* Orig XDP full page model */
 	MEM_TYPE_PAGE_POOL,
 	MEM_TYPE_MAX,
 };
 
-struct xdp_mem_info {
-	u32 type; /* enum xdp_mem_type, but known size type */
+struct pn_mem_info {
+	u32 type; /* enum pn_mem_type, but known size type */
 	u32 id;
 };
 
 struct page_pool;
 
-struct xdp_rxq_info {
+struct pn_rxq_info {
 	struct net_device *dev;
 	u32 queue_index;
 	u32 reg_state;
-	struct xdp_mem_info mem;
+	struct pn_mem_info mem;
 } ____cacheline_aligned; /* perf critical, avoid false-sharing */
 
-struct xdp_buff {
+struct pn_buff {
 	void *data;
 	void *data_end;
 	void *data_meta;
 	void *data_hard_start;
-	struct xdp_rxq_info *rxq;
+	struct pn_rxq_info *rxq;
 };
 
-struct xdp_frame {
+struct pn_frame {
 	void *data;
 	u16 len;
 	u16 headroom;
 	u16 metasize;
-	/* Lifetime of xdp_rxq_info is limited to NAPI/enqueue time,
+	/* Lifetime of pn_rxq_info is limited to NAPI/enqueue time,
 	 * while mem info is valid on remote CPU.
 	 */
-	struct xdp_mem_info mem;
+	struct pn_mem_info mem;
 	struct net_device *dev_rx; /* used by cpumap */
 };
 
-/* Convert xdp_buff to xdp_frame */
+/* Convert pn_buff to pn_frame */
 static inline
-struct xdp_frame *convert_to_xdp_frame(struct xdp_buff *xdp)
+struct pn_frame *convert_to_pn_frame(struct pn_buff *pn)
 {
-	struct xdp_frame *xdp_frame;
+	struct pn_frame *pn_frame;
 	int metasize;
 	int headroom;
 
 	/* Assure headroom is available for storing info */
-	headroom = xdp->data - xdp->data_hard_start;
-	metasize = xdp->data - xdp->data_meta;
+	headroom = pn->data - pn->data_hard_start;
+	metasize = pn->data - pn->data_meta;
 	metasize = metasize > 0 ? metasize : 0;
-	if (unlikely((headroom - metasize) < sizeof(*xdp_frame)))
+	if (unlikely((headroom - metasize) < sizeof(*pn_frame)))
 		return NULL;
 
 	/* Store info in top of packet */
-	xdp_frame = xdp->data_hard_start;
+	pn_frame = pn->data_hard_start;
 
-	xdp_frame->data = xdp->data;
-	xdp_frame->len  = xdp->data_end - xdp->data;
-	xdp_frame->headroom = headroom - sizeof(*xdp_frame);
-	xdp_frame->metasize = metasize;
+	pn_frame->data = pn->data;
+	pn_frame->len  = pn->data_end - pn->data;
+	pn_frame->headroom = headroom - sizeof(*pn_frame);
+	pn_frame->metasize = metasize;
 
-	/* rxq only valid until napi_schedule ends, convert to xdp_mem_info */
-	xdp_frame->mem = xdp->rxq->mem;
+	/* rxq only valid until napi_schedule ends, convert to pn_mem_info */
+	pn_frame->mem = pn->rxq->mem;
 
-	return xdp_frame;
+	return pn_frame;
 }
 
-void xdp_return_frame(struct xdp_frame *xdpf);
-void xdp_return_buff(struct xdp_buff *xdp);
+void pn_return_frame(struct pn_frame *pnf);
+void pn_return_buff(struct pn_buff *pn);
 
-int xdp_rxq_info_reg(struct xdp_rxq_info *xdp_rxq,
+int pn_rxq_info_reg(struct pn_rxq_info *pn_rxq,
 		     struct net_device *dev, u32 queue_index);
-void xdp_rxq_info_unreg(struct xdp_rxq_info *xdp_rxq);
-void xdp_rxq_info_unused(struct xdp_rxq_info *xdp_rxq);
-bool xdp_rxq_info_is_reg(struct xdp_rxq_info *xdp_rxq);
-int xdp_rxq_info_reg_mem_model(struct xdp_rxq_info *xdp_rxq,
-			       enum xdp_mem_type type, void *allocator);
+void pn_rxq_info_unreg(struct pn_rxq_info *pn_rxq);
+void pn_rxq_info_unused(struct pn_rxq_info *pn_rxq);
+bool pn_rxq_info_is_reg(struct pn_rxq_info *pn_rxq);
+int pn_rxq_info_reg_mem_model(struct pn_rxq_info *pn_rxq,
+			       enum pn_mem_type type, void *allocator);
 
 struct sk_buff;
-int xsk_generic_rcv_skb(struct sk_buff *skb);
+int pnsk_generic_rcv_skb(struct sk_buff *skb);
 
 /* Drivers not supporting XDP metadata can use this helper, which
  * rejects any room expansion for metadata as a result.
  */
 static __always_inline void
-xdp_set_data_meta_invalid(struct xdp_buff *xdp)
+pn_set_data_meta_invalid(struct pn_buff *pn)
 {
-	xdp->data_meta = xdp->data + 1;
+	pn->data_meta = pn->data + 1;
 }
 
 static __always_inline bool
-xdp_data_meta_unsupported(const struct xdp_buff *xdp)
+pn_data_meta_unsupported(const struct pn_buff *pn)
 {
-	return unlikely(xdp->data_meta > xdp->data);
+	return unlikely(pn->data_meta > pn->data);
 }
 
 #endif /* __LINUX_NET_XDP_H__ */
diff --git a/include/net/pn_sock.h b/include/net/pn_sock.h
index 185f492..f46af5d 100644
--- a/include/net/pn_sock.h
+++ b/include/net/pn_sock.h
@@ -1,5 +1,5 @@
 /* SPDX-License-Identifier: GPL-2.0
- * AF_XDP internal functions
+ * AF_PN internal functions
  * Copyright(c) 2018 Intel Corporation.
  *
  * This program is free software; you can redistribute it and/or modify it
@@ -12,55 +12,56 @@
  * more details.
  */
 
-#ifndef _LINUX_XDP_SOCK_H
-#define _LINUX_XDP_SOCK_H
+#ifndef _LINUX_PN_SOCK_H
+#define _LINUX_PN_SOCK_H
 
 #include <linux/mutex.h>
 #include <net/sock.h>
 
 struct net_device;
-struct xsk_queue;
-struct xdp_umem;
+struct pnsk_queue;
+struct pn_umem;
 
-struct xdp_sock {
-	/* struct sock must be the first member of struct xdp_sock */
+struct pn_sock {
+	/* struct sock must be the first member of struct pn_sock */
 	struct sock sk;
-	struct xsk_queue *rx;
+	struct pnsk_queue *rx;
 	struct net_device *dev;
-	struct xdp_umem *umem;
+	struct pn_umem *umem;
 	struct list_head flush_node;
 	u16 queue_id;
-	struct xsk_queue *tx ____cacheline_aligned_in_smp;
+	struct pnsk_queue *tx ____cacheline_aligned_in_smp;
 	/* Protects multiple processes in the control path */
 	struct mutex mutex;
 	u64 rx_dropped;
 };
 
-struct xdp_buff;
-#ifdef CONFIG_XDP_SOCKETS
-int xsk_generic_rcv(struct xdp_sock *xs, struct xdp_buff *xdp);
-int xsk_rcv(struct xdp_sock *xs, struct xdp_buff *xdp);
-void xsk_flush(struct xdp_sock *xs);
-bool xsk_is_setup_for_bpf_map(struct xdp_sock *xs);
+struct pn_buff;
+#ifdef CONFIG_PN_SOCKETS
+int pnsk_generic_rcv(struct pn_sock *xs, struct pn_buff *pn);
+int pnsk_rcv(struct pn_sock *xs, struct pn_buff *pn);
+void pnsk_flush(struct pn_sock *xs);
+bool pnsk_is_setup_for_bpf_map(struct pn_sock *xs);
 #else
-static inline int xsk_generic_rcv(struct xdp_sock *xs, struct xdp_buff *xdp)
+static inline int pnsk_generic_rcv(struct pn_sock *xs, struct pn_buff *pn)
 {
 	return -ENOTSUPP;
 }
 
-static inline int xsk_rcv(struct xdp_sock *xs, struct xdp_buff *xdp)
+static inline int pnsk_rcv(struct pn_sock *xs, struct pn_buff *pn)
 {
 	return -ENOTSUPP;
 }
 
-static inline void xsk_flush(struct xdp_sock *xs)
+static inline void pnsk_flush(struct pn_sock *xs)
 {
 }
 
-static inline bool xsk_is_setup_for_bpf_map(struct xdp_sock *xs)
+static inline bool pnsk_is_setup_for_bpf_map(struct pn_sock *xs)
+
 {
 	return false;
 }
-#endif /* CONFIG_XDP_SOCKETS */
+#endif /* CONFIG_PN_SOCKETS */
 
-#endif /* _LINUX_XDP_SOCK_H */
+#endif /* _LINUX_PN_SOCK_H */
diff --git a/include/uapi/linux/bpf.h b/include/uapi/linux/bpf.h
index f747a8f..e852fd9 100644
--- a/include/uapi/linux/bpf.h
+++ b/include/uapi/linux/bpf.h
@@ -94,7 +94,7 @@ enum bpf_prog_type {
 	BPF_PROG_TYPE_SCHED_CLS,
 	BPF_PROG_TYPE_SCHED_ACT,
 	BPF_PROG_TYPE_TRACEPOINT,
-	BPF_PROG_TYPE_XDP,
+	BPF_PROG_TYPE_PN,
 	BPF_PROG_TYPE_PERF_EVENT,
 };
 
@@ -501,24 +501,24 @@ struct bpf_tunnel_key {
 	__u32 tunnel_label;
 };
 
-#define XDP_PACKET_HEADROOM 256
+#define PN_PACKET_HEADROOM 256
 
-/* User return codes for XDP prog type.
- * A valid XDP program must return one of these defined values. All other
+/* User return codes for PN prog type.
+ * A valid PN program must return one of these defined values. All other
  * return codes are reserved for future use. Unknown return codes will result
  * in packet drop.
  */
-enum xdp_action {
-	XDP_ABORTED = 0,
-	XDP_DROP,
-	XDP_PASS,
-	XDP_TX,
+enum pn_action {
+	PN_ABORTED = 0,
+	PN_DROP,
+	PN_PASS,
+	PN_TX,
 };
 
-/* user accessible metadata for XDP packet hook
+/* user accessible metadata for PN packet hook
  * new fields must be added to the end of this structure
  */
-struct xdp_md {
+struct pn_md {
 	__u32 data;
 	__u32 data_end;
 };
diff --git a/include/uapi/linux/if_link.h b/include/uapi/linux/if_link.h
index b4fba66..1735c6c 100644
--- a/include/uapi/linux/if_link.h
+++ b/include/uapi/linux/if_link.h
@@ -156,7 +156,7 @@ enum {
 	IFLA_GSO_MAX_SEGS,
 	IFLA_GSO_MAX_SIZE,
 	IFLA_PAD,
-	IFLA_XDP,
+	IFLA_PN,
 	__IFLA_MAX
 };
 
@@ -872,15 +872,15 @@ enum {
 };
 #define IFLA_OFFLOAD_XSTATS_MAX (__IFLA_OFFLOAD_XSTATS_MAX - 1)
 
-/* XDP section */
+/* PN section */
 
 enum {
-	IFLA_XDP_UNSPEC,
-	IFLA_XDP_FD,
-	IFLA_XDP_ATTACHED,
-	__IFLA_XDP_MAX,
+	IFLA_PN_UNSPEC,
+	IFLA_PN_FD,
+	IFLA_PN_ATTACHED,
+	__IFLA_PN_MAX,
 };
 
-#define IFLA_XDP_MAX (__IFLA_XDP_MAX - 1)
+#define IFLA_PN_MAX (__IFLA_PN_MAX - 1)
 
 #endif /* _UAPI_LINUX_IF_LINK_H */
diff --git a/include/uapi/linux/if_pn.h b/include/uapi/linux/if_pn.h
index 6f74114..f9b1e8c 100644
--- a/include/uapi/linux/if_pn.h
+++ b/include/uapi/linux/if_pn.h
@@ -1,6 +1,6 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
  *
- * if_xdp: XDP socket user-space interface
+ * if_pn: PN socket user-space interface
  * Copyright(c) 2018 Intel Corporation.
  *
  * This program is free software; you can redistribute it and/or modify it
@@ -16,30 +16,30 @@
  *	      Magnus Karlsson <magnus.karlsson@intel.com>
  */
 
-#ifndef _LINUX_IF_XDP_H
-#define _LINUX_IF_XDP_H
+#ifndef _LINUX_IF_PN_H
+#define _LINUX_IF_PN_H
 
 #include <linux/types.h>
 
-/* Options for the sxdp_flags field */
-#define XDP_SHARED_UMEM 1
+/* Options for the spn_flags field */
+#define PN_SHARED_UMEM 1
 
-struct sockaddr_xdp {
-	__u16 sxdp_family;
-	__u32 sxdp_ifindex;
-	__u32 sxdp_queue_id;
-	__u32 sxdp_shared_umem_fd;
-	__u16 sxdp_flags;
+struct sockaddr_pn {
+	__u16 spn_family;
+	__u32 spn_ifindex;
+	__u32 spn_queue_id;
+	__u32 spn_shared_umem_fd;
+	__u16 spn_flags;
 };
 
-/* XDP socket options */
-#define XDP_RX_RING			1
-#define XDP_TX_RING			2
-#define XDP_UMEM_REG			3
-#define XDP_UMEM_FILL_RING		4
-#define XDP_UMEM_COMPLETION_RING	5
+/* PN socket options */
+#define PN_RX_RING			1
+#define PN_TX_RING			2
+#define PN_UMEM_REG			3
+#define PN_UMEM_FILL_RING		4
+#define PN_UMEM_COMPLETION_RING	5
 
-struct xdp_umem_reg {
+struct pn_umem_reg {
 	__u64 addr; /* Start of packet data area */
 	__u64 len; /* Length of packet data area */
 	__u32 frame_size; /* Frame size */
@@ -47,12 +47,12 @@ struct xdp_umem_reg {
 };
 
 /* Pgoff for mmaping the rings */
-#define XDP_PGOFF_RX_RING			 0
-#define XDP_PGOFF_TX_RING		0x80000000
-#define XDP_UMEM_PGOFF_FILL_RING	0x10000000
-#define XDP_UMEM_PGOFF_COMPLETION_RING	0x18000000
+#define PN_PGOFF_RX_RING			 0
+#define PN_PGOFF_TX_RING		0x80000000
+#define PN_UMEM_PGOFF_FILL_RING		0x10000000
+#define PN_UMEM_PGOFF_COMPLETION_RING	0x18000000
 
-struct xdp_desc {
+struct pn_desc {
 	__u32 idx;
 	__u32 len;
 	__u16 offset;
@@ -60,21 +60,21 @@ struct xdp_desc {
 	__u8 padding[5];
 };
 
-struct xdp_ring {
+struct pn_ring {
 	__u32 producer __attribute__((aligned(64)));
 	__u32 consumer __attribute__((aligned(64)));
 };
 
 /* Used for the RX and TX queues for packets */
-struct xdp_rxtx_ring {
-	struct xdp_ring ptrs;
-	struct xdp_desc desc[0] __attribute__((aligned(64)));
+struct pn_rxtx_ring {
+	struct pn_ring ptrs;
+	struct pn_desc desc[0] __attribute__((aligned(64)));
 };
 
 /* Used for the fill and completion queues for buffers */
-struct xdp_umem_ring {
-	struct xdp_ring ptrs;
+struct pn_umem_ring {
+	struct pn_ring ptrs;
 	__u32 desc[0] __attribute__((aligned(64)));
 };
 
-#endif /* _LINUX_IF_XDP_H */
+#endif /* _LINUX_IF_PN_H */
diff --git a/kernel/bpf/Makefile b/kernel/bpf/Makefile
index 3aab70c4..1a04f9b 100644
--- a/kernel/bpf/Makefile
+++ b/kernel/bpf/Makefile
@@ -6,5 +6,5 @@ ifeq ($(CONFIG_PERF_EVENTS),y)
 obj-$(CONFIG_BPF_SYSCALL) += stackmap.o
 endif
 ifeq ($(CONFIG_XDP_SOCKETS),y)
-obj-$(CONFIG_BPF_SYSCALL) += xskmap.o
+obj-$(CONFIG_BPF_SYSCALL) += pnskmap.o
 endif
diff --git a/kernel/bpf/verifier.c b/kernel/bpf/verifier.c
index 9868479..bcc2302 100644
--- a/kernel/bpf/verifier.c
+++ b/kernel/bpf/verifier.c
@@ -636,7 +636,7 @@ static bool may_access_direct_pkt_data(struct bpf_verifier_env *env,
 	switch (env->prog->type) {
 	case BPF_PROG_TYPE_SCHED_CLS:
 	case BPF_PROG_TYPE_SCHED_ACT:
-	case BPF_PROG_TYPE_XDP:
+	case BPF_PROG_TYPE_PN:
 		if (meta)
 			return meta->pkt_access;
 
@@ -1124,8 +1124,8 @@ static int check_map_func_compatibility(struct bpf_map *map, int func_id)
 		    func_id != BPF_FUNC_current_task_under_cgroup)
 			goto error;
 		break;
-/* TODO(JSo): how to implement for xsk??
-	case BPF_MAP_TYPE_XSKMAP:
+/* TODO(JSo): how to implement for pnsk??
+	case BPF_MAP_TYPE_PNSKMAP:
 		if (func_id != BPF_FUNC_redirect_map)
 			goto error;
 		break;
@@ -1154,11 +1154,11 @@ static int check_map_func_compatibility(struct bpf_map *map, int func_id)
 		if (map->map_type != BPF_MAP_TYPE_CGROUP_ARRAY)
 			goto error;
 		break;
-/* TODO(JSo): how to implement for xsk?
+/* TODO(JSo): how to implement for pnsk?
 	case BPF_FUNC_redirect_map:
 		if (map->map_type != BPF_MAP_TYPE_DEVMAP &&
 		    map->map_type != BPF_MAP_TYPE_CPUMAP &&
-		    map->map_type != BPF_MAP_TYPE_XSKMAP)
+		    map->map_type != BPF_MAP_TYPE_PNSKMAP)
 			goto error;
 		break;
 */
diff --git a/net/Kconfig b/net/Kconfig
index 1f480bd..a805856 100644
--- a/net/Kconfig
+++ b/net/Kconfig
@@ -57,7 +57,7 @@ source "net/packet/Kconfig"
 source "net/unix/Kconfig"
 source "net/xfrm/Kconfig"
 source "net/iucv/Kconfig"
-source "net/xdp/Kconfig"
+source "net/pn/Kconfig"
 
 config INET
 	bool "TCP/IP networking"
diff --git a/net/Makefile b/net/Makefile
index 6105b5b..7591098 100644
--- a/net/Makefile
+++ b/net/Makefile
@@ -81,4 +81,4 @@ obj-y				+= l3mdev/
 endif
 obj-$(CONFIG_QRTR)		+= qrtr/
 obj-$(CONFIG_NET_NCSI)		+= ncsi/
-obj-$(CONFIG_XDP_SOCKETS)	+= xdp/
+obj-$(CONFIG_PN_SOCKETS)	+= pn/
diff --git a/net/core/Makefile b/net/core/Makefile
index d91f4ab..78bf4d2 100644
--- a/net/core/Makefile
+++ b/net/core/Makefile
@@ -9,7 +9,7 @@ obj-$(CONFIG_SYSCTL) += sysctl_net_core.o
 
 obj-y		     += dev.o ethtool.o dev_addr_lists.o dst.o netevent.o \
 			neighbour.o rtnetlink.o utils.o link_watch.o filter.o \
-			sock_diag.o dev_ioctl.o tso.o sock_reuseport.o xdp.o
+			sock_diag.o dev_ioctl.o tso.o sock_reuseport.o pn.o
 
 obj-$(CONFIG_XFRM) += flow.o
 obj-y += net-sysfs.o
diff --git a/net/core/dev.c b/net/core/dev.c
index 719eb2e..bbbdba6 100644
--- a/net/core/dev.c
+++ b/net/core/dev.c
@@ -6812,36 +6812,36 @@ int dev_change_proto_down(struct net_device *dev, bool proto_down)
 EXPORT_SYMBOL(dev_change_proto_down);
 
 /**
- *	dev_change_xdp_fd - set or clear a bpf program for a device rx path
+ *	dev_change_pn_fd - set or clear a bpf program for a device rx path
  *	@dev: device
  *	@fd: new program fd or negative value to clear
  *
  *	Set or clear a bpf program for a device
  */
-int dev_change_xdp_fd(struct net_device *dev, int fd)
+int dev_change_pn_fd(struct net_device *dev, int fd)
 {
 	const struct net_device_ops *ops = dev->netdev_ops;
 	struct bpf_prog *prog = NULL;
-	struct netdev_xdp xdp = {};
+	struct netdev_pn pn = {};
 	int err;
 
-	if (!ops->ndo_xdp)
+	if (!ops->ndo_pn)
 		return -EOPNOTSUPP;
 	if (fd >= 0) {
-		prog = bpf_prog_get_type(fd, BPF_PROG_TYPE_XDP);
+		prog = bpf_prog_get_type(fd, BPF_PROG_TYPE_PN);
 		if (IS_ERR(prog))
 			return PTR_ERR(prog);
 	}
 
-	xdp.command = XDP_SETUP_PROG;
-	xdp.prog = prog;
-	err = ops->ndo_xdp(dev, &xdp);
+	pn.command = PN_SETUP_PROG;
+	pn.prog = prog;
+	err = ops->ndo_pn(dev, &pn);
 	if (err < 0 && prog)
 		bpf_prog_put(prog);
 
 	return err;
 }
-EXPORT_SYMBOL(dev_change_xdp_fd);
+EXPORT_SYMBOL(dev_change_pn_fd);
 
 /**
  *	dev_new_index	-	allocate an ifindex
diff --git a/net/core/filter.c b/net/core/filter.c
index e7a46cb..c34f71a 100644
--- a/net/core/filter.c
+++ b/net/core/filter.c
@@ -2501,29 +2501,29 @@ static const struct bpf_func_proto bpf_skb_under_cgroup_proto = {
 	.arg3_type	= ARG_ANYTHING,
 };
 
-static unsigned long bpf_xdp_copy(void *dst_buff, const void *src_buff,
+static unsigned long bpf_pn_copy(void *dst_buff, const void *src_buff,
 				  unsigned long off, unsigned long len)
 {
 	memcpy(dst_buff, src_buff + off, len);
 	return 0;
 }
 
-BPF_CALL_5(bpf_xdp_event_output, struct xdp_buff *, xdp, struct bpf_map *, map,
+BPF_CALL_5(bpf_pn_event_output, struct pn_buff *, pn, struct bpf_map *, map,
 	   u64, flags, void *, meta, u64, meta_size)
 {
-	u64 xdp_size = (flags & BPF_F_CTXLEN_MASK) >> 32;
+	u64 pn_size = (flags & BPF_F_CTXLEN_MASK) >> 32;
 
 	if (unlikely(flags & ~(BPF_F_CTXLEN_MASK | BPF_F_INDEX_MASK)))
 		return -EINVAL;
-	if (unlikely(xdp_size > (unsigned long)(xdp->data_end - xdp->data)))
+	if (unlikely(pn_size > (unsigned long)(pn->data_end - pn->data)))
 		return -EFAULT;
 
-	return bpf_event_output(map, flags, meta, meta_size, xdp, xdp_size,
-				bpf_xdp_copy);
+	return bpf_event_output(map, flags, meta, meta_size, pn, pn_size,
+				bpf_pn_copy);
 }
 
-static const struct bpf_func_proto bpf_xdp_event_output_proto = {
-	.func		= bpf_xdp_event_output,
+static const struct bpf_func_proto bpf_pn_event_output_proto = {
+	.func		= bpf_pn_event_output,
 	.gpl_only	= true,
 	.ret_type	= RET_INTEGER,
 	.arg1_type	= ARG_PTR_TO_CTX,
@@ -2619,11 +2619,11 @@ tc_cls_act_func_proto(enum bpf_func_id func_id)
 }
 
 static const struct bpf_func_proto *
-xdp_func_proto(enum bpf_func_id func_id)
+pn_func_proto(enum bpf_func_id func_id)
 {
 	switch (func_id) {
 	case BPF_FUNC_perf_event_output:
-		return &bpf_xdp_event_output_proto;
+		return &bpf_pn_event_output_proto;
 	case BPF_FUNC_get_smp_processor_id:
 		return &bpf_get_smp_processor_id_proto;
 	default:
@@ -2737,10 +2737,10 @@ static bool tc_cls_act_is_valid_access(int off, int size,
 	return __is_valid_access(off, size, type);
 }
 
-static bool __is_valid_xdp_access(int off, int size,
+static bool __is_valid_pn_access(int off, int size,
 				  enum bpf_access_type type)
 {
-	if (off < 0 || off >= sizeof(struct xdp_md))
+	if (off < 0 || off >= sizeof(struct pn_md))
 		return false;
 	if (off % size != 0)
 		return false;
@@ -2750,7 +2750,7 @@ static bool __is_valid_xdp_access(int off, int size,
 	return true;
 }
 
-static bool xdp_is_valid_access(int off, int size,
+static bool pn_is_valid_access(int off, int size,
 				enum bpf_access_type type,
 				enum bpf_reg_type *reg_type)
 {
@@ -2758,22 +2758,22 @@ static bool xdp_is_valid_access(int off, int size,
 		return false;
 
 	switch (off) {
-	case offsetof(struct xdp_md, data):
+	case offsetof(struct pn_md, data):
 		*reg_type = PTR_TO_PACKET;
 		break;
-	case offsetof(struct xdp_md, data_end):
+	case offsetof(struct pn_md, data_end):
 		*reg_type = PTR_TO_PACKET_END;
 		break;
 	}
 
-	return __is_valid_xdp_access(off, size, type);
+	return __is_valid_pn_access(off, size, type);
 }
 
-void bpf_warn_invalid_xdp_action(u32 act)
+void bpf_warn_invalid_pn_action(u32 act)
 {
-	WARN_ONCE(1, "Illegal XDP return value %u, expect packet loss\n", act);
+	WARN_ONCE(1, "Illegal PN return value %u, expect packet loss\n", act);
 }
-EXPORT_SYMBOL_GPL(bpf_warn_invalid_xdp_action);
+EXPORT_SYMBOL_GPL(bpf_warn_invalid_pn_action);
 
 static u32 sk_filter_convert_ctx_access(enum bpf_access_type type, int dst_reg,
 					int src_reg, int ctx_off,
@@ -2951,7 +2951,7 @@ static u32 tc_cls_act_convert_ctx_access(enum bpf_access_type type, int dst_reg,
 	return insn - insn_buf;
 }
 
-static u32 xdp_convert_ctx_access(enum bpf_access_type type, int dst_reg,
+static u32 pn_convert_ctx_access(enum bpf_access_type type, int dst_reg,
 				  int src_reg, int ctx_off,
 				  struct bpf_insn *insn_buf,
 				  struct bpf_prog *prog)
@@ -2959,15 +2959,15 @@ static u32 xdp_convert_ctx_access(enum bpf_access_type type, int dst_reg,
 	struct bpf_insn *insn = insn_buf;
 
 	switch (ctx_off) {
-	case offsetof(struct xdp_md, data):
-		*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct xdp_buff, data),
+	case offsetof(struct pn_md, data):
+		*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct pn_buff, data),
 				      dst_reg, src_reg,
-				      offsetof(struct xdp_buff, data));
+				      offsetof(struct pn_buff, data));
 		break;
-	case offsetof(struct xdp_md, data_end):
-		*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct xdp_buff, data_end),
+	case offsetof(struct pn_md, data_end):
+		*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct pn_buff, data_end),
 				      dst_reg, src_reg,
-				      offsetof(struct xdp_buff, data_end));
+				      offsetof(struct pn_buff, data_end));
 		break;
 	}
 
@@ -2987,10 +2987,10 @@ static const struct bpf_verifier_ops tc_cls_act_ops = {
 	.gen_prologue		= tc_cls_act_prologue,
 };
 
-static const struct bpf_verifier_ops xdp_ops = {
-	.get_func_proto		= xdp_func_proto,
-	.is_valid_access	= xdp_is_valid_access,
-	.convert_ctx_access	= xdp_convert_ctx_access,
+static const struct bpf_verifier_ops pn_ops = {
+	.get_func_proto		= pn_func_proto,
+	.is_valid_access	= pn_is_valid_access,
+	.convert_ctx_access	= pn_convert_ctx_access,
 };
 
 static struct bpf_prog_type_list sk_filter_type __read_mostly = {
@@ -3008,9 +3008,9 @@ static struct bpf_prog_type_list sched_act_type __read_mostly = {
 	.type	= BPF_PROG_TYPE_SCHED_ACT,
 };
 
-static struct bpf_prog_type_list xdp_type __read_mostly = {
-	.ops	= &xdp_ops,
-	.type	= BPF_PROG_TYPE_XDP,
+static struct bpf_prog_type_list pn_type __read_mostly = {
+	.ops	= &pn_ops,
+	.type	= BPF_PROG_TYPE_PN,
 };
 
 static int __init register_sk_filter_ops(void)
@@ -3018,7 +3018,7 @@ static int __init register_sk_filter_ops(void)
 	bpf_register_prog_type(&sk_filter_type);
 	bpf_register_prog_type(&sched_cls_type);
 	bpf_register_prog_type(&sched_act_type);
-	bpf_register_prog_type(&xdp_type);
+	bpf_register_prog_type(&pn_type);
 
 	return 0;
 }
diff --git a/net/core/pn.c b/net/core/pn.c
index 8fae4c8..2636fa4 100644
--- a/net/core/pn.c
+++ b/net/core/pn.c
@@ -1,4 +1,4 @@
-/* net/core/xdp.c
+/* net/core/pn.c
  *
  * Copyright (c) 2017 Jesper Dangaard Brouer, Red Hat Inc.
  * Released under terms in GPL version 2.  See COPYING.
@@ -10,7 +10,7 @@
 #include <linux/rhashtable.h>
 #include <net/page_pool.h>
 
-#include <net/xdp.h>
+#include <net/pn.h>
 
 #define REG_STATE_NEW		0x0
 #define REG_STATE_REGISTERED	0x1
@@ -26,8 +26,8 @@ static int mem_id_next = MEM_ID_MIN;
 static bool mem_id_init; /* false */
 static struct rhashtable *mem_id_ht;
 
-struct xdp_mem_allocator {
-	struct xdp_mem_info mem;
+struct pn_mem_allocator {
+	struct pn_mem_info mem;
 	union {
 		void *allocator;
 		struct page_pool *page_pool;
@@ -36,22 +36,22 @@ struct xdp_mem_allocator {
 	struct rcu_head rcu;
 };
 
-static u32 xdp_mem_id_hashfn(const void *data, u32 len, u32 seed)
+static u32 pn_mem_id_hashfn(const void *data, u32 len, u32 seed)
 {
 	const u32 *k = data;
 	const u32 key = *k;
 
-	BUILD_BUG_ON(FIELD_SIZEOF(struct xdp_mem_allocator, mem.id)
+	BUILD_BUG_ON(FIELD_SIZEOF(struct pn_mem_allocator, mem.id)
 		     != sizeof(u32));
 
 	/* Use cyclic increasing ID as direct hash key, see rht_bucket_index */
 	return key << RHT_HASH_RESERVED_SPACE;
 }
 
-static int xdp_mem_id_cmp(struct rhashtable_compare_arg *arg,
+static int pn_mem_id_cmp(struct rhashtable_compare_arg *arg,
 			  const void *ptr)
 {
-	const struct xdp_mem_allocator *xa = ptr;
+	const struct pn_mem_allocator *xa = ptr;
 	u32 mem_id = *(u32 *)arg->key;
 
 	return xa->mem.id != mem_id;
@@ -59,21 +59,21 @@ static int xdp_mem_id_cmp(struct rhashtable_compare_arg *arg,
 
 static const struct rhashtable_params mem_id_rht_params = {
 	.nelem_hint = 64,
-	.head_offset = offsetof(struct xdp_mem_allocator, node),
-	.key_offset  = offsetof(struct xdp_mem_allocator, mem.id),
-	.key_len = FIELD_SIZEOF(struct xdp_mem_allocator, mem.id),
+	.head_offset = offsetof(struct pn_mem_allocator, node),
+	.key_offset  = offsetof(struct pn_mem_allocator, mem.id),
+	.key_len = FIELD_SIZEOF(struct pn_mem_allocator, mem.id),
 	.max_size = MEM_ID_MAX,
 	.min_size = 8,
 	.automatic_shrinking = true,
-	.hashfn    = xdp_mem_id_hashfn,
-	.obj_cmpfn = xdp_mem_id_cmp,
+	.hashfn    = pn_mem_id_hashfn,
+	.obj_cmpfn = pn_mem_id_cmp,
 };
 
-static void __xdp_mem_allocator_rcu_free(struct rcu_head *rcu)
+static void __pn_mem_allocator_rcu_free(struct rcu_head *rcu)
 {
-	struct xdp_mem_allocator *xa;
+	struct pn_mem_allocator *xa;
 
-	xa = container_of(rcu, struct xdp_mem_allocator, rcu);
+	xa = container_of(rcu, struct pn_mem_allocator, rcu);
 
 	/* Allow this ID to be reused */
 	ida_simple_remove(&mem_id_pool, xa->mem.id);
@@ -90,10 +90,10 @@ static void __xdp_mem_allocator_rcu_free(struct rcu_head *rcu)
 	kfree(xa);
 }
 
-static void __xdp_rxq_info_unreg_mem_model(struct xdp_rxq_info *xdp_rxq)
+static void __pn_rxq_info_unreg_mem_model(struct pn_rxq_info *pn_rxq)
 {
-	struct xdp_mem_allocator *xa;
-	int id = xdp_rxq->mem.id;
+	struct pn_mem_allocator *xa;
+	int id = pn_rxq->mem.id;
 	int err;
 
 	if (id == 0)
@@ -110,47 +110,47 @@ static void __xdp_rxq_info_unreg_mem_model(struct xdp_rxq_info *xdp_rxq)
 	err = rhashtable_remove_fast(mem_id_ht, &xa->node, mem_id_rht_params);
 	WARN_ON(err);
 
-	call_rcu(&xa->rcu, __xdp_mem_allocator_rcu_free);
+	call_rcu(&xa->rcu, __pn_mem_allocator_rcu_free);
 
 	mutex_unlock(&mem_id_lock);
 }
 
-void xdp_rxq_info_unreg(struct xdp_rxq_info *xdp_rxq)
+void pn_rxq_info_unreg(struct pn_rxq_info *pn_rxq)
 {
 	/* Simplify driver cleanup code paths, allow unreg "unused" */
-	if (xdp_rxq->reg_state == REG_STATE_UNUSED)
+	if (pn_rxq->reg_state == REG_STATE_UNUSED)
 		return;
 
-	WARN(!(xdp_rxq->reg_state == REG_STATE_REGISTERED), "Driver BUG");
+	WARN(!(pn_rxq->reg_state == REG_STATE_REGISTERED), "Driver BUG");
 
-	__xdp_rxq_info_unreg_mem_model(xdp_rxq);
+	__pn_rxq_info_unreg_mem_model(pn_rxq);
 
-	xdp_rxq->reg_state = REG_STATE_UNREGISTERED;
-	xdp_rxq->dev = NULL;
+	pn_rxq->reg_state = REG_STATE_UNREGISTERED;
+	pn_rxq->dev = NULL;
 
 	/* Reset mem info to defaults */
-	xdp_rxq->mem.id = 0;
-	xdp_rxq->mem.type = 0;
+	pn_rxq->mem.id = 0;
+	pn_rxq->mem.type = 0;
 }
-EXPORT_SYMBOL_GPL(xdp_rxq_info_unreg);
+EXPORT_SYMBOL_GPL(pn_rxq_info_unreg);
 
-static void xdp_rxq_info_init(struct xdp_rxq_info *xdp_rxq)
+static void pn_rxq_info_init(struct pn_rxq_info *pn_rxq)
 {
-	memset(xdp_rxq, 0, sizeof(*xdp_rxq));
+	memset(pn_rxq, 0, sizeof(*pn_rxq));
 }
 
 /* Returns 0 on success, negative on failure */
-int xdp_rxq_info_reg(struct xdp_rxq_info *xdp_rxq,
+int pn_rxq_info_reg(struct pn_rxq_info *pn_rxq,
 		     struct net_device *dev, u32 queue_index)
 {
-	if (xdp_rxq->reg_state == REG_STATE_UNUSED) {
+	if (pn_rxq->reg_state == REG_STATE_UNUSED) {
 		WARN(1, "Driver promised not to register this");
 		return -EINVAL;
 	}
 
-	if (xdp_rxq->reg_state == REG_STATE_REGISTERED) {
+	if (pn_rxq->reg_state == REG_STATE_REGISTERED) {
 		WARN(1, "Missing unregister, handled but fix driver");
-		xdp_rxq_info_unreg(xdp_rxq);
+		pn_rxq_info_unreg(pn_rxq);
 	}
 
 	if (!dev) {
@@ -159,26 +159,26 @@ int xdp_rxq_info_reg(struct xdp_rxq_info *xdp_rxq,
 	}
 
 	/* State either UNREGISTERED or NEW */
-	xdp_rxq_info_init(xdp_rxq);
-	xdp_rxq->dev = dev;
-	xdp_rxq->queue_index = queue_index;
+	pn_rxq_info_init(pn_rxq);
+	pn_rxq->dev = dev;
+	pn_rxq->queue_index = queue_index;
 
-	xdp_rxq->reg_state = REG_STATE_REGISTERED;
+	pn_rxq->reg_state = REG_STATE_REGISTERED;
 	return 0;
 }
-EXPORT_SYMBOL_GPL(xdp_rxq_info_reg);
+EXPORT_SYMBOL_GPL(pn_rxq_info_reg);
 
-void xdp_rxq_info_unused(struct xdp_rxq_info *xdp_rxq)
+void pn_rxq_info_unused(struct pn_rxq_info *pn_rxq)
 {
-	xdp_rxq->reg_state = REG_STATE_UNUSED;
+	pn_rxq->reg_state = REG_STATE_UNUSED;
 }
-EXPORT_SYMBOL_GPL(xdp_rxq_info_unused);
+EXPORT_SYMBOL_GPL(pn_rxq_info_unused);
 
-bool xdp_rxq_info_is_reg(struct xdp_rxq_info *xdp_rxq)
+bool pn_rxq_info_is_reg(struct pn_rxq_info *pn_rxq)
 {
-	return (xdp_rxq->reg_state == REG_STATE_REGISTERED);
+	return (pn_rxq->reg_state == REG_STATE_REGISTERED);
 }
-EXPORT_SYMBOL_GPL(xdp_rxq_info_is_reg);
+EXPORT_SYMBOL_GPL(pn_rxq_info_is_reg);
 
 static int __mem_id_init_hash_table(void)
 {
@@ -231,7 +231,7 @@ static int __mem_id_cyclic_get(gfp_t gfp)
 	return id;
 }
 
-static bool __is_supported_mem_type(enum xdp_mem_type type)
+static bool __is_supported_mem_type(enum pn_mem_type type)
 {
 	if (type == MEM_TYPE_PAGE_POOL)
 		return is_page_pool_compiled_in();
@@ -242,15 +242,15 @@ static bool __is_supported_mem_type(enum xdp_mem_type type)
 	return true;
 }
 
-int xdp_rxq_info_reg_mem_model(struct xdp_rxq_info *xdp_rxq,
-			       enum xdp_mem_type type, void *allocator)
+int pn_rxq_info_reg_mem_model(struct pn_rxq_info *pn_rxq,
+			       enum pn_mem_type type, void *allocator)
 {
-	struct xdp_mem_allocator *xdp_alloc;
+	struct pn_mem_allocator *pn_alloc;
 	gfp_t gfp = GFP_KERNEL;
 	int id, errno, ret;
 	void *ptr;
 
-	if (xdp_rxq->reg_state != REG_STATE_REGISTERED) {
+	if (pn_rxq->reg_state != REG_STATE_REGISTERED) {
 		WARN(1, "Missing register, driver bug");
 		return -EFAULT;
 	}
@@ -258,7 +258,7 @@ int xdp_rxq_info_reg_mem_model(struct xdp_rxq_info *xdp_rxq,
 	if (!__is_supported_mem_type(type))
 		return -EOPNOTSUPP;
 
-	xdp_rxq->mem.type = type;
+	pn_rxq->mem.type = type;
 
 	if (!allocator) {
 		if (type == MEM_TYPE_PAGE_POOL)
@@ -277,8 +277,8 @@ int xdp_rxq_info_reg_mem_model(struct xdp_rxq_info *xdp_rxq,
 		}
 	}
 
-	xdp_alloc = kzalloc(sizeof(*xdp_alloc), gfp);
-	if (!xdp_alloc)
+	pn_alloc = kzalloc(sizeof(*pn_alloc), gfp);
+	if (!pn_alloc)
 		return -ENOMEM;
 
 	mutex_lock(&mem_id_lock);
@@ -287,12 +287,12 @@ int xdp_rxq_info_reg_mem_model(struct xdp_rxq_info *xdp_rxq,
 		errno = id;
 		goto err;
 	}
-	xdp_rxq->mem.id = id;
-	xdp_alloc->mem  = xdp_rxq->mem;
-	xdp_alloc->allocator = allocator;
+	pn_rxq->mem.id = id;
+	pn_alloc->mem  = pn_rxq->mem;
+	pn_alloc->allocator = allocator;
 
 	/* Insert allocator into ID lookup table */
-	ptr = rhashtable_insert_slow(mem_id_ht, &id, &xdp_alloc->node);
+	ptr = rhashtable_insert_slow(mem_id_ht, &id, &pn_alloc->node);
 	if (IS_ERR(ptr)) {
 		errno = PTR_ERR(ptr);
 		goto err;
@@ -303,20 +303,20 @@ int xdp_rxq_info_reg_mem_model(struct xdp_rxq_info *xdp_rxq,
 	return 0;
 err:
 	mutex_unlock(&mem_id_lock);
-	kfree(xdp_alloc);
+	kfree(pn_alloc);
 	return errno;
 }
-EXPORT_SYMBOL_GPL(xdp_rxq_info_reg_mem_model);
+EXPORT_SYMBOL_GPL(pn_rxq_info_reg_mem_model);
 
-static void xdp_return(void *data, struct xdp_mem_info *mem)
+static void pn_return(void *data, struct pn_mem_info *mem)
 {
-	struct xdp_mem_allocator *xa;
+	struct pn_mem_allocator *xa;
 	struct page *page;
 
 	switch (mem->type) {
 	case MEM_TYPE_PAGE_POOL:
 		rcu_read_lock();
-		/* mem->id is valid, checked in xdp_rxq_info_reg_mem_model() */
+		/* mem->id is valid, checked in pn_rxq_info_reg_mem_model() */
 		xa = rhashtable_lookup(mem_id_ht, &mem->id, mem_id_rht_params);
 		page = virt_to_head_page(data);
 		if (xa)
@@ -333,19 +333,19 @@ static void xdp_return(void *data, struct xdp_mem_info *mem)
 		put_page(page);
 		break;
 	default:
-		/* Not possible, checked in xdp_rxq_info_reg_mem_model() */
+		/* Not possible, checked in pn_rxq_info_reg_mem_model() */
 		break;
 	}
 }
 
-void xdp_return_frame(struct xdp_frame *xdpf)
+void pn_return_frame(struct pn_frame *pnf)
 {
-	xdp_return(xdpf->data, &xdpf->mem);
+	pn_return(pnf->data, &pnf->mem);
 }
-EXPORT_SYMBOL_GPL(xdp_return_frame);
+EXPORT_SYMBOL_GPL(pn_return_frame);
 
-void xdp_return_buff(struct xdp_buff *xdp)
+void pn_return_buff(struct pn_buff *pn)
 {
-	xdp_return(xdp->data, &xdp->rxq->mem);
+	pn_return(pn->data, &pn->rxq->mem);
 }
-EXPORT_SYMBOL_GPL(xdp_return_buff);
+EXPORT_SYMBOL_GPL(pn_return_buff);
diff --git a/net/core/rtnetlink.c b/net/core/rtnetlink.c
index f3a0ad1..c3bbde9 100644
--- a/net/core/rtnetlink.c
+++ b/net/core/rtnetlink.c
@@ -899,15 +899,15 @@ static size_t rtnl_port_size(const struct net_device *dev,
 		return port_self_size;
 }
 
-static size_t rtnl_xdp_size(const struct net_device *dev)
+static size_t rtnl_pn_size(const struct net_device *dev)
 {
-	size_t xdp_size = nla_total_size(0) +	/* nest IFLA_XDP */
-			  nla_total_size(1);	/* XDP_ATTACHED */
+	size_t pn_size = nla_total_size(0) +	/* nest IFLA_PN */
+			  nla_total_size(1);	/* PN_ATTACHED */
 
-	if (!dev->netdev_ops->ndo_xdp)
+	if (!dev->netdev_ops->ndo_pn)
 		return 0;
 	else
-		return xdp_size;
+		return pn_size;
 }
 
 static noinline size_t if_nlmsg_size(const struct net_device *dev,
@@ -947,7 +947,7 @@ static noinline size_t if_nlmsg_size(const struct net_device *dev,
 	       + nla_total_size(MAX_PHYS_ITEM_ID_LEN) /* IFLA_PHYS_PORT_ID */
 	       + nla_total_size(MAX_PHYS_ITEM_ID_LEN) /* IFLA_PHYS_SWITCH_ID */
 	       + nla_total_size(IFNAMSIZ) /* IFLA_PHYS_PORT_NAME */
-	       + rtnl_xdp_size(dev) /* IFLA_XDP */
+	       + rtnl_pn_size(dev) /* IFLA_PN */
 	       + nla_total_size(1); /* IFLA_PROTO_DOWN */
 
 }
@@ -1254,30 +1254,30 @@ static int rtnl_fill_link_ifmap(struct sk_buff *skb, struct net_device *dev)
 	return 0;
 }
 
-static int rtnl_xdp_fill(struct sk_buff *skb, struct net_device *dev)
+static int rtnl_pn_fill(struct sk_buff *skb, struct net_device *dev)
 {
-	struct netdev_xdp xdp_op = {};
-	struct nlattr *xdp;
+	struct netdev_pn pn_op = {};
+	struct nlattr *pn;
 	int err;
 
-	if (!dev->netdev_ops->ndo_xdp)
+	if (!dev->netdev_ops->ndo_pn)
 		return 0;
-	xdp = nla_nest_start(skb, IFLA_XDP);
-	if (!xdp)
+	pn = nla_nest_start(skb, IFLA_PN);
+	if (!pn)
 		return -EMSGSIZE;
-	xdp_op.command = XDP_QUERY_PROG;
-	err = dev->netdev_ops->ndo_xdp(dev, &xdp_op);
+	pn_op.command = PN_QUERY_PROG;
+	err = dev->netdev_ops->ndo_pn(dev, &pn_op);
 	if (err)
 		goto err_cancel;
-	err = nla_put_u8(skb, IFLA_XDP_ATTACHED, xdp_op.prog_attached);
+	err = nla_put_u8(skb, IFLA_PN_ATTACHED, pn_op.prog_attached);
 	if (err)
 		goto err_cancel;
 
-	nla_nest_end(skb, xdp);
+	nla_nest_end(skb, pn);
 	return 0;
 
 err_cancel:
-	nla_nest_cancel(skb, xdp);
+	nla_nest_cancel(skb, pn);
 	return err;
 }
 
@@ -1377,7 +1377,7 @@ static int rtnl_fill_ifinfo(struct sk_buff *skb, struct net_device *dev,
 	if (rtnl_port_fill(skb, dev, ext_filter_mask))
 		goto nla_put_failure;
 
-	if (rtnl_xdp_fill(skb, dev))
+	if (rtnl_pn_fill(skb, dev))
 		goto nla_put_failure;
 
 	if (dev->rtnl_link_ops || rtnl_have_link_slave_info(dev)) {
@@ -1465,7 +1465,7 @@ static const struct nla_policy ifla_policy[IFLA_MAX+1] = {
 	[IFLA_PHYS_SWITCH_ID]	= { .type = NLA_BINARY, .len = MAX_PHYS_ITEM_ID_LEN },
 	[IFLA_LINK_NETNSID]	= { .type = NLA_S32 },
 	[IFLA_PROTO_DOWN]	= { .type = NLA_U8 },
-	[IFLA_XDP]		= { .type = NLA_NESTED },
+	[IFLA_PN]		= { .type = NLA_NESTED },
 	[IFLA_GROUP]		= { .type = NLA_U32 },
 };
 
@@ -1505,9 +1505,9 @@ static const struct nla_policy ifla_port_policy[IFLA_PORT_MAX+1] = {
 	[IFLA_PORT_RESPONSE]	= { .type = NLA_U16, },
 };
 
-static const struct nla_policy ifla_xdp_policy[IFLA_XDP_MAX + 1] = {
-	[IFLA_XDP_FD]		= { .type = NLA_S32 },
-	[IFLA_XDP_ATTACHED]	= { .type = NLA_U8 },
+static const struct nla_policy ifla_pn_policy[IFLA_PN_MAX + 1] = {
+	[IFLA_PN_FD]		= { .type = NLA_S32 },
+	[IFLA_PN_ATTACHED]	= { .type = NLA_U8 },
 };
 
 static const struct rtnl_link_ops *linkinfo_to_kind_ops(const struct nlattr *nla)
@@ -2172,21 +2172,21 @@ static int do_setlink(const struct sk_buff *skb,
 		status |= DO_SETLINK_NOTIFY;
 	}
 
-	if (tb[IFLA_XDP]) {
-		struct nlattr *xdp[IFLA_XDP_MAX + 1];
+	if (tb[IFLA_PN]) {
+		struct nlattr *pn[IFLA_PN_MAX + 1];
 
-		err = nla_parse_nested(xdp, IFLA_XDP_MAX, tb[IFLA_XDP],
-				       ifla_xdp_policy);
+		err = nla_parse_nested(pn, IFLA_PN_MAX, tb[IFLA_PN],
+				       ifla_pn_policy);
 		if (err < 0)
 			goto errout;
 
-		if (xdp[IFLA_XDP_ATTACHED]) {
+		if (pn[IFLA_PN_ATTACHED]) {
 			err = -EINVAL;
 			goto errout;
 		}
-		if (xdp[IFLA_XDP_FD]) {
-			err = dev_change_xdp_fd(dev,
-						nla_get_s32(xdp[IFLA_XDP_FD]));
+		if (pn[IFLA_PN_FD]) {
+			err = dev_change_pn_fd(dev,
+						nla_get_s32(pn[IFLA_PN_FD]));
 			if (err)
 				goto errout;
 			status |= DO_SETLINK_NOTIFY;
diff --git a/net/core/sock.c b/net/core/sock.c
index a163b21..e22860d 100644
--- a/net/core/sock.c
+++ b/net/core/sock.c
@@ -219,7 +219,7 @@ static const char *const af_family_key_strings[AF_MAX+1] = {
   "sk_lock-AF_RXRPC" , "sk_lock-AF_ISDN"     , "sk_lock-AF_PHONET"   ,
   "sk_lock-AF_IEEE802154", "sk_lock-AF_CAIF" , "sk_lock-AF_ALG"      ,
   "sk_lock-AF_NFC"   , "sk_lock-AF_VSOCK"    , "sk_lock-AF_KCM"      ,
-  "sk_lock-AF_QIPCRTR", "sk_lock-AF_XDP"     , "sk_lock-AF_MAX"
+  "sk_lock-AF_QIPCRTR", "sk_lock-AF_PN"      , "sk_lock-AF_MAX"
 };
 static const char *const af_family_slock_key_strings[AF_MAX+1] = {
   "slock-AF_UNSPEC", "slock-AF_UNIX"     , "slock-AF_INET"     ,
@@ -236,7 +236,7 @@ static const char *const af_family_slock_key_strings[AF_MAX+1] = {
   "slock-AF_RXRPC" , "slock-AF_ISDN"     , "slock-AF_PHONET"   ,
   "slock-AF_IEEE802154", "slock-AF_CAIF" , "slock-AF_ALG"      ,
   "slock-AF_NFC"   , "slock-AF_VSOCK"    , "slock-AF_KCM"      ,
-  "slock-AF_QIPCRTR", "slock-AF_XDP"     , "slock-AF_MAX"
+  "slock-AF_QIPCRTR", "slock-AF_PN"     , "slock-AF_MAX"
 };
 static const char *const af_family_clock_key_strings[AF_MAX+1] = {
   "clock-AF_UNSPEC", "clock-AF_UNIX"     , "clock-AF_INET"     ,
@@ -253,7 +253,7 @@ static const char *const af_family_clock_key_strings[AF_MAX+1] = {
   "clock-AF_RXRPC" , "clock-AF_ISDN"     , "clock-AF_PHONET"   ,
   "clock-AF_IEEE802154", "clock-AF_CAIF" , "clock-AF_ALG"      ,
   "clock-AF_NFC"   , "clock-AF_VSOCK"    , "clock-AF_KCM"      ,
-  "clock-AF_QIPCRTR", "clock-AF_XDP"     , "clock-AF_MAX"
+  "clock-AF_QIPCRTR", "clock-AF_PN"     , "clock-AF_MAX"
 };
 
 /*
diff --git a/net/pn/Kconfig b/net/pn/Kconfig
index d8698a3..be8e05d 100644
--- a/net/pn/Kconfig
+++ b/net/pn/Kconfig
@@ -1,8 +1,8 @@
-config XDP_SOCKETS
-	bool "XDP sockets"
+config PN_SOCKETS
+	bool "PROFINET sockets"
 	depends on BPF_SYSCALL
 	select PAGE_POOL
 	default n
 	help
-	  XDP sockets allows a channel between XDP programs and
-	  userspace applications.
+	  PROFINET sockets allows a channel between CPSW and
+	  userspace implemented PN-Stack.
diff --git a/net/pn/Makefile b/net/pn/Makefile
index 074fb2b..362a33d 100644
--- a/net/pn/Makefile
+++ b/net/pn/Makefile
@@ -1,2 +1,2 @@
-obj-$(CONFIG_XDP_SOCKETS) += xsk.o xdp_umem.o xsk_queue.o
+obj-$(CONFIG_PN_SOCKETS) += pnsk.o pn_umem.o pnsk_queue.o
 
diff --git a/net/pn/pn_umem.c b/net/pn/pn_umem.c
index 2be9fcd..9e49913 100644
--- a/net/pn/pn_umem.c
+++ b/net/pn/pn_umem.c
@@ -1,5 +1,5 @@
 // SPDX-License-Identifier: GPL-2.0
-/* XDP user-space packet buffer
+/* PN user-space packet buffer
  * Copyright(c) 2018 Intel Corporation.
  *
  * This program is free software; you can redistribute it and/or modify it
@@ -20,11 +20,11 @@
 #include <linux/mm.h>
 #include <linux/math64.h>
 
-#include "xdp_umem.h"
+#include "pn_umem.h"
 
-#define XDP_UMEM_MIN_FRAME_SIZE 2048
+#define PN_UMEM_MIN_FRAME_SIZE 2048
 
-int xdp_umem_create(struct xdp_umem **umem)
+int pn_umem_create(struct pn_umem **umem)
 {
 	*umem = kzalloc(sizeof(**umem), GFP_KERNEL);
 
@@ -34,7 +34,7 @@ int xdp_umem_create(struct xdp_umem **umem)
 	return 0;
 }
 
-static void xdp_umem_unpin_pages(struct xdp_umem *umem)
+static void pn_umem_unpin_pages(struct pn_umem *umem)
 {
 	unsigned int i;
 
@@ -51,7 +51,7 @@ static void xdp_umem_unpin_pages(struct xdp_umem *umem)
 	}
 }
 
-static void xdp_umem_unaccount_pages(struct xdp_umem *umem)
+static void pn_umem_unaccount_pages(struct pn_umem *umem)
 {
 	if (umem->user) {
 		atomic_long_sub(umem->npgs, &umem->user->locked_vm);
@@ -59,23 +59,23 @@ static void xdp_umem_unaccount_pages(struct xdp_umem *umem)
 	}
 }
 
-static void xdp_umem_release(struct xdp_umem *umem)
+static void pn_umem_release(struct pn_umem *umem)
 {
 	struct task_struct *task;
 	struct mm_struct *mm;
 
 	if (umem->fq) {
-		xskq_destroy(umem->fq);
+		pnskq_destroy(umem->fq);
 		umem->fq = NULL;
 	}
 
 	if (umem->cq) {
-		xskq_destroy(umem->cq);
+		pnskq_destroy(umem->cq);
 		umem->cq = NULL;
 	}
 
 	if (umem->pgs) {
-		xdp_umem_unpin_pages(umem);
+		pn_umem_unpin_pages(umem);
 
 		task = get_pid_task(umem->pid, PIDTYPE_PID);
 		put_pid(umem->pid);
@@ -90,35 +90,35 @@ static void xdp_umem_release(struct xdp_umem *umem)
 		umem->pgs = NULL;
 	}
 
-	xdp_umem_unaccount_pages(umem);
+	pn_umem_unaccount_pages(umem);
 out:
 	kfree(umem);
 }
 
-static void xdp_umem_release_deferred(struct work_struct *work)
+static void pn_umem_release_deferred(struct work_struct *work)
 {
-	struct xdp_umem *umem = container_of(work, struct xdp_umem, work);
+	struct pn_umem *umem = container_of(work, struct pn_umem, work);
 
-	xdp_umem_release(umem);
+	pn_umem_release(umem);
 }
 
-void xdp_get_umem(struct xdp_umem *umem)
+void pn_get_umem(struct pn_umem *umem)
 {
 	atomic_inc(&umem->users);
 }
 
-void xdp_put_umem(struct xdp_umem *umem)
+void pn_put_umem(struct pn_umem *umem)
 {
 	if (!umem)
 		return;
 
 	if (atomic_dec_and_test(&umem->users)) {
-		INIT_WORK(&umem->work, xdp_umem_release_deferred);
+		INIT_WORK(&umem->work, pn_umem_release_deferred);
 		schedule_work(&umem->work);
 	}
 }
 
-static int xdp_umem_pin_pages(struct xdp_umem *umem)
+static int pn_umem_pin_pages(struct pn_umem *umem)
 {
 	unsigned int gup_flags = FOLL_WRITE;
 	long npgs;
@@ -145,14 +145,14 @@ static int xdp_umem_pin_pages(struct xdp_umem *umem)
 	return 0;
 
 out_pin:
-	xdp_umem_unpin_pages(umem);
+	pn_umem_unpin_pages(umem);
 out_pgs:
 	kfree(umem->pgs);
 	umem->pgs = NULL;
 	return err;
 }
 
-static int xdp_umem_account_pages(struct xdp_umem *umem)
+static int pn_umem_account_pages(struct pn_umem *umem)
 {
 	unsigned long lock_limit, new_npgs, old_npgs;
 
@@ -175,7 +175,7 @@ static int xdp_umem_account_pages(struct xdp_umem *umem)
 	return 0;
 }
 
-int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
+int pn_umem_reg(struct pn_umem *umem, struct pn_umem_reg *mr)
 {
 	u32 frame_size = mr->frame_size, frame_headroom = mr->frame_headroom;
 	u64 addr = mr->addr, size = mr->len;
@@ -185,7 +185,7 @@ int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 	if (!umem)
 		return -EINVAL;
 
-	if (frame_size < XDP_UMEM_MIN_FRAME_SIZE || frame_size > PAGE_SIZE) {
+	if (frame_size < PN_UMEM_MIN_FRAME_SIZE || frame_size > PAGE_SIZE) {
 		/* Strictly speaking we could support this, if:
 		 * - huge pages, or*
 		 * - using an IOMMU, or
@@ -218,7 +218,7 @@ int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 
 	frame_headroom = ALIGN(frame_headroom, 64);
 
-	size_chk = frame_size - frame_headroom - XDP_PACKET_HEADROOM;
+	size_chk = frame_size - frame_headroom - PN_PACKET_HEADROOM;
 	if (size_chk < 0)
 		return -EINVAL;
 
@@ -237,23 +237,23 @@ int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 	umem->nfpplog2 = ilog2(nfpp);
 	atomic_set(&umem->users, 1);
 
-	err = xdp_umem_account_pages(umem);
+	err = pn_umem_account_pages(umem);
 	if (err)
 		goto out;
 
-	err = xdp_umem_pin_pages(umem);
+	err = pn_umem_pin_pages(umem);
 	if (err)
 		goto out_account;
 	return 0;
 
 out_account:
-	xdp_umem_unaccount_pages(umem);
+	pn_umem_unaccount_pages(umem);
 out:
 	put_pid(umem->pid);
 	return err;
 }
 
-bool xdp_umem_validate_queues(struct xdp_umem *umem)
+bool pn_umem_validate_queues(struct pn_umem *umem)
 {
 	return (umem->fq && umem->cq);
 }
diff --git a/net/pn/pn_umem.h b/net/pn/pn_umem.h
index 7e0b2fa..2a202ae 100644
--- a/net/pn/pn_umem.h
+++ b/net/pn/pn_umem.h
@@ -1,5 +1,5 @@
 /* SPDX-License-Identifier: GPL-2.0
- * XDP user-space packet buffer
+ * PN user-space packet buffer
  * Copyright(c) 2018 Intel Corporation.
  *
  * This program is free software; you can redistribute it and/or modify it
@@ -12,21 +12,21 @@
  * more details.
  */
 
-#ifndef XDP_UMEM_H_
-#define XDP_UMEM_H_
+#ifndef PN_UMEM_H_
+#define PN_UMEM_H_
 
 #include <linux/mm.h>
-#include <linux/if_xdp.h>
+#include <linux/if_pn.h>
 #include <linux/workqueue.h>
 
-#include "xsk_queue.h"
-#include "xdp_umem_props.h"
+#include "pnsk_queue.h"
+#include "pn_umem_props.h"
 
-struct xdp_umem {
-	struct xsk_queue *fq;
-	struct xsk_queue *cq;
+struct pn_umem {
+	struct pnsk_queue *fq;
+	struct pnsk_queue *cq;
 	struct page **pgs;
-	struct xdp_umem_props props;
+	struct pn_umem_props props;
 	u32 npgs;
 	u32 frame_headroom;
 	u32 nfpp_mask;
@@ -40,7 +40,7 @@ struct xdp_umem {
 	struct work_struct work;
 };
 
-static inline char *xdp_umem_get_data(struct xdp_umem *umem, u32 idx)
+static inline char *pn_umem_get_data(struct pn_umem *umem, u32 idx)
 {
 	u64 pg, off;
 	char *data;
@@ -52,16 +52,16 @@ static inline char *xdp_umem_get_data(struct xdp_umem *umem, u32 idx)
 	return data + off;
 }
 
-static inline char *xdp_umem_get_data_with_headroom(struct xdp_umem *umem,
+static inline char *pn_umem_get_data_with_headroom(struct pn_umem *umem,
 						    u32 idx)
 {
-	return xdp_umem_get_data(umem, idx) + umem->frame_headroom;
+	return pn_umem_get_data(umem, idx) + umem->frame_headroom;
 }
 
-bool xdp_umem_validate_queues(struct xdp_umem *umem);
-int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr);
-void xdp_get_umem(struct xdp_umem *umem);
-void xdp_put_umem(struct xdp_umem *umem);
-int xdp_umem_create(struct xdp_umem **umem);
+bool pn_umem_validate_queues(struct pn_umem *umem);
+int pn_umem_reg(struct pn_umem *umem, struct pn_umem_reg *mr);
+void pn_get_umem(struct pn_umem *umem);
+void pn_put_umem(struct pn_umem *umem);
+int pn_umem_create(struct pn_umem **umem);
 
-#endif /* XDP_UMEM_H_ */
+#endif /* PN_UMEM_H_ */
diff --git a/net/pn/pn_umem_props.h b/net/pn/pn_umem_props.h
index 77fb5da..a93c1c8 100644
--- a/net/pn/pn_umem_props.h
+++ b/net/pn/pn_umem_props.h
@@ -1,5 +1,5 @@
 /* SPDX-License-Identifier: GPL-2.0
- * XDP user-space packet buffer
+ * PN user-space packet buffer
  * Copyright(c) 2018 Intel Corporation.
  *
  * This program is free software; you can redistribute it and/or modify it
@@ -12,12 +12,12 @@
  * more details.
  */
 
-#ifndef XDP_UMEM_PROPS_H_
-#define XDP_UMEM_PROPS_H_
+#ifndef PN_UMEM_PROPS_H_
+#define PN_UMEM_PROPS_H_
 
-struct xdp_umem_props {
+struct pn_umem_props {
 	u32 frame_size;
 	u32 nframes;
 };
 
-#endif /* XDP_UMEM_PROPS_H_ */
+#endif /* PN_UMEM_PROPS_H_ */
diff --git a/net/pn/pnsk.c b/net/pn/pnsk.c
index 529b6a2..7a4e4ed 100644
--- a/net/pn/pnsk.c
+++ b/net/pn/pnsk.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0
-/* XDP sockets
+/* PN sockets
  *
- * AF_XDP sockets allows a channel between XDP programs and userspace
+ * AF_PN sockets allows a channel between PN programs and userspace
  * applications.
  * Copyright(c) 2018 Intel Corporation.
  *
@@ -18,9 +18,9 @@
  *	      Magnus Karlsson <magnus.karlsson@intel.com>
  */
 
-#define pr_fmt(fmt) "AF_XDP: %s: " fmt, __func__
+#define pr_fmt(fmt) "AF_PN: %s: " fmt, __func__
 
-#include <linux/if_xdp.h>
+#include <linux/if_pn.h>
 #include <linux/init.h>
 #include <linux/sched.h>
 #include <linux/socket.h>
@@ -28,108 +28,108 @@
 #include <linux/uaccess.h>
 #include <linux/net.h>
 #include <linux/netdevice.h>
-#include <net/xdp_sock.h>
-#include <net/xdp.h>
+#include <net/pn_sock.h>
+#include <net/pn.h>
 
-#include "xsk_queue.h"
-#include "xdp_umem.h"
+#include "pnsk_queue.h"
+#include "pn_umem.h"
 
 #define TX_BATCH_SIZE 16
 
-static struct xdp_sock *xdp_sk(struct sock *sk)
+static struct pn_sock *pn_sk(struct sock *sk)
 {
-	return (struct xdp_sock *)sk;
+	return (struct pn_sock *)sk;
 }
 
-bool xsk_is_setup_for_bpf_map(struct xdp_sock *xs)
+bool pnsk_is_setup_for_bpf_map(struct pn_sock *ps)
 {
-	return !!xs->rx;
+	return !!ps->rx;
 }
 
-static int __xsk_rcv(struct xdp_sock *xs, struct xdp_buff *xdp)
+static int __pnsk_rcv(struct pn_sock *ps, struct pn_buff *pn)
 {
-	u32 *id, len = xdp->data_end - xdp->data;
+	u32 *id, len = pn->data_end - pn->data;
 	void *buffer;
 	int err = 0;
 
-	if (xs->dev != xdp->rxq->dev || xs->queue_id != xdp->rxq->queue_index)
+	if (ps->dev != pn->rxq->dev || ps->queue_id != pn->rxq->queue_index)
 		return -EINVAL;
 
-	id = xskq_peek_id(xs->umem->fq);
+	id = pnskq_peek_id(ps->umem->fq);
 	if (!id)
 		return -ENOSPC;
 
-	buffer = xdp_umem_get_data_with_headroom(xs->umem, *id);
-	memcpy(buffer, xdp->data, len);
-	err = xskq_produce_batch_desc(xs->rx, *id, len,
-				      xs->umem->frame_headroom);
+	buffer = pn_umem_get_data_with_headroom(ps->umem, *id);
+	memcpy(buffer, pn->data, len);
+	err = pnskq_produce_batch_desc(ps->rx, *id, len,
+				      ps->umem->frame_headroom);
 	if (!err)
-		xskq_discard_id(xs->umem->fq);
+		pnskq_discard_id(ps->umem->fq);
 
 	return err;
 }
 
-int xsk_rcv(struct xdp_sock *xs, struct xdp_buff *xdp)
+int pnsk_rcv(struct pn_sock *ps, struct pn_buff *pn)
 {
 	int err;
 
-	err = __xsk_rcv(xs, xdp);
+	err = __pnsk_rcv(ps, pn);
 	if (likely(!err))
-		xdp_return_buff(xdp);
+		pn_return_buff(pn);
 	else
-		xs->rx_dropped++;
+		ps->rx_dropped++;
 
 	return err;
 }
 
-void xsk_flush(struct xdp_sock *xs)
+void pnsk_flush(struct pn_sock *ps)
 {
-	xskq_produce_flush_desc(xs->rx);
-	xs->sk.sk_data_ready(&xs->sk);
+	pnskq_produce_flush_desc(ps->rx);
+	ps->sk.sk_data_ready(&ps->sk);
 }
 
-int xsk_generic_rcv(struct xdp_sock *xs, struct xdp_buff *xdp)
+int pnsk_generic_rcv(struct pn_sock *ps, struct pn_buff *pn)
 {
 	int err;
 
-	err = __xsk_rcv(xs, xdp);
+	err = __pnsk_rcv(ps, pn);
 	if (!err)
-		xsk_flush(xs);
+		pnsk_flush(ps);
 	else
-		xs->rx_dropped++;
+		ps->rx_dropped++;
 
 	return err;
 }
 
-static void xsk_destruct_skb(struct sk_buff *skb)
+static void pnsk_destruct_skb(struct sk_buff *skb)
 {
 	u32 id = (u32)(long)skb_shinfo(skb)->destructor_arg;
-	struct xdp_sock *xs = xdp_sk(skb->sk);
+	struct pn_sock *ps = pn_sk(skb->sk);
 
-	WARN_ON_ONCE(xskq_produce_id(xs->umem->cq, id));
+	WARN_ON_ONCE(pnskq_produce_id(ps->umem->cq, id));
 
 	sock_wfree(skb);
 }
 
-static int xsk_generic_xmit(struct sock *sk, struct msghdr *m,
+static int pnsk_generic_xmit(struct sock *sk, struct msghdr *m,
 			    size_t total_len)
 {
 	bool need_wait = !(m->msg_flags & MSG_DONTWAIT);
 	u32 max_batch = TX_BATCH_SIZE;
-	struct xdp_sock *xs = xdp_sk(sk);
+	struct pn_sock *ps = pn_sk(sk);
 	bool sent_frame = false;
-	struct xdp_desc desc;
+	struct pn_desc desc;
 	struct sk_buff *skb;
 	int err = 0;
 
-	if (unlikely(!xs->tx))
+	if (unlikely(!ps->tx))
 		return -ENOBUFS;
 	if (need_wait)
 		return -EOPNOTSUPP;
 
-	mutex_lock(&xs->mutex);
+	mutex_lock(&ps->mutex);
 
-	while (xskq_peek_desc(xs->tx, &desc)) {
+	while (pnskq_peek_desc(ps->tx, &desc)) {
 		char *buffer;
 		u32 id, len;
 
@@ -138,13 +138,13 @@ static int xsk_generic_xmit(struct sock *sk, struct msghdr *m,
 			goto out;
 		}
 
-		if (xskq_reserve_id(xs->umem->cq)) {
+		if (pnskq_reserve_id(ps->umem->cq)) {
 			err = -EAGAIN;
 			goto out;
 		}
 
 		len = desc.len;
-		if (unlikely(len > xs->dev->mtu)) {
+		if (unlikely(len > ps->dev->mtu)) {
 			err = -EMSGSIZE;
 			goto out;
 		}
@@ -157,20 +157,20 @@ static int xsk_generic_xmit(struct sock *sk, struct msghdr *m,
 
 		skb_put(skb, len);
 		id = desc.idx;
-		buffer = xdp_umem_get_data(xs->umem, id) + desc.offset;
+		buffer = pn_umem_get_data(ps->umem, id) + desc.offset;
 		err = skb_store_bits(skb, 0, buffer, len);
 		if (unlikely(err)) {
 			kfree_skb(skb);
 			goto out;
 		}
 
-		skb->dev = xs->dev;
+		skb->dev = ps->dev;
 		skb->priority = sk->sk_priority;
 		skb->mark = sk->sk_mark;
 		skb_shinfo(skb)->destructor_arg = (void *)(long)id;
-		skb->destructor = xsk_destruct_skb;
+		skb->destructor = pnsk_destruct_skb;
 
-		err = dev_direct_xmit(skb, xs->queue_id);
+		err = dev_direct_xmit(skb, ps->queue_id);
 		/* Ignore NET_XMIT_CN as packet might have been sent */
 		if (err == NET_XMIT_DROP || err == NETDEV_TX_BUSY) {
 			err = -EAGAIN;
@@ -179,54 +179,54 @@ static int xsk_generic_xmit(struct sock *sk, struct msghdr *m,
 		}
 
 		sent_frame = true;
-		xskq_discard_desc(xs->tx);
+		pnskq_discard_desc(ps->tx);
 	}
 
 out:
 	if (sent_frame)
 		sk->sk_write_space(sk);
 
-	mutex_unlock(&xs->mutex);
+	mutex_unlock(&ps->mutex);
 	return err;
 }
 
-static int xsk_sendmsg(struct socket *sock, struct msghdr *m, size_t total_len)
+static int pnsk_sendmsg(struct socket *sock, struct msghdr *m, size_t total_len)
 {
 	struct sock *sk = sock->sk;
-	struct xdp_sock *xs = xdp_sk(sk);
+	struct pn_sock *ps = pn_sk(sk);
 
-	if (unlikely(!xs->dev))
+	if (unlikely(!ps->dev))
 		return -ENXIO;
-	if (unlikely(!(xs->dev->flags & IFF_UP)))
+	if (unlikely(!(ps->dev->flags & IFF_UP)))
 		return -ENETDOWN;
 
-	return xsk_generic_xmit(sk, m, total_len);
+	return pnsk_generic_xmit(sk, m, total_len);
 }
 
-static unsigned int xsk_poll(struct file *file, struct socket *sock,
+static unsigned int pnsk_poll(struct file *file, struct socket *sock,
 			     struct poll_table_struct *wait)
 {
 	unsigned int mask = datagram_poll(file, sock, wait);
 	struct sock *sk = sock->sk;
-	struct xdp_sock *xs = xdp_sk(sk);
+	struct pn_sock *ps = pn_sk(sk);
 
-	if (xs->rx && !xskq_empty_desc(xs->rx))
+	if (ps->rx && !pnskq_empty_desc(ps->rx))
 		mask |= POLLIN | POLLRDNORM;
-	if (xs->tx && !xskq_full_desc(xs->tx))
+	if (ps->tx && !pnskq_full_desc(ps->tx))
 		mask |= POLLOUT | POLLWRNORM;
 
 	return mask;
 }
 
-static int xsk_init_queue(u32 entries, struct xsk_queue **queue,
+static int pnsk_init_queue(u32 entries, struct pnsk_queue **queue,
 			  bool umem_queue)
 {
-	struct xsk_queue *q;
+	struct pnsk_queue *q;
 
 	if (entries == 0 || *queue || !is_power_of_2(entries))
 		return -EINVAL;
 
-	q = xskq_create(entries, umem_queue);
+	q = pnskq_create(entries, umem_queue);
 	if (!q)
 		return -ENOMEM;
 
@@ -234,18 +234,18 @@ static int xsk_init_queue(u32 entries, struct xsk_queue **queue,
 	return 0;
 }
 
-static void __xsk_release(struct xdp_sock *xs)
+static void __pnsk_release(struct pn_sock *ps)
 {
-	/* Wait for driver to stop using the xdp socket. */
+	/* Wait for driver to stop using the pn socket. */
 	synchronize_net();
 
-	dev_put(xs->dev);
+	dev_put(ps->dev);
 }
 
-static int xsk_release(struct socket *sock)
+static int pnsk_release(struct socket *sock)
 {
 	struct sock *sk = sock->sk;
-	struct xdp_sock *xs = xdp_sk(sk);
+	struct pn_sock *ps = pn_sk(sk);
 	struct net *net;
 
 	if (!sk)
@@ -257,9 +257,9 @@ static int xsk_release(struct socket *sock)
 	sock_prot_inuse_add(net, sk->sk_prot, -1);
 	local_bh_enable();
 
-	if (xs->dev) {
-		__xsk_release(xs);
-		xs->dev = NULL;
+	if (ps->dev) {
+		__pnsk_release(ps);
+		ps->dev = NULL;
 	}
 
 	sock_orphan(sk);
@@ -271,7 +271,7 @@ static int xsk_release(struct socket *sock)
 	return 0;
 }
 
-static struct socket *xsk_lookup_xsk_from_fd(int fd)
+static struct socket *pnsk_lookup_pnsk_from_fd(int fd)
 {
 	struct socket *sock;
 	int err;
@@ -280,7 +280,7 @@ static struct socket *xsk_lookup_xsk_from_fd(int fd)
 	if (!sock)
 		return ERR_PTR(-ENOTSOCK);
 
-	if (sock->sk->sk_family != PF_XDP) {
+	if (sock->sk->sk_family != PF_PN) {
 		sockfd_put(sock);
 		return ERR_PTR(-ENOPROTOOPT);
 	}
@@ -288,117 +288,117 @@ static struct socket *xsk_lookup_xsk_from_fd(int fd)
 	return sock;
 }
 
-static int xsk_bind(struct socket *sock, struct sockaddr *addr, int addr_len)
+static int pnsk_bind(struct socket *sock, struct sockaddr *addr, int addr_len)
 {
-	struct sockaddr_xdp *sxdp = (struct sockaddr_xdp *)addr;
+	struct sockaddr_pn *spn = (struct sockaddr_pn *)addr;
 	struct sock *sk = sock->sk;
 	struct net_device *dev, *dev_curr;
-	struct xdp_sock *xs = xdp_sk(sk);
-	struct xdp_umem *old_umem = NULL;
+	struct pn_sock *ps = pn_sk(sk);
+	struct pn_umem *old_umem = NULL;
 	int err = 0;
 
-	if (addr_len < sizeof(struct sockaddr_xdp))
+	if (addr_len < sizeof(struct sockaddr_pn))
 		return -EINVAL;
-	if (sxdp->sxdp_family != AF_XDP)
+	if (spn->spn_family != AF_PN)
 		return -EINVAL;
 
-	mutex_lock(&xs->mutex);
-	dev_curr = xs->dev;
-	dev = dev_get_by_index(sock_net(sk), sxdp->sxdp_ifindex);
+	mutex_lock(&ps->mutex);
+	dev_curr = ps->dev;
+	dev = dev_get_by_index(sock_net(sk), spn->spn_ifindex);
 	if (!dev) {
 		err = -ENODEV;
 		goto out_release;
 	}
 
-	if (!xs->rx && !xs->tx) {
+	if (!ps->rx && !ps->tx) {
 		err = -EINVAL;
 		goto out_unlock;
 	}
 
-	if (sxdp->sxdp_queue_id >= dev->num_rx_queues) {
+	if (spn->spn_queue_id >= dev->num_rx_queues) {
 		err = -EINVAL;
 		goto out_unlock;
 	}
 
-	if (sxdp->sxdp_flags & XDP_SHARED_UMEM) {
-		struct xdp_sock *umem_xs;
+	if (spn->spn_flags & PN_SHARED_UMEM) {
+		struct pn_sock *umem_ps;
 		struct socket *sock;
 
-		if (xs->umem) {
+		if (ps->umem) {
 			/* We have already our own. */
 			err = -EINVAL;
 			goto out_unlock;
 		}
 
-		sock = xsk_lookup_xsk_from_fd(sxdp->sxdp_shared_umem_fd);
+		sock = pnsk_lookup_pnsk_from_fd(spn->spn_shared_umem_fd);
 		if (IS_ERR(sock)) {
 			err = PTR_ERR(sock);
 			goto out_unlock;
 		}
 
-		umem_xs = xdp_sk(sock->sk);
-		if (!umem_xs->umem) {
+		umem_ps = pn_sk(sock->sk);
+		if (!umem_ps->umem) {
 			/* No umem to inherit. */
 			err = -EBADF;
 			sockfd_put(sock);
 			goto out_unlock;
-		} else if (umem_xs->dev != dev ||
-			   umem_xs->queue_id != sxdp->sxdp_queue_id) {
+		} else if (umem_ps->dev != dev ||
+			   umem_ps->queue_id != spn->spn_queue_id) {
 			err = -EINVAL;
 			sockfd_put(sock);
 			goto out_unlock;
 		}
 
-		xdp_get_umem(umem_xs->umem);
-		old_umem = xs->umem;
-		xs->umem = umem_xs->umem;
+		pn_get_umem(umem_ps->umem);
+		old_umem = ps->umem;
+		ps->umem = umem_ps->umem;
 		sockfd_put(sock);
-	} else if (!xs->umem || !xdp_umem_validate_queues(xs->umem)) {
+	} else if (!ps->umem || !pn_umem_validate_queues(ps->umem)) {
 		err = -EINVAL;
 		goto out_unlock;
 	} else {
-		/* This xsk has its own umem. */
-		xskq_set_umem(xs->umem->fq, &xs->umem->props);
-		xskq_set_umem(xs->umem->cq, &xs->umem->props);
+		/* This pnsk has its own umem. */
+		pnskq_set_umem(ps->umem->fq, &ps->umem->props);
+		pnskq_set_umem(ps->umem->cq, &ps->umem->props);
 	}
 
 	/* Rebind? */
 	if (dev_curr && (dev_curr != dev ||
-			 xs->queue_id != sxdp->sxdp_queue_id)) {
-		__xsk_release(xs);
+			 ps->queue_id != spn->spn_queue_id)) {
+		__pnsk_release(ps);
 		if (old_umem)
-			xdp_put_umem(old_umem);
+			pn_put_umem(old_umem);
 	}
 
-	xs->dev = dev;
-	xs->queue_id = sxdp->sxdp_queue_id;
+	ps->dev = dev;
+	ps->queue_id = spn->spn_queue_id;
 
-	xskq_set_umem(xs->rx, &xs->umem->props);
-	xskq_set_umem(xs->tx, &xs->umem->props);
+	pnskq_set_umem(ps->rx, &ps->umem->props);
+	pnskq_set_umem(ps->tx, &ps->umem->props);
 
 out_unlock:
 	if (err)
 		dev_put(dev);
 out_release:
-	mutex_unlock(&xs->mutex);
+	mutex_unlock(&ps->mutex);
 	return err;
 }
 
-static int xsk_setsockopt(struct socket *sock, int level, int optname,
+static int pnsk_setsockopt(struct socket *sock, int level, int optname,
 			  char __user *optval, unsigned int optlen)
 {
 	struct sock *sk = sock->sk;
-	struct xdp_sock *xs = xdp_sk(sk);
+	struct pn_sock *ps = pn_sk(sk);
 	int err;
 
-	if (level != SOL_XDP)
+	if (level != SOL_PN)
 		return -ENOPROTOOPT;
 
 	switch (optname) {
-	case XDP_RX_RING:
-	case XDP_TX_RING:
+	case PN_RX_RING:
+	case PN_TX_RING:
 	{
-		struct xsk_queue **q;
+		struct pnsk_queue **q;
 		int entries;
 
 		if (optlen < sizeof(entries))
@@ -406,57 +406,57 @@ static int xsk_setsockopt(struct socket *sock, int level, int optname,
 		if (copy_from_user(&entries, optval, sizeof(entries)))
 			return -EFAULT;
 
-		mutex_lock(&xs->mutex);
-		q = (optname == XDP_TX_RING) ? &xs->tx : &xs->rx;
-		err = xsk_init_queue(entries, q, false);
-		mutex_unlock(&xs->mutex);
+		mutex_lock(&ps->mutex);
+		q = (optname == PN_TX_RING) ? &ps->tx : &ps->rx;
+		err = pnsk_init_queue(entries, q, false);
+		mutex_unlock(&ps->mutex);
 		return err;
 	}
-	case XDP_UMEM_REG:
+	case PN_UMEM_REG:
 	{
-		struct xdp_umem_reg mr;
-		struct xdp_umem *umem;
+		struct pn_umem_reg mr;
+		struct pn_umem *umem;
 
-		if (xs->umem)
+		if (ps->umem)
 			return -EBUSY;
 
 		if (copy_from_user(&mr, optval, sizeof(mr)))
 			return -EFAULT;
 
-		mutex_lock(&xs->mutex);
-		err = xdp_umem_create(&umem);
+		mutex_lock(&ps->mutex);
+		err = pn_umem_create(&umem);
 
-		err = xdp_umem_reg(umem, &mr);
+		err = pn_umem_reg(umem, &mr);
 		if (err) {
 			kfree(umem);
-			mutex_unlock(&xs->mutex);
+			mutex_unlock(&ps->mutex);
 			return err;
 		}
 
 		/* Make sure umem is ready before it can be seen by others */
 		smp_wmb();
 
-		xs->umem = umem;
-		mutex_unlock(&xs->mutex);
+		ps->umem = umem;
+		mutex_unlock(&ps->mutex);
 		return 0;
 	}
-	case XDP_UMEM_FILL_RING:
-	case XDP_UMEM_COMPLETION_RING:
+	case PN_UMEM_FILL_RING:
+	case PN_UMEM_COMPLETION_RING:
 	{
-		struct xsk_queue **q;
+		struct pnsk_queue **q;
 		int entries;
 
-		if (!xs->umem)
+		if (!ps->umem)
 			return -EINVAL;
 
 		if (copy_from_user(&entries, optval, sizeof(entries)))
 			return -EFAULT;
 
-		mutex_lock(&xs->mutex);
-		q = (optname == XDP_UMEM_FILL_RING) ? &xs->umem->fq :
-			&xs->umem->cq;
-		err = xsk_init_queue(entries, q, true);
-		mutex_unlock(&xs->mutex);
+		mutex_lock(&ps->mutex);
+		q = (optname == PN_UMEM_FILL_RING) ? &ps->umem->fq :
+			&ps->umem->cq;
+		err = pnsk_init_queue(entries, q, true);
+		mutex_unlock(&ps->mutex);
 		return err;
 	}
 	default:
@@ -466,28 +466,28 @@ static int xsk_setsockopt(struct socket *sock, int level, int optname,
 	return -ENOPROTOOPT;
 }
 
-static int xsk_mmap(struct file *file, struct socket *sock,
+static int pnsk_mmap(struct file *file, struct socket *sock,
 		    struct vm_area_struct *vma)
 {
 	unsigned long offset = vma->vm_pgoff << PAGE_SHIFT;
 	unsigned long size = vma->vm_end - vma->vm_start;
-	struct xdp_sock *xs = xdp_sk(sock->sk);
-	struct xsk_queue *q = NULL;
+	struct pn_sock *ps = pn_sk(sock->sk);
+	struct pnsk_queue *q = NULL;
 	unsigned long pfn;
 	struct page *qpg;
 
-	if (offset == XDP_PGOFF_RX_RING) {
-		q = xs->rx;
-	} else if (offset == XDP_PGOFF_TX_RING) {
-		q = xs->tx;
+	if (offset == PN_PGOFF_RX_RING) {
+		q = ps->rx;
+	} else if (offset == PN_PGOFF_TX_RING) {
+		q = ps->tx;
 	} else {
-		if (!xs->umem)
+		if (!ps->umem)
 			return -EINVAL;
 
-		if (offset == XDP_UMEM_PGOFF_FILL_RING)
-			q = xs->umem->fq;
-		else if (offset == XDP_UMEM_PGOFF_COMPLETION_RING)
-			q = xs->umem->cq;
+		if (offset == PN_UMEM_PGOFF_FILL_RING)
+			q = ps->umem->fq;
+		else if (offset == PN_UMEM_PGOFF_COMPLETION_RING)
+			q = ps->umem->cq;
 	}
 
 	if (!q)
@@ -502,52 +502,52 @@ static int xsk_mmap(struct file *file, struct socket *sock,
 			       size, vma->vm_page_prot);
 }
 
-static struct proto xsk_proto = {
-	.name =		"XDP",
+static struct proto pnsk_proto = {
+	.name =		"PN",
 	.owner =	THIS_MODULE,
-	.obj_size =	sizeof(struct xdp_sock),
+	.obj_size =	sizeof(struct pn_sock),
 };
 
-static const struct proto_ops xsk_proto_ops = {
-	.family =	PF_XDP,
+static const struct proto_ops pnsk_proto_ops = {
+	.family =	PF_PN,
 	.owner =	THIS_MODULE,
-	.release =	xsk_release,
-	.bind =		xsk_bind,
+	.release =	pnsk_release,
+	.bind =		pnsk_bind,
 	.connect =	sock_no_connect,
 	.socketpair =	sock_no_socketpair,
 	.accept =	sock_no_accept,
 	.getname =	sock_no_getname,
-	.poll =		xsk_poll,
+	.poll =		pnsk_poll,
 	.ioctl =	sock_no_ioctl,
 	.listen =	sock_no_listen,
 	.shutdown =	sock_no_shutdown,
-	.setsockopt =	xsk_setsockopt,
+	.setsockopt =	pnsk_setsockopt,
 	.getsockopt =	sock_no_getsockopt,
-	.sendmsg =	xsk_sendmsg,
+	.sendmsg =	pnsk_sendmsg,
 	.recvmsg =	sock_no_recvmsg,
-	.mmap =		xsk_mmap,
+	.mmap =		pnsk_mmap,
 	.sendpage =	sock_no_sendpage,
 };
 
-static void xsk_destruct(struct sock *sk)
+static void pnsk_destruct(struct sock *sk)
 {
-	struct xdp_sock *xs = xdp_sk(sk);
+	struct pn_sock *ps = pn_sk(sk);
 
 	if (!sock_flag(sk, SOCK_DEAD))
 		return;
 
-	xskq_destroy(xs->rx);
-	xskq_destroy(xs->tx);
-	xdp_put_umem(xs->umem);
+	pnskq_destroy(ps->rx);
+	pnskq_destroy(ps->tx);
+	pn_put_umem(ps->umem);
 
 	sk_refcnt_debug_dec(sk);
 }
 
-static int xsk_create(struct net *net, struct socket *sock, int protocol,
+static int pnsk_create(struct net *net, struct socket *sock, int protocol,
 		      int kern)
 {
 	struct sock *sk;
-	struct xdp_sock *xs;
+	struct pn_sock *ps;
 
 	if (!ns_capable(net->user_ns, CAP_NET_RAW))
 		return -EPERM;
@@ -559,53 +559,53 @@ static int xsk_create(struct net *net, struct socket *sock, int protocol,
 
 	sock->state = SS_UNCONNECTED;
 
-	sk = sk_alloc(net, PF_XDP, GFP_KERNEL, &xsk_proto, kern);
+	sk = sk_alloc(net, PF_PN, GFP_KERNEL, &pnsk_proto, kern);
 	if (!sk)
 		return -ENOBUFS;
 
-	sock->ops = &xsk_proto_ops;
+	sock->ops = &pnsk_proto_ops;
 
 	sock_init_data(sock, sk);
 
-	sk->sk_family = PF_XDP;
+	sk->sk_family = PF_PN;
 
-	sk->sk_destruct = xsk_destruct;
+	sk->sk_destruct = pnsk_destruct;
 	sk_refcnt_debug_inc(sk);
 
-	xs = xdp_sk(sk);
-	mutex_init(&xs->mutex);
+	ps = pn_sk(sk);
+	mutex_init(&ps->mutex);
 
 	local_bh_disable();
-	sock_prot_inuse_add(net, &xsk_proto, 1);
+	sock_prot_inuse_add(net, &pnsk_proto, 1);
 	local_bh_enable();
 
 	return 0;
 }
 
-static const struct net_proto_family xsk_family_ops = {
-	.family = PF_XDP,
-	.create = xsk_create,
+static const struct net_proto_family pnsk_family_ops = {
+	.family = PF_PN,
+	.create = pnsk_create,
 	.owner	= THIS_MODULE,
 };
 
-static int __init xsk_init(void)
+static int __init pnsk_init(void)
 {
 	int err;
 
-	err = proto_register(&xsk_proto, 0 /* no slab */);
+	err = proto_register(&pnsk_proto, 0 /* no slab */);
 	if (err)
 		goto out;
 
-	err = sock_register(&xsk_family_ops);
+	err = sock_register(&pnsk_family_ops);
 	if (err)
 		goto out_proto;
 
 	return 0;
 
 out_proto:
-	proto_unregister(&xsk_proto);
+	proto_unregister(&pnsk_proto);
 out:
 	return err;
 }
 
-fs_initcall(xsk_init);
+fs_initcall(pnsk_init);
diff --git a/net/pn/pnsk_queue.c b/net/pn/pnsk_queue.c
index b237ade2..06cec0b 100644
--- a/net/pn/pnsk_queue.c
+++ b/net/pn/pnsk_queue.c
@@ -1,5 +1,5 @@
 // SPDX-License-Identifier: GPL-2.0
-/* XDP user-space ring structure
+/* PN user-space ring structure
  * Copyright(c) 2018 Intel Corporation.
  *
  * This program is free software; you can redistribute it and/or modify it
@@ -14,9 +14,9 @@
 
 #include <linux/slab.h>
 
-#include "xsk_queue.h"
+#include "pnsk_queue.h"
 
-void xskq_set_umem(struct xsk_queue *q, struct xdp_umem_props *umem_props)
+void pnskq_set_umem(struct pnsk_queue *q, struct pn_umem_props *umem_props)
 {
 	if (!q)
 		return;
@@ -24,20 +24,20 @@ void xskq_set_umem(struct xsk_queue *q, struct xdp_umem_props *umem_props)
 	q->umem_props = *umem_props;
 }
 
-static u32 xskq_umem_get_ring_size(struct xsk_queue *q)
+static u32 pnskq_umem_get_ring_size(struct pnsk_queue *q)
 {
-	return sizeof(struct xdp_umem_ring) + q->nentries * sizeof(u32);
+	return sizeof(struct pn_umem_ring) + q->nentries * sizeof(u32);
 }
 
-static u32 xskq_rxtx_get_ring_size(struct xsk_queue *q)
+static u32 pnskq_rxtx_get_ring_size(struct pnsk_queue *q)
 {
-	return (sizeof(struct xdp_ring) +
-		q->nentries * sizeof(struct xdp_desc));
+	return (sizeof(struct pn_ring) +
+		q->nentries * sizeof(struct pn_desc));
 }
 
-struct xsk_queue *xskq_create(u32 nentries, bool umem_queue)
+struct pnsk_queue *pnskq_create(u32 nentries, bool umem_queue)
 {
-	struct xsk_queue *q;
+	struct pnsk_queue *q;
 	gfp_t gfp_flags;
 	size_t size;
 
@@ -50,10 +50,10 @@ struct xsk_queue *xskq_create(u32 nentries, bool umem_queue)
 
 	gfp_flags = GFP_KERNEL | __GFP_ZERO | __GFP_NOWARN |
 		    __GFP_COMP  | __GFP_NORETRY;
-	size = umem_queue ? xskq_umem_get_ring_size(q) :
-	       xskq_rxtx_get_ring_size(q);
+	size = umem_queue ? pnskq_umem_get_ring_size(q) :
+	       pnskq_rxtx_get_ring_size(q);
 
-	q->ring = (struct xdp_ring *)__get_free_pages(gfp_flags,
+	q->ring = (struct pn_ring *)__get_free_pages(gfp_flags,
 						      get_order(size));
 	if (!q->ring) {
 		kfree(q);
@@ -63,7 +63,7 @@ struct xsk_queue *xskq_create(u32 nentries, bool umem_queue)
 	return q;
 }
 
-void xskq_destroy(struct xsk_queue *q)
+void pnskq_destroy(struct pnsk_queue *q)
 {
 	if (!q)
 		return;
diff --git a/net/pn/pnsk_queue.h b/net/pn/pnsk_queue.h
index 3497e88..455da7da 100644
--- a/net/pn/pnsk_queue.h
+++ b/net/pn/pnsk_queue.h
@@ -1,5 +1,5 @@
 /* SPDX-License-Identifier: GPL-2.0
- * XDP user-space ring structure
+ * PN user-space ring structure
  * Copyright(c) 2018 Intel Corporation.
  *
  * This program is free software; you can redistribute it and/or modify it
@@ -12,31 +12,31 @@
  * more details.
  */
 
-#ifndef _LINUX_XSK_QUEUE_H
-#define _LINUX_XSK_QUEUE_H
+#ifndef _LINUX_PNSK_QUEUE_H
+#define _LINUX_PNSK_QUEUE_H
 
 #include <linux/types.h>
-#include <linux/if_xdp.h>
+#include <linux/if_pn.h>
 
-#include "xdp_umem_props.h"
+#include "pn_umem_props.h"
 
 #define RX_BATCH_SIZE 16
 
-struct xsk_queue {
-	struct xdp_umem_props umem_props;
+struct pnsk_queue {
+	struct pn_umem_props umem_props;
 	u32 ring_mask;
 	u32 nentries;
 	u32 prod_head;
 	u32 prod_tail;
 	u32 cons_head;
 	u32 cons_tail;
-	struct xdp_ring *ring;
+	struct pn_ring *ring;
 	u64 invalid_descs;
 };
 
 /* Common functions operating for both RXTX and umem queues */
 
-static inline u32 xskq_nb_avail(struct xsk_queue *q, u32 dcnt)
+static inline u32 pnskq_nb_avail(struct pnsk_queue *q, u32 dcnt)
 {
 	u32 entries = q->prod_tail - q->cons_tail;
 
@@ -49,7 +49,7 @@ static inline u32 xskq_nb_avail(struct xsk_queue *q, u32 dcnt)
 	return (entries > dcnt) ? dcnt : entries;
 }
 
-static inline u32 xskq_nb_free(struct xsk_queue *q, u32 producer, u32 dcnt)
+static inline u32 pnskq_nb_free(struct pnsk_queue *q, u32 producer, u32 dcnt)
 {
 	u32 free_entries = q->nentries - (producer - q->cons_tail);
 
@@ -63,7 +63,7 @@ static inline u32 xskq_nb_free(struct xsk_queue *q, u32 producer, u32 dcnt)
 
 /* UMEM queue */
 
-static inline bool xskq_is_valid_id(struct xsk_queue *q, u32 idx)
+static inline bool pnskq_is_valid_id(struct pnsk_queue *q, u32 idx)
 {
 	if (unlikely(idx >= q->umem_props.nframes)) {
 		q->invalid_descs++;
@@ -72,13 +72,13 @@ static inline bool xskq_is_valid_id(struct xsk_queue *q, u32 idx)
 	return true;
 }
 
-static inline u32 *xskq_validate_id(struct xsk_queue *q)
+static inline u32 *pnskq_validate_id(struct pnsk_queue *q)
 {
 	while (q->cons_tail != q->cons_head) {
-		struct xdp_umem_ring *ring = (struct xdp_umem_ring *)q->ring;
+		struct pn_umem_ring *ring = (struct pn_umem_ring *)q->ring;
 		unsigned int idx = q->cons_tail & q->ring_mask;
 
-		if (xskq_is_valid_id(q, ring->desc[idx]))
+		if (pnskq_is_valid_id(q, ring->desc[idx]))
 			return &ring->desc[idx];
 
 		q->cons_tail++;
@@ -87,33 +87,33 @@ static inline u32 *xskq_validate_id(struct xsk_queue *q)
 	return NULL;
 }
 
-static inline u32 *xskq_peek_id(struct xsk_queue *q)
+static inline u32 *pnskq_peek_id(struct pnsk_queue *q)
 {
-	struct xdp_umem_ring *ring;
+	struct pn_umem_ring *ring;
 
 	if (q->cons_tail == q->cons_head) {
 		WRITE_ONCE(q->ring->consumer, q->cons_tail);
-		q->cons_head = q->cons_tail + xskq_nb_avail(q, RX_BATCH_SIZE);
+		q->cons_head = q->cons_tail + pnskq_nb_avail(q, RX_BATCH_SIZE);
 
 		/* Order consumer and data */
 		smp_rmb();
 
-		return xskq_validate_id(q);
+		return pnskq_validate_id(q);
 	}
 
-	ring = (struct xdp_umem_ring *)q->ring;
+	ring = (struct pn_umem_ring *)q->ring;
 	return &ring->desc[q->cons_tail & q->ring_mask];
 }
 
-static inline void xskq_discard_id(struct xsk_queue *q)
+static inline void pnskq_discard_id(struct pnsk_queue *q)
 {
 	q->cons_tail++;
-	(void)xskq_validate_id(q);
+	(void)pnskq_validate_id(q);
 }
 
-static inline int xskq_produce_id(struct xsk_queue *q, u32 id)
+static inline int pnskq_produce_id(struct pnsk_queue *q, u32 id)
 {
-	struct xdp_umem_ring *ring = (struct xdp_umem_ring *)q->ring;
+	struct pn_umem_ring *ring = (struct pn_umem_ring *)q->ring;
 
 	ring->desc[q->prod_tail++ & q->ring_mask] = id;
 
@@ -124,9 +124,9 @@ static inline int xskq_produce_id(struct xsk_queue *q, u32 id)
 	return 0;
 }
 
-static inline int xskq_reserve_id(struct xsk_queue *q)
+static inline int pnskq_reserve_id(struct pnsk_queue *q)
 {
-	if (xskq_nb_free(q, q->prod_head, 1) == 0)
+	if (pnskq_nb_free(q, q->prod_head, 1) == 0)
 		return -ENOSPC;
 
 	q->prod_head++;
@@ -135,7 +135,7 @@ static inline int xskq_reserve_id(struct xsk_queue *q)
 
 /* Rx/Tx queue */
 
-static inline bool xskq_is_valid_desc(struct xsk_queue *q, struct xdp_desc *d)
+static inline bool pnskq_is_valid_desc(struct pnsk_queue *q, struct pn_desc *d)
 {
 	u32 buff_len;
 
@@ -154,14 +154,14 @@ static inline bool xskq_is_valid_desc(struct xsk_queue *q, struct xdp_desc *d)
 	return true;
 }
 
-static inline struct xdp_desc *xskq_validate_desc(struct xsk_queue *q,
-						  struct xdp_desc *desc)
+static inline struct pn_desc *pnskq_validate_desc(struct pnsk_queue *q,
+						  struct pn_desc *desc)
 {
 	while (q->cons_tail != q->cons_head) {
-		struct xdp_rxtx_ring *ring = (struct xdp_rxtx_ring *)q->ring;
+		struct pn_rxtx_ring *ring = (struct pn_rxtx_ring *)q->ring;
 		unsigned int idx = q->cons_tail & q->ring_mask;
 
-		if (xskq_is_valid_desc(q, &ring->desc[idx])) {
+		if (pnskq_is_valid_desc(q, &ring->desc[idx])) {
 			if (desc)
 				*desc = ring->desc[idx];
 			return desc;
@@ -173,39 +173,39 @@ static inline struct xdp_desc *xskq_validate_desc(struct xsk_queue *q,
 	return NULL;
 }
 
-static inline struct xdp_desc *xskq_peek_desc(struct xsk_queue *q,
-					      struct xdp_desc *desc)
+static inline struct pn_desc *pnskq_peek_desc(struct pnsk_queue *q,
+					      struct pn_desc *desc)
 {
-	struct xdp_rxtx_ring *ring;
+	struct pn_rxtx_ring *ring;
 
 	if (q->cons_tail == q->cons_head) {
 		WRITE_ONCE(q->ring->consumer, q->cons_tail);
-		q->cons_head = q->cons_tail + xskq_nb_avail(q, RX_BATCH_SIZE);
+		q->cons_head = q->cons_tail + pnskq_nb_avail(q, RX_BATCH_SIZE);
 
 		/* Order consumer and data */
 		smp_rmb();
 
-		return xskq_validate_desc(q, desc);
+		return pnskq_validate_desc(q, desc);
 	}
 
-	ring = (struct xdp_rxtx_ring *)q->ring;
+	ring = (struct pn_rxtx_ring *)q->ring;
 	*desc = ring->desc[q->cons_tail & q->ring_mask];
 	return desc;
 }
 
-static inline void xskq_discard_desc(struct xsk_queue *q)
+static inline void pnskq_discard_desc(struct pnsk_queue *q)
 {
 	q->cons_tail++;
-	(void)xskq_validate_desc(q, NULL);
+	(void)pnskq_validate_desc(q, NULL);
 }
 
-static inline int xskq_produce_batch_desc(struct xsk_queue *q,
+static inline int pnskq_produce_batch_desc(struct pnsk_queue *q,
 					  u32 id, u32 len, u16 offset)
 {
-	struct xdp_rxtx_ring *ring = (struct xdp_rxtx_ring *)q->ring;
+	struct pn_rxtx_ring *ring = (struct pn_rxtx_ring *)q->ring;
 	unsigned int idx;
 
-	if (xskq_nb_free(q, q->prod_head, 1) == 0)
+	if (pnskq_nb_free(q, q->prod_head, 1) == 0)
 		return -ENOSPC;
 
 	idx = (q->prod_head++) & q->ring_mask;
@@ -216,7 +216,7 @@ static inline int xskq_produce_batch_desc(struct xsk_queue *q,
 	return 0;
 }
 
-static inline void xskq_produce_flush_desc(struct xsk_queue *q)
+static inline void pnskq_produce_flush_desc(struct pnsk_queue *q)
 {
 	/* Order producer and data */
 	smp_wmb();
@@ -225,18 +225,18 @@ static inline void xskq_produce_flush_desc(struct xsk_queue *q)
 	WRITE_ONCE(q->ring->producer, q->prod_tail);
 }
 
-static inline bool xskq_full_desc(struct xsk_queue *q)
+static inline bool pnskq_full_desc(struct pnsk_queue *q)
 {
-	return (xskq_nb_avail(q, q->nentries) == q->nentries);
+	return (pnskq_nb_avail(q, q->nentries) == q->nentries);
 }
 
-static inline bool xskq_empty_desc(struct xsk_queue *q)
+static inline bool pnskq_empty_desc(struct pnsk_queue *q)
 {
-	return (xskq_nb_free(q, q->prod_tail, 1) == q->nentries);
+	return (pnskq_nb_free(q, q->prod_tail, 1) == q->nentries);
 }
 
-void xskq_set_umem(struct xsk_queue *q, struct xdp_umem_props *umem_props);
-struct xsk_queue *xskq_create(u32 nentries, bool umem_queue);
-void xskq_destroy(struct xsk_queue *q_ops);
+void pnskq_set_umem(struct pnsk_queue *q, struct pn_umem_props *umem_props);
+struct pnsk_queue *pnskq_create(u32 nentries, bool umem_queue);
+void pnskq_destroy(struct pnsk_queue *q_ops);
 
-#endif /* _LINUX_XSK_QUEUE_H */
+#endif /* _LINUX_PNSK_QUEUE_H */
diff --git a/samples/bpf/pnsock.h b/samples/bpf/pnsock.h
index 533ab81..1ed1dd8 100644
--- a/samples/bpf/pnsock.h
+++ b/samples/bpf/pnsock.h
@@ -1,6 +1,6 @@
 /* SPDX-License-Identifier: GPL-2.0 */
-#ifndef XDPSOCK_H_
-#define XDPSOCK_H_
+#ifndef PNSOCK_H_
+#define PNSOCK_H_
 
 /* Power-of-2 number of sockets */
 #define MAX_SOCKS 4
@@ -8,4 +8,4 @@
 /* Round-robin receive */
 #define RR_LB 0
 
-#endif /* XDPSOCK_H_ */
+#endif /* PNSOCK_H_ */
diff --git a/samples/bpf/pnsock_user.c b/samples/bpf/pnsock_user.c
index 2ed6337..1f80634 100644
--- a/samples/bpf/pnsock_user.c
+++ b/samples/bpf/pnsock_user.c
@@ -17,7 +17,7 @@
 #include <libgen.h>
 /*#include <linux/bpf.h>*/
 #include <linux/if_link.h>
-#include <linux/if_xdp.h>
+#include <linux/if_pn.h>
 /*#include <linux/if_ether.h>*/
 #include <net/if.h>
 #include <signal.h>
@@ -42,18 +42,18 @@
 #include "libbpf.h"
 */
 
-#include "xdpsock.h"
+#include "pnsock.h"
 
-#ifndef SOL_XDP
-#define SOL_XDP 282
+#ifndef SOL_PN
+#define SOL_PN 282
 #endif
 
-#ifndef AF_XDP
-#define AF_XDP 43
+#ifndef AF_PN
+#define AF_PN 43
 #endif
 
-#ifndef PF_XDP
-#define PF_XDP AF_XDP
+#ifndef PF_PN
+#define PF_PN AF_PN
 #endif
 
 #define NUM_FRAMES 256
@@ -78,7 +78,7 @@ enum benchmark_type {
 };
 
 static enum benchmark_type opt_bench = BENCH_RXDROP;
-static u32 opt_xdp_flags;
+static u32 opt_pn_flags;
 static const char *opt_if = "";
 static int opt_ifindex;
 static int opt_queue;
@@ -86,34 +86,34 @@ static int opt_poll;
 static int opt_shared_packet_buffer;
 static int opt_interval = 1;
 
-struct xdp_umem_uqueue {
+struct pn_umem_uqueue {
 	u32 cached_prod;
 	u32 cached_cons;
 	u32 mask;
 	u32 size;
-	struct xdp_umem_ring *ring;
+	struct pn_umem_ring *ring;
 };
 
-struct xdp_umem {
+struct pn_umem {
 	char (*frames)[FRAME_SIZE];
-	struct xdp_umem_uqueue fq;
-	struct xdp_umem_uqueue cq;
+	struct pn_umem_uqueue fq;
+	struct pn_umem_uqueue cq;
 	int fd;
 };
 
-struct xdp_uqueue {
+struct pn_uqueue {
 	u32 cached_prod;
 	u32 cached_cons;
 	u32 mask;
 	u32 size;
-	struct xdp_rxtx_ring *ring;
+	struct pn_rxtx_ring *ring;
 };
 
-struct xdpsock {
-	struct xdp_uqueue rx;
-	struct xdp_uqueue tx;
+struct pnsock {
+	struct pn_uqueue rx;
+	struct pn_uqueue tx;
 	int sfd;
-	struct xdp_umem *umem;
+	struct pn_umem *umem;
 	u32 outstanding_tx;
 	unsigned long rx_npkts;
 	unsigned long tx_npkts;
@@ -123,7 +123,7 @@ struct xdpsock {
 
 #define MAX_SOCKS 4
 static int num_socks;
-struct xdpsock *xsks[MAX_SOCKS];
+struct pnsock *pnsks[MAX_SOCKS];
 
 static unsigned long get_nsecs(void)
 {
@@ -161,7 +161,7 @@ static const char pkt_data[] =
 	"\xe9\x1\x10\x92\x10\x92\x00\x1a\x6d\xa3\x34\x33\x1f\x69\x40\x6b"
 	"\x54\x59\xb6\x14\x2d\x11\x44\xbf\xaf\xd9\xbe\xaa";
 
-static inline u32 umem_nb_free(struct xdp_umem_uqueue *q, u32 nb)
+static inline u32 umem_nb_free(struct pn_umem_uqueue *q, u32 nb)
 {
 	u32 free_entries = q->size - (q->cached_prod - q->cached_cons);
 
@@ -174,7 +174,7 @@ static inline u32 umem_nb_free(struct xdp_umem_uqueue *q, u32 nb)
 	return q->size - (q->cached_prod - q->cached_cons);
 }
 
-static inline u32 xq_nb_free(struct xdp_uqueue *q, u32 ndescs)
+static inline u32 xq_nb_free(struct pn_uqueue *q, u32 ndescs)
 {
 	u32 free_entries = q->cached_cons - q->cached_prod;
 
@@ -186,7 +186,7 @@ static inline u32 xq_nb_free(struct xdp_uqueue *q, u32 ndescs)
 	return q->cached_cons - q->cached_prod;
 }
 
-static inline u32 umem_nb_avail(struct xdp_umem_uqueue *q, u32 nb)
+static inline u32 umem_nb_avail(struct pn_umem_uqueue *q, u32 nb)
 {
 	u32 entries = q->cached_prod - q->cached_cons;
 
@@ -198,7 +198,7 @@ static inline u32 umem_nb_avail(struct xdp_umem_uqueue *q, u32 nb)
 	return (entries > nb) ? nb : entries;
 }
 
-static inline u32 xq_nb_avail(struct xdp_uqueue *q, u32 ndescs)
+static inline u32 xq_nb_avail(struct pn_uqueue *q, u32 ndescs)
 {
 	u32 entries = q->cached_prod - q->cached_cons;
 
@@ -210,8 +210,8 @@ static inline u32 xq_nb_avail(struct xdp_uqueue *q, u32 ndescs)
 	return (entries > ndescs) ? ndescs : entries;
 }
 
-static inline int umem_fill_to_kernel_ex(struct xdp_umem_uqueue *fq,
-					 struct xdp_desc *d,
+static inline int umem_fill_to_kernel_ex(struct pn_umem_uqueue *fq,
+					 struct pn_desc *d,
 					 size_t nb)
 {
 	u32 i;
@@ -232,7 +232,7 @@ static inline int umem_fill_to_kernel_ex(struct xdp_umem_uqueue *fq,
 	return 0;
 }
 
-static inline int umem_fill_to_kernel(struct xdp_umem_uqueue *fq, u32 *d,
+static inline int umem_fill_to_kernel(struct pn_umem_uqueue *fq, u32 *d,
 				      size_t nb)
 {
 	u32 i;
@@ -253,7 +253,7 @@ static inline int umem_fill_to_kernel(struct xdp_umem_uqueue *fq, u32 *d,
 	return 0;
 }
 
-static inline size_t umem_complete_from_kernel(struct xdp_umem_uqueue *cq,
+static inline size_t umem_complete_from_kernel(struct pn_umem_uqueue *cq,
 					       u32 *d, size_t nb)
 {
 	u32 idx, i, entries = umem_nb_avail(cq, nb);
@@ -274,17 +274,17 @@ static inline size_t umem_complete_from_kernel(struct xdp_umem_uqueue *cq,
 	return entries;
 }
 
-static inline void *xq_get_data(struct xdpsock *xsk, __u32 idx, __u32 off)
+static inline void *xq_get_data(struct pnsock *pnsk, __u32 idx, __u32 off)
 {
 	lassert(idx < NUM_FRAMES);
-	return &xsk->umem->frames[idx][off];
+	return &pnsk->umem->frames[idx][off];
 }
 
-static inline int xq_enq(struct xdp_uqueue *uq,
-			 const struct xdp_desc *descs,
+static inline int xq_enq(struct pn_uqueue *uq,
+			 const struct pn_desc *descs,
 			 unsigned int ndescs)
 {
-	struct xdp_rxtx_ring *r = uq->ring;
+	struct pn_rxtx_ring *r = uq->ring;
 	unsigned int i;
 
 	if (xq_nb_free(uq, ndescs) < ndescs)
@@ -304,10 +304,10 @@ static inline int xq_enq(struct xdp_uqueue *uq,
 	return 0;
 }
 
-static inline int xq_enq_tx_only(struct xdp_uqueue *uq,
+static inline int xq_enq_tx_only(struct pn_uqueue *uq,
 				 __u32 idx, unsigned int ndescs)
 {
-	struct xdp_rxtx_ring *q = uq->ring;
+	struct pn_rxtx_ring *q = uq->ring;
 	unsigned int i;
 
 	if (xq_nb_free(uq, ndescs) < ndescs)
@@ -327,11 +327,11 @@ static inline int xq_enq_tx_only(struct xdp_uqueue *uq,
 	return 0;
 }
 
-static inline int xq_deq(struct xdp_uqueue *uq,
-			 struct xdp_desc *descs,
+static inline int xq_deq(struct pn_uqueue *uq,
+			 struct pn_desc *descs,
 			 int ndescs)
 {
-	struct xdp_rxtx_ring *r = uq->ring;
+	struct pn_rxtx_ring *r = uq->ring;
 	unsigned int idx;
 	int i, entries;
 
@@ -405,11 +405,11 @@ static size_t gen_eth_frame(char *frame)
 	return sizeof(pkt_data) - 1;
 }
 
-static struct xdp_umem *xdp_umem_configure(int sfd)
+static struct pn_umem *pn_umem_configure(int sfd)
 {
 	int fq_size = FQ_NUM_DESCS, cq_size = CQ_NUM_DESCS;
-	struct xdp_umem_reg mr;
-	struct xdp_umem *umem;
+	struct pn_umem_reg mr;
+	struct pn_umem *umem;
 	void *bufs;
 
 	umem = calloc(1, sizeof(*umem));
@@ -423,27 +423,27 @@ static struct xdp_umem *xdp_umem_configure(int sfd)
 	mr.frame_size = FRAME_SIZE;
 	mr.frame_headroom = FRAME_HEADROOM;
 
-	lassert(setsockopt(sfd, SOL_XDP, XDP_UMEM_REG, &mr, sizeof(mr)) == 0);
-	lassert(setsockopt(sfd, SOL_XDP, XDP_UMEM_FILL_RING, &fq_size,
+	lassert(setsockopt(sfd, SOL_PN, PN_UMEM_REG, &mr, sizeof(mr)) == 0);
+	lassert(setsockopt(sfd, SOL_PN, PN_UMEM_FILL_RING, &fq_size,
 			   sizeof(int)) == 0);
-	lassert(setsockopt(sfd, SOL_XDP, XDP_UMEM_COMPLETION_RING, &cq_size,
+	lassert(setsockopt(sfd, SOL_PN, PN_UMEM_COMPLETION_RING, &cq_size,
 			   sizeof(int)) == 0);
 
-	umem->fq.ring = mmap(0, sizeof(struct xdp_umem_ring) +
+	umem->fq.ring = mmap(0, sizeof(struct pn_umem_ring) +
 			     FQ_NUM_DESCS * sizeof(u32),
 			     PROT_READ | PROT_WRITE,
 			     MAP_SHARED | MAP_POPULATE, sfd,
-			     XDP_UMEM_PGOFF_FILL_RING);
+			     PN_UMEM_PGOFF_FILL_RING);
 	lassert(umem->fq.ring != MAP_FAILED);
 
 	umem->fq.mask = FQ_NUM_DESCS - 1;
 	umem->fq.size = FQ_NUM_DESCS;
 
-	umem->cq.ring = mmap(0, sizeof(struct xdp_umem_ring) +
+	umem->cq.ring = mmap(0, sizeof(struct pn_umem_ring) +
 			     CQ_NUM_DESCS * sizeof(u32),
 			     PROT_READ | PROT_WRITE,
 			     MAP_SHARED | MAP_POPULATE, sfd,
-			     XDP_UMEM_PGOFF_COMPLETION_RING);
+			     PN_UMEM_PGOFF_COMPLETION_RING);
 	lassert(umem->cq.ring != MAP_FAILED);
 
 	umem->cq.mask = CQ_NUM_DESCS - 1;
@@ -462,76 +462,76 @@ static struct xdp_umem *xdp_umem_configure(int sfd)
 	return umem;
 }
 
-static struct xdpsock *xsk_configure(struct xdp_umem *umem)
+static struct pnsock *pnsk_configure(struct pn_umem *umem)
 {
-	struct sockaddr_xdp sxdp = {};
+	struct sockaddr_pn spn = {};
 	int sfd, ndescs = NUM_DESCS;
-	struct xdpsock *xsk;
+	struct pnsock *pnsk;
 	bool shared = true;
 	u32 i;
 
-	sfd = socket(PF_XDP, SOCK_RAW, 0);
+	sfd = socket(PF_PN, SOCK_RAW, 0);
 	lassert(sfd >= 0);
 
-	xsk = calloc(1, sizeof(*xsk));
-	lassert(xsk);
+	pnsk = calloc(1, sizeof(*pnsk));
+	lassert(pnsk);
 
-	xsk->sfd = sfd;
-	xsk->outstanding_tx = 0;
+	pnsk->sfd = sfd;
+	pnsk->outstanding_tx = 0;
 
 	if (!umem) {
 		shared = false;
-		xsk->umem = xdp_umem_configure(sfd);
+		pnsk->umem = pn_umem_configure(sfd);
 	} else {
-		xsk->umem = umem;
+		pnsk->umem = umem;
 	}
 
-	lassert(setsockopt(sfd, SOL_XDP, XDP_RX_RING,
+	lassert(setsockopt(sfd, SOL_PN, PN_RX_RING,
 			   &ndescs, sizeof(int)) == 0);
-	lassert(setsockopt(sfd, SOL_XDP, XDP_TX_RING,
+	lassert(setsockopt(sfd, SOL_PN, PN_TX_RING,
 			   &ndescs, sizeof(int)) == 0);
 
 	/* Rx */
-	xsk->rx.ring = mmap(NULL,
-			    sizeof(struct xdp_ring) +
-			    NUM_DESCS * sizeof(struct xdp_desc),
+	pnsk->rx.ring = mmap(NULL,
+			    sizeof(struct pn_ring) +
+			    NUM_DESCS * sizeof(struct pn_desc),
 			    PROT_READ | PROT_WRITE,
 			    MAP_SHARED | MAP_POPULATE, sfd,
-			    XDP_PGOFF_RX_RING);
-	lassert(xsk->rx.ring != MAP_FAILED);
+			    PN_PGOFF_RX_RING);
+	lassert(pnsk->rx.ring != MAP_FAILED);
 
 	if (!shared) {
 		for (i = 0; i < NUM_DESCS / 2; i++)
-			lassert(umem_fill_to_kernel(&xsk->umem->fq, &i, 1)
+			lassert(umem_fill_to_kernel(&pnsk->umem->fq, &i, 1)
 				== 0);
 	}
 
 	/* Tx */
-	xsk->tx.ring = mmap(NULL,
-			 sizeof(struct xdp_ring) +
-			 NUM_DESCS * sizeof(struct xdp_desc),
+	pnsk->tx.ring = mmap(NULL,
+			 sizeof(struct pn_ring) +
+			 NUM_DESCS * sizeof(struct pn_desc),
 			 PROT_READ | PROT_WRITE,
 			 MAP_SHARED | MAP_POPULATE, sfd,
-			 XDP_PGOFF_TX_RING);
-	lassert(xsk->tx.ring != MAP_FAILED);
+			 PN_PGOFF_TX_RING);
+	lassert(pnsk->tx.ring != MAP_FAILED);
 
-	xsk->rx.mask = NUM_DESCS - 1;
-	xsk->rx.size = NUM_DESCS;
+	pnsk->rx.mask = NUM_DESCS - 1;
+	pnsk->rx.size = NUM_DESCS;
 
-	xsk->tx.mask = NUM_DESCS - 1;
-	xsk->tx.size = NUM_DESCS;
+	pnsk->tx.mask = NUM_DESCS - 1;
+	pnsk->tx.size = NUM_DESCS;
 
-	sxdp.sxdp_family = PF_XDP;
-	sxdp.sxdp_ifindex = opt_ifindex;
-	sxdp.sxdp_queue_id = opt_queue;
+	spn.spn_family = PF_PN;
+	spn.spn_ifindex = opt_ifindex;
+	spn.spn_queue_id = opt_queue;
 	if (shared) {
-		sxdp.sxdp_flags = XDP_SHARED_UMEM;
-		sxdp.sxdp_shared_umem_fd = umem->fd;
+		spn.spn_flags = PN_SHARED_UMEM;
+		spn.spn_shared_umem_fd = umem->fd;
 	}
 
-	lassert(bind(sfd, (struct sockaddr *)&sxdp, sizeof(sxdp)) == 0);
+	lassert(bind(sfd, (struct sockaddr *)&spn, sizeof(spn)) == 0);
 
-	return xsk;
+	return pnsk;
 }
 
 static void print_benchmark(bool running)
@@ -546,10 +546,10 @@ static void print_benchmark(bool running)
 		bench_str = "l2fwd";
 
 	printf("%s:%d %s ", opt_if, opt_queue, bench_str);
-	/*if (opt_xdp_flags & XDP_FLAGS_SKB_MODE)
-		printf("xdp-skb ");
-	else if (opt_xdp_flags & XDP_FLAGS_DRV_MODE)
-		printf("xdp-drv ");
+	/*if (opt_pn_flags & PN_FLAGS_SKB_MODE)
+		printf("pn-skb ");
+	else if (opt_pn_flags & PN_FLAGS_DRV_MODE)
+		printf("pn-drv ");
 	else
 		printf("	");*/
 
@@ -574,9 +574,9 @@ static void dump_stats(void)
 		char *fmt = "%-15s %'-11.0f %'-11lu\n";
 		double rx_pps, tx_pps;
 
-		rx_pps = (xsks[i]->rx_npkts - xsks[i]->prev_rx_npkts) *
+		rx_pps = (pnsks[i]->rx_npkts - pnsks[i]->prev_rx_npkts) *
 			 1000000000. / dt;
-		tx_pps = (xsks[i]->tx_npkts - xsks[i]->prev_tx_npkts) *
+		tx_pps = (pnsks[i]->tx_npkts - pnsks[i]->prev_tx_npkts) *
 			 1000000000. / dt;
 
 		printf("\n sock%d@", i);
@@ -585,11 +585,11 @@ static void dump_stats(void)
 
 		printf("%-15s %-11s %-11s %-11.2f\n", "", "pps", "pkts",
 		       dt / 1000000000.);
-		printf(fmt, "rx", rx_pps, xsks[i]->rx_npkts);
-		printf(fmt, "tx", tx_pps, xsks[i]->tx_npkts);
+		printf(fmt, "rx", rx_pps, pnsks[i]->rx_npkts);
+		printf(fmt, "tx", tx_pps, pnsks[i]->tx_npkts);
 
-		xsks[i]->prev_rx_npkts = xsks[i]->rx_npkts;
-		xsks[i]->prev_tx_npkts = xsks[i]->tx_npkts;
+		pnsks[i]->prev_rx_npkts = pnsks[i]->rx_npkts;
+		pnsks[i]->prev_tx_npkts = pnsks[i]->tx_npkts;
 	}
 }
 
@@ -608,7 +608,7 @@ static void int_exit(int sig)
 {
 	(void)sig;
 	dump_stats();
-	/*bpf_set_link_xdp_fd(opt_ifindex, -1, opt_xdp_flags);*/
+	/*bpf_set_link_pn_fd(opt_ifindex, -1, opt_pn_flags);*/
 	exit(EXIT_SUCCESS);
 }
 
@@ -620,8 +620,8 @@ static struct option long_options[] = {
 	{"queue", required_argument, 0, 'q'},
 	{"poll", no_argument, 0, 'p'},
 	{"shared-buffer", no_argument, 0, 's'},
-	{"xdp-skb", no_argument, 0, 'S'},
-	{"xdp-native", no_argument, 0, 'N'},
+	{"pn-skb", no_argument, 0, 'S'},
+	{"pn-native", no_argument, 0, 'N'},
 	{"interval", required_argument, 0, 'n'},
 	{0, 0, 0, 0}
 };
@@ -638,8 +638,8 @@ static void usage(const char *prog)
 		"  -q, --queue=n	Use queue n (default 0)\n"
 		"  -p, --poll		Use poll syscall\n"
 		"  -s, --shared-buffer	Use shared packet buffer\n"
-		"  -S, --xdp-skb=n	Use XDP skb-mod\n"
-		"  -N, --xdp-native=n	Enfore XDP native mode\n"
+		"  -S, --pn-skb=n	Use PN skb-mod\n"
+		"  -N, --pn-native=n	Enfore PN native mode\n"
 		"  -n, --interval=n	Specify statistics update interval (default 1 sec).\n"
 		"\n";
 	fprintf(stderr, str, prog);
@@ -681,10 +681,10 @@ static void parse_command_line(int argc, char **argv)
 			opt_poll = 1;
 			break;
 		case 'S':
-			/*opt_xdp_flags |= XDP_FLAGS_SKB_MODE;*/
+			/*opt_pn_flags |= PN_FLAGS_SKB_MODE;*/
 			break;
 		case 'N':
-			/*opt_xdp_flags |= XDP_FLAGS_DRV_MODE;*/
+			/*opt_pn_flags |= PN_FLAGS_DRV_MODE;*/
 			break;
 		case 'n':
 			opt_interval = atoi(optarg);
@@ -712,51 +712,51 @@ static void kick_tx(int fd)
 	lassert(0);
 }
 
-static inline void complete_tx_l2fwd(struct xdpsock *xsk)
+static inline void complete_tx_l2fwd(struct pnsock *pnsk)
 {
 	u32 descs[BATCH_SIZE];
 	unsigned int rcvd;
 	size_t ndescs;
 
-	if (!xsk->outstanding_tx)
+	if (!pnsk->outstanding_tx)
 		return;
 
-	kick_tx(xsk->sfd);
-	ndescs = (xsk->outstanding_tx > BATCH_SIZE) ? BATCH_SIZE :
-		 xsk->outstanding_tx;
+	kick_tx(pnsk->sfd);
+	ndescs = (pnsk->outstanding_tx > BATCH_SIZE) ? BATCH_SIZE :
+		 pnsk->outstanding_tx;
 
 	/* re-add completed Tx buffers */
-	rcvd = umem_complete_from_kernel(&xsk->umem->cq, descs, ndescs);
+	rcvd = umem_complete_from_kernel(&pnsk->umem->cq, descs, ndescs);
 	if (rcvd > 0) {
-		umem_fill_to_kernel(&xsk->umem->fq, descs, rcvd);
-		xsk->outstanding_tx -= rcvd;
-		xsk->tx_npkts += rcvd;
+		umem_fill_to_kernel(&pnsk->umem->fq, descs, rcvd);
+		pnsk->outstanding_tx -= rcvd;
+		pnsk->tx_npkts += rcvd;
 	}
 }
 
-static inline void complete_tx_only(struct xdpsock *xsk)
+static inline void complete_tx_only(struct pnsock *pnsk)
 {
 	u32 descs[BATCH_SIZE];
 	unsigned int rcvd;
 
-	if (!xsk->outstanding_tx)
+	if (!pnsk->outstanding_tx)
 		return;
 
-	kick_tx(xsk->sfd);
+	kick_tx(pnsk->sfd);
 
-	rcvd = umem_complete_from_kernel(&xsk->umem->cq, descs, BATCH_SIZE);
+	rcvd = umem_complete_from_kernel(&pnsk->umem->cq, descs, BATCH_SIZE);
 	if (rcvd > 0) {
-		xsk->outstanding_tx -= rcvd;
-		xsk->tx_npkts += rcvd;
+		pnsk->outstanding_tx -= rcvd;
+		pnsk->tx_npkts += rcvd;
 	}
 }
 
-static void rx_drop(struct xdpsock *xsk)
+static void rx_drop(struct pnsock *pnsk)
 {
-	struct xdp_desc descs[BATCH_SIZE];
+	struct pn_desc descs[BATCH_SIZE];
 	unsigned int rcvd, i;
 
-	rcvd = xq_deq(&xsk->rx, descs, BATCH_SIZE);
+	rcvd = xq_deq(&pnsk->rx, descs, BATCH_SIZE);
 	if (!rcvd)
 		return;
 
@@ -768,15 +768,15 @@ static void rx_drop(struct xdpsock *xsk)
 		char *pkt;
 		char buf[32];
 
-		pkt = xq_get_data(xsk, idx, descs[i].offset);
+		pkt = xq_get_data(pnsk, idx, descs[i].offset);
 		sprintf(buf, "idx=%d", idx);
 		hex_dump(pkt, descs[i].len, buf);
 #endif
 	}
 
-	xsk->rx_npkts += rcvd;
+	pnsk->rx_npkts += rcvd;
 
-	umem_fill_to_kernel_ex(&xsk->umem->fq, descs, rcvd);
+	umem_fill_to_kernel_ex(&pnsk->umem->fq, descs, rcvd);
 }
 
 static void rx_drop_all(void)
@@ -787,7 +787,7 @@ static void rx_drop_all(void)
 	memset(fds, 0, sizeof(fds));
 
 	for (i = 0; i < num_socks; i++) {
-		fds[i].fd = xsks[i]->sfd;
+		fds[i].fd = pnsks[i]->sfd;
 		fds[i].events = POLLIN;
 		timeout = 1000; /* 1sn */
 	}
@@ -800,18 +800,18 @@ static void rx_drop_all(void)
 		}
 
 		for (i = 0; i < num_socks; i++)
-			rx_drop(xsks[i]);
+			rx_drop(pnsks[i]);
 	}
 }
 
-static void tx_only(struct xdpsock *xsk)
+static void tx_only(struct pnsock *pnsk)
 {
 	int timeout, ret, nfds = 1;
 	struct pollfd fds[nfds + 1];
 	unsigned int idx = 0;
 
 	memset(fds, 0, sizeof(fds));
-	fds[0].fd = xsk->sfd;
+	fds[0].fd = pnsk->sfd;
 	fds[0].events = POLLOUT;
 	timeout = 1000; /* 1sn */
 
@@ -821,40 +821,40 @@ static void tx_only(struct xdpsock *xsk)
 			if (ret <= 0)
 				continue;
 
-			if (fds[0].fd != xsk->sfd ||
+			if (fds[0].fd != pnsk->sfd ||
 			    !(fds[0].revents & POLLOUT))
 				continue;
 		}
 
-		if (xq_nb_free(&xsk->tx, BATCH_SIZE) >= BATCH_SIZE) {
-			lassert(xq_enq_tx_only(&xsk->tx, idx, BATCH_SIZE) == 0);
+		if (xq_nb_free(&pnsk->tx, BATCH_SIZE) >= BATCH_SIZE) {
+			lassert(xq_enq_tx_only(&pnsk->tx, idx, BATCH_SIZE) == 0);
 
-			xsk->outstanding_tx += BATCH_SIZE;
+			pnsk->outstanding_tx += BATCH_SIZE;
 			idx += BATCH_SIZE;
 			idx %= NUM_FRAMES;
 		}
 
-		complete_tx_only(xsk);
+		complete_tx_only(pnsk);
 	}
 }
 
-static void l2fwd(struct xdpsock *xsk)
+static void l2fwd(struct pnsock *pnsk)
 {
 	for (;;) {
-		struct xdp_desc descs[BATCH_SIZE];
+		struct pn_desc descs[BATCH_SIZE];
 		unsigned int rcvd, i;
 		int ret;
 
 		for (;;) {
-			complete_tx_l2fwd(xsk);
+			complete_tx_l2fwd(pnsk);
 
-			rcvd = xq_deq(&xsk->rx, descs, BATCH_SIZE);
+			rcvd = xq_deq(&pnsk->rx, descs, BATCH_SIZE);
 			if (rcvd > 0)
 				break;
 		}
 
 		for (i = 0; i < rcvd; i++) {
-			char *pkt = xq_get_data(xsk, descs[i].idx,
+			char *pkt = xq_get_data(pnsk, descs[i].idx,
 						descs[i].offset);
 
 			swap_mac_addresses(pkt);
@@ -867,18 +867,18 @@ static void l2fwd(struct xdpsock *xsk)
 #endif
 		}
 
-		xsk->rx_npkts += rcvd;
+		pnsk->rx_npkts += rcvd;
 
-		ret = xq_enq(&xsk->tx, descs, rcvd);
+		ret = xq_enq(&pnsk->tx, descs, rcvd);
 		lassert(ret == 0);
-		xsk->outstanding_tx += rcvd;
+		pnsk->outstanding_tx += rcvd;
 	}
 }
 
 int main(int argc, char **argv)
 {
 	struct rlimit r = {RLIM_INFINITY, RLIM_INFINITY};
-	char xdp_filename[256];
+	char pn_filename[256];
 	int i, ret, key = 0;
 	pthread_t pt;
 
@@ -890,9 +890,9 @@ int main(int argc, char **argv)
 		exit(EXIT_FAILURE);
 	}
 
-	snprintf(xdp_filename, sizeof(xdp_filename), "%s_kern.o", argv[0]);
+	snprintf(pn_filename, sizeof(pn_filename), "%s_kern.o", argv[0]);
 
-	/*if (load_bpf_file(xdp_filename)) {
+	/*if (load_bpf_file(pn_filename)) {
 		fprintf(stderr, "ERROR: load_bpf_file %s\n", bpf_log_buf);
 		exit(EXIT_FAILURE);
 	}
@@ -903,8 +903,8 @@ int main(int argc, char **argv)
 		exit(EXIT_FAILURE);
 	}
 
-	if (bpf_set_link_xdp_fd(opt_ifindex, prog_fd[0], opt_xdp_flags) < 0) {
-		fprintf(stderr, "ERROR: link set xdp fd failed\n");
+	if (bpf_set_link_pn_fd(opt_ifindex, prog_fd[0], opt_pn_flags) < 0) {
+		fprintf(stderr, "ERROR: link set pn fd failed\n");
 		exit(EXIT_FAILURE);
 	}
 
@@ -916,17 +916,17 @@ int main(int argc, char **argv)
 	*/
 
 	/* Create sockets... */
-	xsks[num_socks++] = xsk_configure(NULL);
+	pnsks[num_socks++] = pnsk_configure(NULL);
 
 #if RR_LB
 	for (i = 0; i < MAX_SOCKS - 1; i++)
-		xsks[num_socks++] = xsk_configure(xsks[0]->umem);
+		pnsks[num_socks++] = pnsk_configure(pnsks[0]->umem);
 #endif
 
 	/* ...and insert them into the map. */
 	/*for (i = 0; i < num_socks; i++) {
 		key = i;
-		ret = bpf_map_update_elem(map_fd[1], &key, &xsks[i]->sfd, 0);
+		ret = bpf_map_update_elem(map_fd[1], &key, &pnsks[i]->sfd, 0);
 		if (ret) {
 			fprintf(stderr, "ERROR: bpf_map_update_elem %d\n", i);
 			exit(EXIT_FAILURE);
@@ -948,9 +948,9 @@ int main(int argc, char **argv)
 	if (opt_bench == BENCH_RXDROP)
 		rx_drop_all();
 	else if (opt_bench == BENCH_TXONLY)
-		tx_only(xsks[0]);
+		tx_only(pnsks[0]);
 	else
-		l2fwd(xsks[0]);
+		l2fwd(pnsks[0]);
 
 	return 0;
 }
diff --git a/security/selinux/hooks.c b/security/selinux/hooks.c
index 51ba4e4..df7ad8f 100644
--- a/security/selinux/hooks.c
+++ b/security/selinux/hooks.c
@@ -1322,8 +1322,8 @@ static inline u16 socket_type_to_security_class(int family, int type, int protoc
 		return SECCLASS_KEY_SOCKET;
 	case PF_APPLETALK:
 		return SECCLASS_APPLETALK_SOCKET;
-	case PF_XDP:
-		return SECCLASS_XDP_SOCKET;
+	case PF_PN:
+		return SECCLASS_PN_SOCKET;
 	}
 
 	return SECCLASS_SOCKET;
diff --git a/security/selinux/include/classmap.h b/security/selinux/include/classmap.h
index 50c1fce..2eeb6a0 100644
--- a/security/selinux/include/classmap.h
+++ b/security/selinux/include/classmap.h
@@ -165,7 +165,7 @@ struct security_class_mapping secclass_map[] = {
 	  { COMMON_CAP_PERMS, NULL } },
 	{ "cap2_userns",
 	  { COMMON_CAP2_PERMS, NULL } },
-	{ "xdp_socket",
+	{ "pn_socket",
 	  { COMMON_SOCK_PERMS, NULL } },
 	{ NULL }
   };
diff --git a/tools/include/uapi/linux/bpf.h b/tools/include/uapi/linux/bpf.h
index b2eb5e0..ab39177 100644
--- a/tools/include/uapi/linux/bpf.h
+++ b/tools/include/uapi/linux/bpf.h
@@ -85,7 +85,7 @@ enum bpf_map_type {
 	BPF_MAP_TYPE_PERCPU_ARRAY,
 	BPF_MAP_TYPE_STACK_TRACE,
 	BPF_MAP_TYPE_CGROUP_ARRAY,
-	BPF_MAP_TYPE_XSKMAP,
+	BPF_MAP_TYPE_PNSKMAP,
 };
 
 enum bpf_prog_type {
@@ -97,6 +97,7 @@ enum bpf_prog_type {
 	BPF_PROG_TYPE_TRACEPOINT,
 	BPF_PROG_TYPE_XDP,
 	BPF_PROG_TYPE_PERF_EVENT,
+	BPF_PROG_TYPE_PN,
 };
 
 #define BPF_PSEUDO_MAP_FD	1
@@ -502,22 +503,22 @@ struct bpf_tunnel_key {
 	__u32 tunnel_label;
 };
 
-/* User return codes for XDP prog type.
- * A valid XDP program must return one of these defined values. All other
+/* User return codes for PN prog type.
+ * A valid PN program must return one of these defined values. All other
  * return codes are reserved for future use. Unknown return codes will result
  * in packet drop.
  */
-enum xdp_action {
-	XDP_ABORTED = 0,
-	XDP_DROP,
-	XDP_PASS,
-	XDP_TX,
+enum pn_action {
+	PN_ABORTED = 0,
+	PN_DROP,
+	PN_PASS,
+	PN_TX,
 };
 
-/* user accessible metadata for XDP packet hook
+/* user accessible metadata for PN packet hook
  * new fields must be added to the end of this structure
  */
-struct xdp_md {
+struct pn_md {
 	__u32 data;
 	__u32 data_end;
 };
-- 
2.7.4

