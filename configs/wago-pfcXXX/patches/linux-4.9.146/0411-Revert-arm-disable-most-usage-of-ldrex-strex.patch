From 82a499134b4b151c22612c686f1a36550c8ee438 Mon Sep 17 00:00:00 2001
From: Heinrich Toews <heinrich.toews@wago.com>
Date: Wed, 11 Mar 2020 17:10:58 +0000
Subject: [PATCH] Revert "arm: disable most usage of ldrex/strex"

This reverts commit 206f7dc9219fb914fada3e6916aa1c82acc5894a.
---
 arch/arm/Kconfig                | 2 +-
 arch/arm/include/asm/atomic.h   | 2 +-
 arch/arm/include/asm/cmpxchg.h  | 8 ++++----
 arch/arm/include/asm/spinlock.h | 2 --
 arch/arm/lib/bitops.h           | 2 +-
 5 files changed, 7 insertions(+), 9 deletions(-)

diff --git a/arch/arm/Kconfig b/arch/arm/Kconfig
index 6403c3e..8c40f7d 100644
--- a/arch/arm/Kconfig
+++ b/arch/arm/Kconfig
@@ -19,7 +19,7 @@ config ARM
 	select EDAC_SUPPORT
 	select EDAC_ATOMIC_SCRUB
 	select GENERIC_ALLOCATOR
-	select GENERIC_ATOMIC64 if (CPU_V7M || CPU_V6 || CPU_32v6K || !AEABI)
+	select GENERIC_ATOMIC64 if (CPU_V7M || CPU_V6 || !CPU_32v6K || !AEABI)
 	select GENERIC_CLOCKEVENTS_BROADCAST if SMP
 	select GENERIC_EARLY_IOREMAP
 	select GENERIC_IDLE_POLL_SETUP
diff --git a/arch/arm/include/asm/atomic.h b/arch/arm/include/asm/atomic.h
index 8d8eb6b..66d0e21 100644
--- a/arch/arm/include/asm/atomic.h
+++ b/arch/arm/include/asm/atomic.h
@@ -30,7 +30,7 @@
 #define atomic_read(v)	READ_ONCE((v)->counter)
 #define atomic_set(v,i)	WRITE_ONCE(((v)->counter), (i))
 
-#if 0
+#if __LINUX_ARM_ARCH__ >= 6
 
 /*
  * ARMv6 UP and SMP safe atomic ops.  We use load exclusive and
diff --git a/arch/arm/include/asm/cmpxchg.h b/arch/arm/include/asm/cmpxchg.h
index 63ce6f2..97882f9 100644
--- a/arch/arm/include/asm/cmpxchg.h
+++ b/arch/arm/include/asm/cmpxchg.h
@@ -5,7 +5,7 @@
 #include <linux/prefetch.h>
 #include <asm/barrier.h>
 
-#if defined(CONFIG_CPU_SA1100) || defined(CONFIG_CPU_SA110) || 1
+#if defined(CONFIG_CPU_SA1100) || defined(CONFIG_CPU_SA110)
 /*
  * On the StrongARM, "swp" is terminally broken since it bypasses the
  * cache totally.  This means that the cache becomes inconsistent, and,
@@ -31,14 +31,14 @@ static inline unsigned long __xchg(unsigned long x, volatile void *ptr, int size
 #ifdef swp_is_buggy
 	unsigned long flags;
 #endif
-#if 0
+#if __LINUX_ARM_ARCH__ >= 6
 	unsigned int tmp;
 #endif
 
 	prefetchw((const void *)ptr);
 
 	switch (size) {
-#if 0
+#if __LINUX_ARM_ARCH__ >= 6
 #ifndef CONFIG_CPU_V6 /* MIN ARCH >= V6K */
 	case 1:
 		asm volatile("@	__xchg1\n"
@@ -120,7 +120,7 @@ static inline unsigned long __xchg(unsigned long x, volatile void *ptr, int size
 
 #include <asm-generic/cmpxchg-local.h>
 
-#if 1
+#if __LINUX_ARM_ARCH__ < 6
 /* min ARCH < ARMv6 */
 
 #ifdef CONFIG_SMP
diff --git a/arch/arm/include/asm/spinlock.h b/arch/arm/include/asm/spinlock.h
index d1684f9..4bec454 100644
--- a/arch/arm/include/asm/spinlock.h
+++ b/arch/arm/include/asm/spinlock.h
@@ -5,8 +5,6 @@
 #error SMP not supported on pre-ARMv6 CPUs
 #endif
 
-#error Can't use spinlocks
-
 #include <linux/prefetch.h>
 #include <asm/barrier.h>
 #include <asm/processor.h>
diff --git a/arch/arm/lib/bitops.h b/arch/arm/lib/bitops.h
index e017860..7d807cf 100644
--- a/arch/arm/lib/bitops.h
+++ b/arch/arm/lib/bitops.h
@@ -1,7 +1,7 @@
 #include <asm/assembler.h>
 #include <asm/unwind.h>
 
-#if 0
+#if __LINUX_ARM_ARCH__ >= 6
 	.macro	bitop, name, instr
 ENTRY(	\name		)
 UNWIND(	.fnstart	)
-- 
2.7.4

