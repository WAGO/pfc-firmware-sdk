From fe7d90c75b1c0f529f290e5e3ed6d326b1e799ae Mon Sep 17 00:00:00 2001
From: Oleg Karfich <oleg.karfich@wago.com>
Date: Wed, 19 Jun 2019 15:11:03 +0200
Subject: [PATCH] rmd: add support for rocket master device

Signed-off-by: Oleg Karfich <oleg.karfich@wago.com>
---
 drivers/char/Kconfig  |   9 +
 drivers/char/Makefile |   1 +
 drivers/char/rmd.c    | 866 ++++++++++++++++++++++++++++++++++++++++++++++++++
 3 files changed, 876 insertions(+)
 create mode 100644 drivers/char/rmd.c

diff --git a/drivers/char/Kconfig b/drivers/char/Kconfig
index 8453a49..832ce8f1 100644
--- a/drivers/char/Kconfig
+++ b/drivers/char/Kconfig
@@ -593,5 +593,14 @@ config TILE_SROM
 
 source "drivers/char/xillybus/Kconfig"
 
+config RMD
+	tristate "Rocket Master Device Driver"
+	depends on OF_ADDRESS && OF_IRQ && HAS_DMA
+	help
+	  The RMD driver interfaces with a peripheral designed on
+	  a programmable logic (FPGA).
+
+	  If unsure, say N.
+
 endmenu
 
diff --git a/drivers/char/Makefile b/drivers/char/Makefile
index 6e6c244..956368c 100644
--- a/drivers/char/Makefile
+++ b/drivers/char/Makefile
@@ -60,3 +60,4 @@ js-rtc-y = rtc.o
 obj-$(CONFIG_TILE_SROM)		+= tile-srom.o
 obj-$(CONFIG_XILLYBUS)		+= xillybus/
 obj-$(CONFIG_POWERNV_OP_PANEL)	+= powernv-op-panel.o
+obj-$(CONFIG_RMD)		+= rmd.o
diff --git a/drivers/char/rmd.c b/drivers/char/rmd.c
new file mode 100644
index 0000000..8d87027
--- /dev/null
+++ b/drivers/char/rmd.c
@@ -0,0 +1,866 @@
+// SPDX-License-Identifier: GPL-2.0+
+
+#include <linux/module.h>
+#include <linux/device.h>
+#include <linux/platform_device.h>
+#include <linux/of.h>
+#include <linux/of_irq.h>
+#include <linux/of_address.h>
+#include <linux/of_device.h>
+#include <linux/of_platform.h>
+#include <linux/err.h>
+#include <linux/dmaengine.h>
+#include <linux/cdev.h>
+#include <linux/interrupt.h>
+#include <linux/slab.h>
+#include <linux/dma-mapping.h>
+#include <linux/uaccess.h>
+#include <linux/rmd.h>
+
+#define RMD_NAME "rmd"
+#define RMD_MAX_DEVICES 42
+#define RMD_TRIPPLE_BUF_SIZE 3
+
+static struct rmd_base_t {
+	int		major;
+	struct class	*class;
+	struct cdev	*cdev;
+	struct mutex	idr_lock;
+	struct idr	idr;
+} rmd_base;
+
+enum rmd_idet_cs_field {
+	COMMIT,
+	DISCARD,
+	READ_FINISH,
+	DATA_VALID
+};
+
+enum rmd_channel_direction {
+	DEV_TO_MEM,
+	MEM_TO_DEV
+};
+
+struct rmd_dma_info {
+	struct dma_chan			*chan;
+	struct dma_async_tx_descriptor	*desc;
+	dma_cookie_t			cookie;
+	dma_addr_t			buf;
+};
+
+struct rmd_channel {
+	/* is empty when in softcopy mode */
+	struct rmd_dma_info		*dma_info;
+	enum rmd_channel_direction	direction;
+
+	/* holds the current user index that indicates
+	 * the memory offset in user mapped buffer.
+	 */
+	unsigned int			user_buf_idx;
+
+	/* buffer index currently used for data transmition
+	 * to/from the FIFO. Not used in case of dma based
+	 * transmission.
+	 */
+	unsigned int			kernel_buf_idx;
+
+	/* Contains the currently free buffer index optionally
+	 * or'd with RMD_FLAG_NEW_DATA.
+	 */
+	unsigned int			tripple_buf_ctrl_offset;
+
+	unsigned char			*buf;
+	dma_addr_t			buf_dma;
+	size_t				buf_size;
+};
+
+struct rmd {
+	struct device		*dev;
+	struct completion	completion;
+	int			minor;
+	unsigned int		irq;
+	unsigned long		is_open;
+
+	struct rmd_info		*info;
+
+	/* ioremaped and mapped rmd memory */
+	__iomem void		*base;
+	phys_addr_t		base_phys;
+
+	/* supported max fifo size in rmd per channel */
+	size_t			fifo_max_size;
+
+	/* holds the last measured cycle time when user waits for
+	 * next cycle
+	 */
+	ktime_t			last_call_time;
+};
+
+struct rmd_info {
+	struct rmd		*rmd;
+	struct rmd_config	config;
+	struct rmd_channel	rx_channel;
+	struct rmd_channel	tx_channel;
+	bool			is_configured;
+	bool			is_tx_enabled;
+
+	/* control structure for pdi and pdo tripple buffer */
+	u32			*tripple_buf_ctrl;
+	size_t			tripple_buf_ctrl_size;
+};
+
+static bool rmd_softcopy_mode(struct rmd_config *config)
+{
+	return config->flags & RMD_FLAG_SOFTCOPY;
+}
+
+static struct rmd *rmd_channel_to_rmd(struct rmd_channel *chan)
+{
+	return (chan->direction == MEM_TO_DEV ?
+		container_of(chan, struct rmd_info, tx_channel) :
+		container_of(chan, struct rmd_info, rx_channel))->rmd;
+}
+
+static u32 *rmd_channel_idet_cs_virt(struct rmd_channel *chan)
+{
+	struct rmd *rmd = rmd_channel_to_rmd(chan);
+
+	return chan->direction == MEM_TO_DEV ?
+		rmd->base + rmd->info->config.idet_pdo_cs_offset :
+		rmd->base + rmd->info->config.idet_pdi_cs_offset;
+}
+
+static int rmd_channel_fifo_offset(struct rmd_channel *chan)
+{
+	struct rmd *rmd = rmd_channel_to_rmd(chan);
+
+	return chan->direction == MEM_TO_DEV ?
+		rmd->info->config.idet_fifo_pdo_offset :
+		rmd->info->config.idet_fifo_pdi_offset;
+}
+
+static size_t rmd_channel_fifo_size(struct rmd_channel *chan)
+{
+	struct rmd *rmd = rmd_channel_to_rmd(chan);
+
+	return chan->direction == MEM_TO_DEV ?
+		rmd->info->config.idet_fifo_pdo_size :
+		rmd->info->config.idet_fifo_pdi_size;
+}
+
+static __iomem u32 *rmd_channel_fifo_virt(struct rmd_channel *chan)
+{
+	return (u32 *)(rmd_channel_to_rmd(chan)->base +
+						rmd_channel_fifo_offset(chan));
+}
+
+static int rmd_channel_buf_offs(struct rmd_channel *chan, int n)
+{
+	return n * rmd_channel_to_rmd(chan)->fifo_max_size;
+}
+
+static u32 *rmd_channel_buf_virt(struct rmd_channel *chan)
+{
+	return (u32 *)(chan->buf +
+			rmd_channel_buf_offs(chan, chan->kernel_buf_idx));
+}
+
+static u32 *rmd_channel_tripple_buf_virt(struct rmd_channel *chan)
+{
+	return rmd_channel_to_rmd(chan)->info->tripple_buf_ctrl +
+						chan->tripple_buf_ctrl_offset;
+}
+
+static u32 rmd_channel_get_bit_pos(struct rmd_channel *chan,
+						enum rmd_idet_cs_field field)
+{
+	struct rmd *rmd = rmd_channel_to_rmd(chan);
+	struct rmd_config *config = &rmd->info->config;
+	u8 bit_offset;
+
+	switch (field) {
+	case COMMIT:
+		bit_offset = config->tx_commit_bit_offset;
+		break;
+	case DISCARD:
+		bit_offset = config->tx_discard_bit_offset;
+		break;
+	case READ_FINISH:
+		bit_offset = config->rx_read_finish_bit_offset;
+		break;
+	case DATA_VALID:
+		bit_offset = config->rx_data_valid_bit_offset;
+		break;
+	}
+
+	return BIT(bit_offset);
+}
+
+static inline void rmd_xchg_free_rx_buffer(struct rmd_channel *chan)
+{
+	u32 *buf_ctrl = rmd_channel_tripple_buf_virt(chan);
+	u32 curr_write_idx = chan->kernel_buf_idx | RMD_FLAG_NEW_DATA;
+	u32 next_write_buf;
+
+	next_write_buf = rmd_atomic_xchg(buf_ctrl, curr_write_idx);
+	chan->kernel_buf_idx = next_write_buf & ~RMD_FLAG_NEW_DATA;
+
+	if (chan->kernel_buf_idx >= RMD_BUFFER_INDEX_SIZE) {
+		chan->kernel_buf_idx = 0;
+		WARN_ONCE(1, "buffer index overflow. data could be corrupt:"
+							"%s\n", __func__);
+	}
+}
+
+static inline void rmd_xchg_free_tx_buffer(struct rmd_channel *chan)
+{
+	int i;
+	u32 *buf_ctrl = rmd_channel_tripple_buf_virt(chan);
+	u32 expected_free_idx, next_read_idx;
+	u32 curr_read_idx = chan->kernel_buf_idx;
+
+	for (i = 0; i < RMD_BUFFER_INDEX_SIZE; i++) {
+
+		/* skip our current index */
+		if (i == curr_read_idx)
+			continue;
+
+		expected_free_idx = i | RMD_FLAG_NEW_DATA;
+
+		next_read_idx = rmd_atomic_cmpxchg(buf_ctrl, expected_free_idx,
+								curr_read_idx);
+
+		if (next_read_idx == expected_free_idx) {
+			chan->kernel_buf_idx = next_read_idx &
+							~RMD_FLAG_NEW_DATA;
+
+			if (chan->kernel_buf_idx >= RMD_BUFFER_INDEX_SIZE) {
+				chan->kernel_buf_idx = 0;
+				WARN_ONCE(1, "buffer index overflow. data could"
+						"be corrupt: %s\n", __func__);
+			}
+
+			break;
+		}
+	}
+}
+
+static void rmd_rx_softcopy(struct rmd_channel *rx)
+{
+	int i;
+	u32 ack, *data;
+	u32 *idet_cs = rmd_channel_idet_cs_virt(rx);
+	u32 *fifo = rmd_channel_fifo_virt(rx);
+	size_t s = rmd_channel_fifo_size(rx);
+
+	data = rmd_channel_buf_virt(rx);
+	for (i = 0; i < s / sizeof(*fifo); i++)
+		data[i] = ioread32(fifo);
+
+	iowrite32(rmd_channel_get_bit_pos(rx, READ_FINISH), idet_cs);
+
+	ack = ioread32(idet_cs) & rmd_channel_get_bit_pos(rx, DATA_VALID);
+	if (ack)
+		rmd_xchg_free_rx_buffer(rx);
+}
+
+static void rmd_tx_softcopy(struct rmd_channel *tx)
+{
+	int i;
+	u32 *fifo = rmd_channel_fifo_virt(tx);
+	size_t s = rmd_channel_fifo_size(tx);
+	u32 *data;
+
+	rmd_xchg_free_tx_buffer(tx);
+
+	data = rmd_channel_buf_virt(tx);
+	for (i = 0; i < s / sizeof(*fifo); i++)
+		iowrite32(data[i], fifo);
+
+	iowrite32(rmd_channel_get_bit_pos(tx, COMMIT),
+						rmd_channel_idet_cs_virt(tx));
+}
+
+static irqreturn_t rmd_isr(int irq, void *data)
+{
+	struct rmd *rmd = data;
+
+	if (!rmd->info || !rmd->info->is_configured)
+		return IRQ_HANDLED;
+
+	if (rmd_softcopy_mode(&rmd->info->config)) {
+		disable_irq_nosync(irq);
+		rmd_rx_softcopy(&rmd->info->rx_channel);
+
+		if (rmd->info->is_tx_enabled)
+			rmd_tx_softcopy(&rmd->info->tx_channel);
+
+		enable_irq(irq);
+	} else {
+		complete(&rmd->completion);
+		disable_irq_nosync(irq);
+	}
+
+	return IRQ_HANDLED;
+}
+
+static int rmd_mem_setup_channel(struct rmd *rmd, struct rmd_channel *chan,
+					enum rmd_channel_direction direction)
+{
+	chan->buf_size = RMD_TRIPPLE_BUF_SIZE * rmd->fifo_max_size;
+	chan->direction = direction;
+
+	chan->buf = kzalloc(chan->buf_size, GFP_USER);
+	if (!chan->buf)
+		return -ENOMEM;
+
+	chan->user_buf_idx = 0;
+	chan->kernel_buf_idx = 1;
+	chan->tripple_buf_ctrl_offset = direction == DEV_TO_MEM ? 0 :
+								sizeof(u32);
+	*rmd_channel_tripple_buf_virt(chan) = 2;
+
+	return 0;
+}
+
+static int rmd_mem_setup(struct rmd_info *info)
+{
+	if (rmd_mem_setup_channel(info->rmd, &info->rx_channel, DEV_TO_MEM))
+		return -EINVAL;
+
+	if (rmd_mem_setup_channel(info->rmd, &info->tx_channel, MEM_TO_DEV)) {
+		kfree(info->rx_channel.buf);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int rmd_dma_setup_channel(struct rmd *rmd, struct rmd_channel *chan,
+					enum rmd_channel_direction direction)
+{
+	int ret;
+	struct rmd_dma_info *dma_info;
+	char *chan_name;
+	enum dma_transfer_direction dma_dir;
+
+	dma_info = kzalloc(sizeof(*dma_info), GFP_KERNEL);
+	if (!dma_info)
+		return -ENOMEM;
+
+	chan->buf_size = RMD_TRIPPLE_BUF_SIZE * rmd->fifo_max_size;
+	chan->direction = direction;
+
+	chan->buf = dmam_alloc_coherent(rmd->dev, chan->buf_size,
+						&chan->buf_dma, GFP_USER);
+	if (!chan->buf) {
+		ret = -ENOMEM;
+		goto free_dma_info;
+	}
+	memset(chan->buf, 0, chan->buf_size);
+
+	if (chan->direction == DEV_TO_MEM) {
+		dma_dir = DMA_DEV_TO_MEM;
+		chan_name = "rx";
+	} else {
+		dma_dir = DMA_MEM_TO_DEV;
+		chan_name = "tx";
+	}
+
+	dma_info->chan = dma_request_chan(rmd->dev, chan_name);
+	if (IS_ERR(dma_info->chan)) {
+		ret = PTR_ERR(dma_info->chan);
+		goto free_chan_buf;
+	}
+
+	dma_info->buf = chan->buf_dma;
+//	dma_info->desc = dmaengine_prep_rmd(dma_info->chan, rmd->base_phys,
+//					&rmd->info->config, dma_dir,
+//					dma_info->buf, rmd->fifo_max_size, 0);
+
+	if (!dma_info->desc) {
+		ret = -EINVAL;
+		goto dma_release_channel;
+	}
+
+	/* get initial buffer index */
+//	ret = dmaengine_switch_rmd(dma_info->chan, -1);
+//	if (ret < 0)
+//		goto dma_release_channel;
+
+	chan->user_buf_idx = ret;
+	chan->dma_info = dma_info;
+
+	dma_info->cookie = dmaengine_submit(dma_info->desc);
+	dma_async_issue_pending(dma_info->chan);
+
+	return 0;
+
+dma_release_channel:
+	dma_release_channel(chan->dma_info->chan);
+free_chan_buf:
+	dmam_free_coherent(rmd->dev, chan->buf_size, chan->buf, chan->buf_dma);
+free_dma_info:
+	kfree(dma_info);
+
+	return ret;
+}
+
+static void rmd_dma_release_channel(struct rmd_channel *chan)
+{
+	struct rmd *rmd = rmd_channel_to_rmd(chan);
+
+	dmaengine_terminate_sync(chan->dma_info->chan);
+	dma_release_channel(chan->dma_info->chan);
+	dmam_free_coherent(rmd->dev, chan->buf_size, chan->buf, chan->buf_dma);
+	kfree(chan->dma_info);
+}
+
+static int rmd_dma_setup(struct rmd_info *info)
+{
+	int ret;
+	struct rmd *rmd = info->rmd;
+
+	ret = rmd_dma_setup_channel(rmd, &info->rx_channel, DEV_TO_MEM);
+	if (ret)
+		return ret;
+
+	ret = rmd_dma_setup_channel(rmd, &info->tx_channel, MEM_TO_DEV);
+	if (ret)
+		rmd_dma_release_channel(&info->rx_channel);
+
+	return ret;
+}
+
+static int rmd_open(struct inode *inode, struct file *filep)
+{
+	struct rmd *rmd;
+	struct rmd_info *info;
+
+	mutex_lock(&rmd_base.idr_lock);
+	rmd = idr_find(&rmd_base.idr, iminor(inode));
+	mutex_unlock(&rmd_base.idr_lock);
+
+	if (!rmd)
+		return -ENODEV;
+
+	/* rmd device is allowed to be opened by
+	 * only one process.
+	 */
+	if (test_and_set_bit(0, &rmd->is_open))
+		return -EBUSY;
+
+	info = kzalloc(sizeof(*info), GFP_KERNEL);
+	if (!info) {
+		clear_bit(0, &rmd->is_open);
+		return -ENOMEM;
+	}
+
+	info->tripple_buf_ctrl_size = PAGE_SIZE;
+	info->tripple_buf_ctrl = kzalloc(info->tripple_buf_ctrl_size, GFP_USER);
+	if (!info->tripple_buf_ctrl) {
+		kfree(info);
+		clear_bit(0, &rmd->is_open);
+		return -ENOMEM;
+	}
+
+	rmd->info = info;
+	info->rmd = rmd;
+
+	filep->private_data = info;
+
+	return 0;
+}
+
+static void rmd_cleanup(struct rmd_info *info)
+{
+	struct rmd *rmd = info->rmd;
+	struct rmd_channel *rx = &info->rx_channel;
+	struct rmd_channel *tx = &info->tx_channel;
+
+	if (!info->is_configured)
+		return;
+
+	info->is_configured = false;
+	info->is_tx_enabled = false;
+
+	if (rmd_softcopy_mode(&info->config)) {
+		disable_irq(rmd->irq);
+		kfree(rx->buf);
+		kfree(tx->buf);
+	} else {
+		rmd_dma_release_channel(rx);
+		rmd_dma_release_channel(tx);
+	}
+}
+
+static int rmd_close(struct inode *inode, struct file *filep)
+{
+	struct rmd_info *info = filep->private_data;
+	struct rmd *rmd = info->rmd;
+
+	rmd_cleanup(info);
+
+	kfree(info);
+	filep->private_data = NULL;
+	clear_bit(0, &rmd->is_open);
+
+	return 0;
+}
+
+static int rmd_wait_for_next_cycle(struct rmd *rmd, void __user *argp)
+{
+	int ret;
+	struct rmd_cycle cycle;
+	ktime_t call_time;
+	s64 delta_time_us;
+
+	call_time = ktime_get();
+	delta_time_us = ktime_us_delta(call_time, rmd->last_call_time);
+	rmd->last_call_time = call_time;
+
+	if (delta_time_us < 0)
+		delta_time_us = 0;
+
+	ret = copy_from_user(&cycle, argp, sizeof(cycle));
+	if (ret)
+		return ret;
+
+	cycle.cycle_time_us = delta_time_us;
+
+	reinit_completion(&rmd->completion);
+	enable_irq(rmd->irq);
+	ret = wait_for_completion_interruptible_timeout(&rmd->completion,
+					usecs_to_jiffies(cycle.timeout_us));
+
+	if (!ret)
+		return -ETIMEDOUT;
+
+	return copy_to_user(argp, &cycle, sizeof(cycle));
+}
+
+static int rmd_setup(struct rmd_info *info, void __user *argp)
+{
+	int ret;
+
+	rmd_cleanup(info);
+
+	if (copy_from_user(&info->config, argp, sizeof(info->config)))
+		return -EFAULT;
+
+	if (rmd_softcopy_mode(&info->config)) {
+		ret = rmd_mem_setup(info);
+		enable_irq(info->rmd->irq);
+	} else {
+		ret = rmd_dma_setup(info);
+	}
+
+	if (!ret)
+		info->is_configured = true;
+
+	return ret;
+}
+
+static long rmd_ioctl(struct file *filep, unsigned int cmd, unsigned long arg)
+{
+	struct rmd_info *info = filep->private_data;
+
+	switch (cmd) {
+	case RMD_SET_CONFIG:
+		return rmd_setup(info, (void __user *)arg);
+
+	case RMD_GET_CONFIG:
+		return copy_to_user((void __user *)arg, &info->config,
+							sizeof(info->config));
+
+	case RMD_GET_BUFFER_SIZE:
+		return info->rmd->fifo_max_size * RMD_TRIPPLE_BUF_SIZE;
+
+	/* TODO: DMA */
+	case RMD_SWITCH_READ_BUFFER:
+		return -EINVAL;
+
+	case RMD_GET_DRV_VERSION:
+		return RMD_DRV_VERSION;
+
+	case RMD_GET_READ_BUFFER_IDX:
+		return info->rx_channel.user_buf_idx;
+
+	case RMD_GET_WRITE_BUFFER_IDX:
+		return info->tx_channel.user_buf_idx;
+
+	/* TODO: DMA */
+	case RMD_SWITCH_WRITE_BUFFER:
+		return -EINVAL;
+
+	case RMD_WAIT_FOR_NEXT_CYCLE:
+		return rmd_wait_for_next_cycle(info->rmd, (void __user *)arg);
+
+	case RMD_ACTIVATE_TX_PATH:
+		info->is_tx_enabled = true;
+		return 0;
+
+	case RMD_GET_READ_TRIPPLE_BUF_CTRL_OFFSET:
+		return info->rx_channel.tripple_buf_ctrl_offset;
+
+	case RMD_GET_WRITE_TRIPPLE_BUF_CTRL_OFFSET:
+		return info->tx_channel.tripple_buf_ctrl_offset;
+
+	case RMD_GET_BUFFER_CHUNK_SIZE:
+		return info->rmd->fifo_max_size;
+
+	case RMD_GET_TRIPPLE_BUF_CTRL_SIZE:
+		return info->tripple_buf_ctrl_size;
+	}
+
+	return -EINVAL;
+};
+
+static int rmd_mmap(struct file *filep, struct vm_area_struct *vma)
+{
+	struct rmd_info *info = filep->private_data;
+	struct rmd *rmd = info->rmd;
+	struct rmd_channel *chan;
+
+	if (!info->is_configured)
+		return -EINVAL;
+
+	if (!(vma->vm_flags & VM_SHARED))
+		return -EINVAL;
+
+	if ((vma->vm_flags & VM_READ) && (vma->vm_flags & VM_WRITE))
+		return remap_pfn_range(vma, vma->vm_start,
+			virt_to_phys(info->tripple_buf_ctrl) >> PAGE_SHIFT,
+			info->tripple_buf_ctrl_size,
+			vma->vm_page_prot);
+
+	if (vma->vm_flags & VM_READ)
+		chan = &info->rx_channel;
+	else if (vma->vm_flags & VM_WRITE)
+		chan = &info->tx_channel;
+	else
+		return -EINVAL;
+
+	if (rmd_softcopy_mode(&info->config))
+		return remap_pfn_range(vma, vma->vm_start,
+					virt_to_phys(chan->buf) >> PAGE_SHIFT,
+					chan->buf_size,
+					vma->vm_page_prot);
+	else
+		return dma_mmap_coherent(rmd->dev, vma, chan->buf,
+					chan->buf_dma, chan->buf_size);
+}
+
+static int of_rmd(struct device *dev, struct rmd *rmd)
+{
+	int ret;
+	struct resource res;
+	struct device_node *np = dev->of_node;
+
+	ret = of_address_to_resource(np, 0, &res);
+	if (ret)
+		return ret;
+
+	rmd->base = devm_ioremap_nocache(dev, res.start, resource_size(&res));
+	if (IS_ERR(rmd->base))
+		return PTR_ERR(rmd->base);
+
+	rmd->base_phys = res.start;
+
+	ret = of_property_read_u32(np, "rmd,fifo-size", &rmd->fifo_max_size);
+	if (ret)
+		return ret;
+
+	rmd->irq = irq_of_parse_and_map(np, 0);
+	ret = devm_request_irq(dev, rmd->irq, rmd_isr, IRQF_SHARED |
+					IRQ_TYPE_EDGE_RISING, RMD_NAME, rmd);
+	if (ret)
+		return ret;
+
+	disable_irq(rmd->irq);
+
+	return 0;
+}
+
+static int rmd_get_minor(struct rmd *rmd)
+{
+	int minor;
+
+	mutex_lock(&rmd_base.idr_lock);
+
+	minor = idr_alloc(&rmd_base.idr, rmd, 0, RMD_MAX_DEVICES, GFP_KERNEL);
+	if (minor < 0) {
+		mutex_unlock(&rmd_base.idr_lock);
+		return minor;
+	}
+
+	rmd->minor = minor;
+
+	mutex_unlock(&rmd_base.idr_lock);
+
+	return 0;
+}
+
+static void rmd_free_minor(struct rmd *rmd)
+{
+	mutex_lock(&rmd_base.idr_lock);
+	idr_remove(&rmd_base.idr, rmd->minor);
+	mutex_unlock(&rmd_base.idr_lock);
+}
+
+static int rmd_probe(struct platform_device *pdev)
+{
+	int ret;
+	struct rmd *rmd;
+	struct device *parent = &pdev->dev;
+
+	rmd = devm_kzalloc(parent, sizeof(*rmd), GFP_KERNEL);
+	if (!rmd)
+		return -ENOMEM;
+
+	ret = of_rmd(parent, rmd);
+	if (ret) {
+		dev_err(parent, "oftree handling failed\n");
+		return ret;
+	}
+
+	init_completion(&rmd->completion);
+
+	ret = rmd_get_minor(rmd);
+	if (ret) {
+		dev_err(parent, "could not get minor number\n");
+		return ret;
+	}
+
+	rmd->dev = device_create(rmd_base.class, parent,
+				MKDEV(rmd_base.major, rmd->minor), rmd,
+				"rmd%d", rmd->minor);
+	if (IS_ERR(rmd->dev)) {
+		dev_err(parent, "could not create rmd device\n");
+		rmd_free_minor(rmd);
+		return PTR_ERR(rmd->dev);
+	}
+
+	platform_set_drvdata(pdev, rmd);
+
+	return 0;
+}
+
+static int rmd_remove(struct platform_device *pdev)
+{
+	struct rmd *rmd = platform_get_drvdata(pdev);
+
+	device_destroy(rmd_base.class, MKDEV(rmd_base.major, rmd->minor));
+	rmd_free_minor(rmd);
+
+	return 0;
+}
+
+static const struct file_operations rmd_fileops = {
+	.owner		= THIS_MODULE,
+	.open		= rmd_open,
+	.release	= rmd_close,
+	.unlocked_ioctl	= rmd_ioctl,
+	.mmap		= rmd_mmap,
+};
+
+static int rmd_major_init(void)
+{
+	int ret;
+	dev_t rmd_dev;
+
+	ret = alloc_chrdev_region(&rmd_dev, 0, RMD_MAX_DEVICES, RMD_NAME);
+	if (ret)
+		goto out;
+
+	rmd_base.cdev = cdev_alloc();
+	if (!rmd_base.cdev)
+		goto out_unregister;
+
+	cdev_init(rmd_base.cdev, &rmd_fileops);
+
+	ret = cdev_add(rmd_base.cdev, rmd_dev, RMD_MAX_DEVICES);
+	if (ret)
+		goto out_del;
+
+	rmd_base.major = MAJOR(rmd_dev);
+
+	return 0;
+
+out_del:
+	cdev_del(rmd_base.cdev);
+out_unregister:
+	unregister_chrdev_region(rmd_dev, RMD_MAX_DEVICES);
+out:
+	return ret;
+}
+
+static void rmd_major_exit(void)
+{
+	unregister_chrdev_region(MKDEV(rmd_base.major, 0), RMD_MAX_DEVICES);
+	cdev_del(rmd_base.cdev);
+}
+
+static const struct of_device_id rmd_of_match[] = {
+	{ .compatible = "wago,rmd", },
+	{ /* sentinel */ },
+};
+MODULE_DEVICE_TABLE(of, rmd_of_match);
+
+static struct platform_driver rmd_platform_driver = {
+	.probe = rmd_probe,
+	.remove = rmd_remove,
+	.driver = {
+		.name = RMD_NAME,
+		.of_match_table = rmd_of_match,
+	},
+};
+
+static int __init rmd_init(void)
+{
+	int ret;
+
+	mutex_init(&rmd_base.idr_lock);
+	idr_init(&rmd_base.idr);
+
+	ret = rmd_major_init();
+	if (ret)
+		goto out;
+
+	rmd_base.class = class_create(THIS_MODULE, RMD_NAME);
+	if (IS_ERR(rmd_base.class)) {
+		ret = PTR_ERR(rmd_base.class);
+		goto out_class;
+	}
+
+	ret = platform_driver_register(&rmd_platform_driver);
+	if (ret)
+		goto out_platform;
+
+	return 0;
+
+out_platform:
+	class_destroy(rmd_base.class);
+out_class:
+	rmd_major_exit();
+out:
+	idr_destroy(&rmd_base.idr);
+	mutex_destroy(&rmd_base.idr_lock);
+	return ret;
+}
+
+static void __exit rmd_exit(void)
+{
+	platform_driver_unregister(&rmd_platform_driver);
+	class_destroy(rmd_base.class);
+	rmd_major_exit();
+	idr_destroy(&rmd_base.idr);
+	mutex_destroy(&rmd_base.idr_lock);
+}
+
+module_init(rmd_init);
+module_exit(rmd_exit);
+
+MODULE_AUTHOR("Oleg Karfich <oleg.karfich@wago.com>");
+MODULE_DESCRIPTION("Driver for WAGO Rocket Master Device");
+MODULE_LICENSE("GPL");
-- 
2.7.4

