diff --git a/.gitmodules b/.gitmodules
new file mode 100644
index 000000000000..9e43bcfb3700
--- /dev/null
+++ b/.gitmodules
@@ -0,0 +1,3 @@
+[submodule "labgrid-wago"]
+	path = labgrid-wago
+	url = ssh://tfs-eng:22/tfs/ProductDevelopment/Linux-BSP/_git/labgrid-wago
diff --git a/Documentation/ABI/testing/configfs-usb-gadget-rndis b/Documentation/ABI/testing/configfs-usb-gadget-rndis
index 9416eda7fe93..52d295607678 100644
--- a/Documentation/ABI/testing/configfs-usb-gadget-rndis
+++ b/Documentation/ABI/testing/configfs-usb-gadget-rndis
@@ -16,4 +16,7 @@ Description:
 		class		USB interface class, default is 02 (hex)
 		subclass	USB interface subclass, default is 06 (hex)
 		protocol	USB interface protocol, default is 00 (hex)
+		use_ms_rndiscmp	Use the MS Windows rndiscmp.inf compatible
+				class 0xEF, subclass 0x04, protocol 0x01
+				instead of the default 0x02/0x06/0x00.
 		=========	=============================================
diff --git a/Documentation/RCU/Design/Expedited-Grace-Periods/Expedited-Grace-Periods.rst b/Documentation/RCU/Design/Expedited-Grace-Periods/Expedited-Grace-Periods.rst
index 72f0f6fbd53c..6f89cf1e567d 100644
--- a/Documentation/RCU/Design/Expedited-Grace-Periods/Expedited-Grace-Periods.rst
+++ b/Documentation/RCU/Design/Expedited-Grace-Periods/Expedited-Grace-Periods.rst
@@ -38,7 +38,7 @@ sections.
 RCU-preempt Expedited Grace Periods
 ===================================
 
-``CONFIG_PREEMPT=y`` kernels implement RCU-preempt.
+``CONFIG_PREEMPTION=y`` kernels implement RCU-preempt.
 The overall flow of the handling of a given CPU by an RCU-preempt
 expedited grace period is shown in the following diagram:
 
@@ -112,7 +112,7 @@ things.
 RCU-sched Expedited Grace Periods
 ---------------------------------
 
-``CONFIG_PREEMPT=n`` kernels implement RCU-sched. The overall flow of
+``CONFIG_PREEMPTION=n`` kernels implement RCU-sched. The overall flow of
 the handling of a given CPU by an RCU-sched expedited grace period is
 shown in the following diagram:
 
diff --git a/Documentation/RCU/Design/Requirements/Requirements.rst b/Documentation/RCU/Design/Requirements/Requirements.rst
index 1ae79a10a8de..17d38480ef5c 100644
--- a/Documentation/RCU/Design/Requirements/Requirements.rst
+++ b/Documentation/RCU/Design/Requirements/Requirements.rst
@@ -78,7 +78,7 @@ RCU treats a nested set as one big RCU read-side critical section.
 Production-quality implementations of ``rcu_read_lock()`` and
 ``rcu_read_unlock()`` are extremely lightweight, and in fact have
 exactly zero overhead in Linux kernels built for production use with
-``CONFIG_PREEMPT=n``.
+``CONFIG_PREEMPTION=n``.
 
 This guarantee allows ordering to be enforced with extremely low
 overhead to readers, for example:
@@ -1182,7 +1182,7 @@ and has become decreasingly so as memory sizes have expanded and memory
 costs have plummeted. However, as I learned from Matt Mackall's
 `bloatwatch <http://elinux.org/Linux_Tiny-FAQ>`__ efforts, memory
 footprint is critically important on single-CPU systems with
-non-preemptible (``CONFIG_PREEMPT=n``) kernels, and thus `tiny
+non-preemptible (``CONFIG_PREEMPTION=n``) kernels, and thus `tiny
 RCU <https://lkml.kernel.org/g/20090113221724.GA15307@linux.vnet.ibm.com>`__
 was born. Josh Triplett has since taken over the small-memory banner
 with his `Linux kernel tinification <https://tiny.wiki.kernel.org/>`__
@@ -1498,7 +1498,7 @@ limitations.
 
 Implementations of RCU for which ``rcu_read_lock()`` and
 ``rcu_read_unlock()`` generate no code, such as Linux-kernel RCU when
-``CONFIG_PREEMPT=n``, can be nested arbitrarily deeply. After all, there
+``CONFIG_PREEMPTION=n``, can be nested arbitrarily deeply. After all, there
 is no overhead. Except that if all these instances of
 ``rcu_read_lock()`` and ``rcu_read_unlock()`` are visible to the
 compiler, compilation will eventually fail due to exhausting memory,
@@ -1771,7 +1771,7 @@ implementation can be a no-op.
 
 However, once the scheduler has spawned its first kthread, this early
 boot trick fails for ``synchronize_rcu()`` (as well as for
-``synchronize_rcu_expedited()``) in ``CONFIG_PREEMPT=y`` kernels. The
+``synchronize_rcu_expedited()``) in ``CONFIG_PREEMPTION=y`` kernels. The
 reason is that an RCU read-side critical section might be preempted,
 which means that a subsequent ``synchronize_rcu()`` really does have to
 wait for something, as opposed to simply returning immediately.
@@ -2010,7 +2010,7 @@ the following:
        5 rcu_read_unlock();
        6 do_something_with(v, user_v);
 
-If the compiler did make this transformation in a ``CONFIG_PREEMPT=n`` kernel
+If the compiler did make this transformation in a ``CONFIG_PREEMPTION=n`` kernel
 build, and if ``get_user()`` did page fault, the result would be a quiescent
 state in the middle of an RCU read-side critical section.  This misplaced
 quiescent state could result in line 4 being a use-after-free access,
@@ -2289,10 +2289,10 @@ decides to throw at it.
 
 The Linux kernel is used for real-time workloads, especially in
 conjunction with the `-rt
-patchset <https://rt.wiki.kernel.org/index.php/Main_Page>`__. The
+patchset <https://wiki.linuxfoundation.org/realtime/>`__. The
 real-time-latency response requirements are such that the traditional
 approach of disabling preemption across RCU read-side critical sections
-is inappropriate. Kernels built with ``CONFIG_PREEMPT=y`` therefore use
+is inappropriate. Kernels built with ``CONFIG_PREEMPTION=y`` therefore use
 an RCU implementation that allows RCU read-side critical sections to be
 preempted. This requirement made its presence known after users made it
 clear that an earlier `real-time
@@ -2414,7 +2414,7 @@ includes ``rcu_read_lock_bh()``, ``rcu_read_unlock_bh()``,
 ``call_rcu_bh()``, ``rcu_barrier_bh()``, and
 ``rcu_read_lock_bh_held()``. However, the update-side APIs are now
 simple wrappers for other RCU flavors, namely RCU-sched in
-CONFIG_PREEMPT=n kernels and RCU-preempt otherwise.
+CONFIG_PREEMPTION=n kernels and RCU-preempt otherwise.
 
 Sched Flavor (Historical)
 ~~~~~~~~~~~~~~~~~~~~~~~~~
@@ -2432,11 +2432,11 @@ not have this property, given that any point in the code outside of an
 RCU read-side critical section can be a quiescent state. Therefore,
 *RCU-sched* was created, which follows “classic” RCU in that an
 RCU-sched grace period waits for pre-existing interrupt and NMI
-handlers. In kernels built with ``CONFIG_PREEMPT=n``, the RCU and
+handlers. In kernels built with ``CONFIG_PREEMPTION=n``, the RCU and
 RCU-sched APIs have identical implementations, while kernels built with
-``CONFIG_PREEMPT=y`` provide a separate implementation for each.
+``CONFIG_PREEMPTION=y`` provide a separate implementation for each.
 
-Note well that in ``CONFIG_PREEMPT=y`` kernels,
+Note well that in ``CONFIG_PREEMPTION=y`` kernels,
 ``rcu_read_lock_sched()`` and ``rcu_read_unlock_sched()`` disable and
 re-enable preemption, respectively. This means that if there was a
 preemption attempt during the RCU-sched read-side critical section,
@@ -2599,10 +2599,10 @@ userspace execution also delimit tasks-RCU read-side critical sections.
 
 The tasks-RCU API is quite compact, consisting only of
 ``call_rcu_tasks()``, ``synchronize_rcu_tasks()``, and
-``rcu_barrier_tasks()``. In ``CONFIG_PREEMPT=n`` kernels, trampolines
+``rcu_barrier_tasks()``. In ``CONFIG_PREEMPTION=n`` kernels, trampolines
 cannot be preempted, so these APIs map to ``call_rcu()``,
 ``synchronize_rcu()``, and ``rcu_barrier()``, respectively. In
-``CONFIG_PREEMPT=y`` kernels, trampolines can be preempted, and these
+``CONFIG_PREEMPTION=y`` kernels, trampolines can be preempted, and these
 three APIs are therefore implemented by separate functions that check
 for voluntary context switches.
 
diff --git a/Documentation/RCU/checklist.rst b/Documentation/RCU/checklist.rst
index 2efed9926c3f..7ed4956043bd 100644
--- a/Documentation/RCU/checklist.rst
+++ b/Documentation/RCU/checklist.rst
@@ -214,7 +214,7 @@ over a rather long period of time, but improvements are always welcome!
 	the rest of the system.
 
 7.	As of v4.20, a given kernel implements only one RCU flavor,
-	which is RCU-sched for PREEMPT=n and RCU-preempt for PREEMPT=y.
+	which is RCU-sched for PREEMPTION=n and RCU-preempt for PREEMPTION=y.
 	If the updater uses call_rcu() or synchronize_rcu(),
 	then the corresponding readers my use rcu_read_lock() and
 	rcu_read_unlock(), rcu_read_lock_bh() and rcu_read_unlock_bh(),
diff --git a/Documentation/RCU/rcubarrier.rst b/Documentation/RCU/rcubarrier.rst
index f64f4413a47c..3b4a24877496 100644
--- a/Documentation/RCU/rcubarrier.rst
+++ b/Documentation/RCU/rcubarrier.rst
@@ -9,7 +9,7 @@ RCU (read-copy update) is a synchronization mechanism that can be thought
 of as a replacement for read-writer locking (among other things), but with
 very low-overhead readers that are immune to deadlock, priority inversion,
 and unbounded latency. RCU read-side critical sections are delimited
-by rcu_read_lock() and rcu_read_unlock(), which, in non-CONFIG_PREEMPT
+by rcu_read_lock() and rcu_read_unlock(), which, in non-CONFIG_PREEMPTION
 kernels, generate no code whatsoever.
 
 This means that RCU writers are unaware of the presence of concurrent
@@ -329,10 +329,10 @@ Answer: This cannot happen. The reason is that on_each_cpu() has its last
 	to smp_call_function() and further to smp_call_function_on_cpu(),
 	causing this latter to spin until the cross-CPU invocation of
 	rcu_barrier_func() has completed. This by itself would prevent
-	a grace period from completing on non-CONFIG_PREEMPT kernels,
+	a grace period from completing on non-CONFIG_PREEMPTION kernels,
 	since each CPU must undergo a context switch (or other quiescent
 	state) before the grace period can complete. However, this is
-	of no use in CONFIG_PREEMPT kernels.
+	of no use in CONFIG_PREEMPTION kernels.
 
 	Therefore, on_each_cpu() disables preemption across its call
 	to smp_call_function() and also across the local call to
diff --git a/Documentation/RCU/stallwarn.rst b/Documentation/RCU/stallwarn.rst
index c9ab6af4d3be..e97d1b4876ef 100644
--- a/Documentation/RCU/stallwarn.rst
+++ b/Documentation/RCU/stallwarn.rst
@@ -25,7 +25,7 @@ warnings:
 
 -	A CPU looping with bottom halves disabled.
 
--	For !CONFIG_PREEMPT kernels, a CPU looping anywhere in the kernel
+-	For !CONFIG_PREEMPTION kernels, a CPU looping anywhere in the kernel
 	without invoking schedule().  If the looping in the kernel is
 	really expected and desirable behavior, you might need to add
 	some calls to cond_resched().
@@ -44,7 +44,7 @@ warnings:
 	result in the ``rcu_.*kthread starved for`` console-log message,
 	which will include additional debugging information.
 
--	A CPU-bound real-time task in a CONFIG_PREEMPT kernel, which might
+-	A CPU-bound real-time task in a CONFIG_PREEMPTION kernel, which might
 	happen to preempt a low-priority task in the middle of an RCU
 	read-side critical section.   This is especially damaging if
 	that low-priority task is not permitted to run on any other CPU,
diff --git a/Documentation/RCU/whatisRCU.rst b/Documentation/RCU/whatisRCU.rst
index fb3ff76c3e73..3b2b1479fd0f 100644
--- a/Documentation/RCU/whatisRCU.rst
+++ b/Documentation/RCU/whatisRCU.rst
@@ -684,7 +684,7 @@ Quick Quiz #1:
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 This section presents a "toy" RCU implementation that is based on
 "classic RCU".  It is also short on performance (but only for updates) and
-on features such as hotplug CPU and the ability to run in CONFIG_PREEMPT
+on features such as hotplug CPU and the ability to run in CONFIG_PREEMPTION
 kernels.  The definitions of rcu_dereference() and rcu_assign_pointer()
 are the same as those shown in the preceding section, so they are omitted.
 ::
@@ -740,7 +740,7 @@ Quick Quiz #2:
 Quick Quiz #3:
 		If it is illegal to block in an RCU read-side
 		critical section, what the heck do you do in
-		PREEMPT_RT, where normal spinlocks can block???
+		CONFIG_PREEMPT_RT, where normal spinlocks can block???
 
 :ref:`Answers to Quick Quiz <8_whatisRCU>`
 
@@ -1094,7 +1094,7 @@ Quick Quiz #2:
 		overhead is **negative**.
 
 Answer:
-		Imagine a single-CPU system with a non-CONFIG_PREEMPT
+		Imagine a single-CPU system with a non-CONFIG_PREEMPTION
 		kernel where a routing table is used by process-context
 		code, but can be updated by irq-context code (for example,
 		by an "ICMP REDIRECT" packet).	The usual way of handling
@@ -1121,10 +1121,10 @@ Answer:
 Quick Quiz #3:
 		If it is illegal to block in an RCU read-side
 		critical section, what the heck do you do in
-		PREEMPT_RT, where normal spinlocks can block???
+		CONFIG_PREEMPT_RT, where normal spinlocks can block???
 
 Answer:
-		Just as PREEMPT_RT permits preemption of spinlock
+		Just as CONFIG_PREEMPT_RT permits preemption of spinlock
 		critical sections, it permits preemption of RCU
 		read-side critical sections.  It also permits
 		spinlocks blocking while in RCU read-side critical
diff --git a/Documentation/admin-guide/kernel-parameters.txt b/Documentation/admin-guide/kernel-parameters.txt
index 26bfe7ae711b..9ee9d99cd811 100644
--- a/Documentation/admin-guide/kernel-parameters.txt
+++ b/Documentation/admin-guide/kernel-parameters.txt
@@ -4085,6 +4085,10 @@
 			value, meaning that RCU_SOFTIRQ is used by default.
 			Specify rcutree.use_softirq=0 to use rcuc kthreads.
 
+			But note that CONFIG_PREEMPT_RT=y kernels disable
+			this kernel boot parameter, forcibly setting it
+			to zero.
+
 	rcutree.rcu_fanout_exact= [KNL]
 			Disable autobalancing of the rcu_node combining
 			tree.  This is used by rcutorture, and might
@@ -4463,6 +4467,13 @@
 			only normal grace-period primitives.  No effect
 			on CONFIG_TINY_RCU kernels.
 
+			But note that CONFIG_PREEMPT_RT=y kernels enables
+			this kernel boot parameter, forcibly setting
+			it to the value one, that is, converting any
+			post-boot attempt at an expedited RCU grace
+			period to instead use normal non-expedited
+			grace-period processing.
+
 	rcupdate.rcu_task_ipi_delay= [KNL]
 			Set time in jiffies during which RCU tasks will
 			avoid sending IPIs, starting with the beginning
diff --git a/Documentation/devicetree/bindings/memory-controllers/omap-gpmc.txt b/Documentation/devicetree/bindings/memory-controllers/omap-gpmc.txt
index c1359f4d48d7..15da496d673a 100644
--- a/Documentation/devicetree/bindings/memory-controllers/omap-gpmc.txt
+++ b/Documentation/devicetree/bindings/memory-controllers/omap-gpmc.txt
@@ -22,6 +22,8 @@ Required properties:
 			can support.
  - gpmc,num-waitpins:	The maximum number of wait pins that controller can
 			support.
+ - gpmc,wait0-active-high: sets high polarity of input pin WAIT0
+ - gpmc,wait1-active-high: sets high polarity of input pin WAIT1
  - ranges:		Must be set up to reflect the memory layout with four
 			integer values for each chip-select line in use:
 
diff --git a/Documentation/driver-api/io-mapping.rst b/Documentation/driver-api/io-mapping.rst
index a966239f04e4..a7830c59481f 100644
--- a/Documentation/driver-api/io-mapping.rst
+++ b/Documentation/driver-api/io-mapping.rst
@@ -20,78 +20,64 @@ A mapping object is created during driver initialization using::
 mappable, while 'size' indicates how large a mapping region to
 enable. Both are in bytes.
 
-This _wc variant provides a mapping which may only be used
-with the io_mapping_map_atomic_wc or io_mapping_map_wc.
+This _wc variant provides a mapping which may only be used with
+io_mapping_map_local_wc() or io_mapping_map_wc().
 
-With this mapping object, individual pages can be mapped either atomically
-or not, depending on the necessary scheduling environment. Of course, atomic
-maps are more efficient::
+With this mapping object, individual pages can be mapped either temporarily
+or long term, depending on the requirements. Of course, temporary maps are
+more efficient.
 
-	void *io_mapping_map_atomic_wc(struct io_mapping *mapping,
-				       unsigned long offset)
+	void *io_mapping_map_local_wc(struct io_mapping *mapping,
+				      unsigned long offset)
 
-'offset' is the offset within the defined mapping region.
-Accessing addresses beyond the region specified in the
-creation function yields undefined results. Using an offset
-which is not page aligned yields an undefined result. The
-return value points to a single page in CPU address space.
+'offset' is the offset within the defined mapping region.  Accessing
+addresses beyond the region specified in the creation function yields
+undefined results. Using an offset which is not page aligned yields an
+undefined result. The return value points to a single page in CPU address
+space.
 
-This _wc variant returns a write-combining map to the
-page and may only be used with mappings created by
-io_mapping_create_wc
+This _wc variant returns a write-combining map to the page and may only be
+used with mappings created by io_mapping_create_wc()
 
-Note that the task may not sleep while holding this page
-mapped.
+Temporary mappings are only valid in the context of the caller. The mapping
+is not guaranteed to be globaly visible.
 
-::
+io_mapping_map_local_wc() has a side effect on X86 32bit as it disables
+migration to make the mapping code work. No caller can rely on this side
+effect.
 
-	void io_mapping_unmap_atomic(void *vaddr)
+Nested mappings need to be undone in reverse order because the mapping
+code uses a stack for keeping track of them::
 
-'vaddr' must be the value returned by the last
-io_mapping_map_atomic_wc call. This unmaps the specified
-page and allows the task to sleep once again.
+ addr1 = io_mapping_map_local_wc(map1, offset1);
+ addr2 = io_mapping_map_local_wc(map2, offset2);
+ ...
+ io_mapping_unmap_local(addr2);
+ io_mapping_unmap_local(addr1);
 
-If you need to sleep while holding the lock, you can use the non-atomic
-variant, although they may be significantly slower.
+The mappings are released with::
 
-::
+	void io_mapping_unmap_local(void *vaddr)
+
+'vaddr' must be the value returned by the last io_mapping_map_local_wc()
+call. This unmaps the specified mapping and undoes eventual side effects of
+the mapping function.
+
+If you need to sleep while holding a mapping, you can use the regular
+variant, although this may be significantly slower::
 
 	void *io_mapping_map_wc(struct io_mapping *mapping,
 				unsigned long offset)
 
-This works like io_mapping_map_atomic_wc except it allows
-the task to sleep while holding the page mapped.
+This works like io_mapping_map_local_wc() except it has no side effects and
+the pointer is globaly visible.
 
-
-::
+The mappings are released with::
 
 	void io_mapping_unmap(void *vaddr)
 
-This works like io_mapping_unmap_atomic, except it is used
-for pages mapped with io_mapping_map_wc.
+Use for pages mapped with io_mapping_map_wc().
 
 At driver close time, the io_mapping object must be freed::
 
 	void io_mapping_free(struct io_mapping *mapping)
-
-Current Implementation
-======================
-
-The initial implementation of these functions uses existing mapping
-mechanisms and so provides only an abstraction layer and no new
-functionality.
-
-On 64-bit processors, io_mapping_create_wc calls ioremap_wc for the whole
-range, creating a permanent kernel-visible mapping to the resource. The
-map_atomic and map functions add the requested offset to the base of the
-virtual address returned by ioremap_wc.
-
-On 32-bit processors with HIGHMEM defined, io_mapping_map_atomic_wc uses
-kmap_atomic_pfn to map the specified page in an atomic fashion;
-kmap_atomic_pfn isn't really supposed to be used with device pages, but it
-provides an efficient mapping for this usage.
-
-On 32-bit processors without HIGHMEM defined, io_mapping_map_atomic_wc and
-io_mapping_map_wc both use ioremap_wc, a terribly inefficient function which
-performs an IPI to inform all processors about the new mapping. This results
-in a significant performance penalty.
diff --git a/arch/Kconfig b/arch/Kconfig
index 69fe7133c765..62fb1035e662 100644
--- a/arch/Kconfig
+++ b/arch/Kconfig
@@ -37,6 +37,7 @@ config OPROFILE
 	tristate "OProfile system profiling"
 	depends on PROFILING
 	depends on HAVE_OPROFILE
+	depends on !PREEMPT_RT
 	select RING_BUFFER
 	select RING_BUFFER_ALLOW_SWAP
 	help
@@ -643,6 +644,12 @@ config HAVE_TIF_NOHZ
 config HAVE_VIRT_CPU_ACCOUNTING
 	bool
 
+config HAVE_VIRT_CPU_ACCOUNTING_IDLE
+	bool
+	help
+	  Architecture has its own way to account idle CPU time and therefore
+	  doesn't implement vtime_account_idle().
+
 config ARCH_HAS_SCALED_CPUTIME
 	bool
 
@@ -657,7 +664,6 @@ config HAVE_VIRT_CPU_ACCOUNTING_GEN
 	  some 32-bit arches may require multiple accesses, so proper
 	  locking is needed to protect against concurrent accesses.
 
-
 config HAVE_IRQ_TIME_ACCOUNTING
 	bool
 	help
diff --git a/arch/alpha/include/asm/kmap_types.h b/arch/alpha/include/asm/kmap_types.h
deleted file mode 100644
index 651714b45729..000000000000
--- a/arch/alpha/include/asm/kmap_types.h
+++ /dev/null
@@ -1,15 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0 */
-#ifndef _ASM_KMAP_TYPES_H
-#define _ASM_KMAP_TYPES_H
-
-/* Dummy header just to define km_type. */
-
-#ifdef CONFIG_DEBUG_HIGHMEM
-#define  __WITH_KM_FENCE
-#endif
-
-#include <asm-generic/kmap_types.h>
-
-#undef __WITH_KM_FENCE
-
-#endif
diff --git a/arch/alpha/include/asm/spinlock_types.h b/arch/alpha/include/asm/spinlock_types.h
index 1d5716bc060b..6883bc952d22 100644
--- a/arch/alpha/include/asm/spinlock_types.h
+++ b/arch/alpha/include/asm/spinlock_types.h
@@ -2,10 +2,6 @@
 #ifndef _ALPHA_SPINLOCK_TYPES_H
 #define _ALPHA_SPINLOCK_TYPES_H
 
-#ifndef __LINUX_SPINLOCK_TYPES_H
-# error "please don't include this file directly"
-#endif
-
 typedef struct {
 	volatile unsigned int lock;
 } arch_spinlock_t;
diff --git a/arch/arc/Kconfig b/arch/arc/Kconfig
index 0a89cc9def65..d8804001d550 100644
--- a/arch/arc/Kconfig
+++ b/arch/arc/Kconfig
@@ -507,6 +507,7 @@ config LINUX_RAM_BASE
 config HIGHMEM
 	bool "High Memory Support"
 	select ARCH_DISCONTIGMEM_ENABLE
+	select KMAP_LOCAL
 	help
 	  With ARC 2G:2G address split, only upper 2G is directly addressable by
 	  kernel. Enable this to potentially allow access to rest of 2G and PAE
diff --git a/arch/arc/include/asm/highmem.h b/arch/arc/include/asm/highmem.h
index 6e5eafb3afdd..a6b8e2c352c4 100644
--- a/arch/arc/include/asm/highmem.h
+++ b/arch/arc/include/asm/highmem.h
@@ -9,17 +9,29 @@
 #ifdef CONFIG_HIGHMEM
 
 #include <uapi/asm/page.h>
-#include <asm/kmap_types.h>
+#include <asm/kmap_size.h>
+
+#define FIXMAP_SIZE		PGDIR_SIZE
+#define PKMAP_SIZE		PGDIR_SIZE
 
 /* start after vmalloc area */
 #define FIXMAP_BASE		(PAGE_OFFSET - FIXMAP_SIZE - PKMAP_SIZE)
-#define FIXMAP_SIZE		PGDIR_SIZE	/* only 1 PGD worth */
-#define KM_TYPE_NR		((FIXMAP_SIZE >> PAGE_SHIFT)/NR_CPUS)
-#define FIXMAP_ADDR(nr)		(FIXMAP_BASE + ((nr) << PAGE_SHIFT))
+
+#define FIX_KMAP_SLOTS		(KM_MAX_IDX * NR_CPUS)
+#define FIX_KMAP_BEGIN		(0UL)
+#define FIX_KMAP_END		((FIX_KMAP_BEGIN + FIX_KMAP_SLOTS) - 1)
+
+#define FIXADDR_TOP		(FIXMAP_BASE + (FIX_KMAP_END << PAGE_SHIFT))
+
+/*
+ * This should be converted to the asm-generic version, but of course this
+ * is needlessly different from all other architectures. Sigh - tglx
+ */
+#define __fix_to_virt(x)	(FIXADDR_TOP - ((x) << PAGE_SHIFT))
+#define __virt_to_fix(x)	(((FIXADDR_TOP - ((x) & PAGE_MASK))) >> PAGE_SHIFT)
 
 /* start after fixmap area */
 #define PKMAP_BASE		(FIXMAP_BASE + FIXMAP_SIZE)
-#define PKMAP_SIZE		PGDIR_SIZE
 #define LAST_PKMAP		(PKMAP_SIZE >> PAGE_SHIFT)
 #define LAST_PKMAP_MASK		(LAST_PKMAP - 1)
 #define PKMAP_ADDR(nr)		(PKMAP_BASE + ((nr) << PAGE_SHIFT))
@@ -29,11 +41,13 @@
 
 extern void kmap_init(void);
 
+#define arch_kmap_local_post_unmap(vaddr)			\
+	local_flush_tlb_kernel_range(vaddr, vaddr + PAGE_SIZE)
+
 static inline void flush_cache_kmaps(void)
 {
 	flush_cache_all();
 }
-
 #endif
 
 #endif
diff --git a/arch/arc/include/asm/kmap_types.h b/arch/arc/include/asm/kmap_types.h
deleted file mode 100644
index fecf7851ec32..000000000000
--- a/arch/arc/include/asm/kmap_types.h
+++ /dev/null
@@ -1,14 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0-only */
-/*
- * Copyright (C) 2015 Synopsys, Inc. (www.synopsys.com)
- */
-
-#ifndef _ASM_KMAP_TYPES_H
-#define _ASM_KMAP_TYPES_H
-
-/*
- * We primarily need to define KM_TYPE_NR here but that in turn
- * is a function of PGDIR_SIZE etc.
- * To avoid circular deps issue, put everything in asm/highmem.h
- */
-#endif
diff --git a/arch/arc/mm/highmem.c b/arch/arc/mm/highmem.c
index 1b9f473c6369..c79912a6b196 100644
--- a/arch/arc/mm/highmem.c
+++ b/arch/arc/mm/highmem.c
@@ -36,9 +36,8 @@
  *   This means each only has 1 PGDIR_SIZE worth of kvaddr mappings, which means
  *   2M of kvaddr space for typical config (8K page and 11:8:13 traversal split)
  *
- * - fixmap anyhow needs a limited number of mappings. So 2M kvaddr == 256 PTE
- *   slots across NR_CPUS would be more than sufficient (generic code defines
- *   KM_TYPE_NR as 20).
+ * - The fixed KMAP slots for kmap_local/atomic() require KM_MAX_IDX slots per
+ *   CPU. So the number of CPUs sharing a single PTE page is limited.
  *
  * - pkmap being preemptible, in theory could do with more than 256 concurrent
  *   mappings. However, generic pkmap code: map_new_virtual(), doesn't traverse
@@ -47,48 +46,6 @@
  */
 
 extern pte_t * pkmap_page_table;
-static pte_t * fixmap_page_table;
-
-void *kmap_atomic_high_prot(struct page *page, pgprot_t prot)
-{
-	int idx, cpu_idx;
-	unsigned long vaddr;
-
-	cpu_idx = kmap_atomic_idx_push();
-	idx = cpu_idx + KM_TYPE_NR * smp_processor_id();
-	vaddr = FIXMAP_ADDR(idx);
-
-	set_pte_at(&init_mm, vaddr, fixmap_page_table + idx,
-		   mk_pte(page, prot));
-
-	return (void *)vaddr;
-}
-EXPORT_SYMBOL(kmap_atomic_high_prot);
-
-void kunmap_atomic_high(void *kv)
-{
-	unsigned long kvaddr = (unsigned long)kv;
-
-	if (kvaddr >= FIXMAP_BASE && kvaddr < (FIXMAP_BASE + FIXMAP_SIZE)) {
-
-		/*
-		 * Because preemption is disabled, this vaddr can be associated
-		 * with the current allocated index.
-		 * But in case of multiple live kmap_atomic(), it still relies on
-		 * callers to unmap in right order.
-		 */
-		int cpu_idx = kmap_atomic_idx();
-		int idx = cpu_idx + KM_TYPE_NR * smp_processor_id();
-
-		WARN_ON(kvaddr != FIXMAP_ADDR(idx));
-
-		pte_clear(&init_mm, kvaddr, fixmap_page_table + idx);
-		local_flush_tlb_kernel_range(kvaddr, kvaddr + PAGE_SIZE);
-
-		kmap_atomic_idx_pop();
-	}
-}
-EXPORT_SYMBOL(kunmap_atomic_high);
 
 static noinline pte_t * __init alloc_kmap_pgtable(unsigned long kvaddr)
 {
@@ -108,10 +65,9 @@ void __init kmap_init(void)
 {
 	/* Due to recursive include hell, we can't do this in processor.h */
 	BUILD_BUG_ON(PAGE_OFFSET < (VMALLOC_END + FIXMAP_SIZE + PKMAP_SIZE));
+	BUILD_BUG_ON(LAST_PKMAP > PTRS_PER_PTE);
+	BUILD_BUG_ON(FIX_KMAP_SLOTS > PTRS_PER_PTE);
 
-	BUILD_BUG_ON(KM_TYPE_NR > PTRS_PER_PTE);
 	pkmap_page_table = alloc_kmap_pgtable(PKMAP_BASE);
-
-	BUILD_BUG_ON(LAST_PKMAP > PTRS_PER_PTE);
-	fixmap_page_table = alloc_kmap_pgtable(FIXMAP_BASE);
+	alloc_kmap_pgtable(FIXMAP_BASE);
 }
diff --git a/arch/arm/Kconfig b/arch/arm/Kconfig
index 002e0cf025f5..e3a7ea0063cc 100644
--- a/arch/arm/Kconfig
+++ b/arch/arm/Kconfig
@@ -31,6 +31,7 @@ config ARM
 	select ARCH_OPTIONAL_KERNEL_RWX if ARCH_HAS_STRICT_KERNEL_RWX
 	select ARCH_OPTIONAL_KERNEL_RWX_DEFAULT if CPU_V7
 	select ARCH_SUPPORTS_ATOMIC_RMW
+	select ARCH_SUPPORTS_RT if HAVE_POSIX_CPU_TIMERS_TASK_WORK
 	select ARCH_USE_BUILTIN_BSWAP
 	select ARCH_USE_CMPXCHG_LOCKREF
 	select ARCH_WANT_DEFAULT_TOPDOWN_MMAP_LAYOUT if MMU
@@ -48,7 +49,7 @@ config ARM
 	select EDAC_ATOMIC_SCRUB
 	select GENERIC_ALLOCATOR
 	select GENERIC_ARCH_TOPOLOGY if ARM_CPU_TOPOLOGY
-	select GENERIC_ATOMIC64 if CPU_V7M || CPU_V6 || !CPU_32v6K || !AEABI
+	select GENERIC_ATOMIC64 if CPU_V7M || CPU_V6 || !CPU_32v6K || !AEABI || ARCH_OMAP3
 	select GENERIC_CLOCKEVENTS_BROADCAST if SMP
 	select GENERIC_IRQ_IPI if SMP
 	select GENERIC_CPU_AUTOPROBE
@@ -66,7 +67,7 @@ config ARM
 	select HARDIRQS_SW_RESEND
 	select HAVE_ARCH_AUDITSYSCALL if AEABI && !OABI_COMPAT
 	select HAVE_ARCH_BITREVERSE if (CPU_32v7M || CPU_32v7) && !CPU_32v6
-	select HAVE_ARCH_JUMP_LABEL if !XIP_KERNEL && !CPU_ENDIAN_BE32 && MMU
+	select HAVE_ARCH_JUMP_LABEL if !XIP_KERNEL && !CPU_ENDIAN_BE32 && MMU && !PREEMPT_RT
 	select HAVE_ARCH_KGDB if !CPU_ENDIAN_BE32 && MMU
 	select HAVE_ARCH_MMAP_RND_BITS if MMU
 	select HAVE_ARCH_SECCOMP
@@ -105,6 +106,7 @@ config ARM
 	select HAVE_PERF_EVENTS
 	select HAVE_PERF_REGS
 	select HAVE_PERF_USER_STACK_DUMP
+	select HAVE_PREEMPT_LAZY
 	select MMU_GATHER_RCU_TABLE_FREE if SMP && ARM_LPAE
 	select HAVE_REGS_AND_STACK_ACCESS_API
 	select HAVE_RSEQ
@@ -120,6 +122,7 @@ config ARM
 	select OLD_SIGSUSPEND3
 	select PCI_SYSCALL if PCI
 	select PERF_USE_VMALLOC
+	select HAVE_POSIX_CPU_TIMERS_TASK_WORK if !KVM
 	select RTC_LIB
 	select SET_FS
 	select SYS_SUPPORTS_APM_EMULATION
@@ -1499,6 +1502,7 @@ config HAVE_ARCH_PFN_VALID
 config HIGHMEM
 	bool "High Memory Support"
 	depends on MMU
+	select KMAP_LOCAL
 	help
 	  The address space of ARM processors is only 4 Gigabytes large
 	  and it has to accommodate user address space, kernel address
diff --git a/arch/arm/boot/dts/Makefile b/arch/arm/boot/dts/Makefile
index ce66ffd5a1bb..086c2c6692de 100644
--- a/arch/arm/boot/dts/Makefile
+++ b/arch/arm/boot/dts/Makefile
@@ -576,8 +576,14 @@ dtb-$(CONFIG_SOC_IMX6Q) += \
 	imx6q-var-dt6customboard.dtb \
 	imx6q-wandboard.dtb \
 	imx6q-wandboard-revb1.dtb \
-	imx6q-wandboard-revd1.dtb \
 	imx6q-zii-rdu2.dtb \
+	imx6q-vtpctp-0004.dtb \
+	imx6q-vtpctp-0005.dtb \
+	imx6q-vtpctp-0007.dtb \
+	imx6q-vtpctp-0010.dtb \
+	imx6q-vtpctp-0015.dtb \
+	imx6q-vtpctp-0021.dtb \
+	imx6q-ec-752_8303.dtb \
 	imx6qp-nitrogen6_max.dtb \
 	imx6qp-nitrogen6_som2.dtb \
 	imx6qp-phytec-mira-rdk-nand.dtb \
@@ -727,6 +733,11 @@ dtb-$(CONFIG_ARCH_OMAP3) += \
 	am3517-craneboard.dtb \
 	am3517-evm.dtb \
 	am3517_mt_ventoux.dtb \
+	am3505-pfc-750_8202.dtb \
+	am3505-pfc-750_8203.dtb \
+	am3505-pfc-750_8204.dtb \
+	am3505-pfc-750_8206.dtb \
+	am3505-pfc-750_8207.dtb \
 	logicpd-torpedo-37xx-devkit.dtb \
 	logicpd-som-lv-37xx-devkit.dtb \
 	omap3430-sdp.dtb \
@@ -821,6 +832,19 @@ dtb-$(CONFIG_SOC_AM33XX) += \
 	am335x-sbc-t335.dtb \
 	am335x-sl50.dtb \
 	am335x-wega-rdk.dtb \
+	am335x-pfc-750_8102.dtb \
+	am335x-pfc-750_8100_8101.dtb \
+	am335x-pfc-750_8208.dtb \
+	am335x-pfc-750_8210.dtb \
+	am335x-pfc-750_8211.dtb \
+	am335x-pfc-750_8212.dtb \
+	am335x-pfc-750_8213.dtb \
+	am335x-pfc-750_8214.dtb \
+	am335x-pfc-750_8215.dtb \
+	am335x-pfc-750_8216.dtb \
+	am335x-pfc-750_8217.dtb \
+	am335x-pfc-768_3301.dtb \
+	am335x-rmcb.dtb \
 	am335x-osd3358-sm-red.dtb
 dtb-$(CONFIG_ARCH_OMAP4) += \
 	omap4-droid-bionic-xt875.dtb \
@@ -1070,7 +1094,8 @@ dtb-$(CONFIG_ARCH_STM32) += \
 	stm32mp157c-ed1.dtb \
 	stm32mp157c-ev1.dtb \
 	stm32mp157c-lxa-mc1.dtb \
-	stm32mp157c-odyssey.dtb
+	stm32mp157c-odyssey.dtb \
+	stm32mp151-cc100.dtb
 dtb-$(CONFIG_MACH_SUN4I) += \
 	sun4i-a10-a1000.dtb \
 	sun4i-a10-ba10-tvbox.dtb \
diff --git a/arch/arm/boot/dts/am335x-pfc-750_8100_8101.dts b/arch/arm/boot/dts/am335x-pfc-750_8100_8101.dts
new file mode 100644
index 000000000000..45f28a38498f
--- /dev/null
+++ b/arch/arm/boot/dts/am335x-pfc-750_8100_8101.dts
@@ -0,0 +1,114 @@
+/*
+ * Copyright (C) 2019 WAGO Kontakttechnik GmbH & Co. KG - https://www.wago.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+/dts-v1/;
+
+#include "am335x-pfc-750_810x.dtsi"
+#include "am335x-pfc-750_8xxx-ksz8863.dtsi"
+#include "am335x-pfc-750_810x-dip.dtsi"
+#include "wago-devconf.dtsi"
+
+/ {
+	model = "WAGO PFC100 750-8100/8101";
+	compatible = "wago,am335x-pfc-750_810x-0028", "wago,am335x-pfc", "ti,am33xx";
+};
+
+&am33xx_pinmux {
+	ksz8863_switch_pins: pinmux_ksz8863_switch_pins {
+		pinctrl-single,pins = <
+			AM33XX_MII1_RXD0_GPIO2_21(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_GPMC_BEN1_GPIO1_28(PIN_INPUT_PULLUP)
+		>;
+	};
+};
+
+&aes {
+	status = "okay";
+};
+
+&bitbang_mdio0 {
+	status = "okay";
+};
+
+&cppi41dma {
+	status = "okay";
+};
+
+&elm {
+	status = "okay";
+};
+
+&ecap0 {
+	status = "okay";
+};
+
+&epwmss0 {
+	status = "okay";
+};
+
+&gpmc {
+	status = "okay";
+};
+
+&i2c0 {
+	status = "okay";
+};
+
+&kbus0 {
+	status = "okay";
+};
+
+&ksz8863_switch {
+	pinctrl-names = "default";
+	pinctrl-0 = <&ksz8863_switch_pins>;
+
+	ksz,reset-gpio = <&gpio2 21 GPIO_ACTIVE_LOW>;
+
+	status = "okay";
+
+	ports {
+		port@0 {
+			reg = <1>;
+		};
+
+		port@1 {
+			reg = <2>;
+		};
+	};
+};
+
+&mac {
+	status = "okay";
+};
+
+&mmc1 {
+	status = "okay";
+};
+
+&oms {
+	status = "okay";
+};
+
+&sham {
+	status = "okay";
+};
+
+&spi0 {
+	status = "okay";
+};
+
+&swcfg_ksz8863 {
+	status = "okay";
+};
+
+&uart4 {
+	status = "okay";
+};
+
+&watchdog {
+	status = "okay";
+};
diff --git a/arch/arm/boot/dts/am335x-pfc-750_8102.dts b/arch/arm/boot/dts/am335x-pfc-750_8102.dts
new file mode 100644
index 000000000000..a6329abb1f1f
--- /dev/null
+++ b/arch/arm/boot/dts/am335x-pfc-750_8102.dts
@@ -0,0 +1,120 @@
+/*
+ * Copyright (C) 2019 WAGO Kontakttechnik GmbH & Co. KG - https://www.wago.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+/dts-v1/;
+
+#include "am335x-pfc-750_810x.dtsi"
+#include "am335x-pfc-750_8xxx-ksz8863.dtsi"
+#include "am335x-pfc-750_8xxx-uart1.dtsi"
+#include "am335x-pfc-750_8xxx-uart4.dtsi"
+#include "wago-devconf.dtsi"
+
+/ {
+	model = "WAGO PFC100 750-8102";
+	compatible = "wago,am335x-pfc-750_810x-000c", "wago,am335x-pfc", "ti,am33xx";
+};
+
+&am33xx_pinmux {
+	ksz8863_switch_pins: pinmux_ksz8863_switch_pins {
+		pinctrl-single,pins = <
+			AM33XX_MII1_RXD0_GPIO2_21(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_GPMC_BEN1_GPIO1_28(PIN_INPUT_PULLUP)
+		>;
+	};
+};
+
+&aes {
+	status = "okay";
+};
+
+&bitbang_mdio0 {
+	status = "okay";
+};
+
+&cppi41dma {
+	status = "okay";
+};
+
+&ecap0 {
+	status = "okay";
+};
+
+&elm {
+	status = "okay";
+};
+
+&epwmss0 {
+	status = "okay";
+};
+
+&gpmc {
+	status = "okay";
+};
+
+&i2c0 {
+	status = "okay";
+};
+
+&kbus0 {
+	status = "okay";
+};
+
+&ksz8863_switch {
+	pinctrl-names = "default";
+	pinctrl-0 = <&ksz8863_switch_pins>;
+
+	ksz,reset-gpio = <&gpio2 21 GPIO_ACTIVE_LOW>;
+
+	status = "okay";
+
+	ports {
+		port@0 {
+			reg = <1>;
+		};
+
+		port@1 {
+			reg = <2>;
+		};
+	};
+
+};
+
+&mac {
+	status = "okay";
+};
+
+&mmc1 {
+	status = "okay";
+};
+
+&oms {
+	status = "okay";
+};
+
+&sham {
+	status = "okay";
+};
+
+&spi0 {
+	status = "okay";
+};
+
+&swcfg_ksz8863 {
+	status = "okay";
+};
+
+&uart1 {
+	status = "okay";
+};
+
+&uart4 {
+	status = "okay";
+};
+
+&watchdog {
+	status = "okay";
+};
diff --git a/arch/arm/boot/dts/am335x-pfc-750_810x-dip.dtsi b/arch/arm/boot/dts/am335x-pfc-750_810x-dip.dtsi
new file mode 100644
index 000000000000..667a1519962d
--- /dev/null
+++ b/arch/arm/boot/dts/am335x-pfc-750_810x-dip.dtsi
@@ -0,0 +1,38 @@
+/*
+ * Copyright (C) 2019 WAGO Kontakttechnik GmbH & Co. KG - https://www.wago.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include "am33xx-pinfunc.h"
+
+&am33xx_pinmux {
+	pinctrl-names = "default";
+	pinctrl-0 = <&uart1_dip_pins>;
+
+	uart1_dip_pins: pinmux_uart1_dip_pins { /* optional uart1 with dip switch */
+		pinctrl-single,pins = <
+			AM33XX_UART1_TXD_GPIO0_15(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_UART1_RXD_GPIO0_14(PIN_INPUT_PULLDOWN)
+			AM33XX_UART1_RTSN_GPIO0_13(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_UART1_CTSN_GPIO0_12(PIN_INPUT_PULLDOWN)
+			AM33XX_MCASP0_FSR_GPIO3_19(PIN_INPUT_PULLDOWN)
+		>;
+	};
+};
+
+/ {
+	dip-switch {
+		/* ti,sn74lv165a 8-bit shifit register */
+		compatible = "ti,sn74lv165a";
+
+		gpios = <&gpio0 15 GPIO_ACTIVE_HIGH /* clk */
+			 &gpio0 14 GPIO_ACTIVE_LOW  /* output */
+			 &gpio0 13 GPIO_ACTIVE_LOW  /* load */
+		>;
+
+		clk,period = <2>;
+	};
+};
diff --git a/arch/arm/boot/dts/am335x-pfc-750_810x.dtsi b/arch/arm/boot/dts/am335x-pfc-750_810x.dtsi
new file mode 100644
index 000000000000..de5e925e807b
--- /dev/null
+++ b/arch/arm/boot/dts/am335x-pfc-750_810x.dtsi
@@ -0,0 +1,315 @@
+/*
+ * Copyright (C) 2019 WAGO Kontakttechnik GmbH & Co. KG - https://www.wago.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include "am335x-pfc.dtsi"
+#include "am335x-pfc-750_8xxx-uart4.dtsi"
+#include "am335x-pfc-750_8xxx-leds.dtsi"
+#include "am335x-pfc-750_8xxx-nvram.dtsi"
+#include "am335x-pfc-750_8xxx-kbus.dtsi"
+
+/ {
+	memory {
+		device_type = "memory";
+		reg = <0x80000000 0x10000000>; /* 256 MB */
+	};
+};
+
+/delete-node/ &led_bar61;
+
+&oms_reset_all {
+	gpios = <&gpio3 15 GPIO_ACTIVE_LOW>;
+};
+
+&cpu_0 {
+	/*
+	 * To consider voltage drop between PMIC and SoC,
+	 * tolerance value is reduced to 2% from 4% and
+	 * voltage value is increased as a precaution.
+	 */
+	operating-points = <
+		/* kHz    uV */
+		600000  1100000
+		300000  950000
+	>;
+};
+
+&wsysinit {
+	board,variant = "pfc100";
+	adjtimex,frequency = <2000000>;
+};
+
+&watchdog {
+	pinctrl-names = "default";
+	pinctrl-0 = <&watchdog_pins>;
+
+	en-gpios = <&gpio0 17 GPIO_ACTIVE_LOW>;
+	gpios = <&gpio0 16 GPIO_ACTIVE_LOW>;
+};
+
+&gpmc {
+	pinctrl-names = "default";
+	pinctrl-0 = <&gpmc_data_pins>, <&gpmc_addr_pins>, <&gpmc_ctrl_pins>;
+
+	num-cs = <2>;
+	num-waitpins = <1>;
+	ranges = <
+		0 0 0x08000000 0x01000000 /* CS0: NAND, 16M */
+		1 0 0x01000000 0x01000000 /* CS1: NVRAM, 16M */
+	>;
+
+	nand: nand@0,0 {
+		compatible = "ti,omap2-nand";
+		reg = <0 0 4>; /* CS0, offset 0, IO size 4 */
+		nand-bus-width = <8>;
+		ti,nand-ecc-opt = "bch8";
+		gpmc,device-nand = "true";
+		gpmc,device-width = <1>;
+		gpmc,sync-clk-ps = <0>;
+		gpmc,cs-on-ns = <0>;
+		gpmc,cs-rd-off-ns = <44>;
+		gpmc,cs-wr-off-ns = <44>;
+		gpmc,adv-on-ns = <6>;
+		gpmc,adv-rd-off-ns = <34>;
+		gpmc,adv-wr-off-ns = <44>;
+		gpmc,we-on-ns = <0>;
+		gpmc,we-off-ns = <40>;
+		gpmc,oe-on-ns = <0>;
+		gpmc,oe-off-ns = <54>;
+		gpmc,access-ns = <64>;
+		gpmc,rd-cycle-ns = <82>;
+		gpmc,wr-cycle-ns = <82>;
+		gpmc,wait-pin = <0>;
+		gpmc,wait-on-read;
+		gpmc,wait-on-write;
+		gpmc,bus-turnaround-ns = <0>;
+		gpmc,cycle2cycle-delay-ns = <0>;
+		gpmc,clk-activation-ns = <0>;
+		gpmc,wait-monitoring-ns = <0>;
+		gpmc,wr-access-ns = <40>;
+		gpmc,wr-data-mux-bus-ns = <0>;
+
+		#address-cells = <1>;
+		#size-cells = <1>;
+		ti,elm-id = <&elm>;
+	};
+
+	nvram: nor@1,0 { /* NVRAM Device 128k */
+		reg = <1 0 0x01000000>;
+		bank-width = <2>; /* 1: 8bit, 2: 16bit */
+		gpmc,sync-clk-ps = <0>;
+		gpmc,cs-on-ns = <0>;
+		gpmc,cs-rd-off-ns = <50>;
+		gpmc,cs-wr-off-ns = <30>;
+		gpmc,adv-on-ns = <0>;
+		gpmc,adv-rd-off-ns = <0>;
+		gpmc,adv-wr-off-ns = <0>;
+		gpmc,oe-on-ns = <0>;
+		gpmc,oe-off-ns = <50>;
+		gpmc,we-on-ns = <0>;
+		gpmc,we-off-ns = <30>;
+		gpmc,rd-cycle-ns = <50>;
+		gpmc,wr-cycle-ns = <30>;
+		gpmc,access-ns = <40>;
+		gpmc,wr-access-ns = <10>;
+		gpmc,wr-data-mux-bus-ns = <0>;
+		gpmc,cycle2cycle-delay-ns = <10>;
+		gpmc,cycle2cycle-samecsen;
+		gpmc,cycle2cycle-diffcsen;
+
+		#address-cells = <1>;
+		#size-cells = <1>;
+	};
+};
+
+&am33xx_pinmux {
+	sdcard_cd_pins: pinmux_sdcard_cd_pins {
+		pinctrl-single,pins = <
+			AM33XX_MCASP0_ACLKX_GPIO3_14(PIN_INPUT_PULLUP)
+		>;
+	};
+
+	gpmc_data_pins: pinmux_gpmc_data_pins {
+		pinctrl-single,pins = <
+			AM33XX_GPMC_AD0(PIN_INPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_AD1(PIN_INPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_AD2(PIN_INPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_AD3(PIN_INPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_AD4(PIN_INPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_AD5(PIN_INPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_AD6(PIN_INPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_AD7(PIN_INPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_AD8(PIN_INPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_AD9(PIN_INPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_AD10(PIN_INPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_AD11(PIN_INPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_AD12(PIN_INPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_AD13(PIN_INPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_AD14(PIN_INPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_AD15(PIN_INPUT_PULLDOWN | SLEWCTRL_FAST)
+		>;
+	};
+
+	gpmc_addr_pins: pinmux_gpmc_addr_pins {
+		pinctrl-single,pins = <
+			AM33XX_LCD_DATA0_GPMC_A0(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_DATA1_GPMC_A1(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_DATA2_GPMC_A2(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_DATA3_GPMC_A3(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_DATA4_GPMC_A4(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_DATA5_GPMC_A5(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_DATA6_GPMC_A6(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_DATA7_GPMC_A7(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_VSYNC_GPMC_A8(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_HSYNC_GPMC_A9(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_PCLK_GPMC_A10(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_AC_BIAS_EN_GPMC_A11(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_DATA8_GPMC_A12(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_DATA9_GPMC_A13(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_DATA10_GPMC_A14(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_DATA11_GPMC_A15(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_DATA12_GPMC_A16(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_DATA13_GPMC_A17(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_DATA14_GPMC_A18(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_DATA15_GPMC_A19(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+		>;
+	};
+
+	gpmc_ctrl_pins: pinmux_gpmc_ctrl_pins {
+		pinctrl-single,pins = <
+			AM33XX_GPMC_BEN0_CLE(PIN_OUTPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_ADVN_ALE(PIN_OUTPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_OEN_REN(PIN_OUTPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_WEN(PIN_OUTPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_CSN0(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_GPMC_CSN1(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_GPMC_CSN2_GPMC_BE1N(PIN_OUTPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_WAIT0(PIN_INPUT_PULLUP)
+			AM33XX_GPMC_CLK_GPIO2_1(PIN_INPUT_PULLDOWN)
+		>;
+	};
+
+	kbus_pins: pinmux_kbus_pins {
+		pinctrl-single,pins = <
+			AM33XX_GPMC_A7_GPIO1_23(PIN_OUTPUT_PULLUP | SLEWCTRL_SLOW)
+			AM33XX_GPMC_A9_GPIO1_25(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_GPMC_A8_GPIO1_24(PIN_OUTPUT_PULLUP | SLEWCTRL_SLOW)
+			AM33XX_XDMA_EVENT_INTR1_GPIO0_20(PIN_INPUT_PULLUP)
+			AM33XX_NNMI(PIN_INPUT_PULLUP)
+			AM33XX_GPMC_A6_GPIO1_22(PIN_INPUT_PULLUP)
+			AM33XX_MII1_RX_ER_GPIO3_2(PIN_INPUT_PULLUP)
+			AM33XX_MII1_TX_EN_GPIO3_3(PIN_INPUT_PULLUP)
+			AM33XX_MII1_RX_DV_GPIO3_4(PIN_INPUT_PULLUP)
+			AM33XX_XDMA_EVENT_INTR0_GPIO0_19(PIN_INPUT_PULLUP)
+		>;
+	};
+
+	watchdog_pins: pinmux_watchdog_pins {
+		pinctrl-single,pins = <
+			AM33XX_MII1_TXD3_GPIO0_16(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_MII1_TXD2_GPIO0_17(PIN_OUTPUT_PULLUP | SLEWCTRL_SLOW)
+		>;
+	};
+
+	ecap0_pins: pinmux_ecap0_pins {
+		pinctrl-single,pins = <
+			AM33XX_SPI0_CS1_ECAP1_IN_PWM1_OUT(PIN_INPUT)
+		>;
+	};
+
+	rmii2_pins: pinmux_rmii2_pins {
+		pinctrl-single,pins = <
+			/* RMII 2 */
+			AM33XX_GPMC_A0_RMII2_TXEN(PIN_OUTPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_A4_RMII2_TXD1(PIN_OUTPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_A5_RMII2_TXD0(PIN_OUTPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_A10_RMII2_RXD1(PIN_INPUT_PULLDOWN)
+			AM33XX_GPMC_A11_RMII2_RXD0(PIN_INPUT_PULLDOWN)
+			AM33XX_GPMC_WPN_RMII2_RXERR(PIN_INPUT_PULLDOWN)
+			AM33XX_MII1_COL_RMII2_REFCLK(PIN_INPUT_PULLDOWN)
+			AM33XX_GPMC_CSN3_RMII2_CRS_DV(PIN_INPUT)
+		>;
+	};
+
+	rmii2_sleep_pins: pinmux_rmii2_sleep_pins {
+		pinctrl-single,pins = <
+			/* RMII 2 */
+			AM33XX_GPMC_A0_GPIO1_16(PIN_INPUT_PULLDOWN)
+			AM33XX_GPMC_A4_GPIO1_20(PIN_INPUT_PULLDOWN)
+			AM33XX_GPMC_A5_GPIO1_21(PIN_INPUT_PULLDOWN)
+			AM33XX_GPMC_A10_GPIO1_26(PIN_INPUT_PULLDOWN)
+			AM33XX_GPMC_A11_GPIO1_27(PIN_INPUT_PULLDOWN)
+			AM33XX_GPMC_WPN_GPIO0_31(PIN_INPUT_PULLDOWN)
+			AM33XX_MII1_COL_GPIO3_0(PIN_INPUT_PULLDOWN)
+			AM33XX_GPMC_CSN3_GPIO2_0(PIN_INPUT_PULLDOWN)
+		>;
+	};
+};
+
+&kbus0 {
+	pinctrl-names = "default";
+	pinctrl-0 = <&kbus_pins>;
+
+	/*
+	 * announce used tty-device in /sys/class/wago/system.
+	 * is needed to link this device to /dev/ttyKbus
+	 */
+	kbus,tty-device = "ttyO4"; /* corresponds to &uart4*/
+
+	/* some should be active low, keep all to high for compatibility reasons */
+	kbus,nrst-gpios   = <&gpio1 25 GPIO_ACTIVE_LOW>;
+	kbus,nsync-gpios  = <&gpio1 22 GPIO_ACTIVE_HIGH>;
+	kbus,cmdsel-gpios = <&gpio1 24 GPIO_ACTIVE_HIGH>;
+	kbus,nirq-gpios   = <&gpio1 23 GPIO_ACTIVE_HIGH>;
+	kbus,nerr-gpios   = <&gpio3  2 GPIO_ACTIVE_HIGH>;
+	kbus,nrdy-gpios   = <&gpio0 20 GPIO_ACTIVE_HIGH>;
+};
+
+&tps {
+	interrupts = <47>;
+	interrupt-parent = <&intc>;
+};
+
+/* do we need this ?? */
+&tscadc {
+	adc {
+		//AIN0 	= +5V0 Local Bus voltage	pmic-in
+		//AIN1	= +3V3  IO system voltage 	dcdc4_reg
+		//AIN2-7	not used
+		ti,adc-channels = <8>;
+	};
+};
+
+&ecap0 {
+	pinctrl-names = "default";
+	pinctrl-0 = <&ecap0_pins>;
+};
+
+&ocmcram {
+	ti,no_idle_on_suspend;
+};
+
+&mmc1 {
+	pinctrl-names = "default";
+	pinctrl-0 = <
+		&mmc1_pins
+		&sdcard_cd_pins
+	>;
+
+	cd-debounce-delay-ms = <7>;
+	cd-gpios = <&gpio3 14 GPIO_ACTIVE_LOW>;
+};
+
+&mac {
+	pinctrl-names = "default", "sleep";
+	pinctrl-0 = <&rmii2_pins>;
+	pinctrl-1 = <&rmii2_sleep_pins>;
+	mac_control = <0x18021>; /* force 100MBit full-duplex */
+	active_slave = <1>;
+};
+
+/include/ "pxc-nandparts.dtsi"
diff --git a/arch/arm/boot/dts/am335x-pfc-750_8208-netx.dtsi b/arch/arm/boot/dts/am335x-pfc-750_8208-netx.dtsi
new file mode 100644
index 000000000000..e6596d21c50b
--- /dev/null
+++ b/arch/arm/boot/dts/am335x-pfc-750_8208-netx.dtsi
@@ -0,0 +1,87 @@
+/*
+ * Copyright (C) 2019 WAGO Kontakttechnik GmbH & Co. KG - https://www.wago.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include "am33xx-pinfunc.h"
+
+/ {
+	uio_netx: uio_netx_dma_fb_asic@8000000 {
+		compatible = "uio_pdrv_genirq";
+		pinctrl-names = "default";
+		pinctrl-0 = <&netx_pins>;
+
+		reg = <0x04000000 0x40000>; /* 256k (min: 64k, max: 256k) */
+
+		/* gpio0_19: DMA-FB-ASIC  */
+		interrupt-parent = <&gpio0>;
+		interrupts = <19 IRQ_TYPE_LEVEL_LOW>;
+
+		status = "disabled";
+	};
+};
+
+&am33xx_pinmux {
+	netx_pins: pinmux_netx_pins {
+		pinctrl-single,pins = <
+			AM33XX_GPMC_BEN1_GPMC_CSN6(PIN_OUTPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_MII1_RXD2_UART3_TXD(PIN_OUTPUT_PULLDOWN)
+			AM33XX_MII1_RXD3_UART3_RXD(PIN_INPUT_PULLDOWN)
+			AM33XX_XDMA_EVENT_INTR0_GPIO0_19(PIN_INPUT_PULLDOWN)
+			AM33XX_LCD_DATA14_GPIO0_10(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+		>;
+	};
+};
+
+&wsysinit {
+	dp,reset;
+	dp,gpio-rst = <&gpio0 10 GPIO_ACTIVE_LOW>; /* nRST-FB */
+};
+
+&gpmc {
+	netX51: nor@6,0 {	/* FIXME: Timings are to be updated!!! */
+		reg = <6 0 0x40000>; /* 256k (min: 64k, max: 256k) */
+		bank-width = <2>;    /* 16bit Device (2 Bytes) */
+
+		gpmc,cs-on-ns = <0>;
+		gpmc,cs-rd-off-ns = <150>;
+		gpmc,cs-wr-off-ns = <70>;
+
+		gpmc,adv-on-ns = <0>;
+		gpmc,adv-rd-off-ns = <0>;
+		gpmc,adv-wr-off-ns = <0>;
+
+		gpmc,oe-on-ns = <0>;
+		gpmc,oe-off-ns = <150>;
+
+		gpmc,we-on-ns = <0>;
+		gpmc,we-off-ns = <70>;
+
+		gpmc,rd-cycle-ns = <150>;
+		gpmc,wr-cycle-ns = <70>;
+
+		gpmc,access-ns = <130>;
+		gpmc,wr-access-ns = <50>;
+
+		gpmc,wr-data-mux-bus-ns = <0>;
+
+		gpmc,cycle2cycle-delay-ns = <20>;
+		gpmc,cycle2cycle-samecsen;
+		gpmc,cycle2cycle-diffcsen;
+
+		/* gpmc,time-para-granularity; */
+		gpmc,mux-add-data = <0>; /* non-multiplexing mode */
+
+		/* First rising edge of GPMC_CLK at start access time */
+		gpmc,clk-activation-ns = <0>;
+
+		/* =1, WAIT pin is monitored one GPMC_CLK cycle before valid data */
+		gpmc,wait-monitoring-ns = <0>;
+
+		#address-cells = <1>;
+		#size-cells = <1>;
+	};
+};
diff --git a/arch/arm/boot/dts/am335x-pfc-750_8208.dts b/arch/arm/boot/dts/am335x-pfc-750_8208.dts
new file mode 100644
index 000000000000..0cb2fbdc0b92
--- /dev/null
+++ b/arch/arm/boot/dts/am335x-pfc-750_8208.dts
@@ -0,0 +1,111 @@
+/*
+ * Copyright (C) 2019 WAGO Kontakttechnik GmbH & Co. KG - https://www.wago.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+/dts-v1/;
+
+#include "am335x-pfc-750_8208.dtsi"
+#include "am335x-pfc-750_8xxx-ksz8863.dtsi"
+#include "am335x-pfc-750_8xxx-uart1.dtsi"
+#include "am335x-pfc-750_8208-netx.dtsi"
+#include "am335x-pfc-750_82xx-dcan.dtsi"
+#include "wago-devconf.dtsi"
+
+/ {
+	model = "WAGO PFC200 750-8208";
+	compatible = "wago,am335x-pfc-750_8208-004e", "wago,am335x-pfc", "ti,am33xx";
+};
+
+&am33xx_pinmux {
+	ksz8863_switch_pins: pinmux_ksz8863_switch_pins {
+		pinctrl-single,pins = <
+			AM33XX_MII1_RX_CLK_GPIO3_10(PIN_OUTPUT_PULLDOWN)
+			AM33XX_SPI0_CS1_GPIO0_6(PIN_INPUT_PULLDOWN)
+		>;
+	};
+};
+
+&aes {
+	status = "okay";
+};
+
+&bitbang_mdio0 {
+	status = "okay";
+};
+
+&cppi41dma {
+	status = "okay";
+};
+
+&dcan0 {
+	status = "okay";
+};
+
+&elm {
+	status = "okay";
+};
+
+&gpmc {
+	status = "okay";
+};
+
+&i2c0 {
+	status = "okay";
+};
+
+&kbus0 {
+	status = "okay";
+};
+
+&ksz8863_switch {
+	pinctrl-names = "default";
+	pintctrl-0 = <&ksz8863_switch_pins>;
+
+	ksz,reset-gpio = <&gpio3 10 GPIO_ACTIVE_LOW>;
+	reg = <0>;
+
+	status = "okay";
+};
+
+&mac {
+	status = "okay";
+};
+
+&mmc1 {
+	status = "okay";
+};
+
+&oms {
+	status = "okay";
+};
+
+&sham {
+	status = "okay";
+};
+
+&spi0 {
+	status = "okay";
+};
+
+&swcfg_ksz8863 {
+	status = "okay";
+};
+
+&uart1 {
+	status = "okay";
+};
+
+&uart4 {
+	status = "okay";
+};
+
+&uio_netx {
+	status = "okay";
+};
+
+&watchdog {
+	status = "okay";
+};
diff --git a/arch/arm/boot/dts/am335x-pfc-750_8208.dtsi b/arch/arm/boot/dts/am335x-pfc-750_8208.dtsi
new file mode 100644
index 000000000000..42137533ed33
--- /dev/null
+++ b/arch/arm/boot/dts/am335x-pfc-750_8208.dtsi
@@ -0,0 +1,333 @@
+/*
+ * Copyright (C) 2019 WAGO Kontakttechnik GmbH & Co. KG - https://www.wago.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include "am335x-pfc.dtsi"
+#include "am335x-pfc-750_8xxx-uart4.dtsi"
+#include "am335x-pfc-750_8xxx-leds.dtsi"
+#include "am335x-pfc-750_8xxx-nvram.dtsi"
+#include "am335x-pfc-750_8xxx-kbus.dtsi"
+
+/ {
+	memory: memory {
+		device_type = "memory";
+		reg = <0x80000000 0x10000000>; /* 256 MB */
+	};
+};
+
+&cpu_0 {
+	operating-points = <
+		/* kHz    uV */
+		600000  1100000
+		300000  950000
+	>;
+};
+
+&am33xx_pinmux {
+	rmii1_pins: pinmux_rmii1_pins {
+		pinctrl-single,pins = <
+			/* reserved, not yet used (RMII1) */
+			AM33XX_MII1_TX_EN_RMII1_TXEN(PIN_OUTPUT_PULLDOWN)
+			AM33XX_MII1_TXD0_RMII1_TXD0(PIN_OUTPUT_PULLDOWN)
+			AM33XX_MII1_TXD1_RMII1_TXD1(PIN_OUTPUT_PULLDOWN)
+			AM33XX_MII1_RXD0_RMII1_RXD0(PIN_INPUT_PULLDOWN)
+			AM33XX_MII1_RXD1_RMII1_RXD1(PIN_INPUT_PULLDOWN)
+			AM33XX_GPMC_CLK_GPIO2_1(PIN_OUTPUT_PULLDOWN)
+			AM33XX_GPMC_A1_GPIO1_17(PIN_INPUT_PULLDOWN)
+			AM33XX_RMII1_REF_CLK(PIN_INPUT)
+			AM33XX_MII1_CRS_RMII1_CRS_DV(PIN_INPUT_PULLDOWN)
+			AM33XX_MII1_RX_ER_RMII1_RXERR(PIN_INPUT_PULLDOWN)
+		>;
+	};
+
+	rmii2_pins: pinmux_rmii2_pins {
+		pinctrl-single,pins = <
+			/* RMII 2 */
+			AM33XX_GPMC_A0_RMII2_TXEN(PIN_OUTPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_A4_RMII2_TXD1(PIN_OUTPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_A5_RMII2_TXD0(PIN_OUTPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_A10_RMII2_RXD1(PIN_INPUT_PULLDOWN)
+			AM33XX_GPMC_A11_RMII2_RXD0(PIN_INPUT_PULLDOWN)
+			AM33XX_GPMC_WPN_RMII2_RXERR(PIN_INPUT_PULLDOWN)
+			AM33XX_MII1_COL_RMII2_REFCLK(PIN_INPUT_PULLDOWN)
+			AM33XX_GPMC_CSN3_RMII2_CRS_DV(PIN_INPUT)
+		>;
+	};
+
+	rmii2_sleep_pins: pinmux_rmii2_sleep_pins {
+		pinctrl-single,pins = <
+			/* RMII 2 */
+			AM33XX_GPMC_A0_GPIO1_16(PIN_INPUT_PULLDOWN)
+			AM33XX_GPMC_A4_GPIO1_20(PIN_INPUT_PULLDOWN)
+			AM33XX_GPMC_A5_GPIO1_21(PIN_INPUT_PULLDOWN)
+			AM33XX_GPMC_A10_GPIO1_26(PIN_INPUT_PULLDOWN)
+			AM33XX_GPMC_A11_GPIO1_27(PIN_INPUT_PULLDOWN)
+			AM33XX_GPMC_WPN_GPIO0_31(PIN_INPUT_PULLDOWN)
+			AM33XX_MII1_COL_GPIO3_0(PIN_INPUT_PULLDOWN)
+			AM33XX_GPMC_CSN3_GPIO2_0(PIN_INPUT_PULLDOWN)
+		>;
+	};
+
+	watchdog_pins: pinmux_watchdog_pins {
+		pinctrl-single,pins = <
+			AM33XX_EMU0_GPIO3_7(PIN_OUTPUT_PULLDOWN)
+			AM33XX_EMU1_GPIO3_8(PIN_OUTPUT_PULLDOWN)
+		>;
+	};
+
+	sdcard_cd_pins: pinmux_sdcard_cd_pins {
+		pinctrl-single,pins = <
+			AM33XX_MCASP0_ACLKX_GPIO3_14(PIN_INPUT_PULLUP)
+		>;
+	};
+
+	sdcard_wp_pins: pinmux_sdcard_wp_pins {
+		pinctrl-single,pins = <
+			AM33XX_ECAP0_IN_PWM0_OUT_MMC0_SDWP(PIN_INPUT_PULLDOWN)
+		>;
+	};
+
+	kbus_pins: pinmux_kbus_pins {
+		pinctrl-single,pins = <
+			AM33XX_GPMC_A7_GPIO1_23(PIN_OUTPUT_PULLUP | SLEWCTRL_SLOW)
+			AM33XX_GPMC_A9_GPIO1_25(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_GPMC_A8_GPIO1_24(PIN_OUTPUT_PULLUP | SLEWCTRL_SLOW)
+			AM33XX_XDMA_EVENT_INTR1_GPIO0_20(PIN_INPUT_PULLUP)
+			AM33XX_GPMC_A6_GPIO1_22(PIN_INPUT_PULLUP)
+			AM33XX_MII1_TX_CLK_GPIO3_9(PIN_INPUT_PULLUP)
+		>;
+	};
+
+	gpmc_data_pins: pinmux_gpmc_data_pins {
+		pinctrl-single,pins = <
+			AM33XX_GPMC_AD0(PIN_INPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_AD1(PIN_INPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_AD2(PIN_INPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_AD3(PIN_INPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_AD4(PIN_INPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_AD5(PIN_INPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_AD6(PIN_INPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_AD7(PIN_INPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_AD8(PIN_INPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_AD9(PIN_INPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_AD10(PIN_INPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_AD11(PIN_INPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_AD12(PIN_INPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_AD13(PIN_INPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_AD14(PIN_INPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_AD15(PIN_INPUT_PULLDOWN | SLEWCTRL_FAST)
+		>;
+	};
+
+	gpmc_addr_pins: pinmux_gpmc_addr_pins {
+		pinctrl-single,pins = <
+			AM33XX_LCD_DATA0_GPMC_A0(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_DATA1_GPMC_A1(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_DATA2_GPMC_A2(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_DATA3_GPMC_A3(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_DATA4_GPMC_A4(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_DATA5_GPMC_A5(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_DATA6_GPMC_A6(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_DATA7_GPMC_A7(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_VSYNC_GPMC_A8(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_HSYNC_GPMC_A9(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_PCLK_GPMC_A10(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_AC_BIAS_EN_GPMC_A11(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_DATA8_GPMC_A12(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_DATA9_GPMC_A13(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_DATA10_GPMC_A14(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_DATA11_GPMC_A15(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_DATA12_GPMC_A16(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_DATA13_GPMC_A17(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_DATA15_GPMC_A19(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+		>;
+	};
+
+	gpmc_ctrl_pins: pinmux_gpmc_ctrl_pins {
+		pinctrl-single,pins = <
+			AM33XX_GPMC_BEN0_CLE(PIN_OUTPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_ADVN_ALE(PIN_OUTPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_OEN_REN(PIN_OUTPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_WEN(PIN_OUTPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_CSN0(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_GPMC_CSN1(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_GPMC_CSN2_GPMC_BE1N(PIN_OUTPUT_PULLDOWN | SLEWCTRL_FAST)
+			AM33XX_GPMC_WAIT0(PIN_INPUT_PULLUP)
+		>;
+	};
+};
+
+&wsysinit {
+	board,variant = "pfc200v2";
+	adjtimex,frequency = <2000000>;
+};
+
+&watchdog {
+	pinctrl-names = "default";
+	pinctrl-0 = <&watchdog_pins>;
+
+	en-gpios = <&gpio3 7 GPIO_ACTIVE_LOW>;
+	gpios = <&gpio3 8 GPIO_ACTIVE_LOW>;
+};
+
+&mac {
+	pinctrl-names = "default", "sleep";
+	pinctrl-0 = <&rmii2_pins>;
+	pinctrl-1 = <&rmii2_sleep_pins>;
+	mac_control = <0x18021>; /* force 100MBit full-duplex */
+	active_slave = <1>;
+};
+
+&gpmc {
+	pinctrl-names = "default";
+	pinctrl-0 = <&gpmc_data_pins>, <&gpmc_addr_pins>, <&gpmc_ctrl_pins>;
+
+	num-cs = <7>;
+	num-waitpins = <3>;
+	ranges = <
+		0 0 0x08000000 0x01000000 /* CS0: NAND, 16M */
+		1 0 0x01000000 0x01000000 /* CS1: NVRAM, 16M */
+		6 0 0x04000000 0x01000000 /* CS6: NETX/DPC31, 16M */
+	>;
+
+	nand: nand@0,0 {
+		compatible = "ti,omap2-nand";
+		reg = <0 0 4>; /* CS0, offset 0, IO size 4 */
+		nand-bus-width = <8>;
+		ti,nand-ecc-opt = "bch8";
+		gpmc,device-nand = "true";
+		gpmc,device-width = <1>;
+		gpmc,sync-clk-ps = <0>;
+		gpmc,cs-on-ns = <0>;
+		gpmc,cs-rd-off-ns = <44>;
+		gpmc,cs-wr-off-ns = <44>;
+		gpmc,adv-on-ns = <6>;
+		gpmc,adv-rd-off-ns = <34>;
+		gpmc,adv-wr-off-ns = <44>;
+		gpmc,we-on-ns = <0>;
+		gpmc,we-off-ns = <40>;
+		gpmc,oe-on-ns = <0>;
+		gpmc,oe-off-ns = <54>;
+		gpmc,access-ns = <64>;
+		gpmc,rd-cycle-ns = <82>;
+		gpmc,wr-cycle-ns = <82>;
+		gpmc,wait-pin = <0>;
+		gpmc,wait-on-read;
+		gpmc,wait-on-write;
+		gpmc,bus-turnaround-ns = <0>;
+		gpmc,cycle2cycle-delay-ns = <0>;
+		gpmc,clk-activation-ns = <0>;
+		gpmc,wait-monitoring-ns = <0>;
+		gpmc,wr-access-ns = <40>;
+		gpmc,wr-data-mux-bus-ns = <0>;
+
+		#address-cells = <1>;
+		#size-cells = <1>;
+		ti,elm-id = <&elm>;
+	};
+
+	nvram: nor@1,0 { /* NVRAM Device 128k */
+		reg = <1 0 0x01000000>;
+		bank-width = <2>; /* 1: 8bit, 2: 16bit */
+		gpmc,sync-clk-ps = <0>;
+		gpmc,cs-on-ns = <0>;
+		gpmc,cs-rd-off-ns = <50>;
+		gpmc,cs-wr-off-ns = <30>;
+		gpmc,adv-on-ns = <0>;
+		gpmc,adv-rd-off-ns = <0>;
+		gpmc,adv-wr-off-ns = <0>;
+		gpmc,oe-on-ns = <0>;
+		gpmc,oe-off-ns = <50>;
+		gpmc,we-on-ns = <0>;
+		gpmc,we-off-ns = <30>;
+		gpmc,rd-cycle-ns = <50>;
+		gpmc,wr-cycle-ns = <30>;
+		gpmc,access-ns = <40>;
+		gpmc,wr-access-ns = <10>;
+		gpmc,wr-data-mux-bus-ns = <0>;
+		gpmc,cycle2cycle-delay-ns = <10>;
+		gpmc,cycle2cycle-samecsen;
+		gpmc,cycle2cycle-diffcsen;
+
+		#address-cells = <1>;
+		#size-cells = <1>;
+	};
+};
+
+&kbus0 {
+	pinctrl-names = "default";
+	pinctrl-0 = <&kbus_pins>;
+
+	/*
+	 * announce used tty-device in /sys/class/wago/system.
+	 * is needed to link this device to /dev/ttyKbus
+	 */
+	kbus,tty-device = "ttyO4"; /* corresponds to &uart4*/
+
+	/* some should be active low, keep all to high for compatibility reasons */
+	kbus,nrst-gpios   = <&gpio1 25 GPIO_ACTIVE_LOW>;
+	kbus,nsync-gpios  = <&gpio1 22 GPIO_ACTIVE_HIGH>;
+	kbus,cmdsel-gpios = <&gpio1 24 GPIO_ACTIVE_HIGH>;
+	kbus,nirq-gpios   = <&gpio1 23 GPIO_ACTIVE_HIGH>;
+	kbus,nerr-gpios   = <&gpio3  9 GPIO_ACTIVE_HIGH>;
+	kbus,nrdy-gpios   = <&gpio0 20 GPIO_ACTIVE_HIGH>;
+};
+
+&lb61_0 {
+	label = "bf-red";
+};
+
+&lb61_1 {
+	label = "bf-green";
+};
+
+&lb61_2 {
+	label = "dia-red";
+};
+
+&lb61_3 {
+	label = "dia-green";
+};
+
+&lb60_10 {
+	label = "can-red";
+};
+
+&lb60_11 {
+	label = "can-green";
+};
+
+&tps {
+	interrupts = <47>;
+	interrupt-parent = <&intc>;
+};
+
+&tscadc {
+	adc {
+		/* AIN0 = +5V0 Local Bus voltage pmic-in
+		 * AIN1 = +3V3 IO system voltage dcdc4_reg
+		 * AIN2-7 not used
+		 */
+		ti,adc-channels = <8>;
+	};
+};
+
+&ocmcram {
+	ti,no_idle_on_suspend;
+};
+
+&mmc1 {
+	pinctrl-names = "default";
+	pinctrl-0 = <
+		&mmc1_pins
+		&sdcard_cd_pins
+		&sdcard_wp_pins
+	>;
+	cd-gpios = <&gpio3 14 GPIO_ACTIVE_LOW>;
+};
+
+/include/ "pxc-nandparts.dtsi"
diff --git a/arch/arm/boot/dts/am335x-pfc-750_8210.dts b/arch/arm/boot/dts/am335x-pfc-750_8210.dts
new file mode 100644
index 000000000000..3c2cfb2b44bd
--- /dev/null
+++ b/arch/arm/boot/dts/am335x-pfc-750_8210.dts
@@ -0,0 +1,160 @@
+/*
+ * Copyright (C) 2019 WAGO Kontakttechnik GmbH & Co. KG - https://www.wago.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+/dts-v1/;
+
+#include "am335x-pfc-750_821x.dtsi"
+#include "am335x-pfc-750_821x-mv88e6321.dtsi"
+#include "wago-devconf.dtsi"
+
+/{
+	model = "WAGO PFC200 750-8210";
+	compatible = "wago,am335x-pfc-750_821x-1007", "wago,am335x-pfc", "ti,am33xx";
+};
+
+&aes {
+	status = "okay";
+};
+
+&cppi41dma {
+	status = "okay";
+};
+
+&elm {
+	status = "okay";
+};
+
+&gpmc {
+	status = "okay";
+};
+
+&i2c0 {
+	status = "okay";
+};
+
+&kbus0 {
+	status = "okay";
+};
+
+&mac {
+	status = "okay";
+};
+
+&mmc1 {
+	status = "okay";
+};
+
+&mmc2 {
+	status = "okay";
+};
+
+&oms {
+	status = "okay";
+};
+
+&sham {
+	status = "okay";
+};
+
+&spi0 {
+	status = "okay";
+};
+
+&uart0 {
+	status = "okay";
+};
+
+&uart5 {
+	status = "okay";
+};
+
+&watchdog {
+	status = "okay";
+};
+
+&wsysinit {
+	tty,rs232-485 = "nop";
+	profinet-capable;
+};
+
+&am33xx_pinmux {
+	ext_phy0_pins: pinmux_ext_phy0_pins {
+		pinctrl-single,pins = <
+			AM33XX_GPMC_WPN_GPIO0_31(PIN_INPUT)
+		>;
+	};
+
+	ext_phy1_pins: pinmux_ext_phy1_pins {
+		pinctrl-single,pins = <
+			AM33XX_MII1_RX_ER_GPIO3_2(PIN_INPUT)
+		>;
+	};
+
+};
+
+&swcfg_mv88e6321 {
+	status = "okay";
+};
+
+&mv88e6321_switch {
+	eeprom = /bits/ 8 <0x20 0x80 0x09 0x5E 0x40 0x80 0x09 0x5E 0x01 0x7D 0x01 0x7D 0x01 0x7D 0x01 0x7D
+			   0xF9 0x7F 0x40 0x19 0xF8 0x7F 0x60 0x94 0xF9 0x7F 0x40 0x19 0xF8 0x7F 0x80 0x94
+			   0xF9 0x7F 0x40 0x19 0xF8 0x7F 0x00 0x94 0xF9 0x7F 0x40 0x19 0xF8 0x7F 0x20 0x94
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF>;
+
+	mdio {
+		ext_phy0: ethernet-phy@0 {
+			compatible = "ethernet-phy-idD565.A401", "ethernet-phy-ieee802.3-c22";
+			pinctrl-names = "default";
+			pinctrl-0 = <&ext_phy0_pins>;
+			reg = <0>;
+			max-speed = <100>;
+			interrupt-parent = <&gpio3>;
+			interrupts = <2 IRQ_TYPE_EDGE_FALLING>;
+		};
+
+		ext_phy1: ethernet-phy@1 {
+			compatible = "ethernet-phy-idD565.A401", "ethernet-phy-ieee802.3-c22";
+			pinctrl-names = "default";
+			pinctrl-0 = <&ext_phy1_pins>;
+			reg = <1>;
+			max-speed = <100>;
+			interrupt-parent = <&gpio0>;
+			interrupts = <31 IRQ_TYPE_EDGE_FALLING>;
+		};
+	};
+
+	ports {
+		port@0 {
+			reg = <0>;
+			label = "ethX1";
+			phy-mode = "sgmii";
+			phy-handle = <&ext_phy0>;
+			phy-external;
+		};
+
+		port@1 {
+			reg = <1>;
+			label = "ethX2";
+			phy-mode = "sgmii";
+			phy-handle = <&ext_phy1>;
+			phy-external;
+		};
+	};
+};
diff --git a/arch/arm/boot/dts/am335x-pfc-750_8211.dts b/arch/arm/boot/dts/am335x-pfc-750_8211.dts
new file mode 100644
index 000000000000..761ebcdc97cd
--- /dev/null
+++ b/arch/arm/boot/dts/am335x-pfc-750_8211.dts
@@ -0,0 +1,140 @@
+/*
+ * Copyright (C) 2019 WAGO Kontakttechnik GmbH & Co. KG - https://www.wago.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+/dts-v1/;
+
+#include "am335x-pfc-750_821x.dtsi"
+#include "am335x-pfc-750_82xx-dcan.dtsi"
+#include "am335x-pfc-750_821x-mv88e6321.dtsi"
+#include "am335x-pfc-750_8xxx-uart1-two-wire.dtsi"
+#include "wago-devconf.dtsi"
+
+/{
+	model = "WAGO PFC200 750-8211";
+	compatible = "wago,am335x-pfc-750_821x-1004", "wago,am335x-pfc", "ti,am33xx";
+};
+
+&aes {
+	status = "okay";
+};
+
+&cppi41dma {
+	status = "okay";
+};
+
+&elm {
+	status = "okay";
+};
+
+&gpmc {
+	status = "okay";
+};
+
+&i2c0 {
+	status = "okay";
+};
+
+&kbus0 {
+	status = "okay";
+};
+
+&mac {
+	status = "okay";
+};
+
+&mmc1 {
+	status = "okay";
+};
+
+&mmc2 {
+	status = "okay";
+};
+
+&oms {
+	status = "okay";
+};
+
+&sham {
+	status = "okay";
+};
+
+&swcfg_mv88e6321 {
+	status = "okay";
+};
+
+&spi0 {
+	status = "okay";
+};
+
+&uart0 {
+	status = "okay";
+};
+
+&uart1 {
+	/delete-property/ rs485en-gpio;
+	/delete-property/ rs485-rts-active-high;
+	/delete-property/ rs485-rts-delay;
+	status = "okay";
+};
+
+&uart5 {
+	status = "okay";
+};
+
+&watchdog {
+	status = "okay";
+};
+
+&wsysinit {
+	tty,rs232-485 = "nop";
+	profinet-capable;
+};
+
+&mv88e6321_switch {
+	eeprom = /bits/ 8 <0x20 0x80 0x09 0x5E 0x40 0x80 0x09 0x5E 0x01 0x7D 0x01 0x7D 0x01 0x7D 0x01 0x7D
+			   0xF9 0x7F 0x40 0x19 0xF8 0x7F 0x60 0x94 0xF9 0x7F 0x40 0x19 0xF8 0x7F 0x80 0x94
+			   0xF9 0x7F 0x40 0x19 0xF8 0x7F 0x00 0x94 0xF9 0x7F 0x40 0x19 0xF8 0x7F 0x20 0x94
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF>;
+
+	led-ctrl = /bits/ 16 <0x0000 0x0000 0x0000 0x0088 0x0088 0x0000 0x0000 0x0000>;
+
+	ports {
+		port@0 {
+			reg = <0>;
+			label = "ethX1";
+			phy-mode = "sgmii";
+
+			fixed-link {
+				speed = <100>;
+				full-duplex;
+			};
+		};
+
+		port@1 {
+			reg = <1>;
+			label = "ethX2";
+			phy-mode = "sgmii";
+
+			fixed-link {
+				speed = <100>;
+				full-duplex;
+			};
+		};
+	};
+};
diff --git a/arch/arm/boot/dts/am335x-pfc-750_8212.dts b/arch/arm/boot/dts/am335x-pfc-750_8212.dts
new file mode 100644
index 000000000000..0ccdd23df295
--- /dev/null
+++ b/arch/arm/boot/dts/am335x-pfc-750_8212.dts
@@ -0,0 +1,90 @@
+/*
+ * Copyright (C) 2019 WAGO Kontakttechnik GmbH & Co. KG - https://www.wago.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+/dts-v1/;
+
+#include "am335x-pfc-750_821x.dtsi"
+#include "am335x-pfc-750_8xxx-ksz8863.dtsi"
+#include "am335x-pfc-750_8xxx-uart1.dtsi"
+#include "wago-devconf.dtsi"
+
+/ {
+	model = "WAGO PFC200 750-8212";
+	compatible = "wago,am335x-pfc-750_821x-000c", "wago,am335x-pfc", "ti,am33xx";
+};
+
+&aes {
+	status = "okay";
+};
+
+&bitbang_mdio0 {
+	status = "okay";
+};
+
+&elm {
+	status = "okay";
+};
+
+&gpmc {
+	status = "okay";
+};
+
+&i2c0 {
+	status = "okay";
+};
+
+&kbus0 {
+	status = "okay";
+};
+
+&ksz8863_switch {
+	status = "okay";
+};
+
+&mac {
+	status = "okay";
+};
+
+&mmc1 {
+	status = "okay";
+};
+
+&mmc2 {
+	status = "okay";
+};
+
+&oms {
+	status = "okay";
+};
+
+&sham {
+	status = "okay";
+};
+
+&spi0 {
+	status = "okay";
+};
+
+&swcfg_ksz8863 {
+	status = "okay";
+};
+
+&uart0 {
+	status = "okay";
+};
+
+&uart1 {
+	status = "okay";
+};
+
+&uart5 {
+	status = "okay";
+};
+
+&watchdog {
+	status = "okay";
+};
diff --git a/arch/arm/boot/dts/am335x-pfc-750_8213.dts b/arch/arm/boot/dts/am335x-pfc-750_8213.dts
new file mode 100644
index 000000000000..43892b064ee3
--- /dev/null
+++ b/arch/arm/boot/dts/am335x-pfc-750_8213.dts
@@ -0,0 +1,98 @@
+/*
+ * Copyright (C) 2019 WAGO Kontakttechnik GmbH & Co. KG - https://www.wago.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+/dts-v1/;
+
+#include "am335x-pfc-750_821x.dtsi"
+#include "am335x-pfc-750_8xxx-ksz8863.dtsi"
+#include "am335x-pfc-750_82xx-dcan.dtsi"
+#include "wago-devconf.dtsi"
+
+/ {
+	model = "WAGO PFC200 750-8213";
+	compatible = "wago,am335x-pfc-750_821x-000a", "wago,am335x-pfc", "ti,am33xx";
+};
+
+&aes {
+	status = "okay";
+};
+
+&bitbang_mdio0 {
+	status = "okay";
+};
+
+&dcan0 {
+	status = "okay";
+};
+
+&elm {
+	status = "okay";
+};
+
+&gpmc {
+	status = "okay";
+};
+
+&i2c0 {
+	status = "okay";
+};
+
+&kbus0 {
+	status = "okay";
+};
+
+&ksz8863_switch {
+	status = "okay";
+};
+
+&mac {
+	status = "okay";
+};
+
+&mmc1 {
+	status = "okay";
+};
+
+&mmc2 {
+	status = "okay";
+};
+
+&oms {
+	status = "okay";
+};
+
+&sham {
+	status = "okay";
+};
+
+&spi0 {
+	status = "okay";
+};
+
+&swcfg_ksz8863 {
+	status = "okay";
+};
+
+&uart0 {
+	status = "okay";
+};
+
+&uart5 {
+	status = "okay";
+};
+
+&watchdog {
+	status = "okay";
+};
+
+&lb60_10 {
+	label = "can-red";
+};
+
+&lb60_11 {
+	label = "can-green";
+};
diff --git a/arch/arm/boot/dts/am335x-pfc-750_8214.dts b/arch/arm/boot/dts/am335x-pfc-750_8214.dts
new file mode 100644
index 000000000000..e9b752cfba5a
--- /dev/null
+++ b/arch/arm/boot/dts/am335x-pfc-750_8214.dts
@@ -0,0 +1,103 @@
+/*
+ * Copyright (C) 2019 WAGO Kontakttechnik GmbH & Co. KG - https://www.wago.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+/dts-v1/;
+
+#include "am335x-pfc-750_821x.dtsi"
+#include "am335x-pfc-750_8xxx-uart1.dtsi"
+#include "am335x-pfc-750_8xxx-ksz8863.dtsi"
+#include "am335x-pfc-750_82xx-dcan.dtsi"
+#include "wago-devconf.dtsi"
+
+/ {
+	model = "WAGO PFC200 750-8214";
+	compatible = "wago,am335x-pfc-750_821x-000e", "wago,am335x-pfc", "ti,am33xx";
+};
+
+&aes {
+	status = "okay";
+};
+
+&bitbang_mdio0 {
+	status = "okay";
+};
+
+&dcan0 {
+	status = "okay";
+};
+
+&elm {
+	status = "okay";
+};
+
+&gpmc {
+	status = "okay";
+};
+
+&i2c0 {
+	status = "okay";
+};
+
+&kbus0 {
+	status = "okay";
+};
+
+&ksz8863_switch {
+	status = "okay";
+};
+
+&mac {
+	status = "okay";
+};
+
+&mmc1 {
+	status = "okay";
+};
+
+&mmc2 {
+	status = "okay";
+};
+
+&oms {
+	status = "okay";
+};
+
+&sham {
+	status = "okay";
+};
+
+&spi0 {
+	status = "okay";
+};
+
+&swcfg_ksz8863 {
+	status = "okay";
+};
+
+&uart0 {
+	status = "okay";
+};
+
+&uart1 {
+	status = "okay";
+};
+
+&uart5 {
+	status = "okay";
+};
+
+&watchdog {
+	status = "okay";
+};
+
+&lb60_10 {
+	label = "can-red";
+};
+
+&lb60_11 {
+	label = "can-green";
+};
diff --git a/arch/arm/boot/dts/am335x-pfc-750_8215-tcam.dtsi b/arch/arm/boot/dts/am335x-pfc-750_8215-tcam.dtsi
new file mode 100644
index 000000000000..83b0cfef8d6d
--- /dev/null
+++ b/arch/arm/boot/dts/am335x-pfc-750_8215-tcam.dtsi
@@ -0,0 +1,505 @@
+/*
+ * Copyright (C) 2019 WAGO Kontakttechnik GmbH & Co. KG - https://www.wago.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+/{
+	tcam {
+		compatible = "mv88e6321,tcam";
+		tcam-mode-port-mask = <0x58>;   /* use tcam */
+		debug-port = <0x03>;
+
+		/* ethX11: forwarding mrp test frames to ethX12 */
+		ethX11_mrp_test_forwarding {
+			id = <1>;
+			frame-type-mask = <3>;
+			frame-type = <0>;
+			spv-mask = <0x7F>;
+			spv = <0x08>;       /* use tcam entry for Port 3 -> ethX11 */
+			ppri-mask = <0x0>;
+			ppri = <0x0>;
+			pvid-mask = <0x0>;
+			pvid = <0x0>;
+			next-id = <0>;
+
+			dpv-override;
+			dpv-data = <0x10>;  /* frame forwarding to Port 4 -> ethX12 */
+
+			action-override;
+			action-data = <0x0>;
+
+			frame-data = [
+				01 15 4E 00 00 01 00 00 00 00 00 00 00 00 00 00
+				88 E3 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				];
+
+			frame-data-mask = [
+				FF FF FF FF FF FF 00 00 00 00 00 00 00 00 00 00
+				FF FF 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				];
+		};
+
+		/* ethX12: forwarding mrp test frames to ethX12 */
+		ethX12_mrp_test_forwarding {
+			id = <2>;
+			frame-type-mask = <3>;
+			frame-type = <0>;
+			spv-mask = <0x7F>;
+			spv = <0x10>;       /* use tcam entry for Port 4 -> ethX12 */
+			ppri-mask = <0x0>;
+			ppri = <0x0>;
+			pvid-mask = <0x0>;
+			pvid = <0x0>;
+			next-id = <0>;
+
+			dpv-override;
+			dpv-data = <0x08>;  /* frame forwarding to Port 3 -> ethX11 */
+
+			action-override;
+			action-data = <0x0>;
+
+			frame-data = [
+				01 15 4E 00 00 01 00 00 00 00 00 00 00 00 00 00
+				88 E3 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				];
+
+			frame-data-mask = [
+				FF FF FF FF FF FF 00 00 00 00 00 00 00 00 00 00
+				FF FF 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				];
+		};
+
+		/* ethX11: forwarding mrp control frames to ethX12 and cpu */
+		ethX11_mrp_control_forwarding {
+			id = <3>;
+			frame-type-mask = <3>;
+			frame-type = <0>;
+			spv-mask = <0x7F>;
+			spv = <0x08>;       /* use tcam entry for Port 3 -> ethX11 */
+			ppri-mask = <0x0>;
+			ppri = <0x0>;
+			pvid-mask = <0x0>;
+			pvid = <0x0>;
+			next-id = <0>;
+
+			dpv-override;
+			dpv-data = <0x50>;  /* frame forwarding to Port 4 -> ethX12 and cpu*/
+
+			action-override;
+			action-data = <0x0>;
+
+			frame-data = [
+				01 15 4E 00 00 02 00 00 00 00 00 00 00 00 00 00
+				88 E3 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				];
+
+			frame-data-mask = [
+				FF FF FF FF FF FF 00 00 00 00 00 00 00 00 00 00
+				FF FF 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				];
+		};
+
+		/* ethX12: forwarding mrp control frames to ethX12 */
+		ethX12_mrp_control_forwarding {
+			id = <4>;
+			frame-type-mask = <3>;
+			frame-type = <0>;
+			spv-mask = <0x7F>;
+			spv = <0x10>;       /* use tcam entry for Port 4 -> ethX12 */
+			ppri-mask = <0x0>;
+			ppri = <0x0>;
+			pvid-mask = <0x0>;
+			pvid = <0x0>;
+			next-id = <0>;
+
+			dpv-override;
+			dpv-data = <0x48>;  /* frame forwarding to Port 3 -> ethX11 and cpu */
+
+			action-override;
+			action-data = <0x0>;
+
+			frame-data = [
+				01 15 4E 00 00 02 00 00 00 00 00 00 00 00 00 00
+				88 E3 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				];
+
+			frame-data-mask = [
+				FF FF FF FF FF FF 00 00 00 00 00 00 00 00 00 00
+				FF FF 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				];
+		};
+
+		/* ethX11: dcp ident boundary - Egress of Frames with MAC '01-0E-CF-00-00-00' on
+		 * ethX11 must be prevented Ingress on ethX12 is allowed, forward to CPU port is allowed
+		 */
+		ethX11_dcp_ident_boundary {
+			id = <5>;
+			frame-type-mask = <3>;
+			frame-type = <0>;
+			spv-mask = <0x7F>;
+			spv = <0x10>;       /* use tcam entry for Port 4 -> ethX12 */
+			ppri-mask = <0x0>;
+			ppri = <0x0>;
+			pvid-mask = <0x0>;
+			pvid = <0x0>;
+			next-id = <0>;
+
+			dpv-override;
+			dpv-data = <0x40>;  /* frame forwarding to Port 6 -> CPU  */
+
+			action-override;
+			action-data = <0x0>;
+
+			frame-data = [
+				01 0E CF 00 00 00 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				];
+
+			frame-data-mask = [
+				FF FF FF FF FF FF 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				];
+		};
+
+		/* ethX12: dcp ident boundary - Egress of Frames with MAC '01-0E-CF-00-00-00'
+		 * on ethX12 must be prevented Ingress on ethX11 is allowed, forward to CPU port is allowed
+		 */
+		ethX12_dcp_ident_boundary {
+			id = <6>;
+			frame-type-mask = <3>;
+			frame-type = <0>;
+			spv-mask = <0x7F>;
+			spv = <0x08>;       /* use tcam entry for Port 3 -> ethX11 */
+			ppri-mask = <0x0>;
+			ppri = <0x0>;
+			pvid-mask = <0x0>;
+			pvid = <0x0>;
+			next-id = <0>;
+
+			dpv-override;
+			dpv-data = <0x40>;  /* frame forwarding to Port 6 -> CPU  */
+
+			action-override;
+			action-data = <0x0>;
+
+			frame-data = [
+				01 0E CF 00 00 00 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				];
+
+			frame-data-mask = [
+				FF FF FF FF FF FF 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				];
+		};
+		/* ethX11: dcp hello boundary - Egress of Frames with MAC '01-0E-CF-00-00-01'
+		 * on ethX11 must be prevented Ingress on ethX12 is allowed, forward to CPU port is allowed
+		 */
+		ethX11_dcp_hello_boundary {
+			id = <7>;
+			frame-type-mask = <3>;
+			frame-type = <0>;
+			spv-mask = <0x7F>;
+			spv = <0x10>;       /* use tcam entry for Port 4 -> ethX12 */
+			ppri-mask = <0x0>;
+			ppri = <0x0>;
+			pvid-mask = <0x0>;
+			pvid = <0x0>;
+			next-id = <0>;
+
+			dpv-override;
+			dpv-data = <0x40>;  /* frame forwarding to Port 6 -> CPU  */
+
+			action-override;
+			action-data = <0x0>;
+
+			frame-data = [
+				01 0E CF 00 00 01 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				];
+
+			frame-data-mask = [
+				FF FF FF FF FF FF 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				];
+		};
+
+		/* ethX12: dcp hello boundary - Egress of Frames with MAC '01-0E-CF-00-00-01'
+		 * on ethX12 must be prevented Ingress on ethX11 is allowed, forward to CPU port is allowed
+		 */
+		ethX12_dcp_hello_boundary {
+			id = <8>;
+			frame-type-mask = <3>;
+			frame-type = <0>;
+			spv-mask = <0x7F>;
+			spv = <0x08>;       /*  */
+			ppri-mask = <0x0>;
+			ppri = <0x0>;
+			pvid-mask = <0x0>;
+			pvid = <0x0>;
+			next-id = <0>;
+
+			dpv-override;
+			dpv-data = <0x40>;  /* frame forwarding to Port 6 -> CPU  */
+
+			action-override;
+			action-data = <0x0>;
+
+			frame-data = [
+				01 0E CF 00 00 01 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				];
+
+			frame-data-mask = [
+				FF FF FF FF FF FF 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				];
+		};
+
+		/* ethX11: forwarding all frames to CPU */
+		ethX11_all_forwarding_to_cpu {
+			id = <9>;
+			frame-type-mask = <3>;
+			frame-type = <0>;
+			spv-mask = <0x7F>;
+			spv = <0x08>;       /* use tcam entry for Port 3 -> ethX11 */
+			ppri-mask = <0x0>;
+			ppri = <0x0>;
+			pvid-mask = <0x0>;
+			pvid = <0x0>;
+			next-id = <0>;
+
+			dpv-override;
+			dpv-data = <0x40>;  /* frame forwarding to Port 6 -> CPU */
+
+			action-override;
+			action-data = <0x0>;
+
+			frame-data = [
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				];
+
+			frame-data-mask = [
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				];
+		};
+
+		/* ethX12: forwarding all frames to CPU */
+		ethX12_all_forwarding_to_cpu {
+			id = <10>;
+			frame-type-mask = <3>;
+			frame-type = <0>;
+			spv-mask = <0x7F>;
+			spv = <0x10>;       /* use tcam entry for Port 4 -> ethX12 */
+			ppri-mask = <0x0>;
+			ppri = <0x0>;
+			pvid-mask = <0x0>;
+			pvid = <0x0>;
+			next-id = <0>;
+
+			dpv-override;
+			dpv-data = <0x40>;  /* frame forwarding to Port 6 -> CPU */
+
+			action-override;
+			action-data = <0x0>;
+
+			frame-data = [
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				];
+
+			frame-data-mask = [
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				];
+		};
+
+		/* Forward DLR beacon frames
+		 */
+		dlr_forward_beacon_ethX11_to_ethX12 {
+			id = <11>;
+			frame-type-mask = <3>;
+			frame-type = <0>;
+			spv-mask = <0x7F>;
+			spv = <0x08>;         /* switch port 3 -> ethX11 */
+			ppri-mask = <0x0>;
+			ppri = <0x0>;
+			pvid-mask = <0x0>;
+			pvid = <0x0>;
+			next-id = <0>;
+
+			dpv-override;
+			dpv-data = <0x10>;  /* forward to Port 4 -> ethX12 */
+
+			action-override;
+			action-data = <0x0>;
+
+			frame-data = [
+				01 21 6C 00 00 01 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				];
+
+			frame-data-mask = [
+				FF FF FF FF FF FF 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				];
+		};
+
+		dlr_forward_beacon_ethX12_to_ethX11 {
+			id = <12>;
+			frame-type-mask = <3>;
+			frame-type = <0>;
+			spv-mask = <0x7F>;
+			spv = <0x10>;         /* switch port 4 -> ethX12 */
+			ppri-mask = <0x0>;
+			ppri = <0x0>;
+			pvid-mask = <0x0>;
+			pvid = <0x0>;
+			next-id = <0>;
+
+			dpv-override;
+			dpv-data = <0x08>;  /* forward to port 3 -> ethX11 */
+
+			action-override;
+			action-data = <0x0>;
+
+			frame-data = [
+				01 21 6C 00 00 01 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				];
+
+			frame-data-mask = [
+				FF FF FF FF FF FF 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				];
+		};
+
+		/* DLR Peer-to-Peer frames are only forwarded to CPU port
+		 */
+		dlr_p2p_all_to_cpu {
+			id = <13>;
+			frame-type-mask = <3>;
+			frame-type = <0>;
+			spv-mask = <0x40>; /* All but CPU port */
+			spv = <0x0>;
+			ppri-mask = <0x0>;
+			ppri = <0x0>;
+			pvid-mask = <0x0>;
+			pvid = <0x0>;
+			next-id = <0>;
+
+			dpv-override;
+			dpv-data = <0x40>;  /* frame forwarding to Port 6 -> CPU */
+
+			action-override;
+			action-data = <0x0>;
+
+			frame-data = [
+				01 21 6C 00 00 02 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				];
+
+			frame-data-mask = [
+				FF FF FF FF FF FF 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				];
+		};
+
+		/* DLR Gateway ethX11 to ethX12
+		 */
+		dlr_gateway_ethX11_to_ethX12 {
+			id = <14>;
+			frame-type-mask = <3>;
+			frame-type = <0>;
+			spv-mask = <0x7F>;
+			spv = <0x08>;         /* switch port 3 -> ethX11 */
+			ppri-mask = <0x0>;
+			ppri = <0x0>;
+			pvid-mask = <0x0>;
+			pvid = <0x0>;
+			next-id = <0>;
+
+			dpv-override;
+			dpv-data = <0x10>;  /* forward to Port 4 -> ethX12 */
+
+			action-override;
+			action-data = <0x0>;
+
+			frame-data = [
+				01 21 6C 00 00 04 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				];
+
+			frame-data-mask = [
+				FF FF FF FF FF FF 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				];
+		};
+
+		/* DLR Gateway frames from ethX12 to ethX11
+		 */
+		dlr_gateway_ethX12_to_ethX11 {
+			id = <15>;
+			frame-type-mask = <3>;
+			frame-type = <0>;
+			spv-mask = <0x7F>;
+			spv = <0x10>;         /* switch port 4 -> ethX12 */
+			ppri-mask = <0x0>;
+			ppri = <0x0>;
+			pvid-mask = <0x0>;
+			pvid = <0x0>;
+			next-id = <0>;
+
+			dpv-override;
+			dpv-data = <0x08>;  /* forward to port 3 -> ethX11 */
+
+			action-override;
+			action-data = <0x0>;
+
+			frame-data = [
+				01 21 6C 00 00 04 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				];
+
+			frame-data-mask = [
+				FF FF FF FF FF FF 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+				];
+		};
+	};
+};
diff --git a/arch/arm/boot/dts/am335x-pfc-750_8215.dts b/arch/arm/boot/dts/am335x-pfc-750_8215.dts
new file mode 100644
index 000000000000..87bedfd7d2ef
--- /dev/null
+++ b/arch/arm/boot/dts/am335x-pfc-750_8215.dts
@@ -0,0 +1,215 @@
+/*
+ * Copyright (C) 2019 WAGO Kontakttechnik GmbH & Co. KG - https://www.wago.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+/dts-v1/;
+
+#include "am335x-pfc-750_821x.dtsi"
+#include "am335x-pfc-750_82xx-dcan.dtsi"
+#include "am335x-pfc-750_821x-mv88e6321.dtsi"
+#include "am335x-pfc-750_8215-tcam.dtsi"
+#include "wago-devconf.dtsi"
+
+/{
+	model = "WAGO PFC200 750-8215";
+	compatible = "wago,am335x-pfc-750_821x-0182", "wago,am335x-pfc", "ti,am33xx";
+};
+
+&aes {
+	status = "okay";
+};
+
+&cppi41dma {
+	status = "okay";
+};
+
+&dcan0 {
+	status = "okay";
+};
+
+&elm {
+	status = "okay";
+};
+
+&gpmc {
+	status = "okay";
+};
+
+&i2c0 {
+	status = "okay";
+};
+
+&kbus0 {
+	status = "okay";
+};
+
+&mac {
+	status = "okay";
+};
+
+&mmc1 {
+	status = "okay";
+};
+
+&mmc2 {
+	status = "okay";
+};
+
+&oms {
+	status = "okay";
+};
+
+&sham {
+	status = "okay";
+};
+
+&spi0 {
+	status = "okay";
+};
+
+&uart0 {
+	status = "okay";
+};
+
+&uart5 {
+	status = "okay";
+};
+
+&usb {
+	status = "okay";
+};
+
+&usb_ctrl_mod {
+	status = "okay";
+};
+
+&usb1_phy {
+	status = "okay";
+};
+
+&usb1 {
+	pinctrl-names = "default";
+	pinctr-0 = <&usb1_pins>;
+
+	dr_mode = "host";
+	status = "okay";
+};
+
+&watchdog {
+	status = "okay";
+};
+
+&wsysinit {
+	tty,rs232-485 = "nop";
+	profinet-capable;
+};
+
+&am33xx_pinmux {
+	ext_phy0_pins: pinmux_ext_phy0_pins {
+		pinctrl-single,pins = <
+			AM33XX_GPMC_WPN_GPIO0_31(PIN_INPUT)
+		>;
+	};
+
+	ext_phy1_pins: pinmux_ext_phy1_pins {
+		pinctrl-single,pins = <
+			AM33XX_MII1_RX_ER_GPIO3_2(PIN_INPUT)
+		>;
+	};
+
+	usb1_pins: pinmux_usb0_pins {
+		pinctrl-single,pins = <
+			AM33XX_USB1_DRVVBUS(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+		>;
+	};
+};
+
+&swcfg_mv88e6321 {
+	status = "okay";
+};
+
+&mv88e6321_switch {
+	eeprom = /bits/ 8 <0x20 0x80 0x09 0x5E 0x40 0x80 0x09 0x5E 0x01 0x7D 0x01 0x7D 0x01 0x7D 0x01 0x7D
+			   0xF9 0x7F 0x40 0x19 0xF8 0x7F 0x60 0x94 0xF9 0x7F 0x40 0x19 0xF8 0x7F 0x80 0x94
+			   0xF9 0x7F 0x40 0x19 0xF8 0x7F 0x00 0x94 0xF9 0x7F 0x40 0x19 0xF8 0x7F 0x20 0x94
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF>;
+
+	mdio {
+		ext_phy0: ethernet-phy@0 {
+			compatible = "ethernet-phy-idD565.A401", "ethernet-phy-ieee802.3-c22";
+			pinctrl-names = "default";
+			pinctrl-0 = <&ext_phy0_pins>;
+			reg = <0>;
+			max-speed = <100>;
+			interrupt-parent = <&gpio0>;
+			interrupts = <31 IRQ_TYPE_EDGE_FALLING>;
+		};
+
+		ext_phy1: ethernet-phy@1 {
+			compatible = "ethernet-phy-idD565.A401", "ethernet-phy-ieee802.3-c22";
+			pinctrl-names = "default";
+			pinctrl-0 = <&ext_phy1_pins>;
+			reg = <1>;
+			max-speed = <100>;
+			interrupt-parent = <&gpio3>;
+			interrupts = <2 IRQ_TYPE_EDGE_FALLING>;
+		};
+	};
+
+	ports {
+		port@0 {
+			reg = <0>;
+			label = "ethX1";
+			phy-mode = "sgmii";
+			phy-handle = <&ext_phy0>;
+			phy-external;
+		};
+
+		port@1 {
+			reg = <1>;
+			label = "ethX2";
+			phy-mode = "sgmii";
+			phy-handle = <&ext_phy1>;
+			phy-external;
+		};
+	};
+};
+
+&lb61_0 {
+	label = "bf-red";
+};
+
+&lb61_1 {
+	label = "bf-green";
+};
+
+&lb61_2 {
+	label = "dia-red";
+};
+
+&lb61_3 {
+	label = "dia-green";
+};
+
+&lb60_10 {
+	label = "can-red";
+};
+
+&lb60_11 {
+	label = "can-green";
+};
diff --git a/arch/arm/boot/dts/am335x-pfc-750_8216.dts b/arch/arm/boot/dts/am335x-pfc-750_8216.dts
new file mode 100644
index 000000000000..ed951e3304cb
--- /dev/null
+++ b/arch/arm/boot/dts/am335x-pfc-750_8216.dts
@@ -0,0 +1,120 @@
+/*
+ * Copyright (C) 2019 WAGO Kontakttechnik GmbH & Co. KG - https://www.wago.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+/dts-v1/;
+
+#include "am335x-pfc-750_821x.dtsi"
+#include "am335x-pfc-750_8xxx-ksz8863.dtsi"
+#include "am335x-pfc-750_8xxx-uart1.dtsi"
+#include "am335x-pfc-750_82xx-dcan.dtsi"
+#include "am335x-pfc-750_821x-dpc31.dtsi"
+#include "wago-devconf.dtsi"
+
+/ {
+	model = "WAGO PFC200 750-8216";
+	compatible = "wago,am335x-pfc-750_821x-000f", "wago,am335x-pfc", "ti,am33xx";
+};
+
+&aes {
+	status = "okay";
+};
+
+&bitbang_mdio0 {
+	status = "okay";
+};
+
+&dcan0 {
+	status = "okay";
+};
+
+&elm {
+	status = "okay";
+};
+
+&gpmc {
+	status = "okay";
+};
+
+&i2c0 {
+	status = "okay";
+};
+
+&kbus0 {
+	status = "okay";
+};
+
+&ksz8863_switch {
+	status = "okay";
+};
+
+&mac {
+	status = "okay";
+};
+
+&mmc1 {
+	status = "okay";
+};
+
+&mmc2 {
+	status = "okay";
+};
+
+&oms {
+	status = "okay";
+};
+
+&sham {
+	status = "okay";
+};
+
+&spi0 {
+	status = "okay";
+};
+
+&swcfg_ksz8863 {
+	status = "okay";
+};
+
+&uart0 {
+	status = "okay";
+};
+
+&uart1 {
+	status = "okay";
+};
+
+&uart5 {
+	status = "okay";
+};
+
+&watchdog {
+	status = "okay";
+};
+
+&lb61_0 {
+	label = "bf-red";
+};
+
+&lb61_1 {
+	label = "bf-green";
+};
+
+&lb61_2 {
+	label = "dia-red";
+};
+
+&lb61_3 {
+	label = "dia-green";
+};
+
+&lb60_10 {
+	label = "can-red";
+};
+
+&lb60_11 {
+	label = "can-green";
+};
diff --git a/arch/arm/boot/dts/am335x-pfc-750_8217.dts b/arch/arm/boot/dts/am335x-pfc-750_8217.dts
new file mode 100644
index 000000000000..816243982d45
--- /dev/null
+++ b/arch/arm/boot/dts/am335x-pfc-750_8217.dts
@@ -0,0 +1,186 @@
+/*
+ * Copyright (C) 2019 WAGO Kontakttechnik GmbH & Co. KG - https://www.wago.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+/dts-v1/;
+
+#include "am335x-pfc-750_821x.dtsi"
+#include "am335x-pfc-750_8xxx-ksz8863.dtsi"
+#include "am335x-pfc-750_8xxx-uart1.dtsi"
+#include "wago-devconf.dtsi"
+
+/ {
+	model = "WAGO PFC200 750-8217";
+	compatible = "wago,am335x-pfc-750_821x-1006", "wago,am335x-pfc", "ti,am33xx";
+};
+
+&aes {
+	status = "okay";
+};
+
+&bitbang_mdio0 {
+	status = "okay";
+};
+
+&cppi41dma {
+	status = "okay";
+};
+
+&elm {
+	status = "okay";
+};
+
+&gpmc {
+	status = "okay";
+};
+
+&i2c0 {
+	status = "okay";
+};
+
+&kbus0 {
+	status = "okay";
+};
+
+&ksz8863_switch {
+	status = "okay";
+};
+
+&lb61_0 {
+	label = "s4-red";
+};
+
+&lb61_1 {
+	label = "s4-green";
+};
+
+&lb61_2 {
+	label = "s3-red";
+};
+
+&lb61_3 {
+	label = "s3-green";
+};
+
+&lb61_4 {
+	label = "s2-red";
+};
+
+&lb61_5 {
+	label = "s2-green";
+};
+
+&lb61_6 {
+	label = "s1-red";
+};
+
+&lb61_7 {
+	label = "s1-green";
+};
+
+&lb61_8 {
+	label = "wds-red";
+};
+
+&lb61_9 {
+	label = "wds-green";
+};
+
+&lb61_10 {
+	label = "net-red";
+};
+
+&lb61_11 {
+	label = "net-green";
+};
+
+&lb60_10 {
+	label = "usr-red";
+};
+
+&lb60_11 {
+	label = "usr-green";
+};
+
+&mac {
+	status = "okay";
+};
+
+&mmc1 {
+	status = "okay";
+};
+
+&mmc2 {
+	status = "okay";
+};
+
+&oms {
+	status = "okay";
+};
+
+&sham {
+	status = "okay";
+};
+
+&spi0 {
+	status = "okay";
+};
+
+&swcfg_ksz8863 {
+	status = "okay";
+};
+
+&uart0 {
+	status = "okay";
+};
+
+&uart1 {
+	status = "okay";
+};
+
+&uart5 {
+	status = "okay";
+};
+
+&usb {
+	status = "okay";
+};
+
+&usb_ctrl_mod {
+	status = "okay";
+};
+
+&usb0_phy {
+	status = "disabled";
+};
+
+&usb1_phy {
+	status = "okay";
+};
+
+&usb0 {
+	status = "disabled";
+};
+
+&usb1 {
+	pinctrl-names = "default";
+	pinctr-0 = <&usb1_pins>;
+
+	status = "okay";
+	dr_mode = "host";
+};
+
+&watchdog {
+	status = "okay";
+};
+
+&am33xx_pinmux {
+	usb1_pins: pinmux_usb1_pins {
+		pinctrl-single,pins = <
+			AM33XX_USB1_DRVVBUS(PIN_OUTPUT_PULLUP | SLEWCTRL_SLOW)
+		>;
+	};
+};
diff --git a/arch/arm/boot/dts/am335x-pfc-750_821x-dpc31.dtsi b/arch/arm/boot/dts/am335x-pfc-750_821x-dpc31.dtsi
new file mode 100644
index 000000000000..53f0e6d1734a
--- /dev/null
+++ b/arch/arm/boot/dts/am335x-pfc-750_821x-dpc31.dtsi
@@ -0,0 +1,80 @@
+/*
+ * Copyright (C) 2019 WAGO Kontakttechnik GmbH & Co. KG - https://www.wago.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+/ {
+	UIO_DPC31_XINT@0x1000000 {
+		compatible = "uio_pdrv_genirq";
+		reg = <0x4000000 0x4000>; /* 8k * 2 = 16k, needed because of 16Bit addressing */
+		interrupt-parent = <&gpio0>;
+		interrupts = <6 IRQ_TYPE_EDGE_FALLING>;
+	};
+
+	UIO_DPC31_SYNC@0x1000000 {
+		compatible = "uio_pdrv_genirq";
+		interrupt-parent = <&gpio1>;
+		interrupts = <25 IRQ_TYPE_EDGE_RISING>;
+	};
+};
+
+&am33xx_pinmux {
+	dpc31_pins: pinmux_dpc31_pins {
+		pinctrl-single,pins = <
+			AM33XX_SPI0_CS1_GPIO0_6(PIN_INPUT) /* SPI0_CS1/GPIO0_6 | DIRQ-FB-ASIC */
+			AM33XX_GPMC_CSN0(PIN_OUTPUT_PULLUP | SLEWCTRL_SLOW) /* GPMC_CSN0/GPIO1_29 | Chip-Select 0 */
+			AM33XX_GPMC_A9_GPIO1_25(PIN_INPUT) /* RGMII2_RD2/GPIO1_25 | DPSYNC */
+		>;
+	};
+};
+
+&wsysinit {
+	pinctrl-names = "default";
+	pinctrl-0 = <&dpc31_pins>;
+
+	dp,reset;
+	dp,gpio-rst = <&io_expander_70 1 GPIO_ACTIVE_HIGH>; /* dpc31 reset */
+};
+
+&gpmc {
+	dpc31: nor@x0,0 {
+		reg = <0 0 0x1000000>; /* 8k * 2 = 16k, needed because of 16Bit addressing
+					* minimum is 16M - set it here */
+		bank-width = <2>; /* 1: 8bit, 2: 16bit */
+		gpmc,sync-clk-ps = <0>;
+
+		gpmc,cs-on-ns = <10>;
+		gpmc,cs-rd-off-ns = <140>;
+		gpmc,cs-wr-off-ns = <100>;
+
+		gpmc,adv-on-ns = <0>;
+		gpmc,adv-rd-off-ns = <0>;
+		gpmc,adv-wr-off-ns = <0>;
+
+		gpmc,oe-on-ns = <10>;
+		gpmc,oe-off-ns = <130>;
+
+		gpmc,we-on-ns = <1>;
+		gpmc,we-off-ns = <110>;
+
+		gpmc,rd-cycle-ns = <140>;
+		gpmc,wr-cycle-ns = <120>;
+
+		gpmc,access-ns = <120>;
+		gpmc,wr-access-ns = <10>;
+
+		gpmc,wr-data-mux-bus-ns = <0>;
+
+		gpmc,cycle2cycle-delay-ns = <0>;
+
+		gpmc,mux-add-data = <0>; /* non multiplexing mode */
+
+		gpmc,clk-activation-ns = <0>;
+
+		#address-cells = <1>;
+		#size-cells = <1>;
+	};
+};
diff --git a/arch/arm/boot/dts/am335x-pfc-750_821x-mv88e6321.dtsi b/arch/arm/boot/dts/am335x-pfc-750_821x-mv88e6321.dtsi
new file mode 100644
index 000000000000..890bc5ede465
--- /dev/null
+++ b/arch/arm/boot/dts/am335x-pfc-750_821x-mv88e6321.dtsi
@@ -0,0 +1,125 @@
+/*
+ * Copyright (C) 2019 WAGO Kontakttechnik GmbH & Co. KG - https://www.wago.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+/ {
+	swcfg_mv88e6321: swcfg_mv88e6321 {
+		compatible = "swcfg,mv88e6321";
+
+		swcfg,mii-bus = <&davinci_mdio>;
+
+		swcfg,alias = "mv88e6321";
+		swcfg,cpu_port = <5>;
+		swcfg,ports = <7>;
+		swcfg,vlans = <1>;
+		swcfg,switch = <&mv88e6321_switch>;
+
+		status = "disabled";
+	};
+};
+
+&am33xx_pinmux {
+	davinci_mdio_default_pins: pinmux_davinci_mdio_default_pins {
+		pinctrl-single,pins = <
+			AM33XX_MDIO_DATA(PIN_INPUT_PULLUP | SLEWCTRL_FAST)
+			AM33XX_MDC_MDIO_CLK(PIN_OUTPUT_PULLUP)
+		>;
+	};
+
+	davinci_mdio_sleep_pins: pinmux_davinci_mdio_sleep_pins {
+		pinctrl-single,pins = <
+			AM33XX_MDIO_GPIO0_0(PIN_INPUT_PULLDOWN)
+			AM33XX_MDC_GPIO0_1(PIN_INPUT_PULLDOWN)
+		>;
+	};
+
+	switch0_pins: pinmux_switch0_pins {
+		pinctrl-single,pins = <
+			AM33XX_LCD_DATA15_GPIO0_11(PIN_INPUT)
+		>;
+	};
+};
+
+&davinci_mdio {
+	pinctrl-names = "default", "sleep";
+	pinctrl-0 = <&davinci_mdio_default_pins>;
+	pinctrl-1 = <&davinci_mdio_sleep_pins>;
+	status = "okay";
+
+	mv88e6321_switch: switch@0 {
+		eeprom-length = <256>;
+
+		compatible = "marvell,mv88e6085";
+		pinctrl-names = "default";
+		pinctrl-0 = <&switch0_pins>;
+
+		#address-cells = <1>;
+		#size-cells = <0>;
+
+		reg = <0>;
+		dsa,member = <0 0>;
+		phy-scan;
+
+		interrupt-controller;
+		#interrupt-cells = <2>;
+		reset-gpios = <&io_expander_70 4 GPIO_ACTIVE_LOW>;
+		interrupt-parent = <&gpio0>;
+		interrupts = <11 IRQ_TYPE_LEVEL_LOW>;
+
+		mdio {
+			#address-cells = <1>;
+			#size-cells = <0>;
+
+			int_phy3: ethernet-phy@3 {
+				compatible = "ethernet-phy-id0141.0C00", "ethernet-phy-ieee802.3-c22";
+				reg = <3>;
+				max-speed = <100>;
+				interrupt-parent = <&mv88e6321_switch>;
+				interrupts = <3 IRQ_TYPE_LEVEL_HIGH>;
+				eee-broken-100tx;
+				eee-broken-1000t;
+			};
+
+			int_phy4: ethernet-phy@4 {
+				compatible = "ethernet-phy-id0141.0C00", "ethernet-phy-ieee802.3-c22";
+				reg = <4>;
+				max-speed = <100>;
+				interrupt-parent = <&mv88e6321_switch>;
+				interrupts = <4 IRQ_TYPE_LEVEL_HIGH>;
+				eee-broken-100tx;
+				eee-broken-1000t;
+			};
+		};
+
+		ports {
+			#address-cells = <1>;
+			#size-cells = <0>;
+
+			port@3 {
+				reg = <3>;
+				label = "ethX11";
+				phy-handle = <&int_phy3>;
+			};
+
+			port@4 {
+				reg = <4>;
+				label = "ethX12";
+				phy-handle = <&int_phy4>;
+			};
+
+			port@6 {
+				reg = <6>;
+				label = "cpu";
+				ethernet = <&mac>;
+				fixed-link {
+					speed = <100>;
+					full-duplex;
+				};
+			};
+		};
+	};
+};
diff --git a/arch/arm/boot/dts/am335x-pfc-750_821x.dtsi b/arch/arm/boot/dts/am335x-pfc-750_821x.dtsi
new file mode 100644
index 000000000000..107b9088b96a
--- /dev/null
+++ b/arch/arm/boot/dts/am335x-pfc-750_821x.dtsi
@@ -0,0 +1,381 @@
+/*
+ * Copyright (C) 2019 WAGO Kontakttechnik GmbH & Co. KG - https://www.wago.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include "am335x-pfc.dtsi"
+#include "am335x-pfc-750_8xxx-leds.dtsi"
+#include "am335x-pfc-750_8xxx-nvram.dtsi"
+#include "am335x-pfc-750_8xxx-kbus.dtsi"
+
+/ {
+	memory {
+		device_type = "memory";
+		reg = <0x80000000 0x20000000>; /* 512 MB */
+	};
+};
+
+&cpu_0 {
+	operating-points = <
+		1000000  1325000
+		 800000  1260000
+		 720000  1200000
+		 600000  1100000
+		 300000  950000
+	>;
+};
+
+&am33xx_pinmux {
+	pinctrl-names = "default";
+	pinctrl-0 = <
+		&gpio0_pins
+		&gpio1_pins
+	>;
+
+	watchdog_pins: pinmux_watchdog_pins {
+		pinctrl-single,pins = <
+			AM33XX_EMU1_GPIO3_8(PIN_OUTPUT_PULLDOWN)
+		>;
+	};
+
+	uart1_pins: pinmux_uart1_pins {
+		pinctrl-single,pins = <
+			AM33XX_UART1_TXD(PIN_OUTPUT_PULLUP | SLEWCTRL_SLOW)
+			AM33XX_UART1_RXD(PIN_INPUT_PULLUP)
+			AM33XX_UART1_CTSN(PIN_INPUT_PULLUP)
+			AM33XX_UART1_RTSN(PIN_OUTPUT_PULLUP | SLEWCTRL_SLOW)
+			AM33XX_MCASP0_FSR_GPIO3_19(PIN_OUTPUT_PULLUP | SLEWCTRL_SLOW)
+		>;
+	};
+
+	uart5_pins: pinmux_uart5_pins {
+		pinctrl-single,pins = <
+			AM33XX_MII1_RX_DV_UART5_TXD(PIN_OUTPUT | SLEWCTRL_SLOW)
+			AM33XX_LCD_DATA14_UART5_RXD(PIN_INPUT_PULLUP)
+		>;
+	};
+
+	spi0_pins: pinmux_spi0_pins {
+		pinctrl-single,pins = <
+			AM33XX_SPI0_SCLK(PIN_INPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_SPI0_D0(PIN_INPUT)
+			AM33XX_SPI0_D1(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_SPI0_CS0(PIN_OUTPUT_PULLUP | SLEWCTRL_SLOW)
+		>;
+
+	};
+
+	spi1_pins: pinmux_spi1_pins {
+		pinctrl-single,pins = <
+			AM33XX_UART0_CTSN_SPI1_D0(PIN_INPUT)
+			AM33XX_UART0_RTSN_SPI1_D1(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_MCASP0_ACLKX_SPI1_SCLK(PIN_INPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_ECAP0_IN_PWM0_OUT_SPI1_CS1(PIN_OUTPUT_PULLUP | SLEWCTRL_SLOW)
+		>;
+	};
+
+	sdcard_cd_pins: pinmux_sdcard_cd_pins {
+		pinctrl-single,pins = <
+			AM33XX_GPMC_A3_GPIO1_19(PIN_INPUT_PULLUP)
+		>;
+	};
+
+	sdcard_wp_pins: pinmux_sdcard_wp_pins {
+		pinctrl-single,pins = <
+			AM33XX_GPMC_ADVN_ALE_GPIO2_2(PIN_INPUT_PULLUP)
+		>;
+	};
+
+	emmc_pins: pinmux_emmc_pins {
+		pinctrl-single,pins = <
+			AM33XX_GPMC_CSN1_MMC1_CLK(PIN_INPUT_PULLUP | SLEWCTRL_FAST)
+			AM33XX_GPMC_CSN2_MMC1_CMD(PIN_INPUT_PULLUP | SLEWCTRL_FAST)
+			AM33XX_MII1_TX_CLK_MMC1_DAT0(PIN_INPUT_PULLUP | SLEWCTRL_FAST)
+			AM33XX_MII1_RX_CLK_MMC1_DAT1(PIN_INPUT_PULLUP | SLEWCTRL_FAST)
+			AM33XX_MII1_RXD3_MMC1_DAT2(PIN_INPUT_PULLUP | SLEWCTRL_FAST)
+			AM33XX_MII1_RXD2_MMC1_DAT3(PIN_INPUT_PULLUP | SLEWCTRL_FAST)
+		>;
+	};
+
+	gpmc_data_pins: pinmux_gpmc_data_pins {
+		pinctrl-single,pins = <
+			AM33XX_GPMC_AD0(PIN_INPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_GPMC_AD1(PIN_INPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_GPMC_AD2(PIN_INPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_GPMC_AD3(PIN_INPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_GPMC_AD4(PIN_INPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_GPMC_AD5(PIN_INPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_GPMC_AD6(PIN_INPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_GPMC_AD7(PIN_INPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_GPMC_AD8(PIN_INPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_GPMC_AD9(PIN_INPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_GPMC_AD10(PIN_INPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_GPMC_AD11(PIN_INPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_GPMC_AD12(PIN_INPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_GPMC_AD13(PIN_INPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_GPMC_AD14(PIN_INPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_GPMC_AD15(PIN_INPUT_PULLDOWN | SLEWCTRL_SLOW)
+		>;
+	};
+
+	gpmc_addr_pins: pinmux_gpmc_addr_pins {
+		pinctrl-single,pins = <
+			AM33XX_LCD_DATA0_GPMC_A0(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_DATA1_GPMC_A1(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_DATA2_GPMC_A2(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_DATA3_GPMC_A3(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_DATA4_GPMC_A4(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_DATA5_GPMC_A5(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_DATA6_GPMC_A6(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_DATA7_GPMC_A7(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_VSYNC_GPMC_A8(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_HSYNC_GPMC_A9(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_PCLK_GPMC_A10(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_AC_BIAS_EN_GPMC_A11(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_DATA8_GPMC_A12(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_DATA9_GPMC_A13(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_DATA10_GPMC_A14(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_DATA11_GPMC_A15(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_DATA12_GPMC_A16(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_DATA13_GPMC_A17(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+		>;
+	};
+
+	gpmc_ctrl_pins: pinmux_gpmc_ctrl_pins {
+		pinctrl-single,pins = <
+			/* GPMC CONTROL */
+			AM33XX_GPMC_OEN_REN(PIN_OUTPUT_PULLUP | SLEWCTRL_SLOW)
+			AM33XX_GPMC_WEN(PIN_OUTPUT_PULLUP | SLEWCTRL_SLOW)
+			AM33XX_GPMC_WAIT0_GPMC_CSN4(PIN_OUTPUT_PULLUP | SLEWCTRL_SLOW)
+			AM33XX_GPMC_BEN0_CLE(PIN_OUTPUT_PULLUP | SLEWCTRL_SLOW)
+			AM33XX_GPMC_BEN1(PIN_OUTPUT_PULLUP | SLEWCTRL_SLOW)
+			AM33XX_GPMC_CLK_GPMC_WAIT1(PIN_INPUT_PULLUP)
+		>;
+	};
+
+	rmii2_pins: pinmux_rmii2_pins {
+		pinctrl-single,pins = <
+			/* RMII 2 */
+			AM33XX_GPMC_A0_RMII2_TXEN(PIN_OUTPUT | SLEWCTRL_FAST)
+			AM33XX_GPMC_A4_RMII2_TXD1(PIN_OUTPUT | SLEWCTRL_FAST)
+			AM33XX_GPMC_A5_RMII2_TXD0(PIN_OUTPUT | SLEWCTRL_FAST)
+			AM33XX_GPMC_A10_RMII2_RXD1(PIN_INPUT)
+			AM33XX_GPMC_A11_RMII2_RXD0(PIN_INPUT)
+			AM33XX_MII1_COL_RMII2_REFCLK(PIN_INPUT)
+			AM33XX_GPMC_CSN3_RMII2_CRS_DV(PIN_INPUT)
+		>;
+	};
+
+	rmii2_sleep_pins: pinmux_rmii2_sleep_pins {
+		pinctrl-single,pins = <
+			/* RMII 2 */
+			AM33XX_GPMC_A0_RMII2_TXEN(PIN_OUTPUT | SLEWCTRL_FAST)
+			AM33XX_GPMC_A4_RMII2_TXD1(PIN_OUTPUT | SLEWCTRL_FAST)
+			AM33XX_GPMC_A5_RMII2_TXD0(PIN_OUTPUT | SLEWCTRL_FAST)
+			AM33XX_GPMC_A10_RMII2_RXD1(PIN_INPUT)
+			AM33XX_GPMC_A11_RMII2_RXD0(PIN_INPUT)
+			AM33XX_MII1_COL_RMII2_REFCLK(PIN_INPUT)
+			AM33XX_GPMC_CSN3_RMII2_CRS_DV(PIN_INPUT)
+		>;
+	};
+
+	rmii1_pins: pinmux_rmii1_pins {
+		pinctrl-single,pins = <
+			/* RMII1 could used on Marvell based devices only; default off*/
+			AM33XX_RMII1_REF_CLK_GPIO0_29(PIN_INPUT)
+			AM33XX_MII1_TX_EN_GPIO3_3(PIN_INPUT)
+			AM33XX_MII1_TXD0_GPIO0_28(PIN_INPUT)
+			AM33XX_MII1_TXD1_GPIO0_21(PIN_INPUT)
+			AM33XX_MII1_RXD0_GPIO2_21(PIN_INPUT)
+			AM33XX_MII1_RXD1_GPIO2_20(PIN_INPUT)
+			AM33XX_MII1_CRS_GPIO3_1(PIN_INPUT)
+		>;
+	};
+
+	rmii1_sleep_pins: pinmux_rmii1_sleep_pins {
+		pinctrl-single,pins = <
+			/* RMII1 could used on Marvell based devices only; default off*/
+			AM33XX_RMII1_REF_CLK_GPIO0_29(PIN_INPUT)
+			AM33XX_MII1_TX_EN_GPIO3_3(PIN_INPUT)
+			AM33XX_MII1_TXD0_GPIO0_28(PIN_INPUT)
+			AM33XX_MII1_TXD1_GPIO0_21(PIN_INPUT)
+			AM33XX_MII1_RXD0_GPIO2_21(PIN_INPUT)
+			AM33XX_MII1_RXD1_GPIO2_20(PIN_INPUT)
+			AM33XX_MII1_CRS_GPIO3_1(PIN_INPUT)
+		>;
+	};
+
+	kbus_pins: pinmux_kbus_pins {
+		pinctrl-single,pins = <
+			AM33XX_GPMC_A6_GPIO1_22(PIN_INPUT_PULLUP)
+			AM33XX_GPMC_A7_GPIO1_23(PIN_INPUT_PULLUP)
+			AM33XX_XDMA_EVENT_INTR1_GPIO0_20(PIN_INPUT_PULLUP)
+			AM33XX_EMU0_GPIO3_7(PIN_OUTPUT_PULLUP | SLEWCTRL_SLOW)
+			AM33XX_GPMC_A8_GPIO1_24(PIN_INPUT_PULLUP)
+		>;
+	};
+
+	gpio0_pins: pinmux_gpio0_pins {
+		pinctrl-single,pins = <
+			AM33XX_XDMA_EVENT_INTR0_GPIO0_19(PIN_OUTPUT_PULLUP | SLEWCTRL_SLOW)
+		>;
+	};
+
+	gpio1_pins: pinmux_gpio1_pins {
+		pinctrl-single,pins = <
+			AM33XX_GPMC_A1_GPIO1_17(PIN_INPUT_PULLUP)
+			AM33XX_GPMC_A2_GPIO1_18(PIN_OUTPUT_PULLUP | SLEWCTRL_SLOW)
+		>;
+	};
+};
+
+&wsysinit {
+	board,variant = "pfc200v3";
+};
+
+&watchdog {
+	pinctrl-names = "default";
+	pinctrl-0 = <&watchdog_pins>;
+
+	en-gpios = <&io_expander_70 0 GPIO_ACTIVE_LOW>;
+	gpios = <&gpio3 8 GPIO_ACTIVE_LOW>;
+};
+
+&gpmc {
+	pinctrl-names = "default";
+	pinctrl-0 = <&gpmc_data_pins>, <&gpmc_addr_pins>, <&gpmc_ctrl_pins>;
+
+	num-cs = <7>;	/* only two devices */
+	num-waitpins = <3>;
+	ranges = <
+		4 0 0x01000000 0x01000000	/* CS4: NVRAM, 16M */
+		0 0 0x04000000 0x01000000	/* CS0: NETX/DPC31, 16M */
+	>;
+
+	nvram: nor@4,0 { /* NVRAM Device 128k */
+		reg = <4 0 0x01000000>;
+		bank-width = <2>; /* 1: 8bit, 2: 16bit */
+		gpmc,sync-clk-ps = <0>;
+		gpmc,cs-on-ns = <0>;
+		gpmc,cs-rd-off-ns = <50>;
+		gpmc,cs-wr-off-ns = <30>;
+		gpmc,adv-on-ns = <0>;
+		gpmc,adv-rd-off-ns = <0>;
+		gpmc,adv-wr-off-ns = <0>;
+		gpmc,oe-on-ns = <0>;
+		gpmc,oe-off-ns = <50>;
+		gpmc,we-on-ns = <0>;
+		gpmc,we-off-ns = <30>;
+		gpmc,rd-cycle-ns = <50>;
+		gpmc,wr-cycle-ns = <30>;
+		gpmc,access-ns = <40>;
+		gpmc,wr-access-ns = <10>;
+		gpmc,wr-data-mux-bus-ns = <0>;
+		gpmc,cycle2cycle-delay-ns = <10>;
+		gpmc,cycle2cycle-samecsen;
+		gpmc,cycle2cycle-diffcsen;
+
+		#address-cells = <1>;
+		#size-cells = <1>;
+	};
+};
+
+&uart5 { /* KBUS Firmware download */
+	pinctrl-names = "default";
+	pinctrl-0 = <&uart5_pins>;
+};
+
+&kbus0 { /* KBUS on cs0 */
+	pinctrl-names = "default";
+	pinctrl-0 = <&kbus_pins>;
+
+	/*
+	 * announce used tty-device in /sys/class/wago/system.
+	 * is needed to link this device to /dev/ttyKbus
+	 */
+	kbus,tty-device = "ttyO5"; /* corresponds to &uart5*/
+
+	/* some should be active low, keep all to high for compatibility reasons */
+	kbus,nrst-gpio   = <&io_expander_70 3 GPIO_ACTIVE_LOW>;
+	kbus,nsync-gpio  = <&gpio1 22 GPIO_ACTIVE_HIGH>;
+	kbus,cmdsel-gpio = <&gpio3 7 GPIO_ACTIVE_HIGH>;
+	kbus,nirq-gpio   = <&gpio1 23 GPIO_ACTIVE_HIGH>;
+	kbus,nerr-gpio   = <&gpio1 24 GPIO_ACTIVE_HIGH>;
+	kbus,nrdy-gpio   = <&gpio0 20 GPIO_ACTIVE_HIGH>;
+};
+
+&i2c0 {
+	clock-frequency = <100000>;
+
+	io_expander_70: pca9554@70 { /* IO Port Expander only outputs*/
+		compatible = "nxp,pca9554";
+		#address-cells = <1>;
+		#size-cells = <0>;
+		#gpio-cells = <2>;
+		gpio-controller;
+		reg = <0x70>;
+	};
+};
+
+&lb60_10 {
+	label = "u7-red";
+};
+
+&lb60_11 {
+	label = "u7-green";
+};
+
+&rtc_i2c {
+	trim-data = <0 1 31>;
+};
+
+&tps {
+	interrupt-parent = <&gpio3>;
+	interrupts = <20 IRQ_TYPE_LEVEL_LOW>;
+};
+
+&mac {
+	pinctrl-names = "default", "sleep";
+	pinctrl-0 = <
+		&rmii2_pins
+		&rmii1_pins
+	>;
+	pinctrl-1 = <
+		&rmii2_sleep_pins
+		&rmii1_sleep_pins
+	>;
+	mac_control = <0x18021>; /* force 100MBit full-duplex */
+	active_slave = <1>;
+};
+
+&mmc1 {
+	pinctrl-names = "default";
+	pinctrl-0 = <
+		&mmc1_pins
+		&sdcard_cd_pins
+		&sdcard_wp_pins
+	>;
+
+	cd-debounce-delay-ms = <7>;
+	cd-gpios = <&gpio1 19 GPIO_ACTIVE_LOW>;
+};
+
+&mmc2 {
+	pinctrl-names = "default";
+	pinctrl-0 = <&emmc_pins>;
+	vmmc-supply = <&dcdc4_reg>;
+	bus-width = <4>;
+	ti,non-removable;
+};
+
+&gpio1 {
+	boot_select {
+		gpio-hog;
+		gpios = <18 GPIO_ACTIVE_LOW>;
+		output-high;
+		line-name = "boot_select";
+	};
+};
diff --git a/arch/arm/boot/dts/am335x-pfc-750_82xx-dcan.dtsi b/arch/arm/boot/dts/am335x-pfc-750_82xx-dcan.dtsi
new file mode 100644
index 000000000000..b64292406747
--- /dev/null
+++ b/arch/arm/boot/dts/am335x-pfc-750_82xx-dcan.dtsi
@@ -0,0 +1,23 @@
+/*
+ * Copyright (C) 2019 WAGO Kontakttechnik GmbH & Co. KG - https://www.wago.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include "am33xx-pinfunc.h"
+
+&am33xx_pinmux {
+	dcan0_pins: pinmux_dcan0_pins {
+		pinctrl-single,pins = <
+			AM33XX_MII1_TXD3_DCAN0_TX(PIN_OUTPUT_PULLDOWN)
+			AM33XX_MII1_TXD2_DCAN0_RX(PIN_INPUT_PULLDOWN)
+		>;
+	};
+};
+
+&dcan0 {
+	pinctrl-names = "default";
+	pinctrl-0 = <&dcan0_pins>;
+};
diff --git a/arch/arm/boot/dts/am335x-pfc-750_8xxx-kbus.dtsi b/arch/arm/boot/dts/am335x-pfc-750_8xxx-kbus.dtsi
new file mode 100644
index 000000000000..44529c853c50
--- /dev/null
+++ b/arch/arm/boot/dts/am335x-pfc-750_8xxx-kbus.dtsi
@@ -0,0 +1,26 @@
+/*
+ * Copyright (C) 2019 WAGO Kontakttechnik GmbH & Co. KG - https://www.wago.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+&spi0 {
+	spi-rt;
+	spi-rt-prio = <81>;
+
+	kbus0: kbus@0 {
+		compatible = "wago,spi-kbus";
+		reg = <0>;
+		spi-max-frequency = <10000000>;
+		kbus,dma-boost;
+		kbus,dma-boost-prio = <85>;
+		kbus,dma-boost-irq-thread = "49000000";
+		kbus,dma-default-prio = <50>;
+
+		kbus,reset-on-boot;
+
+		status = "disabled";
+	};
+};
diff --git a/arch/arm/boot/dts/am335x-pfc-750_8xxx-ksz8863.dtsi b/arch/arm/boot/dts/am335x-pfc-750_8xxx-ksz8863.dtsi
new file mode 100644
index 000000000000..9dd9ddfdd345
--- /dev/null
+++ b/arch/arm/boot/dts/am335x-pfc-750_8xxx-ksz8863.dtsi
@@ -0,0 +1,66 @@
+/*
+ * Copyright (C) 2019 WAGO Kontakttechnik GmbH & Co. KG - https://www.wago.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+
+/ {
+	swcfg_ksz8863: swcfg_ksz8863 {
+		compatible = "swcfg,ksz8863";
+		swcfg,mii-bus = <&bitbang_mdio0>;
+		swcfg,alias = "ksz8863";
+		swcfg,cpu_port = <2>;
+		swcfg,ports = <3>;
+		swcfg,vlans = <16>;
+		swcfg,switch = <&ksz8863_switch>;
+
+		status = "disabled";
+	};
+};
+
+&bitbang_mdio0 {
+	ksz8863_switch: switch@0 {
+		compatible = "micrel,ksz8863";
+
+		#address-cells = <1>;
+		#size-cells = <0>;
+
+		ksz,reset-gpio = <&io_expander_70 4 GPIO_ACTIVE_LOW>;
+
+		reg = <0>;
+		dsa,member = <0 0>;
+
+		ksz,reset-switch;
+		ksz,disable-internal-ldo;
+
+		status = "disabled";
+
+		ports {
+			#address-cells = <1>;
+			#size-cells = <0>;
+
+			port@0 {
+				reg = <2>;
+				label = "ethX1";
+			};
+
+			port@1 {
+				reg = <1>;
+				label = "ethX2";
+			};
+
+			port@2 {
+				reg = <3>;
+				label = "cpu";
+				ethernet = <&mac>;
+				fixed-link {
+					speed = <100>;
+					full-duplex;
+				};
+			};
+		};
+	};
+};
diff --git a/arch/arm/boot/dts/am335x-pfc-750_8xxx-leds.dtsi b/arch/arm/boot/dts/am335x-pfc-750_8xxx-leds.dtsi
new file mode 100644
index 000000000000..1d01ac6f78fd
--- /dev/null
+++ b/arch/arm/boot/dts/am335x-pfc-750_8xxx-leds.dtsi
@@ -0,0 +1,156 @@
+/*
+ * Copyright (C) 2019 WAGO Kontakttechnik GmbH & Co. KG - https://www.wago.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+&i2c0 {
+	pinctrl-names = "default";
+	pinctrl-0 = <&i2c0_pins>;
+
+	clock-frequency = <400000>;
+
+	led_bar60: pca9552@60 {
+		compatible = "nxp,pca9552";
+		#address-cells = <1>;
+		#size-cells = <0>;
+		reg = <0x60>;
+
+		lb60_0: sys-red@0 {
+			label = "sys-red";
+			reg = <0>;
+			linux,default-trigger = "timer";
+		};
+
+		lb60_1: sys-green@1 {
+			label = "sys-green";
+			reg = <1>;
+			linux,default-trigger = "timer";
+		};
+
+		lb60_2: run-red@2 {
+			label = "run-red";
+			reg = <2>;
+		};
+
+		lb60_3: run-green@3 {
+			label = "run-green";
+			reg = <3>;
+		};
+
+		lb60_4: io-red@4 {
+			label = "io-red";
+			reg = <4>;
+		};
+
+		lb60_5: io-green@5 {
+			label = "io-green";
+			reg = <5>;
+		};
+
+		lb60_6: ms-red@6 {
+			label = "ms-red";
+			reg = <6>;
+		};
+
+		lb60_7: ms-green@7 {
+			label = "ms-green";
+			reg = <7>;
+		};
+
+		lb60_8: ns-red@8 {
+			label = "ns-red";
+			reg = <8>;
+		};
+
+		lb60_9: ns-green@9 {
+			label = "ns-green";
+			reg = <9>;
+		};
+
+		lb60_10: usr_r@10 {
+			label = "usr-red";
+			reg = <10>;
+		};
+
+		lb60_11: usr_g@11 {
+			label = "usr-green";
+			reg = <11>;
+		};
+
+		lb60_15: sys-over-red@15 {
+			label = "sys-over-red";
+			reg = <15>;
+			linux,default-trigger = "default-on";
+		};
+	};
+
+	led_bar61: pca9552@61 {
+		compatible = "nxp,pca9552";
+		#address-cells = <1>;
+		#size-cells = <0>;
+		reg = <0x61>;
+
+		lb61_0: u6-red@0 {
+			label = "u6-red";
+			reg = <0>;
+		};
+
+		lb61_1: u6-green@1 {
+			label = "u6-green";
+			reg = <1>;
+		};
+
+		lb61_2: u5-red@2 {
+			label = "u5-red";
+			reg = <2>;
+		};
+
+		lb61_3: u5-green@3 {
+			label = "u5-green";
+			reg = <3>;
+		};
+
+		lb61_4: u4-red@4 {
+			label = "u4-red";
+			reg = <4>;
+		};
+
+		lb61_5: u4-green@5 {
+			label = "u4-green";
+			reg = <5>;
+		};
+
+		lb61_6: u3-red@6 {
+			label = "u3-red";
+			reg = <6>;
+		};
+
+		lb61_7: u3-green@7 {
+			label = "u3-green";
+			reg = <7>;
+		};
+
+		lb61_8: u2-red@8 {
+			label = "u2-red";
+			reg = <8>;
+		};
+
+		lb61_9: u2-green@9 {
+			label = "u2-green";
+			reg = <9>;
+		};
+
+		lb61_10: u1-red@10 {
+			label = "u1-red";
+			reg = <10>;
+		};
+
+		lb61_11: u1-green@11 {
+			label = "u1-green";
+			reg = <11>;
+		};
+	};
+};
diff --git a/arch/arm/boot/dts/am335x-pfc-750_8xxx-nvram.dtsi b/arch/arm/boot/dts/am335x-pfc-750_8xxx-nvram.dtsi
new file mode 100644
index 000000000000..e7df8856e75c
--- /dev/null
+++ b/arch/arm/boot/dts/am335x-pfc-750_8xxx-nvram.dtsi
@@ -0,0 +1,14 @@
+/*
+ * Copyright (C) 2019 WAGO Kontakttechnik GmbH & Co. KG - https://www.wago.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+/ {
+	uio_nvram: UIO_NVRAM@0x1000000 {
+		compatible = "uio_pdrv_genirq";
+		reg = <0x01000000 0x00020000>; /* 128k 16bit NVRAM */
+	};
+};
diff --git a/arch/arm/boot/dts/am335x-pfc-750_8xxx-uart1-two-wire.dtsi b/arch/arm/boot/dts/am335x-pfc-750_8xxx-uart1-two-wire.dtsi
new file mode 100644
index 000000000000..92e96eaf5870
--- /dev/null
+++ b/arch/arm/boot/dts/am335x-pfc-750_8xxx-uart1-two-wire.dtsi
@@ -0,0 +1,24 @@
+/*
+ * Copyright (C) 2019 WAGO Kontakttechnik GmbH & Co. KG - https://www.wago.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include "am33xx-pinfunc.h"
+
+&am33xx_pinmux {
+	uart1_pins: pinmux_uart1_pins {
+		pinctrl-single,pins = <
+			AM33XX_UART1_TXD(PIN_OUTPUT | SLEWCTRL_SLOW)
+			AM33XX_UART1_RXD(PIN_INPUT)
+		>;
+	};
+};
+
+&uart1 {
+	compatible = "ti,omap3-uart";
+	pinctrl-names = "default";
+	pinctrl-0 = <&uart1_pins>;
+};
diff --git a/arch/arm/boot/dts/am335x-pfc-750_8xxx-uart1.dtsi b/arch/arm/boot/dts/am335x-pfc-750_8xxx-uart1.dtsi
new file mode 100644
index 000000000000..4987769dbc22
--- /dev/null
+++ b/arch/arm/boot/dts/am335x-pfc-750_8xxx-uart1.dtsi
@@ -0,0 +1,32 @@
+/*
+ * Copyright (C) 2019 WAGO Kontakttechnik GmbH & Co. KG - https://www.wago.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include "am33xx-pinfunc.h"
+
+&am33xx_pinmux {
+	uart1_pins: pinmux_uart1_pins {
+		pinctrl-single,pins = <
+			AM33XX_UART1_TXD(PIN_OUTPUT | SLEWCTRL_SLOW)
+			AM33XX_UART1_RXD(PIN_INPUT)
+			AM33XX_UART1_CTSN(PIN_INPUT)
+			AM33XX_UART1_RTSN(PIN_OUTPUT | SLEWCTRL_SLOW)
+			AM33XX_MCASP0_FSR_GPIO3_19(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+		>;
+	};
+};
+
+&uart1 {
+	compatible = "ti,omap3-uart-rtu";
+	pinctrl-names = "default";
+	pinctrl-0 = <&uart1_pins>;
+
+	rs485en-gpio = <&gpio3 19 GPIO_ACTIVE_LOW>;
+	rs485-rts-active-high;
+
+	rs485-rts-delay = <1 1>;
+};
diff --git a/arch/arm/boot/dts/am335x-pfc-750_8xxx-uart4.dtsi b/arch/arm/boot/dts/am335x-pfc-750_8xxx-uart4.dtsi
new file mode 100644
index 000000000000..6359a99132ea
--- /dev/null
+++ b/arch/arm/boot/dts/am335x-pfc-750_8xxx-uart4.dtsi
@@ -0,0 +1,23 @@
+/*
+ * Copyright (C) 2019 WAGO Kontakttechnik GmbH & Co. KG - https://www.wago.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include "am33xx-pinfunc.h"
+
+&am33xx_pinmux {
+	uart4_pins: pinmux_uart4_pins {
+		pinctrl-single,pins = <
+			AM33XX_UART0_CTSN_UART4_RXD(PIN_INPUT_PULLUP)
+			AM33XX_UART0_RTSN_UART4_TXD(PIN_OUTPUT_PULLUP | SLEWCTRL_SLOW)
+		>;
+	};
+};
+
+&uart4 {
+	pinctrl-names = "default";
+	pinctrl-0 = <&uart4_pins>;
+};
diff --git a/arch/arm/boot/dts/am335x-pfc-768_3301.dts b/arch/arm/boot/dts/am335x-pfc-768_3301.dts
new file mode 100644
index 000000000000..f68926553b1b
--- /dev/null
+++ b/arch/arm/boot/dts/am335x-pfc-768_3301.dts
@@ -0,0 +1,171 @@
+/*
+ * Copyright (C) 2019 WAGO Kontakttechnik GmbH & Co. KG - https://www.wago.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+/dts-v1/;
+
+#include "am335x-pfc-768_330x.dtsi"
+
+/{
+	model = "WAGO PFC200 768-3301";
+	compatible = "wago,am335x-pfc-768_330x-3005", "wago,am335x-pfc", "ti,am33xx";
+};
+
+&encsw {
+	status ="okay";
+};
+
+&aes {
+	status = "okay";
+};
+
+&cppi41dma {
+	status = "okay";
+};
+
+&elm {
+	status = "okay";
+};
+
+&oms {
+	status = "okay";
+};
+
+&oms_reset_all {
+	gpios = <&gpio3 15 GPIO_ACTIVE_LOW>;
+};
+
+&oms_stop {
+	gpios = <&gpio2 23 GPIO_ACTIVE_LOW>;
+};
+
+&sham {
+	status = "okay";
+};
+
+&uart0 {
+	status = "okay";
+};
+
+&usb {
+	status = "okay";
+};
+
+&usb_ctrl_mod {
+	status = "okay";
+};
+
+&usb0_phy {
+	status = "okay";
+};
+
+&usb1_phy {
+	status = "disabled";
+};
+
+&usb0 {
+	dr_mode = "otg";
+	status = "okay";
+};
+
+&usb1 {
+	status = "disabled";
+};
+
+&spi1 {
+	status = "okay";
+
+	flash: s25fl016k@1 {
+		#address-cells = <1>;
+		#size-cells = <1>;
+		compatible = "s25fl016k", "jedec,spi-nor";
+		reg = <1>;
+		spi-max-frequency = <20000000>;
+
+		/* This property indicates, that during probing its not possible
+		 * to read out the jedec-id or any other registers (e.g. spi ip
+		 * is pinmuxed to be offline during boot up because of an
+		 * concurrent external spi master). Instead assume that the
+		 * user configures the correctly used device in the compatible
+		 * string and prepares the environment correctly when accessing
+		 * the device.
+		 */
+		spi-nor,offline-probe;
+
+		partition@0 {
+			label = "firmware";
+			reg = <0x0 0x200000>;
+		};
+	};
+};
+
+&watchdog {
+	status = "okay";
+};
+
+&i2c0 {
+	status = "okay";
+};
+
+&swcfg_mv88e6321 {
+	status = "okay";
+};
+
+&mac {
+	status = "okay";
+};
+
+&mmc1 {
+	status = "okay";
+};
+
+&mmc2 {
+	status = "okay";
+};
+
+&gpmc {
+	status = "okay";
+};
+
+&fpga {
+	status = "okay";
+};
+
+&rmd {
+	status = "okay";
+
+	pinctrl-names = "default", "sleep", "active";
+	pinctrl-1 = <&spi1_sleep_pins>;
+	pinctrl-2 = <&spi1_pins>;
+
+	nrst-gpios = <&gpio2 17 GPIO_ACTIVE_HIGH>;
+	nce-gpios = <&gpio0 30 GPIO_ACTIVE_HIGH>;
+	nconfig-gpios = <&gpio0 16 GPIO_ACTIVE_LOW>;
+};
+
+&uio_rmd_mem {
+	status = "okay";
+};
+
+&uio_rmd_irq0 {
+	status = "okay";
+};
+
+&spi0 {
+	status = "okay";
+
+	spi-sram@0 {
+		#address-cells = <1>;
+		#size-cells = <1>;
+		compatible = "anvo-systems,anv32aa1w";
+		reg = <0>;
+		spi-max-frequency = <40000000>;
+	};
+};
+
+&cc_logic_usb_c {
+	status = "okay";
+};
diff --git a/arch/arm/boot/dts/am335x-pfc-768_330x-rlb.dtsi b/arch/arm/boot/dts/am335x-pfc-768_330x-rlb.dtsi
new file mode 100644
index 000000000000..1832a931c919
--- /dev/null
+++ b/arch/arm/boot/dts/am335x-pfc-768_330x-rlb.dtsi
@@ -0,0 +1,167 @@
+/*
+ * Copyright (C) 2020 WAGO Kontakttechnik GmbH & Co. KG - https://www.wago.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+/ {
+	uio_rmd_mem: uio_rmd_mem@1000000 {
+		status = "disabled";
+		compatible = "uio_pdrv_genirq";
+		reg = <0x01000000 0x01000000>;
+	};
+
+	uio_rmd_irq0: uio_rmd_irq0 {
+		status = "disabled";
+		compatible = "uio_pdrv_genirq";
+		interrupts = <7>;
+		interrupt-parent = <&intc>;
+	};
+
+	uio_rmd_irq1: uio_rmd_irq1 {
+		status = "disabled";
+		compatible = "uio_pdrv_genirq";
+		interrupts = <25 IRQ_TYPE_EDGE_RISING>;
+		interrupt-parent = <&gpio2>;
+	};
+
+	rmd: rmd {
+		status = "disabled";
+		compatible = "wago,rmd";
+		pinctrl-names = "default";
+		pinctrl-0 = <&rmd_pins>, <&spi1_sleep_pins>;
+
+		rmd,fifo-size = <0x1000>;
+		reg = <0x01000000 0x01000000>;
+
+		/*
+		 * Crossbar Mapped Channels
+		 *
+		 * 29: pi_x_dma_event_intr1
+		 * 30: pi_x_dma_event_intr2
+		 *
+		 * Map 30 and 29 to open channels 12 and 13
+		 *
+		 */
+		dmas = <&edma_xbar 12 0 30>, <&edma_xbar 13 0 29>;
+		dma-names = "rx", "tx";
+
+		interrupts = <13 IRQ_TYPE_EDGE_RISING>;
+		interrupt-parent = <&gpio2>;
+	};
+};
+
+&am33xx_pinmux {
+	gpmc_data_pins: pinmux_gpmc_data_pins {
+		pinctrl-single,pins = <
+			AM33XX_GPMC_AD0(PIN_INPUT_PULLDOWN  | SLEWCTRL_SLOW)
+			AM33XX_GPMC_AD1(PIN_INPUT_PULLDOWN  | SLEWCTRL_SLOW)
+			AM33XX_GPMC_AD2(PIN_INPUT_PULLDOWN  | SLEWCTRL_SLOW)
+			AM33XX_GPMC_AD3(PIN_INPUT_PULLDOWN  | SLEWCTRL_SLOW)
+			AM33XX_GPMC_AD4(PIN_INPUT_PULLDOWN  | SLEWCTRL_SLOW)
+			AM33XX_GPMC_AD5(PIN_INPUT_PULLDOWN  | SLEWCTRL_SLOW)
+			AM33XX_GPMC_AD6(PIN_INPUT_PULLDOWN  | SLEWCTRL_SLOW)
+			AM33XX_GPMC_AD7(PIN_INPUT_PULLDOWN  | SLEWCTRL_SLOW)
+			AM33XX_GPMC_AD8(PIN_INPUT_PULLDOWN  | SLEWCTRL_SLOW)
+			AM33XX_GPMC_AD9(PIN_INPUT_PULLDOWN  | SLEWCTRL_SLOW)
+			AM33XX_GPMC_AD10(PIN_INPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_GPMC_AD11(PIN_INPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_GPMC_AD12(PIN_INPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_GPMC_AD13(PIN_INPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_GPMC_AD14(PIN_INPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_GPMC_AD15(PIN_INPUT_PULLDOWN | SLEWCTRL_SLOW)
+		>;
+	};
+
+	gpmc_addr_pins: pinmux_gpmc_addr_pins {
+		pinctrl-single,pins = <
+			AM33XX_LCD_VSYNC_GPMC_A8(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+		>;
+	};
+
+	gpmc_ctrl_pins: pinmux_gpmc_ctrl_pins {
+		pinctrl-single,pins = <
+			AM33XX_GPMC_OEN_REN(PIN_OUTPUT_PULLUP  | SLEWCTRL_FAST)
+			AM33XX_GPMC_WEN(PIN_OUTPUT_PULLUP      | SLEWCTRL_FAST)
+			AM33XX_GPMC_CSN0(PIN_OUTPUT_PULLUP     | SLEWCTRL_SLOW)
+			AM33XX_GPMC_BEN0_CLE(PIN_OUTPUT_PULLUP | SLEWCTRL_FAST)
+			AM33XX_GPMC_BEN1(PIN_OUTPUT_PULLUP     | SLEWCTRL_FAST)
+			AM33XX_GPMC_CLK_GPMC_WAIT1(PIN_INPUT_PULLUP)
+			AM33XX_GPMC_ADVN_ALE(PIN_OUTPUT)
+		>;
+	};
+
+	fpga_pins: pinmux_fpga_pins {
+		pinctrl-single,pins = <
+			AM33XX_MII1_TXD3_GPIO0_16(PIN_OUTPUT_PULLUP) /* nCONFIG */
+			AM33XX_LCD_DATA10_GPIO2_16(PIN_INPUT) /* CONFIG-DONE */
+			AM33XX_LCD_PCLK_GPIO2_24(PIN_INPUT) /* nSTATUS */
+			AM33XX_LCD_DATA11_GPIO2_17(PIN_OUTPUT) /* nRST */
+			AM33XX_GPMC_WAIT0_GPIO0_30(PIN_OUTPUT) /* nCE */
+			AM33XX_MII1_TXD2_GPIO0_17(PIN_INPUT) /* INIT-DONE */
+		>;
+	};
+
+	rmd_pins: pinmux_rmd_pins {
+		pinctrl-single,pins = <
+			AM33XX_GPMC_A7_GPIO1_23(PIN_INPUT) /* DBG0 */
+			AM33XX_GPMC_A6_GPIO1_22(PIN_INPUT) /* DBG3 */
+
+			AM33XX_LCD_DATA7_GPIO2_13(PIN_INPUT) /* SYNC1 */
+			AM33XX_LCD_DATA8_GPIO2_14(PIN_INPUT) /* SYNC2 */
+			AM33XX_LCD_DATA9_GPIO2_15(PIN_INPUT) /* SYNC3 */
+
+			AM33XX_LCD_DATA6_GPIO2_12(PIN_INPUT) /* DMA2-RMD */
+			AM33XX_LCD_AC_BIAS_EN_GPIO2_25(PIN_INPUT) /* IRQ1-RMD */
+			AM33XX_XDMA_EVENT_INTR1(PIN_INPUT) /* DMA1-RMD */
+			AM33XX_SPI0_CS1_XDMA_EVENT_INTR2(PIN_INPUT) /* DMA0-RMD */
+		>;
+	};
+};
+
+&gpmc {
+	status = "disabled";
+
+	pinctrl-names = "default";
+	pinctrl-0 = <&gpmc_data_pins>, <&gpmc_addr_pins>, <&gpmc_ctrl_pins>;
+
+	ranges = <0 0 0x01000000 0x01000000>;
+
+	fpga: fpga@0,0 {
+		status = "disabled";
+
+		reg = <0 0 0x01000000>;
+		bank-width = <2>; /* 16 Bit */
+
+		gpmc,sync-clk-ps = <0>;
+		gpmc,cs-on-ns = <0>;
+		gpmc,cs-rd-off-ns = <80>;
+		gpmc,cs-wr-off-ns = <70>;
+
+		gpmc,adv-on-ns = <0>;
+		gpmc,adv-rd-off-ns = <10>;
+		gpmc,adv-wr-off-ns = <10>;
+
+		gpmc,we-on-ns = <20>;
+		gpmc,we-off-ns = <70>;
+		gpmc,oe-on-ns = <10>;
+		gpmc,oe-off-ns = <80>;
+		gpmc,oe-extra-delay;
+
+		gpmc,access-ns = <80>;
+		gpmc,rd-cycle-ns = <100>;
+		gpmc,wr-cycle-ns = <90>;
+		gpmc,page-burst-access-ns = <0>;
+
+		gpmc,bus-turnaround-ns  = <0>;
+		gpmc,wr-access-ns = <70>;
+		gpmc,wr-data-mux-bus-ns = <20>;
+
+		gpmc,wait-pin = <1>;
+		gpmc,wait-on-read;
+		gpmc,wait-on-write;
+		gpmc,mux-add-data = <2>; /* address-data multiplexing */
+	};
+};
diff --git a/arch/arm/boot/dts/am335x-pfc-768_330x.dtsi b/arch/arm/boot/dts/am335x-pfc-768_330x.dtsi
new file mode 100644
index 000000000000..0389482aeb34
--- /dev/null
+++ b/arch/arm/boot/dts/am335x-pfc-768_330x.dtsi
@@ -0,0 +1,358 @@
+/*
+ * Copyright (C) 2019 WAGO Kontakttechnik GmbH & Co. KG - https://www.wago.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include "am335x-pfc.dtsi"
+#include "am335x-pfc-750_8xxx-leds.dtsi"
+#include "am335x-pfc-750_821x-mv88e6321.dtsi"
+#include "am335x-pfc-768_330x-rlb.dtsi"
+#include "wago-devconf.dtsi"
+
+/ {
+	memory {
+		device_type = "memory";
+		reg = <0x80000000 0x20000000>; /* 512 MB */
+	};
+
+	encsw: dip-switch {
+		status = "disabled";
+		compatible = "encsw";
+		encsw-gpios = <
+			&io_expander_71 0 GPIO_ACTIVE_LOW
+			&io_expander_71 1 GPIO_ACTIVE_LOW
+			&io_expander_71 2 GPIO_ACTIVE_LOW
+			&io_expander_71 3 GPIO_ACTIVE_LOW
+			&io_expander_71 4 GPIO_ACTIVE_LOW
+			&io_expander_71 5 GPIO_ACTIVE_LOW
+			&io_expander_71 6 GPIO_ACTIVE_LOW
+			&io_expander_71 7 GPIO_ACTIVE_LOW
+			>;
+	};
+};
+
+&cpu_0 {
+	operating-points = <
+		1000000  1325000
+		 800000  1260000
+		 720000  1200000
+		 600000  1100000
+		 300000  950000
+	>;
+};
+
+&am33xx_pinmux {
+	pinctrl-names = "default";
+	pinctrl-0 = <
+		&gpio0_pins
+		&gpio1_pins
+		&gpio2_pins
+	>;
+
+	watchdog_pins: pinmux_watchdog_pins {
+		pinctrl-single,pins = <
+			AM33XX_EMU1_GPIO3_8(PIN_OUTPUT_PULLDOWN)
+		>;
+	};
+
+	spi1_pins: pinmux_spi1_pins {
+		pinctrl-single,pins = <
+			AM33XX_UART0_CTSN_SPI1_D0(PIN_INPUT)
+			AM33XX_UART0_RTSN_SPI1_D1(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_MCASP0_ACLKX_SPI1_SCLK(PIN_INPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_ECAP0_IN_PWM0_OUT_SPI1_CS1(PIN_OUTPUT_PULLUP | SLEWCTRL_SLOW)
+		>;
+	};
+
+	spi1_sleep_pins: pinmux_spi1_sleep_pins {
+		pinctrl-single,pins = <
+			AM33XX_UART0_CTSN_GPIO1_8(PIN_INPUT_PULLDOWN)
+			AM33XX_UART0_RTSN_GPIO1_9(PIN_INPUT_PULLDOWN)
+			AM33XX_MCASP0_ACLKX_GPIO3_14(PIN_INPUT_PULLDOWN)
+			AM33XX_ECAP0_IN_PWM0_OUT_GPIO0_7(PIN_INPUT_PULLDOWN)
+		>;
+	};
+
+	mmc1_cd_pins: pinmux_sdcard_cd_pins {
+		pinctrl-single,pins = <
+			AM33XX_GPMC_A3_GPIO1_19(PIN_INPUT_PULLUP)
+		>;
+	};
+
+	emmc_pins: pinmux_emmc_pins {
+		pinctrl-single,pins = <
+			AM33XX_GPMC_CSN1_MMC1_CLK(PIN_INPUT_PULLUP | SLEWCTRL_FAST)
+			AM33XX_GPMC_CSN2_MMC1_CMD(PIN_INPUT_PULLUP | SLEWCTRL_FAST)
+			AM33XX_MII1_TX_CLK_MMC1_DAT0(PIN_INPUT_PULLUP | SLEWCTRL_FAST)
+			AM33XX_MII1_RX_CLK_MMC1_DAT1(PIN_INPUT_PULLUP | SLEWCTRL_FAST)
+			AM33XX_MII1_RXD3_MMC1_DAT2(PIN_INPUT_PULLUP | SLEWCTRL_FAST)
+			AM33XX_MII1_RXD2_MMC1_DAT3(PIN_INPUT_PULLUP | SLEWCTRL_FAST)
+		>;
+	};
+
+	rmii2_pins: pinmux_rmii2_pins {
+		pinctrl-single,pins = <
+			AM33XX_GPMC_A0_RMII2_TXEN(PIN_OUTPUT | SLEWCTRL_FAST)
+			AM33XX_GPMC_A4_RMII2_TXD1(PIN_OUTPUT | SLEWCTRL_FAST)
+			AM33XX_GPMC_A5_RMII2_TXD0(PIN_OUTPUT | SLEWCTRL_FAST)
+			AM33XX_GPMC_A10_RMII2_RXD1(PIN_INPUT)
+			AM33XX_GPMC_A11_RMII2_RXD0(PIN_INPUT)
+			AM33XX_MII1_COL_RMII2_REFCLK(PIN_INPUT)
+			AM33XX_GPMC_CSN3_RMII2_CRS_DV(PIN_INPUT)
+		>;
+	};
+
+	rmii2_sleep_pins: pinmux_rmii2_sleep_pins {
+		pinctrl-single,pins = <
+			AM33XX_GPMC_A0_GPIO1_16(PIN_INPUT_PULLDOWN)
+			AM33XX_GPMC_A4_GPIO1_20(PIN_INPUT_PULLDOWN)
+			AM33XX_GPMC_A5_GPIO1_21(PIN_INPUT_PULLDOWN)
+			AM33XX_GPMC_A10_GPIO1_26(PIN_INPUT_PULLDOWN)
+			AM33XX_GPMC_A11_GPIO1_27(PIN_INPUT_PULLDOWN)
+			AM33XX_MII1_COL_GPIO3_0(PIN_INPUT_PULLDOWN)
+			AM33XX_GPMC_CSN3_GPIO2_0(PIN_INPUT_PULLDOWN)
+		>;
+	};
+
+	rmii1_sleep_pins: pinmux_rmii1_sleep_pins {
+		pinctrl-single,pins = <
+			/* all pins are hard-wired to GROUND */
+			AM33XX_RMII1_REF_CLK_GPIO0_29(PIN_INPUT)
+			AM33XX_MII1_TX_EN_GPIO3_3(PIN_INPUT)
+			AM33XX_MII1_TXD0_GPIO0_28(PIN_INPUT)
+			AM33XX_MII1_TXD1_GPIO0_21(PIN_INPUT)
+			AM33XX_MII1_RXD0_GPIO2_21(PIN_INPUT)
+			AM33XX_MII1_RXD1_GPIO2_20(PIN_INPUT)
+			AM33XX_MII1_CRS_GPIO3_1(PIN_INPUT)
+			AM33XX_MII1_RX_ER_GPIO3_2(PIN_INPUT)
+		>;
+	};
+
+	gpio2_pins: pinmux_gpio2_pins {
+		pinctrl-single,pins = <
+			AM33XX_LCD_HSYNC_GPIO2_23(PIN_INPUT) /* BAS Stop */
+		>;
+	};
+
+	gpio1_pins: pinmux_gpio1_pins {
+		pinctrl-single,pins = <
+			AM33XX_GPMC_A2_GPIO1_18(PIN_OUTPUT_PULLUP | SLEWCTRL_SLOW)
+			AM33XX_GPMC_A8_GPIO1_24(PIN_INPUT) /* DIAG-UF1 */
+			AM33XX_GPMC_A1_GPIO1_17(PIN_INPUT) /* DIAG-US1 */
+			AM33XX_GPMC_A9_GPIO1_25(PIN_INPUT) /* nINT-DIP */
+		>;
+	};
+
+	gpio0_pins: pinmux_gpio0_pins {
+		pinctrl-single,pins = <
+			AM33XX_XDMA_EVENT_INTR0_GPIO0_19(PIN_INPUT) /* nRST-LEDServer */
+		>;
+	};
+
+	ptn5150_pins: pinmux_ptn5150_pins {
+		pinctrl-single,pins = <
+			AM33XX_GPMC_WPN_GPIO0_31(PIN_INPUT) /* nINT-MCU-CC */
+			/* USB0 VBUS activate */
+			AM33XX_USB0_DRVVBUS_GPIO0_18(PIN_OUTPUT_PULLDOWN)
+		>;
+	};
+
+	fpga_pins: pinmux_fpga_pins {
+		pinctrl-single,pins = <
+			AM33XX_MII1_TXD3_GPIO0_16(PIN_OUTPUT_PULLUP) /* nCONFIG */
+			AM33XX_LCD_DATA10_GPIO2_16(PIN_INPUT) /* CONFIG-DONE */
+			AM33XX_LCD_PCLK_GPIO2_24(PIN_INPUT) /* nSTATUS */
+			AM33XX_LCD_DATA11_GPIO2_17(PIN_OUTPUT) /* nRST */
+			AM33XX_GPMC_WAIT0_GPIO0_30(PIN_OUTPUT) /* nCE */
+			AM33XX_MII1_TXD2_GPIO0_17(PIN_INPUT) /* INIT-DONE */
+		>;
+	};
+};
+
+&wsysinit {
+	board,variant = "pfc200adv";
+	tty,rs232-485 = "nop";
+	tty,service   = "ttyGS0";
+};
+
+&watchdog {
+	pinctrl-names = "default";
+	pinctrl-0 = <&watchdog_pins>;
+
+	en-gpios = <&io_expander_70 0 GPIO_ACTIVE_LOW>;
+	gpios = <&gpio3 8 GPIO_ACTIVE_LOW>;
+};
+
+&mv88e6321_switch {
+	ports {
+		port@3 {
+			label = "ethX1";
+		};
+
+		port@4 {
+			label = "ethX2";
+		};
+	};
+};
+
+&i2c0 {
+	clock-frequency = <100000>;
+
+	/* IO Port Expander outputs only */
+	io_expander_70: pca9538@70 {
+		compatible = "nxp,pca9538";
+		#address-cells = <1>;
+		#size-cells = <0>;
+		#gpio-cells = <2>;
+		gpio-controller;
+		reg = <0x70>;
+	};
+
+	/* IO Port Expander for DIP-switch, inputs only */
+	io_expander_71: pca9538@71 {
+		compatible = "nxp,pca9538";
+		#address-cells = <1>;
+		#size-cells = <0>;
+		reg = <0x71>;
+		#gpio-cells = <2>;
+		gpio-controller;
+	};
+
+	/*
+	 * PTN5150H
+	 * CC logic for USB Type-C applications
+	 * yet no driver configured!
+	 *
+	 * Initially functions in device only mode.
+	 * But it's possible to enable both directions.
+	 *
+	 * Pins: nRST-CC (&io_expander_70 6)  and nINT-CC
+	 */
+	cc_logic_usb_c: cclogic@1e {
+		compatible = "nxp,ptn5150";
+		reg = <0x1e>;
+		int-gpio = <&gpio0 31 GPIO_ACTIVE_HIGH>;
+		vbus-gpio = <&gpio0 18 GPIO_ACTIVE_HIGH>;
+		pinctrl-names = "default";
+		pinctrl-0 = <&ptn5150_pins>;
+	};
+};
+
+/delete-node/ &led_bar61;
+
+&lb60_0 {
+	label = "ds-red";
+};
+
+&lb60_1 {
+	label = "ds-green";
+};
+
+&lb60_2 {
+	label = "ec-red";
+};
+
+&lb60_3 {
+	label = "ec-blue";
+};
+
+&lb60_4 {
+	label = "h11-red";
+};
+
+&lb60_5 {
+	label = "h11-green";
+};
+
+&lb60_6 {
+	label = "h12-red";
+};
+
+&lb60_7 {
+	label = "h12-green";
+};
+
+&lb60_8 {
+	label = "run-red";
+};
+
+&lb60_9 {
+	label = "run-green";
+};
+
+&lb60_10 {
+	label = "app-red";
+};
+
+&lb60_11 {
+	label = "app-green";
+};
+
+&rtc_i2c {
+	trim-data = <0 1 31>;
+};
+
+&tps {
+	interrupt-parent = <&gpio3>;
+	interrupts = <20 IRQ_TYPE_LEVEL_LOW>;
+};
+
+&mac {
+	pinctrl-0 = <
+		&rmii2_pins
+		&rmii1_sleep_pins
+	>;
+
+	pinctrl-1 = <
+		&rmii2_sleep_pins
+	>;
+};
+
+&mmc1 {
+	pinctrl-names = "default";
+	pinctrl-0 = <
+		&mmc1_pins
+		&mmc1_cd_pins
+	>;
+
+	cd-gpios = <&gpio1 19 GPIO_ACTIVE_LOW>;
+};
+
+&mmc2 {
+	pinctrl-names = "default";
+	pinctrl-0 = <&emmc_pins>;
+	vmmc-supply = <&dcdc4_reg>;
+	bus-width = <4>;
+	ti,non-removable;
+};
+
+&gpio1 {
+	boot_select {
+		gpio-hog;
+		gpios = <18 GPIO_ACTIVE_LOW>;
+		output-high;
+		line-name = "boot_select";
+	};
+};
+
+&mv88e6321_switch {
+	eeprom = /bits/ 8 <0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF
+			   0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF 0xFF>;
+};
diff --git a/arch/arm/boot/dts/am335x-pfc.dtsi b/arch/arm/boot/dts/am335x-pfc.dtsi
new file mode 100644
index 000000000000..56270d1ab11e
--- /dev/null
+++ b/arch/arm/boot/dts/am335x-pfc.dtsi
@@ -0,0 +1,341 @@
+/*
+ * Copyright (C) 2019 WAGO Kontakttechnik GmbH & Co. KG - https://www.wago.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <dt-bindings/interrupt-controller/irq.h>
+#include "am33xx.dtsi"
+#include "am33xx-pinfunc.h"
+
+/ {
+	compatible = "wago,am335x-pfc", "ti,am33xx";
+
+	cpus {
+		cpu_0: cpu@0 {
+			cpu0-supply = <&dcdc2_reg>;
+		};
+	};
+
+	/* XE164 +3V3  System voltage */
+	vxe164_reg: fixedregulator@0 {
+		compatible = "regulator-fixed";
+		regulator-name = "vxe164";
+		regulator-min-microvolt = <3300000>;
+		regulator-max-microvolt = <3300000>;
+		vin-supply = <&dcdc4_reg>;
+	};
+
+	/* SD Card +3V3 Systemvoltage over power switch */
+	vsd_reg: fixedregulator@1 {
+		compatible = "regulator-fixed";
+		regulator-name = "v_sd";
+		regulator-min-microvolt = <3300000>;
+		regulator-max-microvolt = <3300000>;
+		vin-supply = <&dcdc4_reg>;
+	};
+
+	wsysinit: wsysinit_init {
+		compatible = "wago,sysinit";
+
+		tty,service   = "ttyO0";
+		tty,rs232-485 = "ttyO1";
+
+		/* sysclock adjustments, empirical values */
+		adjtimex,tick = <10000>;
+		adjtimex,frequency = <200000>;
+	};
+
+	/* this name of the gpio-keys device is a
+	 * historical heritage from 3.6.11 kernel.
+	 * the device-name is checked in omsd. So we
+	 * need to adapt this configuration
+	 */
+	oms: PAC-Operating-Mode-Switch {
+		compatible = "gpio-keys";
+		#address-cells = <1>;
+		#size-cells = <0>;
+		autorepeat;
+		status = "disabled";
+
+		oms_run: run {
+			label = "RUN";
+			gpios = <&gpio3 18 GPIO_ACTIVE_LOW>;
+			linux,code = <1>;
+			linux,input-type = <5>;
+			debounce-interval = <1>;
+		};
+
+		oms_stop: stop {
+			label = "STOP";
+			gpios = <&gpio3 17 GPIO_ACTIVE_LOW>;
+			linux,code = <2>;
+			linux,input-type = <5>;
+			debounce-interval = <1>;
+		};
+
+		oms_reset: reset {
+			label = "RESET";
+			gpios = <&gpio3 16 GPIO_ACTIVE_LOW>;
+			linux,code = <3>;
+			linux,input-type = <1>;
+			debounce-interval = <1>;
+		};
+
+		oms_reset_all: reset_all {
+			label = "RESET_ALL";
+			gpios = <&gpio3 15 GPIO_ACTIVE_HIGH>;
+			linux,code = <4>;
+			linux,input-type = <1>;
+			debounce-interval = <1>;
+		};
+	};
+
+	watchdog: watchdog {
+		/* XC6124 */
+		compatible = "linux,wdt-gpio";
+
+		hw_algo = "toggle";
+		hw_margin_ms = <1600>;
+		rt-prio = <91>;
+
+		status = "disabled";
+	};
+};
+
+&am33xx_pinmux {
+	uart0_pins: pinmux_uart0_pins {
+		pinctrl-single,pins = <
+			AM33XX_UART0_RXD(PIN_INPUT_PULLUP)
+			AM33XX_UART0_TXD(PIN_OUTPUT_PULLUP | SLEWCTRL_SLOW)
+		>;
+	};
+
+	spi0_pins: pinmux_spi0_pins {
+		pinctrl-single,pins = <
+			AM33XX_SPI0_SCLK(PIN_INPUT_PULLUP | SLEWCTRL_SLOW)
+			AM33XX_SPI0_D0(PIN_INPUT_PULLUP)
+			AM33XX_SPI0_D1(PIN_OUTPUT_PULLUP | SLEWCTRL_SLOW)
+			AM33XX_SPI0_CS0(PIN_OUTPUT_PULLUP | SLEWCTRL_SLOW)
+		>;
+	};
+
+	i2c0_pins: pinmux_i2c0_pins {
+		pinctrl-single,pins = <
+			AM33XX_I2C0_SDA(PIN_INPUT_PULLUP | SLEWCTRL_SLOW)
+			AM33XX_I2C0_SCL(PIN_INPUT_PULLUP | SLEWCTRL_SLOW)
+		>;
+	};
+
+	gpio_bitbang_mdio_pins: pinmux_gpio_bitbang_mdio_pins {
+		pinctrl-single,pins = <
+			AM33XX_MDIO_GPIO0_0(PIN_INPUT_PULLUP | SLEWCTRL_FAST)
+			AM33XX_MDC_GPIO0_1(PIN_OUTPUT_PULLUP)
+		>;
+	};
+
+	gpio_bitbang_mdio_sleep_pins: pinmux_gpio_bitbang_mdio_sleep_pins {
+		pinctrl-single,pins = <
+			AM33XX_MDIO_GPIO0_0(PIN_INPUT_PULLUP)
+			AM33XX_MDC_GPIO0_1(PIN_INPUT_PULLDOWN)
+		>;
+	};
+
+	gpio3_pins: pinmux_gpio3_pins {
+		pinctrl-single,pins = <
+			AM33XX_MCASP0_FSX_GPIO3_15(PIN_INPUT)
+			AM33XX_MCASP0_AXR0_GPIO3_16(PIN_INPUT)
+			AM33XX_MCASP0_AHCLKR_GPIO3_17(PIN_INPUT)
+			AM33XX_MCASP0_ACLKR_GPIO3_18(PIN_INPUT)
+			AM33XX_MCASP0_AXR1_GPIO3_20(PIN_INPUT_PULLUP)
+			AM33XX_MCASP0_AHCLKX_GPIO3_21(PIN_INPUT_PULLUP)
+		>;
+	};
+
+	mmc1_pins: pinmux_mmc1_pins {
+		pinctrl-single,pins = <
+			AM33XX_MMC0_CLK(PIN_INPUT_PULLUP | SLEWCTRL_FAST)
+			AM33XX_MMC0_CMD(PIN_INPUT_PULLUP | SLEWCTRL_FAST)
+			AM33XX_MMC0_DAT0(PIN_INPUT_PULLUP | SLEWCTRL_FAST)
+			AM33XX_MMC0_DAT1(PIN_INPUT_PULLUP | SLEWCTRL_FAST)
+			AM33XX_MMC0_DAT2(PIN_INPUT_PULLUP | SLEWCTRL_FAST)
+			AM33XX_MMC0_DAT3(PIN_INPUT_PULLUP | SLEWCTRL_FAST)
+		>;
+	};
+
+
+};
+
+&uart0 {
+	pinctrl-names = "default";
+	pinctrl-0 = <&uart0_pins>;
+	status = "okay";
+};
+
+&spi0 {
+	pinctrl-names = "default";
+	pinctrl-0 = <&spi0_pins>;
+};
+
+&i2c0 {
+	pinctrl-names = "default";
+	pinctrl-0 = <&i2c0_pins>;
+
+	clock-frequency = <400000>;
+
+	eeprom: at24cs128n@54 {
+		compatible = "atmel,24c512";
+		reg = <0x54>;
+
+		pagesize = <128>;
+	};
+
+	rtc_i2c: rtc_r2221t@32 {
+		compatible = "ricoh,r2221tl";
+		reg = <0x32>;
+
+		interrupt-parent = <&gpio3>;
+		interrupts = <21 IRQ_TYPE_LEVEL_LOW>;
+		trim-data = <0 0 13>;
+	};
+
+	tps: tps65218@24 {
+		compatible = "ti,tps65218";
+		reg = <0x24>;
+		interrupt-controller;
+		ti,strict-supply-voltage-supervision = <1>;
+		ti,under-voltage-hyst-microvolt = <400000>;
+		ti,under-voltage-limit-microvolt = <3350000>;
+		#interrupt-cells = <2>;
+
+		dcdc1_reg: regulator-dcdc1 {
+			regulator-name = "vdd_core";
+			regulator-min-microvolt = <1100000>;
+			regulator-max-microvolt = <1100000>;
+			regulator-boot-on;
+			regulator-always-on;
+		};
+
+		dcdc2_reg: regulator-dcdc2 {
+			regulator-name = "vdd_mpu";
+			regulator-min-microvolt = <912000>;
+			regulator-max-microvolt = <1378000>;
+			regulator-boot-on;
+			regulator-always-on;
+		};
+
+		dcdc3_reg: regulator-dcdc3 {
+			regulator-name = "vdcdc3";
+			regulator-min-microvolt = <1500000>;
+			regulator-max-microvolt = <1500000>;
+			regulator-boot-on;
+			regulator-always-on;
+		};
+
+		dcdc4_reg: regulator-dcdc4 {
+			regulator-name = "vdd_logic";
+			regulator-min-microvolt = <3300000>;
+			regulator-max-microvolt = <3300000>;
+			regulator-boot-on;
+			regulator-always-on;
+		};
+
+		loadswitch2:  regulator-ls2 {
+			/* Should be set to 100 mA according to SLP00000843.002
+			 * but 4G Modem is not being enumerated correctly,
+			 * so we increase it.
+			 */
+			regulator-min-microamp = <200000>;
+			regulator-max-microamp = <200000>;
+			regulator-boot-on;
+			regulator-always-on;
+		};
+
+		loadswitch3: regulator-ls3 {
+			regulator-min-microamp = <1000000>;
+			regulator-max-microamp = <1000000>;
+			regulator-boot-on;
+			regulator-always-on;
+		};
+
+		ldo1: regulator-ldo1 {
+			regulator-always-on;
+		};
+
+		regulator-pwrbutton {
+			// dummy entry to suppres warning on boot
+			compatible = "ti,tps65218-pwrbutton";
+			status = "disabled";
+		};
+
+		regulator-gpio {
+			// dummy entry to suppres warning on boot
+			compatible = "ti,tps65218-gpio";
+			status = "disabled";
+		};
+	};
+};
+
+&bitbang_mdio0 {
+	pinctrl-names = "default", "sleep";
+	pinctrl-0 = <&gpio_bitbang_mdio_pins>;
+	pinctrl-1 = <&gpio_bitbang_mdio_sleep_pins>;
+	gpios = <&gpio0 1 0   /* 0: mdc  */
+		 &gpio0 0 0>; /* 1: mdio */
+};
+
+/*
+ * Use a fixed phy for emac1.
+ * that's rgmii2
+ */
+&cpsw_emac1 {
+	phy-mode = "rmii";
+	fixed-link {
+		speed = <100>;
+		full-duplex;
+	};
+};
+
+&mmc1 {
+	pinctrl-names = "default";
+	pinctrl-0 = <&mmc1_pins>;
+
+	vmmc-supply = <&dcdc4_reg>;
+	bus-width = <4>;
+};
+
+&gpio3 {
+	pinctrl-names = "default";
+	pinctrl-0 = <&gpio3_pins>;
+};
+
+&wdt2_module {
+	ti,no-reset-on-init;
+	ti,no-idle;
+};
+
+&usb {
+	status = "disabled";
+};
+
+&usb_ctrl_mod {
+	status = "disabled";
+};
+
+&usb0_phy {
+	status = "disabled";
+};
+
+&usb1_phy {
+	status = "disabled";
+};
+
+&usb0 {
+	status = "disabled";
+};
+
+&usb1 {
+	status = "disabled";
+};
diff --git a/arch/arm/boot/dts/am335x-rmcb.dts b/arch/arm/boot/dts/am335x-rmcb.dts
new file mode 100644
index 000000000000..17b1a79e9d3f
--- /dev/null
+++ b/arch/arm/boot/dts/am335x-rmcb.dts
@@ -0,0 +1,625 @@
+/*
+ * Copyright (C) 2020 WAGO Kontakttechnik GmbH & Co. KG - https://www.wago.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+/dts-v1/;
+
+#include "am335x-pfc.dtsi"
+#include "am335x-pfc-750_8xxx-ksz8863.dtsi"
+#include "am335x-pfc-768_330x-rlb.dtsi"
+#include "wago-devconf.dtsi"
+
+/delete-node/ &vxe164_reg;
+/delete-node/ &vsd_reg;
+/delete-node/ &tps;
+/delete-node/ &gpio3_pins;
+
+/{
+	model = "WAGO RMCB";
+	compatible = "wago,am335x-rmcb-0028", "wago,am335x-pfc", "ti,am33xx";
+
+	memory {
+		device_type = "memory";
+		reg = <0x80000000 0x8000000>; /* 128 MB */
+	};
+
+	vrlb_reg: fixedregulator@0 {
+		compatible = "regulator-fixed";
+		regulator-name = "v_rlb";
+		regulator-min-microvolt = <3300000>;
+		regulator-max-microvolt = <3300000>;
+	};
+
+	gpio-io_expander_22-leds {
+		status = "okay";
+		compatible = "gpio-leds";
+
+		ds-red@0 {
+			label = "ds-red";
+			gpios = <&io_expander_22 0 1>;
+			linux,default-trigger = "timer";
+		};
+
+		ds-green@1 {
+			label = "ds-green";
+			gpios = <&io_expander_22 1 1>;
+			linux,default-trigger = "timer";
+		};
+
+		ec-red@2 {
+			label = "ec-red";
+			gpios = <&io_expander_22 2 1>;
+			linux,default-trigger = "none";
+		};
+
+		ec-green@3 {
+			label = "ec-green";
+			gpios = <&io_expander_22 3 1>;
+			linux,default-trigger = "none";
+		};
+
+		h11-red@4 {
+			label = "h11-red";
+			gpios = <&io_expander_22 4 1>;
+			linux,default-trigger = "none";
+		};
+
+		h11-green@5 {
+			label = "h11-green";
+			gpios = <&io_expander_22 5 1>;
+			linux,default-trigger = "none";
+		};
+
+		h12-red@6 {
+			label = "h12-red";
+			gpios = <&io_expander_22 6 1>;
+			linux,default-trigger = "none";
+		};
+
+		h12-green@7 {
+			label = "h12-green";
+			gpios = <&io_expander_22 7 1>;
+			linux,default-trigger = "none";
+		};
+
+		run-red@8 {
+			label = "run-red";
+			gpios = <&io_expander_22 8 1>;
+			linux,default-trigger = "none";
+		};
+
+		run-green@9 {
+			label = "run-green";
+			gpios = <&io_expander_22 9 1>;
+			linux,default-trigger = "none";
+		};
+
+		app-red@10 {
+			label = "app-red";
+			gpios = <&io_expander_22 10 1>;
+			linux,default-trigger = "none";
+		};
+
+		app-green@11 {
+			label = "app-green";
+			gpios = <&io_expander_22 11 1>;
+			linux,default-trigger = "none";
+		};
+
+		led7-red@12 {
+			label = "led7-red";
+			gpios = <&io_expander_22 12 1>;
+			linux,default-trigger = "none";
+		};
+
+		led7-green@13 {
+			label = "led7-green";
+			gpios = <&io_expander_22 13 1>;
+			linux,default-trigger = "none";
+		};
+
+		led8-red@14 {
+			label = "led8-red";
+			gpios = <&io_expander_22 14 1>;
+			linux,default-trigger = "none";
+		};
+
+		led8-green@15 {
+			label = "led8-green";
+			gpios = <&io_expander_22 15 1>;
+			linux,default-trigger = "none";
+		};
+	};
+
+	dip-switch {
+		status = "okay";
+		compatible = "encsw";
+		encsw-gpios = < &io_expander_20 0 GPIO_ACTIVE_LOW
+				&io_expander_20 1 GPIO_ACTIVE_LOW
+				&io_expander_20 2 GPIO_ACTIVE_LOW
+				&io_expander_20 3 GPIO_ACTIVE_LOW
+				&io_expander_20 4 GPIO_ACTIVE_LOW
+				&io_expander_20 5 GPIO_ACTIVE_LOW
+				&io_expander_20 6 GPIO_ACTIVE_LOW
+				&io_expander_20 7 GPIO_ACTIVE_LOW >;
+	};
+};
+
+&uio_rmd_mem {
+	status = "okay";
+};
+
+&uio_rmd_irq0 {
+	status = "okay";
+};
+
+&rmd {
+	status = "okay";
+	pinctrl-0 = <&rmd_pins>;
+	dmas = <&edma_xbar 12 0 28>, <&edma_xbar 13 0 29>;
+	interrupts = <9 IRQ_TYPE_EDGE_RISING>;
+	interrupt-parent = <&gpio3>;
+};
+
+&cpu_0 {
+	cpu0-supply = <&vdd1_reg>;
+	operating-points = <
+		1000000  1325000
+		 800000  1260000
+		 720000  1200000
+		 600000  1100000
+		 300000  950000
+	>;
+};
+
+&wsysinit {
+	board,variant = "pfc200adv";
+	tty,rs232-485 = "nop";
+	tty,service   = "ttyO0";
+};
+
+/* Use polled gpio-keys driver until the interrupt driven gpio-keys driver is
+ * fixed. Due to the fact that the gpio expander (pca9535) does not support debounce
+ * functions like a "real" gpio-controller, the gpio-keys driver falls back to
+ * timer driven debouncing. That has the bug, that the timmer is throw ~3333 minutes
+ * later. If the timer fires, the driver works like a charm.
+ */
+&oms {
+	status = "okay";
+
+	/* is needed for irq support when we go back to gpio-keys driver */
+	pinctrl-names = "default";
+	pinctrl-0 = <&io_expander_21_irq_pin>;
+
+	compatible = "gpio-keys-polled";
+	poll-interval = <100>;
+};
+
+&oms_run {
+	gpios = <&io_expander_21 1 GPIO_ACTIVE_LOW>;
+};
+
+&oms_stop {
+	gpios = <&io_expander_21 0 GPIO_ACTIVE_LOW>;
+};
+
+&oms_reset {
+	gpios = <&io_expander_21 2 GPIO_ACTIVE_LOW>; /* Board-Label: Taster 1 */
+};
+
+&oms_reset_all {
+	gpios = <&io_expander_21 3 GPIO_ACTIVE_LOW>; /* Board-Label: Taster 2 */
+};
+
+&watchdog {
+	status = "okay";
+	pinctrl-0 = <&watchdog_pins>;
+
+	en-gpios = <&gpio3 4 GPIO_ACTIVE_LOW>;
+	gpios = <&gpio0 7 GPIO_ACTIVE_LOW>;
+};
+
+&i2c0 {
+	status = "okay";
+
+	tps: tps65910@2d {
+		reg = <0x2d>;
+	};
+
+	io_expander_20: pcf8575@20 {
+		compatible = "nxp,pcf8574";
+		reg = <0x20>;
+		gpio-controller;
+		#gpio-cells = <2>;
+	};
+
+	io_expander_21: pca9535@21 {
+		compatible = "nxp,pca9535";
+		reg = <0x21>;
+
+		gpio-controller;
+		#gpio-cells = <2>;
+		interrupt-parent = <&gpio3>;
+		interrupts = <15 IRQ_TYPE_LEVEL_LOW>;
+		interrupt-controller;
+		#interrupt-cells = <2>;
+	};
+
+	io_expander_22: pca9535@22 {
+		compatible = "nxp,pca9535";
+		reg = <0x22>;
+
+		gpio-controller;
+		#gpio-cells = <2>;
+	};
+};
+
+#include "tps65910.dtsi"
+
+&tps {
+	vcc1-supply = <&vrlb_reg>;
+	vcc2-supply = <&vrlb_reg>;
+	vcc3-supply = <&vrlb_reg>;
+	vcc4-supply = <&vrlb_reg>;
+	vcc5-supply = <&vrlb_reg>;
+	vcc6-supply = <&vrlb_reg>;
+	vcc7-supply = <&vrlb_reg>;
+	vccio-supply = <&vrlb_reg>;
+};
+
+&vrtc_reg {
+	regulator-always-on;
+};
+
+&vio_reg {
+	regulator-name = "v_ddr";
+	regulator-min-microvolt = <1500000>;
+	regulator-max-microvolt = <1500000>;
+	regulator-always-on;
+	regulator-boot-on;
+};
+
+&vdd1_reg {
+	/* VDD_MPU voltage limits 0.95V - 1.26V with +/-4% tolerance */
+	regulator-name = "vdd_mpu";
+	regulator-min-microvolt = <912500>;
+	regulator-max-microvolt = <1312500>;
+	regulator-boot-on;
+	regulator-always-on;
+};
+
+&vdd2_reg {
+	/* VDD_CORE voltage limits 0.95V - 1.1V with +/-4% tolerance */
+	regulator-name = "vdd_core";
+	regulator-min-microvolt = <1100000>;
+	regulator-max-microvolt = <1100000>;
+	regulator-boot-on;
+	regulator-always-on;
+};
+
+&vdd3_reg {
+	regulator-name = "v_otg";
+};
+
+&vdig2_reg {
+	regulator-name = "v_pll";
+	regulator-min-microvolt = <1800000>;
+	regulator-max-microvolt = <1800000>;
+	regulator-boot-on;
+	regulator-always-on;
+};
+
+&vaux2_reg {
+	regulator-name = "v_peth";
+	regulator-min-microvolt = <3300000>;
+	regulator-max-microvolt = <3300000>;
+	regulator-always-on;
+	regulator-boot-on;
+};
+
+&vaux33_reg {
+	regulator-name = "v_ceth";
+	regulator-min-microvolt = <3300000>;
+	regulator-max-microvolt = <3300000>;
+	regulator-always-on;
+	regulator-boot-on;
+};
+
+&vmmc_reg {
+	regulator-name = "v_io";
+	regulator-min-microvolt = <3300000>;
+	regulator-max-microvolt = <3300000>;
+	regulator-always-on;
+	regulator-boot-on;
+};
+
+&rtc_i2c {
+	status = "okay";
+	trim-data = <0 1 31>;
+};
+
+&bitbang_mdio0 {
+	status = "okay";
+};
+
+&mmc1 {
+	status = "okay";
+	vmmc-supply = <&vmmc_reg>;
+
+	pinctrl-names = "default";
+	pinctrl-0 = <
+		&mmc1_pins
+		&mmc1_cd_pins
+	>;
+	cd-gpios = <&gpio3 14 GPIO_ACTIVE_HIGH>;
+};
+
+&spi0 {
+	status = "okay";
+	pinctrl-names = "default";
+	pinctrl-0 = <&spi0_pins>;
+
+	fpga_ps: fpga_ps@0 {
+		compatible = "altr,passive-serial";
+		pinctrl-names = "default";
+		pinctrl-0 = <&fpga_pins>;
+
+		reg = <0>;
+		spi-max-frequency = <48000000>;
+
+		reset-gpios = <&io_expander_21 5 GPIO_ACTIVE_LOW>;
+		gpio-nconfig = <&gpio3 7 GPIO_ACTIVE_HIGH>;
+		gpio-nstatus = <&gpio2 18 GPIO_ACTIVE_HIGH>;
+		gpio-confdone = <&gpio2 19 GPIO_ACTIVE_HIGH>;
+	};
+};
+
+&mac {
+	status = "okay";
+	pinctrl-0 = <&rmii2_pins>, <&rmii1_pins>;
+	pinctrl-1 = <&rmii2_sleep_pins>, <&rmii1_sleep_pins>;
+	active_slave = <0>;
+};
+
+/*
+ * Use a fixed phy for emac0.
+ * that's rmii1
+ */
+&cpsw_emac0 {
+	status = "okay";
+	phy-mode = "rmii";
+
+	fixed-link {
+		speed = <100>;
+		full-duplex;
+	};
+};
+
+/*
+ * disable explicitly the unused emac1 interface
+ * when using cpsw in switched mode (!dual_emac).
+ * Otherwise the cpsw ip hangs after sending some frames.
+ */
+&cpsw_emac1 {
+	status = "disabled";
+};
+
+&gpmc {
+	status = "okay";
+
+	ranges = < 0 0 0x08000000 0x01000000	/* CS0: NAND */
+		   2 0 0x01000000 0x01000000 >;	/* CS2: FPGA, 16M */
+};
+
+&fpga {
+	status = "okay";
+
+	reg = <2 0 0x01000000>;
+	bank-width = <2>; /* 16 Bit */
+
+	gpmc,sync-clk-ps = <0>;
+	gpmc,cs-on-ns = <0>;
+	gpmc,cs-rd-off-ns = <90>;
+	gpmc,cs-wr-off-ns = <100>;
+
+	gpmc,adv-on-ns = <10>;
+	gpmc,adv-rd-off-ns = <100>;
+	gpmc,adv-wr-off-ns = <100>;
+
+	gpmc,we-on-ns = <10>;
+	gpmc,we-off-ns = <90>;
+	gpmc,oe-on-ns = <10>;
+	gpmc,oe-off-ns = <90>;
+	gpmc,oe-extra-delay;
+
+	gpmc,access-ns = <90>;
+	gpmc,rd-cycle-ns = <100>;
+	gpmc,wr-cycle-ns = <100>;
+	gpmc,page-burst-access-ns = <0>;
+
+	gpmc,bus-turnaround-ns  = <20>;
+	gpmc,wr-access-ns = <90>;
+	gpmc,wr-data-mux-bus-ns = <30>;
+	gpmc,cycle2cycle-diffcsen;
+	gpmc,cycle2cycle-samecsen; /* skipping this would lead to buggy 32bit accesss */
+	gpmc,cycle2cycle-delay-ns = <10>;
+
+	gpmc,wait-pin = <0>;
+	gpmc,wait-on-read;
+	gpmc,wait-on-write;
+	gpmc,mux-add-data = <2>; /* address-data multiplexing */
+};
+
+&gpio3 {
+	/delete-property/ pinctrl-names;
+	/delete-property/ pinctrl-0;
+};
+
+&ksz8863_switch {
+	status = "okay";
+	ksz,reset-gpio = <&io_expander_21 7 GPIO_ACTIVE_LOW>;
+	/delete-property/ ksz,disable-internal-ldo;
+
+	ports {
+		port@0 { reg = <1>; };
+		port@1 { reg = <2>; };
+	};
+};
+
+&swcfg_ksz8863 {
+	status = "okay";
+};
+
+&aes {
+	status = "okay";
+};
+
+&cppi41dma {
+	status = "okay";
+};
+
+&elm {
+	status = "okay";
+};
+
+&sham {
+	status = "okay";
+};
+
+&uart0 {
+	status = "okay";
+};
+
+&usb {
+	status = "okay";
+};
+
+&usb_ctrl_mod {
+	status = "okay";
+};
+
+&usb0_phy {
+	status = "okay";
+};
+
+&usb0 {
+	status = "okay";
+	dr_mode = "peripheral";
+};
+
+&am33xx_pinmux {
+	watchdog_pins: pinmux_watchdog_pins {
+		pinctrl-single,pins = <
+			AM33XX_MII1_RX_DV_GPIO3_4(PIN_OUTPUT_PULLDOWN)
+			AM33XX_ECAP0_IN_PWM0_OUT_GPIO0_7(PIN_OUTPUT_PULLDOWN)
+		>;
+	};
+
+	mmc1_cd_pins: pinmux_sdcard_cd_pins {
+		pinctrl-single,pins = <
+			AM33XX_MCASP0_ACLKX_GPIO3_14(PIN_INPUT_PULLUP)
+		>;
+	};
+
+	spi0_pins: pinmux_spi0_pins {
+		pinctrl-single,pins = <
+			AM33XX_SPI0_SCLK(PIN_INPUT)
+			AM33XX_SPI0_D0(PIN_INPUT)
+			AM33XX_SPI0_D1(PIN_INPUT)
+			AM33XX_SPI0_CS0(PIN_OUTPUT_PULLUP)
+			AM33XX_SPI0_CS1(PIN_OUTPUT_PULLUP)
+		>;
+	};
+
+	rmii2_pins: pinmux_rmii2_pins {
+		pinctrl-single,pins = <
+			AM33XX_GPMC_A0_GPIO1_16(PIN_INPUT_PULLDOWN)
+			AM33XX_GPMC_A4_GPIO1_20(PIN_INPUT_PULLDOWN)
+			AM33XX_GPMC_A5_GPIO1_21(PIN_INPUT_PULLDOWN)
+			AM33XX_GPMC_A10_GPIO1_26(PIN_INPUT_PULLDOWN)
+			AM33XX_GPMC_A11_GPIO1_27(PIN_INPUT_PULLDOWN)
+			AM33XX_MII1_COL_GPIO3_0(PIN_INPUT_PULLDOWN)
+			AM33XX_GPMC_CSN3_GPIO2_0(PIN_INPUT_PULLDOWN)
+		>;
+	};
+
+	rmii2_sleep_pins: pinmux_rmii2_sleep_pins {
+		pinctrl-single,pins = <
+			AM33XX_GPMC_A0_GPIO1_16(PIN_INPUT_PULLDOWN)
+			AM33XX_GPMC_A4_GPIO1_20(PIN_INPUT_PULLDOWN)
+			AM33XX_GPMC_A5_GPIO1_21(PIN_INPUT_PULLDOWN)
+			AM33XX_GPMC_A10_GPIO1_26(PIN_INPUT_PULLDOWN)
+			AM33XX_GPMC_A11_GPIO1_27(PIN_INPUT_PULLDOWN)
+			AM33XX_MII1_COL_GPIO3_0(PIN_INPUT_PULLDOWN)
+			AM33XX_GPMC_CSN3_GPIO2_0(PIN_INPUT_PULLDOWN)
+		>;
+	};
+
+	rmii1_pins: pinmux_rmii1_pins {
+		pinctrl-single,pins = <
+			AM33XX_RMII1_REF_CLK(PIN_INPUT)
+			AM33XX_MII1_TX_EN_RMII1_TXEN(PIN_OUTPUT | SLEWCTRL_FAST)
+			AM33XX_MII1_TXD0_RMII1_TXD0(PIN_OUTPUT | SLEWCTRL_FAST)
+			AM33XX_MII1_TXD1_RMII1_TXD1(PIN_OUTPUT | SLEWCTRL_FAST)
+			AM33XX_MII1_RXD0_RMII1_RXD0(PIN_INPUT)
+			AM33XX_MII1_RXD1_RMII1_RXD1(PIN_INPUT)
+			AM33XX_MII1_CRS_RMII1_CRS_DV(PIN_INPUT)
+		>;
+	};
+
+	rmii1_sleep_pins: pinmux_rmii1_sleep_pins {
+		pinctrl-single,pins = <
+			AM33XX_RMII1_REF_CLK_GPIO0_29(PIN_INPUT)
+			AM33XX_MII1_TX_EN_GPIO3_3(PIN_INPUT)
+			AM33XX_MII1_TXD0_GPIO0_28(PIN_INPUT)
+			AM33XX_MII1_TXD1_GPIO0_21(PIN_INPUT)
+			AM33XX_MII1_RXD0_GPIO2_21(PIN_INPUT)
+			AM33XX_MII1_RXD1_GPIO2_20(PIN_INPUT)
+			AM33XX_MII1_CRS_GPIO3_1(PIN_INPUT)
+		>;
+	};
+
+	gpmc_ctrl_pins: pinmux_gpmc_ctrl_pins {
+		pinctrl-single,pins = <
+			AM33XX_GPMC_OEN_REN(PIN_OUTPUT_PULLUP  | SLEWCTRL_FAST)
+			AM33XX_GPMC_WEN(PIN_OUTPUT_PULLUP      | SLEWCTRL_FAST)
+			AM33XX_GPMC_CSN0(PIN_OUTPUT_PULLUP     | SLEWCTRL_SLOW)
+			AM33XX_GPMC_CSN2(PIN_OUTPUT_PULLUP     | SLEWCTRL_SLOW)
+			AM33XX_GPMC_BEN0_CLE(PIN_OUTPUT_PULLUP | SLEWCTRL_FAST)
+			AM33XX_GPMC_WAIT0(PIN_INPUT_PULLUP)
+			AM33XX_GPMC_ADVN_ALE(PIN_OUTPUT)
+		>;
+	};
+
+	gpmc_addr_pins: pinmux_gpmc_addr_pins {
+		pinctrl-single,pins = <
+			AM33XX_LCD_VSYNC_GPMC_A8(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+			AM33XX_LCD_HSYNC_GPMC_A9(PIN_OUTPUT_PULLDOWN | SLEWCTRL_SLOW)
+		>;
+	};
+
+	fpga_pins: pinmux_fpga_pins {
+		pinctrl-single,pins = <
+			AM33XX_EMU0_GPIO3_7(PIN_OUTPUT_PULLUP)
+			AM33XX_MII1_RXD3_GPIO2_18(PIN_INPUT)
+			AM33XX_MII1_RXD2_GPIO2_19(PIN_INPUT)
+		>;
+	};
+
+	rmd_pins: pinmux_rmd_pins {
+		pinctrl-single,pins = <
+			AM33XX_MII1_TX_CLK_GPIO3_9(PIN_INPUT)
+			AM33XX_XDMA_EVENT_INTR0(PIN_INPUT)
+			AM33XX_XDMA_EVENT_INTR1(PIN_INPUT)
+		>;
+	};
+
+	io_expander_21_irq_pin: pinmux_io_expander_21_irq {
+		pinctrl-single,pins = <
+			AM33XX_MCASP0_FSX_GPIO3_15(PIN_INPUT)
+		>;
+	};
+};
diff --git a/arch/arm/boot/dts/am33xx-l4.dtsi b/arch/arm/boot/dts/am33xx-l4.dtsi
index ea20e4bdf040..9c4f8d4b4728 100644
--- a/arch/arm/boot/dts/am33xx-l4.dtsi
+++ b/arch/arm/boot/dts/am33xx-l4.dtsi
@@ -380,7 +380,7 @@ target-module@33000 {			/* 0x44e33000, ap 27 18.0 */
 			ranges = <0x0 0x33000 0x1000>;
 		};
 
-		target-module@35000 {			/* 0x44e35000, ap 29 50.0 */
+		wdt2_module: target-module@35000 {			/* 0x44e35000, ap 29 50.0 */
 			compatible = "ti,sysc-omap2", "ti,sysc";
 			reg = <0x35000 0x4>,
 			      <0x35010 0x4>,
@@ -739,6 +739,13 @@ davinci_mdio: mdio@1000 {
 					status = "disabled";
 				};
 
+				bitbang_mdio0: gpio_mdio {
+					compatible = "virtual,mdio-gpio";
+					#address-cells = <1>;
+					#size-cells = <0>;
+					status = "disabled";
+				};
+
 				cpsw_emac0: slave@200 {
 					/* Filled in by U-Boot */
 					mac-address = [ 00 00 00 00 00 00 ];
@@ -2137,4 +2144,3 @@ target-module@24000 {			/* 0x48324000, ap 103 68.0 */
 		};
 	};
 };
-
diff --git a/arch/arm/boot/dts/am33xx-pinfunc.h b/arch/arm/boot/dts/am33xx-pinfunc.h
new file mode 100644
index 000000000000..d254acdb1e93
--- /dev/null
+++ b/arch/arm/boot/dts/am33xx-pinfunc.h
@@ -0,0 +1,1064 @@
+/*
+ * Copyright (C) 2019 WAGO Kontakttechnik GmbH & Co. GK - https://www.wago.com/
+ *
+ * The code contained herein is licensed under the GNU General Public
+ * License. You may obtain a copy of the GNU General Public License
+ * Version 2 or later at the following locations:
+ *
+ * http://www.opensource.org/licenses/gpl-license.html
+ * http://www.gnu.org/copyleft/gpl.html
+ */
+
+#ifndef __DTS_AM33XX_PINFUNC_H
+#define __DTS_AM33XX_PINFUNC_H
+
+#include <dt-bindings/gpio/gpio.h>
+#include <dt-bindings/pinctrl/am33xx.h>
+
+
+#define AM33XX_PIN_OFFSET				0x800
+#define AM33XX_PIN(offset, mode, flags) \
+	AM33XX_PADCONF(offset, flags, mode)
+
+#define AM33XX_PIN_GPMC_AD0(mode, flags)		AM33XX_PIN(0x800, mode, flags)
+#define AM33XX_PIN_GPMC_AD1(mode, flags)		AM33XX_PIN(0x804, mode, flags)
+#define AM33XX_PIN_GPMC_AD2(mode, flags)		AM33XX_PIN(0x808, mode, flags)
+#define AM33XX_PIN_GPMC_AD3(mode, flags)		AM33XX_PIN(0x80C, mode, flags)
+#define AM33XX_PIN_GPMC_AD4(mode, flags)		AM33XX_PIN(0x810, mode, flags)
+#define AM33XX_PIN_GPMC_AD5(mode, flags)		AM33XX_PIN(0x814, mode, flags)
+#define AM33XX_PIN_GPMC_AD6(mode, flags)		AM33XX_PIN(0x818, mode, flags)
+#define AM33XX_PIN_GPMC_AD7(mode, flags)		AM33XX_PIN(0x81C, mode, flags)
+#define AM33XX_PIN_GPMC_AD8(mode, flags)		AM33XX_PIN(0x820, mode, flags)
+#define AM33XX_PIN_GPMC_AD9(mode, flags)		AM33XX_PIN(0x824, mode, flags)
+#define AM33XX_PIN_GPMC_AD10(mode, flags)		AM33XX_PIN(0x828, mode, flags)
+#define AM33XX_PIN_GPMC_AD11(mode, flags)		AM33XX_PIN(0x82C, mode, flags)
+#define AM33XX_PIN_GPMC_AD12(mode, flags)		AM33XX_PIN(0x830, mode, flags)
+#define AM33XX_PIN_GPMC_AD13(mode, flags)		AM33XX_PIN(0x834, mode, flags)
+#define AM33XX_PIN_GPMC_AD14(mode, flags)		AM33XX_PIN(0x838, mode, flags)
+#define AM33XX_PIN_GPMC_AD15(mode, flags)		AM33XX_PIN(0x83C, mode, flags)
+#define AM33XX_PIN_GPMC_A0(mode, flags)			AM33XX_PIN(0x840, mode, flags)
+#define AM33XX_PIN_GPMC_A1(mode, flags)			AM33XX_PIN(0x844, mode, flags)
+#define AM33XX_PIN_GPMC_A2(mode, flags)			AM33XX_PIN(0x848, mode, flags)
+#define AM33XX_PIN_GPMC_A3(mode, flags)			AM33XX_PIN(0x84C, mode, flags)
+#define AM33XX_PIN_GPMC_A4(mode, flags)			AM33XX_PIN(0x850, mode, flags)
+#define AM33XX_PIN_GPMC_A5(mode, flags)			AM33XX_PIN(0x854, mode, flags)
+#define AM33XX_PIN_GPMC_A6(mode, flags)			AM33XX_PIN(0x858, mode, flags)
+#define AM33XX_PIN_GPMC_A7(mode, flags)			AM33XX_PIN(0x85C, mode, flags)
+#define AM33XX_PIN_GPMC_A8(mode, flags)			AM33XX_PIN(0x860, mode, flags)
+#define AM33XX_PIN_GPMC_A9(mode, flags)			AM33XX_PIN(0x864, mode, flags)
+#define AM33XX_PIN_GPMC_A10(mode, flags)		AM33XX_PIN(0x868, mode, flags)
+#define AM33XX_PIN_GPMC_A11(mode, flags)		AM33XX_PIN(0x86C, mode, flags)
+#define AM33XX_PIN_GPMC_WAIT0(mode, flags)		AM33XX_PIN(0x870, mode, flags)
+#define AM33XX_PIN_GPMC_WPN(mode, flags)		AM33XX_PIN(0x874, mode, flags)
+#define AM33XX_PIN_GPMC_BEN1(mode, flags)		AM33XX_PIN(0x878, mode, flags)
+#define AM33XX_PIN_GPMC_CSN0(mode, flags)		AM33XX_PIN(0x87C, mode, flags)
+#define AM33XX_PIN_GPMC_CSN1(mode, flags)		AM33XX_PIN(0x880, mode, flags)
+#define AM33XX_PIN_GPMC_CSN2(mode, flags)		AM33XX_PIN(0x884, mode, flags)
+#define AM33XX_PIN_GPMC_CSN3(mode, flags)		AM33XX_PIN(0x888, mode, flags)
+#define AM33XX_PIN_GPMC_CLK(mode, flags)		AM33XX_PIN(0x88C, mode, flags)
+#define AM33XX_PIN_GPMC_ADVN_ALE(mode, flags)		AM33XX_PIN(0x890, mode, flags)
+#define AM33XX_PIN_GPMC_OEN_REN(mode, flags)		AM33XX_PIN(0x894, mode, flags)
+#define AM33XX_PIN_GPMC_WEN(mode, flags)		AM33XX_PIN(0x898, mode, flags)
+#define AM33XX_PIN_GPMC_BEN0_CLE(mode, flags)		AM33XX_PIN(0x89C, mode, flags)
+#define AM33XX_PIN_LCD_DATA0(mode, flags)		AM33XX_PIN(0x8A0, mode, flags)
+#define AM33XX_PIN_LCD_DATA1(mode, flags)		AM33XX_PIN(0x8A4, mode, flags)
+#define AM33XX_PIN_LCD_DATA2(mode, flags)		AM33XX_PIN(0x8A8, mode, flags)
+#define AM33XX_PIN_LCD_DATA3(mode, flags)		AM33XX_PIN(0x8AC, mode, flags)
+#define AM33XX_PIN_LCD_DATA4(mode, flags)		AM33XX_PIN(0x8B0, mode, flags)
+#define AM33XX_PIN_LCD_DATA5(mode, flags)		AM33XX_PIN(0x8B4, mode, flags)
+#define AM33XX_PIN_LCD_DATA6(mode, flags)		AM33XX_PIN(0x8B8, mode, flags)
+#define AM33XX_PIN_LCD_DATA7(mode, flags)		AM33XX_PIN(0x8BC, mode, flags)
+#define AM33XX_PIN_LCD_DATA8(mode, flags)		AM33XX_PIN(0x8C0, mode, flags)
+#define AM33XX_PIN_LCD_DATA9(mode, flags)		AM33XX_PIN(0x8C4, mode, flags)
+#define AM33XX_PIN_LCD_DATA10(mode, flags)		AM33XX_PIN(0x8C8, mode, flags)
+#define AM33XX_PIN_LCD_DATA11(mode, flags)		AM33XX_PIN(0x8CC, mode, flags)
+#define AM33XX_PIN_LCD_DATA12(mode, flags)		AM33XX_PIN(0x8D0, mode, flags)
+#define AM33XX_PIN_LCD_DATA13(mode, flags)		AM33XX_PIN(0x8D4, mode, flags)
+#define AM33XX_PIN_LCD_DATA14(mode, flags)		AM33XX_PIN(0x8D8, mode, flags)
+#define AM33XX_PIN_LCD_DATA15(mode, flags)		AM33XX_PIN(0x8DC, mode, flags)
+#define AM33XX_PIN_LCD_VSYNC(mode, flags)		AM33XX_PIN(0x8E0, mode, flags)
+#define AM33XX_PIN_LCD_HSYNC(mode, flags)		AM33XX_PIN(0x8E4, mode, flags)
+#define AM33XX_PIN_LCD_PCLK(mode, flags)		AM33XX_PIN(0x8E8, mode, flags)
+#define AM33XX_PIN_LCD_AC_BIAS_EN(mode, flags)		AM33XX_PIN(0x8EC, mode, flags)
+#define AM33XX_PIN_MMC0_DAT3(mode, flags)		AM33XX_PIN(0x8F0, mode, flags)
+#define AM33XX_PIN_MMC0_DAT2(mode, flags)		AM33XX_PIN(0x8F4, mode, flags)
+#define AM33XX_PIN_MMC0_DAT1(mode, flags)		AM33XX_PIN(0x8F8, mode, flags)
+#define AM33XX_PIN_MMC0_DAT0(mode, flags)		AM33XX_PIN(0x8FC, mode, flags)
+#define AM33XX_PIN_MMC0_CLK(mode, flags)		AM33XX_PIN(0x900, mode, flags)
+#define AM33XX_PIN_MMC0_CMD(mode, flags)		AM33XX_PIN(0x904, mode, flags)
+#define AM33XX_PIN_MII1_COL(mode, flags)		AM33XX_PIN(0x908, mode, flags)
+#define AM33XX_PIN_MII1_CRS(mode, flags)		AM33XX_PIN(0x90C, mode, flags)
+#define AM33XX_PIN_MII1_RX_ER(mode, flags)		AM33XX_PIN(0x910, mode, flags)
+#define AM33XX_PIN_MII1_TX_EN(mode, flags)		AM33XX_PIN(0x914, mode, flags)
+#define AM33XX_PIN_MII1_RX_DV(mode, flags)		AM33XX_PIN(0x918, mode, flags)
+#define AM33XX_PIN_MII1_TXD3(mode, flags)		AM33XX_PIN(0x91C, mode, flags)
+#define AM33XX_PIN_MII1_TXD2(mode, flags)		AM33XX_PIN(0x920, mode, flags)
+#define AM33XX_PIN_MII1_TXD1(mode, flags)		AM33XX_PIN(0x924, mode, flags)
+#define AM33XX_PIN_MII1_TXD0(mode, flags)		AM33XX_PIN(0x928, mode, flags)
+#define AM33XX_PIN_MII1_TX_CLK(mode, flags)		AM33XX_PIN(0x92C, mode, flags)
+#define AM33XX_PIN_MII1_RX_CLK(mode, flags)		AM33XX_PIN(0x930, mode, flags)
+#define AM33XX_PIN_MII1_RXD3(mode, flags)		AM33XX_PIN(0x934, mode, flags)
+#define AM33XX_PIN_MII1_RXD2(mode, flags)		AM33XX_PIN(0x938, mode, flags)
+#define AM33XX_PIN_MII1_RXD1(mode, flags)		AM33XX_PIN(0x93C, mode, flags)
+#define AM33XX_PIN_MII1_RXD0(mode, flags)		AM33XX_PIN(0x940, mode, flags)
+#define AM33XX_PIN_RMII1_REF_CLK(mode, flags)		AM33XX_PIN(0x944, mode, flags)
+#define AM33XX_PIN_MDIO(mode, flags)			AM33XX_PIN(0x948, mode, flags)
+#define AM33XX_PIN_MDC(mode, flags)			AM33XX_PIN(0x94C, mode, flags)
+#define AM33XX_PIN_SPI0_SCLK(mode, flags)		AM33XX_PIN(0x950, mode, flags)
+#define AM33XX_PIN_SPI0_D0(mode, flags)			AM33XX_PIN(0x954, mode, flags)
+#define AM33XX_PIN_SPI0_D1(mode, flags)			AM33XX_PIN(0x958, mode, flags)
+#define AM33XX_PIN_SPI0_CS0(mode, flags)		AM33XX_PIN(0x95C, mode, flags)
+#define AM33XX_PIN_SPI0_CS1(mode, flags)		AM33XX_PIN(0x960, mode, flags)
+#define AM33XX_PIN_ECAP0_IN_PWM0_OUT(mode, flags)	AM33XX_PIN(0x964, mode, flags)
+#define AM33XX_PIN_UART0_CTSN(mode, flags)		AM33XX_PIN(0x968, mode, flags)
+#define AM33XX_PIN_UART0_RTSN(mode, flags)		AM33XX_PIN(0x96C, mode, flags)
+#define AM33XX_PIN_UART0_RXD(mode, flags)		AM33XX_PIN(0x970, mode, flags)
+#define AM33XX_PIN_UART0_TXD(mode, flags)		AM33XX_PIN(0x974, mode, flags)
+#define AM33XX_PIN_UART1_CTSN(mode, flags)		AM33XX_PIN(0x978, mode, flags)
+#define AM33XX_PIN_UART1_RTSN(mode, flags)		AM33XX_PIN(0x97C, mode, flags)
+#define AM33XX_PIN_UART1_RXD(mode, flags)		AM33XX_PIN(0x980, mode, flags)
+#define AM33XX_PIN_UART1_TXD(mode, flags)		AM33XX_PIN(0x984, mode, flags)
+#define AM33XX_PIN_I2C0_SDA(mode, flags)		AM33XX_PIN(0x988, mode, flags)
+#define AM33XX_PIN_I2C0_SCL(mode, flags)		AM33XX_PIN(0x98C, mode, flags)
+#define AM33XX_PIN_MCASP0_ACLKX(mode, flags)		AM33XX_PIN(0x990, mode, flags)
+#define AM33XX_PIN_MCASP0_FSX(mode, flags)		AM33XX_PIN(0x994, mode, flags)
+#define AM33XX_PIN_MCASP0_AXR0(mode, flags)		AM33XX_PIN(0x998, mode, flags)
+#define AM33XX_PIN_MCASP0_AHCLKR(mode, flags)		AM33XX_PIN(0x99C, mode, flags)
+#define AM33XX_PIN_MCASP0_ACLKR(mode, flags)		AM33XX_PIN(0x9A0, mode, flags)
+#define AM33XX_PIN_MCASP0_FSR(mode, flags)		AM33XX_PIN(0x9A4, mode, flags)
+#define AM33XX_PIN_MCASP0_AXR1(mode, flags)		AM33XX_PIN(0x9A8, mode, flags)
+#define AM33XX_PIN_MCASP0_AHCLKX(mode, flags)		AM33XX_PIN(0x9AC, mode, flags)
+#define AM33XX_PIN_XDMA_EVENT_INTR0(mode, flags)	AM33XX_PIN(0x9B0, mode, flags)
+#define AM33XX_PIN_XDMA_EVENT_INTR1(mode, flags)	AM33XX_PIN(0x9B4, mode, flags)
+#define AM33XX_PIN_NNMI(mode, flags)			AM33XX_PIN(0x9C0, mode, flags)
+#define AM33XX_PIN_TMS(mode, flags)			AM33XX_PIN(0x9D0, mode, flags)
+#define AM33XX_PIN_TDI(mode, flags)			AM33XX_PIN(0x9D0, mode, flags)
+#define AM33XX_PIN_TDO(mode, flags)			AM33XX_PIN(0x9D4, mode, flags)
+#define AM33XX_PIN_TCK(mode, flags)			AM33XX_PIN(0x9D8, mode, flags)
+#define AM33XX_PIN_TRSTN(mode, flags)			AM33XX_PIN(0x9DC, mode, flags)
+#define AM33XX_PIN_EMU0(mode, flags)			AM33XX_PIN(0x9E4, mode, flags)
+#define AM33XX_PIN_EMU1(mode, flags)			AM33XX_PIN(0x9E8, mode, flags)
+#define AM33XX_PIN_USB0_DRVVBUS(mode, flags)		AM33XX_PIN(0xA1C, mode, flags)
+#define AM33XX_PIN_USB1_DRVVBUS(mode, flags)		AM33XX_PIN(0xA34, mode, flags)
+
+#define AM33XX_ECAP0_IN_PWM0_OUT(flags)					AM33XX_PIN_ECAP0_IN_PWM0_OUT(MUX_MODE0, flags)
+#define AM33XX_ECAP0_IN_PWM0_OUT_UART3_TXD(flags)			AM33XX_PIN_ECAP0_IN_PWM0_OUT(MUX_MODE1, flags)
+#define AM33XX_ECAP0_IN_PWM0_OUT_SPI1_CS1(flags)			AM33XX_PIN_ECAP0_IN_PWM0_OUT(MUX_MODE2, flags)
+#define AM33XX_ECAP0_IN_PWM0_OUT_PR1_ECAP0_ECAP_CAPIN_APWM_O(flags)	AM33XX_PIN_ECAP0_IN_PWM0_OUT(MUX_MODE3, flags)
+#define AM33XX_ECAP0_IN_PWM0_OUT_SPI1_SCLK(flags)			AM33XX_PIN_ECAP0_IN_PWM0_OUT(MUX_MODE4, flags)
+#define AM33XX_ECAP0_IN_PWM0_OUT_MMC0_SDWP(flags)			AM33XX_PIN_ECAP0_IN_PWM0_OUT(MUX_MODE5, flags)
+#define AM33XX_ECAP0_IN_PWM0_OUT_XDMA_EVENT_INTR2(flags)		AM33XX_PIN_ECAP0_IN_PWM0_OUT(MUX_MODE6, flags)
+#define AM33XX_ECAP0_IN_PWM0_OUT_GPIO0_7(flags)				AM33XX_PIN_ECAP0_IN_PWM0_OUT(MUX_MODE7, flags)
+
+#define AM33XX_EMU0(flags)						AM33XX_PIN_EMU0(MUX_MODE0, flags)
+#define AM33XX_EMU0_GPIO3_7(flags)					AM33XX_PIN_EMU0(MUX_MODE7, flags)
+
+#define AM33XX_EMU1(flags)						AM33XX_PIN_EMU1(MUX_MODE0, flags)
+#define AM33XX_EMU1_GPIO3_8(flags)					AM33XX_PIN_EMU1(MUX_MODE7, flags)
+
+#define AM33XX_GPMC_A0(flags)						AM33XX_PIN_GPMC_A0(MUX_MODE0, flags)
+#define AM33XX_GPMC_A0_GMII2_TXEN(flags)				AM33XX_PIN_GPMC_A0(MUX_MODE1, flags)
+#define AM33XX_GPMC_A0_RGMII2_TCTL(flags)				AM33XX_PIN_GPMC_A0(MUX_MODE2, flags)
+#define AM33XX_GPMC_A0_RMII2_TXEN(flags)				AM33XX_PIN_GPMC_A0(MUX_MODE3, flags)
+#define AM33XX_GPMC_A0_GPMC_A16(flags)					AM33XX_PIN_GPMC_A0(MUX_MODE4, flags)
+#define AM33XX_GPMC_A0_PR1_MII_MT1_CLK(flags)				AM33XX_PIN_GPMC_A0(MUX_MODE5, flags)
+#define AM33XX_GPMC_A0_EHRPWM1_TRIPZONE_INPUT(flags)			AM33XX_PIN_GPMC_A0(MUX_MODE6, flags)
+#define AM33XX_GPMC_A0_GPIO1_16(flags)					AM33XX_PIN_GPMC_A0(MUX_MODE7, flags)
+
+#define AM33XX_GPMC_A1(flags)						AM33XX_PIN_GPMC_A1(MUX_MODE0, flags)
+#define AM33XX_GPMC_A1_GMII2_RXDV(flags)				AM33XX_PIN_GPMC_A1(MUX_MODE1, flags)
+#define AM33XX_GPMC_A1_RGMII2_RCTL(flags)				AM33XX_PIN_GPMC_A1(MUX_MODE2, flags)
+#define AM33XX_GPMC_A1_MMC2_DAT0(flags)					AM33XX_PIN_GPMC_A1(MUX_MODE3, flags)
+#define AM33XX_GPMC_A1_GPMC_A17(flags)					AM33XX_PIN_GPMC_A1(MUX_MODE4, flags)
+#define AM33XX_GPMC_A1_PR1_MII1_TXD3(flags)				AM33XX_PIN_GPMC_A1(MUX_MODE5, flags)
+#define AM33XX_GPMC_A1_EHRPWM0_SYNCO(flags)				AM33XX_PIN_GPMC_A1(MUX_MODE6, flags)
+#define AM33XX_GPMC_A1_GPIO1_17(flags)					AM33XX_PIN_GPMC_A1(MUX_MODE7, flags)
+
+#define AM33XX_GPMC_A2(flags)						AM33XX_PIN_GPMC_A2(MUX_MODE0, flags)
+#define AM33XX_GPMC_A2_GMII2_TXD3(flags)				AM33XX_PIN_GPMC_A2(MUX_MODE1, flags)
+#define AM33XX_GPMC_A2_RGMII2_TD3(flags)				AM33XX_PIN_GPMC_A2(MUX_MODE2, flags)
+#define AM33XX_GPMC_A2_MMC2_DAT1(flags)					AM33XX_PIN_GPMC_A2(MUX_MODE3, flags)
+#define AM33XX_GPMC_A2_GPMC_A18(flags)					AM33XX_PIN_GPMC_A2(MUX_MODE4, flags)
+#define AM33XX_GPMC_A2_PR1_MII1_TXD2(flags)				AM33XX_PIN_GPMC_A2(MUX_MODE5, flags)
+#define AM33XX_GPMC_A2_EHRPWM1A(flags)					AM33XX_PIN_GPMC_A2(MUX_MODE6, flags)
+#define AM33XX_GPMC_A2_GPIO1_18(flags)					AM33XX_PIN_GPMC_A2(MUX_MODE7, flags)
+
+#define AM33XX_GPMC_A3(flags)						AM33XX_PIN_GPMC_A3(MUX_MODE0, flags)
+#define AM33XX_GPMC_A3_GMII2_TXD2(flags)				AM33XX_PIN_GPMC_A3(MUX_MODE1, flags)
+#define AM33XX_GPMC_A3_RGMII2_TD2(flags)				AM33XX_PIN_GPMC_A3(MUX_MODE2, flags)
+#define AM33XX_GPMC_A3_MMC2_DAT2(flags)					AM33XX_PIN_GPMC_A3(MUX_MODE3, flags)
+#define AM33XX_GPMC_A3_GPMC_A19(flags)					AM33XX_PIN_GPMC_A3(MUX_MODE4, flags)
+#define AM33XX_GPMC_A3_PR1_MII1_TXD1(flags)				AM33XX_PIN_GPMC_A3(MUX_MODE5, flags)
+#define AM33XX_GPMC_A3_EHRPWM1B(flags)					AM33XX_PIN_GPMC_A3(MUX_MODE6, flags)
+#define AM33XX_GPMC_A3_GPIO1_19(flags)					AM33XX_PIN_GPMC_A3(MUX_MODE7, flags)
+
+#define AM33XX_GPMC_A4(flags)						AM33XX_PIN_GPMC_A4(MUX_MODE0, flags)
+#define AM33XX_GPMC_A4_GMII2_TXD1(flags)				AM33XX_PIN_GPMC_A4(MUX_MODE1, flags)
+#define AM33XX_GPMC_A4_RGMII2_TD1(flags)				AM33XX_PIN_GPMC_A4(MUX_MODE2, flags)
+#define AM33XX_GPMC_A4_RMII2_TXD1(flags)				AM33XX_PIN_GPMC_A4(MUX_MODE3, flags)
+#define AM33XX_GPMC_A4_GPMC_A20(flags)					AM33XX_PIN_GPMC_A4(MUX_MODE4, flags)
+#define AM33XX_GPMC_A4_PR1_MII1_TXD0(flags)				AM33XX_PIN_GPMC_A4(MUX_MODE5, flags)
+#define AM33XX_GPMC_A4_EQEP1A_IN(flags)					AM33XX_PIN_GPMC_A4(MUX_MODE6, flags)
+#define AM33XX_GPMC_A4_GPIO1_20(flags)					AM33XX_PIN_GPMC_A4(MUX_MODE7, flags)
+
+#define AM33XX_GPMC_A5(flags)						AM33XX_PIN_GPMC_A5(MUX_MODE0, flags)
+#define AM33XX_GPMC_A5_GMII2_TXD0(flags)				AM33XX_PIN_GPMC_A5(MUX_MODE1, flags)
+#define AM33XX_GPMC_A5_RGMII2_TD0(flags)				AM33XX_PIN_GPMC_A5(MUX_MODE2, flags)
+#define AM33XX_GPMC_A5_RMII2_TXD0(flags)				AM33XX_PIN_GPMC_A5(MUX_MODE3, flags)
+#define AM33XX_GPMC_A5_GPMC_A21(flags)					AM33XX_PIN_GPMC_A5(MUX_MODE4, flags)
+#define AM33XX_GPMC_A5_PR1_MII1_RXD3(flags)				AM33XX_PIN_GPMC_A5(MUX_MODE5, flags)
+#define AM33XX_GPMC_A5_EQEP1B_IN(flags)					AM33XX_PIN_GPMC_A5(MUX_MODE6, flags)
+#define AM33XX_GPMC_A5_GPIO1_21(flags)					AM33XX_PIN_GPMC_A5(MUX_MODE7, flags)
+
+#define AM33XX_GPMC_A6(flags)						AM33XX_PIN_GPMC_A6(MUX_MODE0, flags)
+#define AM33XX_GPMC_A6_GMII2_TXCLK(flags)				AM33XX_PIN_GPMC_A6(MUX_MODE1, flags)
+#define AM33XX_GPMC_A6_RGMII2_TCLK(flags)				AM33XX_PIN_GPMC_A6(MUX_MODE2, flags)
+#define AM33XX_GPMC_A6_MMC2_DAT4(flags)					AM33XX_PIN_GPMC_A6(MUX_MODE3, flags)
+#define AM33XX_GPMC_A6_GPMC_A22(flags)					AM33XX_PIN_GPMC_A6(MUX_MODE4, flags)
+#define AM33XX_GPMC_A6_PR1_MII1_RXD2(flags)				AM33XX_PIN_GPMC_A6(MUX_MODE5, flags)
+#define AM33XX_GPMC_A6_EQEP1_INDEX(flags)				AM33XX_PIN_GPMC_A6(MUX_MODE6, flags)
+#define AM33XX_GPMC_A6_GPIO1_22(flags)					AM33XX_PIN_GPMC_A6(MUX_MODE7, flags)
+
+#define AM33XX_GPMC_A7(flags)						AM33XX_PIN_GPMC_A7(MUX_MODE0, flags)
+#define AM33XX_GPMC_A7_GMII2_RXCLK(flags)				AM33XX_PIN_GPMC_A7(MUX_MODE1, flags)
+#define AM33XX_GPMC_A7_RGMII2_RCLK(flags)				AM33XX_PIN_GPMC_A7(MUX_MODE2, flags)
+#define AM33XX_GPMC_A7_MMC2_DAT5(flags)					AM33XX_PIN_GPMC_A7(MUX_MODE3, flags)
+#define AM33XX_GPMC_A7_GPMC_A23(flags)					AM33XX_PIN_GPMC_A7(MUX_MODE4, flags)
+#define AM33XX_GPMC_A7_PR1_MII1_RXD1(flags)				AM33XX_PIN_GPMC_A7(MUX_MODE5, flags)
+#define AM33XX_GPMC_A7_EQEP1_STROBE(flags)				AM33XX_PIN_GPMC_A7(MUX_MODE6, flags)
+#define AM33XX_GPMC_A7_GPIO1_23(flags)					AM33XX_PIN_GPMC_A7(MUX_MODE7, flags)
+
+#define AM33XX_GPMC_A8(flags)						AM33XX_PIN_GPMC_A8(MUX_MODE0, flags)
+#define AM33XX_GPMC_A8_GMII2_RXD3(flags)				AM33XX_PIN_GPMC_A8(MUX_MODE1, flags)
+#define AM33XX_GPMC_A8_RGMII2_RD3(flags)				AM33XX_PIN_GPMC_A8(MUX_MODE2, flags)
+#define AM33XX_GPMC_A8_MMC2_DAT6(flags)					AM33XX_PIN_GPMC_A8(MUX_MODE3, flags)
+#define AM33XX_GPMC_A8_GPMC_A24(flags)					AM33XX_PIN_GPMC_A8(MUX_MODE4, flags)
+#define AM33XX_GPMC_A8_PR1_MII1_RXD0(flags)				AM33XX_PIN_GPMC_A8(MUX_MODE5, flags)
+#define AM33XX_GPMC_A8_MCASP0_ACLKX(flags)				AM33XX_PIN_GPMC_A8(MUX_MODE6, flags)
+#define AM33XX_GPMC_A8_GPIO1_24(flags)					AM33XX_PIN_GPMC_A8(MUX_MODE7, flags)
+
+#define AM33XX_GPMC_A9(flags)						AM33XX_PIN_GPMC_A9(MUX_MODE0, flags)
+#define AM33XX_GPMC_A9_GMII2_RXD2(flags)				AM33XX_PIN_GPMC_A9(MUX_MODE1, flags)
+#define AM33XX_GPMC_A9_RGMII2_RD2(flags)				AM33XX_PIN_GPMC_A9(MUX_MODE2, flags)
+#define AM33XX_GPMC_A9_MMC2_DAT7(flags)					AM33XX_PIN_GPMC_A9(MUX_MODE3, flags)
+#define AM33XX_GPMC_A9_RMII2_CRS_DV(flags)				AM33XX_PIN_GPMC_A9(MUX_MODE3, flags)
+#define AM33XX_GPMC_A9_GPMC_A25(flags)					AM33XX_PIN_GPMC_A9(MUX_MODE4, flags)
+#define AM33XX_GPMC_A9_PR1_MII_MR1_CLK(flags)				AM33XX_PIN_GPMC_A9(MUX_MODE5, flags)
+#define AM33XX_GPMC_A9_MCASP0_FSX(flags)				AM33XX_PIN_GPMC_A9(MUX_MODE6, flags)
+#define AM33XX_GPMC_A9_GPIO1_25(flags)					AM33XX_PIN_GPMC_A9(MUX_MODE7, flags)
+
+#define AM33XX_GPMC_A10(flags)						AM33XX_PIN_GPMC_A10(MUX_MODE0, flags)
+#define AM33XX_GPMC_A10_GMII2_RXD1(flags)				AM33XX_PIN_GPMC_A10(MUX_MODE1, flags)
+#define AM33XX_GPMC_A10_RGMII2_RD1(flags)				AM33XX_PIN_GPMC_A10(MUX_MODE2, flags)
+#define AM33XX_GPMC_A10_RMII2_RXD1(flags)				AM33XX_PIN_GPMC_A10(MUX_MODE3, flags)
+#define AM33XX_GPMC_A10_GPMC_A26(flags)					AM33XX_PIN_GPMC_A10(MUX_MODE4, flags)
+#define AM33XX_GPMC_A10_PR1_MII1_RXDV(flags)				AM33XX_PIN_GPMC_A10(MUX_MODE5, flags)
+#define AM33XX_GPMC_A10_MCASP0_AXR0(flags)				AM33XX_PIN_GPMC_A10(MUX_MODE6, flags)
+#define AM33XX_GPMC_A10_GPIO1_26(flags)					AM33XX_PIN_GPMC_A10(MUX_MODE7, flags)
+
+#define AM33XX_GPMC_A11(flags)						AM33XX_PIN_GPMC_A11(MUX_MODE0, flags)
+#define AM33XX_GPMC_A11_GMII2_RXD0(flags)				AM33XX_PIN_GPMC_A11(MUX_MODE1, flags)
+#define AM33XX_GPMC_A11_RGMII2_RD0(flags)				AM33XX_PIN_GPMC_A11(MUX_MODE2, flags)
+#define AM33XX_GPMC_A11_RMII2_RXD0(flags)				AM33XX_PIN_GPMC_A11(MUX_MODE3, flags)
+#define AM33XX_GPMC_A11_GPMC_A27(flags)					AM33XX_PIN_GPMC_A11(MUX_MODE4, flags)
+#define AM33XX_GPMC_A11_PR1_MII1_RXER(flags)				AM33XX_PIN_GPMC_A11(MUX_MODE5, flags)
+#define AM33XX_GPMC_A11_MCASP0_AXR1(flags)				AM33XX_PIN_GPMC_A11(MUX_MODE6, flags)
+#define AM33XX_GPMC_A11_GPIO1_27(flags)					AM33XX_PIN_GPMC_A11(MUX_MODE7, flags)
+
+#define AM33XX_GPMC_AD0(flags)						AM33XX_PIN_GPMC_AD0(MUX_MODE0, flags)
+#define AM33XX_GPMC_AD0_MMC1_DAT0(flags)				AM33XX_PIN_GPMC_AD0(MUX_MODE1, flags)
+#define AM33XX_GPMC_AD0_GPIO1_0(flags)					AM33XX_PIN_GPMC_AD0(MUX_MODE7, flags)
+
+#define AM33XX_GPMC_AD1(flags)						AM33XX_PIN_GPMC_AD1(MUX_MODE0, flags)
+#define AM33XX_GPMC_AD1_MMC1_DAT1(flags)				AM33XX_PIN_GPMC_AD1(MUX_MODE1, flags)
+#define AM33XX_GPMC_AD1_GPIO1_1(flags)					AM33XX_PIN_GPMC_AD1(MUX_MODE7, flags)
+
+#define AM33XX_GPMC_AD2(flags)						AM33XX_PIN_GPMC_AD2(MUX_MODE0, flags)
+#define AM33XX_GPMC_AD2_MMC1_DAT2(flags)				AM33XX_PIN_GPMC_AD2(MUX_MODE1, flags)
+#define AM33XX_GPMC_AD2_GPIO1_2(flags)					AM33XX_PIN_GPMC_AD2(MUX_MODE7, flags)
+
+#define AM33XX_GPMC_AD3(flags)						AM33XX_PIN_GPMC_AD3(MUX_MODE0, flags)
+#define AM33XX_GPMC_AD3_MMC1_DAT3(flags)				AM33XX_PIN_GPMC_AD3(MUX_MODE1, flags)
+#define AM33XX_GPMC_AD3_GPIO1_3(flags)					AM33XX_PIN_GPMC_AD3(MUX_MODE7, flags)
+
+#define AM33XX_GPMC_AD4(flags)						AM33XX_PIN_GPMC_AD4(MUX_MODE0, flags)
+#define AM33XX_GPMC_AD4_MMC1_DAT4(flags)				AM33XX_PIN_GPMC_AD4(MUX_MODE1, flags)
+#define AM33XX_GPMC_AD4_GPIO1_4(flags)					AM33XX_PIN_GPMC_AD4(MUX_MODE7, flags)
+
+#define AM33XX_GPMC_AD5(flags)						AM33XX_PIN_GPMC_AD5(MUX_MODE0, flags)
+#define AM33XX_GPMC_AD5_MMC1_DAT5(flags)				AM33XX_PIN_GPMC_AD5(MUX_MODE1, flags)
+#define AM33XX_GPMC_AD5_GPIO1_5(flags)					AM33XX_PIN_GPMC_AD5(MUX_MODE7, flags)
+
+#define AM33XX_GPMC_AD6(flags)						AM33XX_PIN_GPMC_AD6(MUX_MODE0, flags)
+#define AM33XX_GPMC_AD6_MMC1_DAT6(flags)				AM33XX_PIN_GPMC_AD6(MUX_MODE1, flags)
+#define AM33XX_GPMC_AD6_GPIO1_6(flags)					AM33XX_PIN_GPMC_AD6(MUX_MODE7, flags)
+
+#define AM33XX_GPMC_AD7(flags)						AM33XX_PIN_GPMC_AD7(MUX_MODE0, flags)
+#define AM33XX_MMC1_DAT7(flags)						AM33XX_PIN_GPMC_AD7(MUX_MODE1, flags)
+#define AM33XX_GPMC_AD7_GPIO1_7(flags)					AM33XX_PIN_GPMC_AD7(MUX_MODE7, flags)
+
+#define AM33XX_GPMC_AD8(flags)						AM33XX_PIN_GPMC_AD8(MUX_MODE0, flags)
+#define AM33XX_GPMC_AD8_LCD_DATA23(flags)				AM33XX_PIN_GPMC_AD8(MUX_MODE1, flags)
+#define AM33XX_GPMC_AD8_MMC1_DAT0(flags)				AM33XX_PIN_GPMC_AD8(MUX_MODE2, flags)
+#define AM33XX_GPMC_AD8_MMC2_DAT4(flags)				AM33XX_PIN_GPMC_AD8(MUX_MODE3, flags)
+#define AM33XX_GPMC_AD8_EHRPWM2A(flags)					AM33XX_PIN_GPMC_AD8(MUX_MODE4, flags)
+#define AM33XX_GPMC_AD8_PR1_MII_MT0_CLK(flags)				AM33XX_PIN_GPMC_AD8(MUX_MODE5, flags)
+#define AM33XX_GPMC_AD8_GPIO0_22(flags)					AM33XX_PIN_GPMC_AD8(MUX_MODE7, flags)
+
+#define AM33XX_GPMC_AD9(flags)						AM33XX_PIN_GPMC_AD9(MUX_MODE0, flags)
+#define AM33XX_GPMC_AD9_LCD_DATA22(flags)				AM33XX_PIN_GPMC_AD9(MUX_MODE1, flags)
+#define AM33XX_GPMC_AD9_MMC1_DAT1(flags)				AM33XX_PIN_GPMC_AD9(MUX_MODE2, flags)
+#define AM33XX_GPMC_AD9_MMC2_DAT5(flags)				AM33XX_PIN_GPMC_AD9(MUX_MODE3, flags)
+#define AM33XX_GPMC_AD9_EHRPWM2B(flags)					AM33XX_PIN_GPMC_AD9(MUX_MODE4, flags)
+#define AM33XX_GPMC_AD9_PR1_MII0_COL(flags)				AM33XX_PIN_GPMC_AD9(MUX_MODE5, flags)
+#define AM33XX_GPMC_AD9_GPIO0_23(flags)					AM33XX_PIN_GPMC_AD9(MUX_MODE7, flags)
+
+#define AM33XX_GPMC_AD10(flags)						AM33XX_PIN_GPMC_AD10(MUX_MODE0, flags)
+#define AM33XX_GPMC_AD10_LCD_DATA21(flags)				AM33XX_PIN_GPMC_AD10(MUX_MODE1, flags)
+#define AM33XX_GPMC_AD10_MMC1_DAT2(flags)				AM33XX_PIN_GPMC_AD10(MUX_MODE2, flags)
+#define AM33XX_GPMC_AD10_MMC2_DAT6(flags)				AM33XX_PIN_GPMC_AD10(MUX_MODE3, flags)
+#define AM33XX_GPMC_AD10_EHRPWM2_TRIPZONE_INPUT(flags)			AM33XX_PIN_GPMC_AD10(MUX_MODE4, flags)
+#define AM33XX_GPMC_AD10_PR1_MII0_TXEN(flags)				AM33XX_PIN_GPMC_AD10(MUX_MODE5, flags)
+#define AM33XX_GPMC_AD10_GPIO0_26(flags)				AM33XX_PIN_GPMC_AD10(MUX_MODE7, flags)
+
+#define AM33XX_GPMC_AD11(flags)						AM33XX_PIN_GPMC_AD11(MUX_MODE0, flags)
+#define AM33XX_GPMC_AD11_LCD_DATA20(flags)				AM33XX_PIN_GPMC_AD11(MUX_MODE1, flags)
+#define AM33XX_GPMC_AD11_MMC1_DAT3(flags)				AM33XX_PIN_GPMC_AD11(MUX_MODE2, flags)
+#define AM33XX_GPMC_AD11_MMC2_DAT7(flags)				AM33XX_PIN_GPMC_AD11(MUX_MODE3, flags)
+#define AM33XX_GPMC_AD11_EHRPWM0_SYNCO(flags)				AM33XX_PIN_GPMC_AD11(MUX_MODE4, flags)
+#define AM33XX_GPMC_AD11_PR1_MII0_TXD3(flags)				AM33XX_PIN_GPMC_AD11(MUX_MODE5, flags)
+#define AM33XX_GPMC_AD11_GPIO0_27(flags)				AM33XX_PIN_GPMC_AD11(MUX_MODE7, flags)
+
+#define AM33XX_GPMC_AD12(flags)						AM33XX_PIN_GPMC_AD12(MUX_MODE0, flags)
+#define AM33XX_GPMC_AD12_LCD_DATA19(flags)				AM33XX_PIN_GPMC_AD12(MUX_MODE1, flags)
+#define AM33XX_GPMC_AD12_MMC1_DAT4(flags)				AM33XX_PIN_GPMC_AD12(MUX_MODE2, flags)
+#define AM33XX_GPMC_AD12_MMC2_DAT0(flags)				AM33XX_PIN_GPMC_AD12(MUX_MODE3, flags)
+#define AM33XX_GPMC_AD12_EQEP2A_IN(flags)				AM33XX_PIN_GPMC_AD12(MUX_MODE4, flags)
+#define AM33XX_GPMC_AD12_PR1_MII0_TXD2(flags)				AM33XX_PIN_GPMC_AD12(MUX_MODE5, flags)
+#define AM33XX_GPMC_AD12_PR1_PRU0_PRU_R30_14(flags)			AM33XX_PIN_GPMC_AD12(MUX_MODE6, flags)
+#define AM33XX_GPMC_AD12_GPIO1_12(flags)				AM33XX_PIN_GPMC_AD12(MUX_MODE7, flags)
+
+#define AM33XX_GPMC_AD13(flags)						AM33XX_PIN_GPMC_AD13(MUX_MODE0, flags)
+#define AM33XX_GPMC_AD13_LCD_DATA18(flags)				AM33XX_PIN_GPMC_AD13(MUX_MODE1, flags)
+#define AM33XX_GPMC_AD13_MMC1_DAT5(flags)				AM33XX_PIN_GPMC_AD13(MUX_MODE2, flags)
+#define AM33XX_GPMC_AD13_MMC2_DAT1(flags)				AM33XX_PIN_GPMC_AD13(MUX_MODE3, flags)
+#define AM33XX_GPMC_AD13_EQEP2B_IN(flags)				AM33XX_PIN_GPMC_AD13(MUX_MODE4, flags)
+#define AM33XX_GPMC_AD13_PR1_MII0_TXD1(flags)				AM33XX_PIN_GPMC_AD13(MUX_MODE5, flags)
+#define AM33XX_GPMC_AD13_PR1_PRU0_PRU_R30_15(flags)			AM33XX_PIN_GPMC_AD13(MUX_MODE6, flags)
+#define AM33XX_GPMC_AD13_GPIO1_13(flags)				AM33XX_PIN_GPMC_AD13(MUX_MODE7, flags)
+
+#define AM33XX_GPMC_AD14(flags)						AM33XX_PIN_GPMC_AD14(MUX_MODE0, flags)
+#define AM33XX_GPMC_AD14_LCD_DATA17(flags)				AM33XX_PIN_GPMC_AD14(MUX_MODE1, flags)
+#define AM33XX_GPMC_AD14_MMC1_DAT6(flags)				AM33XX_PIN_GPMC_AD14(MUX_MODE2, flags)
+#define AM33XX_GPMC_AD14_MMC2_DAT2(flags)				AM33XX_PIN_GPMC_AD14(MUX_MODE3, flags)
+#define AM33XX_GPMC_AD14_EQEP2_INDEX(flags)				AM33XX_PIN_GPMC_AD14(MUX_MODE4, flags)
+#define AM33XX_GPMC_AD14_PR1_MII0_TXD0(flags)				AM33XX_PIN_GPMC_AD14(MUX_MODE5, flags)
+#define AM33XX_GPMC_AD14_PR1_PRU0_PRU_R31_14(flags)			AM33XX_PIN_GPMC_AD14(MUX_MODE6, flags)
+#define AM33XX_GPMC_AD14_GPIO1_14(flags)				AM33XX_PIN_GPMC_AD14(MUX_MODE7, flags)
+
+#define AM33XX_GPMC_AD15(flags)						AM33XX_PIN_GPMC_AD15(MUX_MODE0, flags)
+#define AM33XX_GPMC_AD15_LCD_DATA16(flags)				AM33XX_PIN_GPMC_AD15(MUX_MODE1, flags)
+#define AM33XX_GPMC_AD15_MMC1_DAT7(flags)				AM33XX_PIN_GPMC_AD15(MUX_MODE2, flags)
+#define AM33XX_GPMC_AD15_MMC2_DAT3(flags)				AM33XX_PIN_GPMC_AD15(MUX_MODE3, flags)
+#define AM33XX_GPMC_AD15_EQEP2_STROBE(flags)				AM33XX_PIN_GPMC_AD15(MUX_MODE4, flags)
+#define AM33XX_GPMC_AD15_PR1_ECAP0_ECAP_CAPIN_APWM_O(flags)		AM33XX_PIN_GPMC_AD15(MUX_MODE5, flags)
+#define AM33XX_GPMC_AD15_PR1_PRU0_PRU_R31_15(flags)			AM33XX_PIN_GPMC_AD15(MUX_MODE6, flags)
+#define AM33XX_GPMC_AD15_GPIO1_15(flags)				AM33XX_PIN_GPMC_AD15(MUX_MODE7, flags)
+
+#define AM33XX_GPMC_ADVN_ALE(flags)					AM33XX_PIN_GPMC_ADVN_ALE(MUX_MODE0, flags)
+#define AM33XX_GPMC_ADVN_ALE_TIMER4(flags)				AM33XX_PIN_GPMC_ADVN_ALE(MUX_MODE2, flags)
+#define AM33XX_GPMC_ADVN_ALE_GPIO2_2(flags)				AM33XX_PIN_GPMC_ADVN_ALE(MUX_MODE7, flags)
+
+#define AM33XX_GPMC_BEN0_CLE(flags)					AM33XX_PIN_GPMC_BEN0_CLE(MUX_MODE0, flags)
+#define AM33XX_GPMC_BEN0_CLE_TIMER5(flags)				AM33XX_PIN_GPMC_BEN0_CLE(MUX_MODE2, flags)
+#define AM33XX_GPMC_BEN0_CLE_GPIO2_5(flags)				AM33XX_PIN_GPMC_BEN0_CLE(MUX_MODE7, flags)
+
+#define AM33XX_GPMC_BEN1(flags)						AM33XX_PIN_GPMC_BEN1(MUX_MODE0, flags)
+#define AM33XX_GPMC_BEN1_GMII2_COL(flags)				AM33XX_PIN_GPMC_BEN1(MUX_MODE1, flags)
+#define AM33XX_GPMC_BEN1_GPMC_CSN6(flags)				AM33XX_PIN_GPMC_BEN1(MUX_MODE2, flags)
+#define AM33XX_GPMC_BEN1_MMC2_DAT3(flags)				AM33XX_PIN_GPMC_BEN1(MUX_MODE3, flags)
+#define AM33XX_GPMC_BEN1_GPMC_DIR(flags)				AM33XX_PIN_GPMC_BEN1(MUX_MODE4, flags)
+#define AM33XX_GPMC_BEN1_PR1_MII1_RXLINK(flags)				AM33XX_PIN_GPMC_BEN1(MUX_MODE5, flags)
+#define AM33XX_GPMC_BEN1_MCASP0_ACLKR(flags)				AM33XX_PIN_GPMC_BEN1(MUX_MODE6, flags)
+#define AM33XX_GPMC_BEN1_GPIO1_28(flags)				AM33XX_PIN_GPMC_BEN1(MUX_MODE7, flags)
+
+#define AM33XX_GPMC_CLK(flags)						AM33XX_PIN_GPMC_CLK(MUX_MODE0, flags)
+#define AM33XX_GPMC_CLK_LCD_MEMORY_CLK(flags)				AM33XX_PIN_GPMC_CLK(MUX_MODE1, flags)
+#define AM33XX_GPMC_CLK_GPMC_WAIT1(flags)				AM33XX_PIN_GPMC_CLK(MUX_MODE2, flags)
+#define AM33XX_GPMC_CLK_MMC2_CLK(flags)					AM33XX_PIN_GPMC_CLK(MUX_MODE3, flags)
+#define AM33XX_GPMC_CLK_PR1_MII1_CRS(flags)				AM33XX_PIN_GPMC_CLK(MUX_MODE4, flags)
+#define AM33XX_GPMC_CLK_PR1_MDIO_MDCLK(flags)				AM33XX_PIN_GPMC_CLK(MUX_MODE5, flags)
+#define AM33XX_GPMC_CLK_MCASP0_FSR(flags)				AM33XX_PIN_GPMC_CLK(MUX_MODE6, flags)
+#define AM33XX_GPMC_CLK_GPIO2_1(flags)					AM33XX_PIN_GPMC_CLK(MUX_MODE7, flags)
+
+#define AM33XX_GPMC_CSN0(flags)						AM33XX_PIN_GPMC_CSN0(MUX_MODE0, flags)
+#define AM33XX_GPMC_CSN0_GPIO1_29(flags)				AM33XX_PIN_GPMC_CSN0(MUX_MODE7, flags)
+
+#define AM33XX_GPMC_CSN1(flags)						AM33XX_PIN_GPMC_CSN1(MUX_MODE0, flags)
+#define AM33XX_GPMC_CSN1_GPMC_CLK(flags)				AM33XX_PIN_GPMC_CSN1(MUX_MODE1, flags)
+#define AM33XX_GPMC_CSN1_MMC1_CLK(flags)				AM33XX_PIN_GPMC_CSN1(MUX_MODE2, flags)
+#define AM33XX_GPMC_CSN1_PR1_EDIO_DATA_IN6(flags)			AM33XX_PIN_GPMC_CSN1(MUX_MODE3, flags)
+#define AM33XX_GPMC_CSN1_PR1_EDIO_DATA_OUT6(flags)			AM33XX_PIN_GPMC_CSN1(MUX_MODE4, flags)
+#define AM33XX_GPMC_CSN1_PR1_PRU1_PRU_R30_12(flags)			AM33XX_PIN_GPMC_CSN1(MUX_MODE5, flags)
+#define AM33XX_GPMC_CSN1_PR1_PRU1_PRU_R31_12(flags)			AM33XX_PIN_GPMC_CSN1(MUX_MODE6, flags)
+#define AM33XX_GPMC_CSN1_GPIO1_30(flags)				AM33XX_PIN_GPMC_CSN1(MUX_MODE7, flags)
+
+#define AM33XX_GPMC_CSN2(flags)						AM33XX_PIN_GPMC_CSN2(MUX_MODE0, flags)
+#define AM33XX_GPMC_CSN2_GPMC_BE1N(flags)				AM33XX_PIN_GPMC_CSN2(MUX_MODE1, flags)
+#define AM33XX_GPMC_CSN2_MMC1_CMD(flags)				AM33XX_PIN_GPMC_CSN2(MUX_MODE2, flags)
+#define AM33XX_GPMC_CSN2_PR1_EDIO_DATA_IN7(flags)			AM33XX_PIN_GPMC_CSN2(MUX_MODE3, flags)
+#define AM33XX_GPMC_CSN2_PR1_EDIO_DATA_OUT7(flags)			AM33XX_PIN_GPMC_CSN2(MUX_MODE4, flags)
+#define AM33XX_GPMC_CSN2_PR1_PRU1_PRU_R30_13(flags)			AM33XX_PIN_GPMC_CSN2(MUX_MODE5, flags)
+#define AM33XX_GPMC_CSN2_PR1_PRU1_PRU_R31_13(flags)			AM33XX_PIN_GPMC_CSN2(MUX_MODE6, flags)
+#define AM33XX_GPMC_CSN2_GPIO1_31(flags)				AM33XX_PIN_GPMC_CSN2(MUX_MODE7, flags)
+
+#define AM33XX_GPMC_CSN3(flags)						AM33XX_PIN_GPMC_CSN3(MUX_MODE0, flags)
+#define AM33XX_GPMC_CSN3_GPMC_CSN3_GPMC_A3(flags)			AM33XX_PIN_GPMC_CSN3(MUX_MODE1, flags)
+#define AM33XX_GPMC_CSN3_RMII2_CRS_DV(flags)				AM33XX_PIN_GPMC_CSN3(MUX_MODE2, flags)
+#define AM33XX_GPMC_CSN3_MMC2_CMD(flags)				AM33XX_PIN_GPMC_CSN3(MUX_MODE3, flags)
+#define AM33XX_GPMC_CSN3_PR1_MII0_CRS(flags)				AM33XX_PIN_GPMC_CSN3(MUX_MODE4, flags)
+#define AM33XX_GPMC_CSN3_PR1_MDIO_DATA(flags)				AM33XX_PIN_GPMC_CSN3(MUX_MODE5, flags)
+#define AM33XX_GPMC_CSN3_EMU4(flags)					AM33XX_PIN_GPMC_CSN3(MUX_MODE6, flags)
+#define AM33XX_GPMC_CSN3_GPIO2_0(flags)					AM33XX_PIN_GPMC_CSN3(MUX_MODE7, flags)
+
+#define AM33XX_GPMC_OEN_REN(flags)					AM33XX_PIN_GPMC_OEN_REN(MUX_MODE0, flags)
+#define AM33XX_GPMC_OEN_REN_TIMER7(flags)				AM33XX_PIN_GPMC_OEN_REN(MUX_MODE2, flags)
+#define AM33XX_GPMC_OEN_REN_GPIO2_3(flags)				AM33XX_PIN_GPMC_OEN_REN(MUX_MODE7, flags)
+
+#define AM33XX_GPMC_WAIT0(flags)					AM33XX_PIN_GPMC_WAIT0(MUX_MODE0, flags)
+#define AM33XX_GPMC_WAIT0_GMII2_CRS(flags)				AM33XX_PIN_GPMC_WAIT0(MUX_MODE1, flags)
+#define AM33XX_GPMC_WAIT0_GPMC_CSN4(flags)				AM33XX_PIN_GPMC_WAIT0(MUX_MODE2, flags)
+#define AM33XX_GPMC_WAIT0_RMII2_CRS_DV(flags)				AM33XX_PIN_GPMC_WAIT0(MUX_MODE3, flags)
+#define AM33XX_GPMC_WAIT0_MMC1_SDCD(flags)				AM33XX_PIN_GPMC_WAIT0(MUX_MODE4, flags)
+#define AM33XX_GPMC_WAIT0_PR1_MII1_COL(flags)				AM33XX_PIN_GPMC_WAIT0(MUX_MODE5, flags)
+#define AM33XX_GPMC_WAIT0_UART4_RXD(flags)				AM33XX_PIN_GPMC_WAIT0(MUX_MODE6, flags)
+#define AM33XX_GPMC_WAIT0_GPIO0_30(flags)				AM33XX_PIN_GPMC_WAIT0(MUX_MODE7, flags)
+
+#define AM33XX_GPMC_WEN(flags)						AM33XX_PIN_GPMC_WEN(MUX_MODE0, flags)
+#define AM33XX_GPMC_WEN_TIMER6(flags)					AM33XX_PIN_GPMC_WEN(MUX_MODE2, flags)
+#define AM33XX_GPMC_WEN_GPIO2_4(flags)					AM33XX_PIN_GPMC_WEN(MUX_MODE7, flags)
+
+#define AM33XX_GPMC_WPN(flags)						AM33XX_PIN_GPMC_WPN(MUX_MODE0, flags)
+#define AM33XX_GPMC_WPN_GMII2_RXERR(flags)				AM33XX_PIN_GPMC_WPN(MUX_MODE1, flags)
+#define AM33XX_GPMC_WPN_GPMC_CSN5(flags)				AM33XX_PIN_GPMC_WPN(MUX_MODE2, flags)
+#define AM33XX_GPMC_WPN_RMII2_RXERR(flags)				AM33XX_PIN_GPMC_WPN(MUX_MODE3, flags)
+#define AM33XX_GPMC_WPN_MMC2_SDCD(flags)				AM33XX_PIN_GPMC_WPN(MUX_MODE4, flags)
+#define AM33XX_GPMC_WPN_PR1_MII1_TXEN(flags)				AM33XX_PIN_GPMC_WPN(MUX_MODE5, flags)
+#define AM33XX_GPMC_WPN_UART4_TXD(flags)				AM33XX_PIN_GPMC_WPN(MUX_MODE6, flags)
+#define AM33XX_GPMC_WPN_GPIO0_31(flags)					AM33XX_PIN_GPMC_WPN(MUX_MODE7, flags)
+
+#define AM33XX_I2C0_SDA(flags)						AM33XX_PIN_I2C0_SDA(MUX_MODE0, flags)
+#define AM33XX_I2C0_SDA_TIMER4(flags)					AM33XX_PIN_I2C0_SDA(MUX_MODE1, flags)
+#define AM33XX_I2C0_SDA_UART2_CTSN(flags)				AM33XX_PIN_I2C0_SDA(MUX_MODE2, flags)
+#define AM33XX_I2C0_SDA_ECAP2_IN_PWM2_OUT(flags)			AM33XX_PIN_I2C0_SDA(MUX_MODE3, flags)
+#define AM33XX_I2C0_SDA_GPIO3_5(flags)					AM33XX_PIN_I2C0_SDA(MUX_MODE7, flags)
+
+#define AM33XX_I2C0_SCL(flags)						AM33XX_PIN_I2C0_SCL(MUX_MODE0, flags)
+#define AM33XX_I2C0_SCL_TIMER7(flags)					AM33XX_PIN_I2C0_SCL(MUX_MODE1, flags)
+#define AM33XX_I2C0_SCL_UART2_RTSN(flags)				AM33XX_PIN_I2C0_SCL(MUX_MODE2, flags)
+#define AM33XX_I2C0_SCL_ECAP1_IN_PWM1_OUT(flags)			AM33XX_PIN_I2C0_SCL(MUX_MODE3, flags)
+#define AM33XX_I2C0_SCL_GPIO3_6(flags)					AM33XX_PIN_I2C0_SCL(MUX_MODE7, flags)
+
+#define AM33XX_LCD_AC_BIAS_EN(flags)					AM33XX_PIN_LCD_AC_BIAS_EN(MUX_MODE0, flags)
+#define AM33XX_LCD_AC_BIAS_EN_GPMC_A11(flags)				AM33XX_PIN_LCD_AC_BIAS_EN(MUX_MODE1, flags)
+#define AM33XX_LCD_AC_BIAS_EN_PR1_MII1_CRS(flags)			AM33XX_PIN_LCD_AC_BIAS_EN(MUX_MODE2, flags)
+#define AM33XX_LCD_AC_BIAS_EN_PR1_EDIO_DATA_IN5(flags)			AM33XX_PIN_LCD_AC_BIAS_EN(MUX_MODE3, flags)
+#define AM33XX_LCD_AC_BIAS_EN_PR1_EDIO_DATA_OUT5(flags)			AM33XX_PIN_LCD_AC_BIAS_EN(MUX_MODE4, flags)
+#define AM33XX_LCD_AC_BIAS_EN_PR1_PRU1_PRU_R30_11(flags)		AM33XX_PIN_LCD_AC_BIAS_EN(MUX_MODE5, flags)
+#define AM33XX_LCD_AC_BIAS_EN_PR1_PRU1_PRU_R31_11(flags)		AM33XX_PIN_LCD_AC_BIAS_EN(MUX_MODE6, flags)
+#define AM33XX_LCD_AC_BIAS_EN_GPIO2_25(flags)				AM33XX_PIN_LCD_AC_BIAS_EN(MUX_MODE7, flags)
+
+#define AM33XX_LCD_DATA0(flags)						AM33XX_PIN_LCD_DATA0(MUX_MODE0, flags)
+#define AM33XX_LCD_DATA0_GPMC_A0(flags)					AM33XX_PIN_LCD_DATA0(MUX_MODE1, flags)
+#define AM33XX_LCD_DATA0_PR1_MII_MT0_CLK(flags)				AM33XX_PIN_LCD_DATA0(MUX_MODE2, flags)
+#define AM33XX_LCD_DATA0_EHRPWM2A(flags)				AM33XX_PIN_LCD_DATA0(MUX_MODE3, flags)
+#define AM33XX_LCD_DATA0_PR1_PRU1_PRU_R30_0(flags)			AM33XX_PIN_LCD_DATA0(MUX_MODE5, flags)
+#define AM33XX_LCD_DATA0_PR1_PRU1_PRU_R31_0(flags)			AM33XX_PIN_LCD_DATA0(MUX_MODE6, flags)
+#define AM33XX_LCD_DATA0_GPIO2_6(flags)					AM33XX_PIN_LCD_DATA0(MUX_MODE7, flags)
+
+#define AM33XX_LCD_DATA1(flags)						AM33XX_PIN_LCD_DATA1(MUX_MODE0, flags)
+#define AM33XX_LCD_DATA1_GPMC_A1(flags)					AM33XX_PIN_LCD_DATA1(MUX_MODE1, flags)
+#define AM33XX_LCD_DATA1_PR1_MII0_TXEN(flags)				AM33XX_PIN_LCD_DATA1(MUX_MODE2, flags)
+#define AM33XX_LCD_DATA1_EHRPWM2B(flags)				AM33XX_PIN_LCD_DATA1(MUX_MODE3, flags)
+#define AM33XX_LCD_DATA1_PR1_PRU1_PRU_R30_1(flags)			AM33XX_PIN_LCD_DATA1(MUX_MODE5, flags)
+#define AM33XX_LCD_DATA1_PR1_PRU1_PRU_R31_1(flags)			AM33XX_PIN_LCD_DATA1(MUX_MODE6, flags)
+#define AM33XX_LCD_DATA1_GPIO2_7(flags)					AM33XX_PIN_LCD_DATA1(MUX_MODE7, flags)
+
+#define AM33XX_LCD_DATA2(flags)						AM33XX_PIN_LCD_DATA2(MUX_MODE0, flags)
+#define AM33XX_LCD_DATA2_GPMC_A2(flags)					AM33XX_PIN_LCD_DATA2(MUX_MODE1, flags)
+#define AM33XX_LCD_DATA2_PR1_MII0_TXD3(flags)				AM33XX_PIN_LCD_DATA2(MUX_MODE2, flags)
+#define AM33XX_LCD_DATA2_EHRPWM2_TRIPZONE_INPUT(flags)			AM33XX_PIN_LCD_DATA2(MUX_MODE3, flags)
+#define AM33XX_LCD_DATA2_PR1_PRU1_PRU_R30_2(flags)			AM33XX_PIN_LCD_DATA2(MUX_MODE5, flags)
+#define AM33XX_LCD_DATA2_PR1_PRU1_PRU_R31_2(flags)			AM33XX_PIN_LCD_DATA2(MUX_MODE6, flags)
+#define AM33XX_LCD_DATA2_GPIO2_8(flags)					AM33XX_PIN_LCD_DATA2(MUX_MODE7, flags)
+
+#define AM33XX_LCD_DATA3(flags)						AM33XX_PIN_LCD_DATA3(MUX_MODE0, flags)
+#define AM33XX_LCD_DATA3_GPMC_A3(flags)					AM33XX_PIN_LCD_DATA3(MUX_MODE1, flags)
+#define AM33XX_LCD_DATA3_PR1_MII0_TXD2(flags)				AM33XX_PIN_LCD_DATA3(MUX_MODE2, flags)
+#define AM33XX_LCD_DATA3_LCD_DATA3_EHRPWM0_SYNCO(flags)			AM33XX_PIN_LCD_DATA3(MUX_MODE3, flags)
+#define AM33XX_LCD_DATA3_PR1_PRU1_PRU_R30_3(flags)			AM33XX_PIN_LCD_DATA3(MUX_MODE5, flags)
+#define AM33XX_LCD_DATA3_PR1_PRU1_PRU_R31_3(flags)			AM33XX_PIN_LCD_DATA3(MUX_MODE6, flags)
+#define AM33XX_LCD_DATA3_GPIO2_9(flags)					AM33XX_PIN_LCD_DATA3(MUX_MODE7, flags)
+
+#define AM33XX_LCD_DATA4(flags)						AM33XX_PIN_LCD_DATA4(MUX_MODE0, flags)
+#define AM33XX_LCD_DATA4_GPMC_A4(flags)					AM33XX_PIN_LCD_DATA4(MUX_MODE1, flags)
+#define AM33XX_LCD_DATA4_PR1_MII0_TXD1(flags)				AM33XX_PIN_LCD_DATA4(MUX_MODE2, flags)
+#define AM33XX_LCD_DATA4_EQEP2A_IN(flags)				AM33XX_PIN_LCD_DATA4(MUX_MODE3, flags)
+#define AM33XX_LCD_DATA4_PR1_PRU1_PRU_R30_4(flags)			AM33XX_PIN_LCD_DATA4(MUX_MODE5, flags)
+#define AM33XX_LCD_DATA4_PR1_PRU1_PRU_R31_4(flags)			AM33XX_PIN_LCD_DATA4(MUX_MODE6, flags)
+#define AM33XX_LCD_DATA4_GPIO2_10(flags)				AM33XX_PIN_LCD_DATA4(MUX_MODE7, flags)
+
+#define AM33XX_LCD_DATA5(flags)						AM33XX_PIN_LCD_DATA5(MUX_MODE0, flags)
+#define AM33XX_LCD_DATA5_GPMC_A5(flags)					AM33XX_PIN_LCD_DATA5(MUX_MODE1, flags)
+#define AM33XX_LCD_DATA5_PR1_MII0_TXD0(flags)				AM33XX_PIN_LCD_DATA5(MUX_MODE2, flags)
+#define AM33XX_LCD_DATA5_EQEP2B_IN(flags)				AM33XX_PIN_LCD_DATA5(MUX_MODE3, flags)
+#define AM33XX_LCD_DATA5_PR1_PRU1_PRU_R30_5(flags)			AM33XX_PIN_LCD_DATA5(MUX_MODE5, flags)
+#define AM33XX_LCD_DATA5_PR1_PRU1_PRU_R31_5(flags)			AM33XX_PIN_LCD_DATA5(MUX_MODE6, flags)
+#define AM33XX_LCD_DATA5_GPIO2_11(flags)				AM33XX_PIN_LCD_DATA5(MUX_MODE7, flags)
+
+#define AM33XX_LCD_DATA6(flags)						AM33XX_PIN_LCD_DATA6(MUX_MODE0, flags)
+#define AM33XX_LCD_DATA6_GPMC_A6(flags)					AM33XX_PIN_LCD_DATA6(MUX_MODE1, flags)
+#define AM33XX_LCD_DATA6_PR1_EDIO_DATA_IN6(flags)			AM33XX_PIN_LCD_DATA6(MUX_MODE2, flags)
+#define AM33XX_LCD_DATA6_EQEP2_INDEX(flags)				AM33XX_PIN_LCD_DATA6(MUX_MODE3, flags)
+#define AM33XX_LCD_DATA6_PR1_EDIO_DATA_OUT6(flags)			AM33XX_PIN_LCD_DATA6(MUX_MODE4, flags)
+#define AM33XX_LCD_DATA6_PR1_PRU1_PRU_R30_6(flags)			AM33XX_PIN_LCD_DATA6(MUX_MODE5, flags)
+#define AM33XX_LCD_DATA6_PR1_PRU1_PRU_R31_6(flags)			AM33XX_PIN_LCD_DATA6(MUX_MODE6, flags)
+#define AM33XX_LCD_DATA6_GPIO2_12(flags)				AM33XX_PIN_LCD_DATA6(MUX_MODE7, flags)
+
+#define AM33XX_LCD_DATA7(flags)						AM33XX_PIN_LCD_DATA7(MUX_MODE0, flags)
+#define AM33XX_LCD_DATA7_GPMC_A7(flags)					AM33XX_PIN_LCD_DATA7(MUX_MODE1, flags)
+#define AM33XX_LCD_DATA7_PR1_EDIO_DATA_IN7(flags)			AM33XX_PIN_LCD_DATA7(MUX_MODE2, flags)
+#define AM33XX_LCD_DATA7_EQEP2_STROBE(flags)				AM33XX_PIN_LCD_DATA7(MUX_MODE3, flags)
+#define AM33XX_LCD_DATA7_PR1_EDIO_DATA_OUT7(flags)			AM33XX_PIN_LCD_DATA7(MUX_MODE4, flags)
+#define AM33XX_LCD_DATA7_PR1_PRU1_PRU_R30_7(flags)			AM33XX_PIN_LCD_DATA7(MUX_MODE5, flags)
+#define AM33XX_LCD_DATA7_PR1_PRU1_PRU_R31_7(flags)			AM33XX_PIN_LCD_DATA7(MUX_MODE6, flags)
+#define AM33XX_LCD_DATA7_GPIO2_13(flags)				AM33XX_PIN_LCD_DATA7(MUX_MODE7, flags)
+
+#define AM33XX_LCD_DATA8(flags)						AM33XX_PIN_LCD_DATA8(MUX_MODE0, flags)
+#define AM33XX_LCD_DATA8_GPMC_A12(flags)				AM33XX_PIN_LCD_DATA8(MUX_MODE1, flags)
+#define AM33XX_LCD_DATA8_EHRPWM1_TRIPZONE_INPUT(flags)			AM33XX_PIN_LCD_DATA8(MUX_MODE2, flags)
+#define AM33XX_LCD_DATA8_MCASP0_ACLKX(flags)				AM33XX_PIN_LCD_DATA8(MUX_MODE3, flags)
+#define AM33XX_LCD_DATA8_UART5_TXD(flags)				AM33XX_PIN_LCD_DATA8(MUX_MODE4, flags)
+#define AM33XX_LCD_DATA8_PR1_MII0_RXD3(flags)				AM33XX_PIN_LCD_DATA8(MUX_MODE5, flags)
+#define AM33XX_LCD_DATA8_UART2_CTSN(flags)				AM33XX_PIN_LCD_DATA8(MUX_MODE6, flags)
+#define AM33XX_LCD_DATA8_GPIO2_14(flags)				AM33XX_PIN_LCD_DATA8(MUX_MODE7, flags)
+
+#define AM33XX_LCD_DATA9(flags)						AM33XX_PIN_LCD_DATA9(MUX_MODE0, flags)
+#define AM33XX_LCD_DATA9_GPMC_A13(flags)				AM33XX_PIN_LCD_DATA9(MUX_MODE1, flags)
+#define AM33XX_LCD_DATA9_EHRPWM0_SYNCO(flags)				AM33XX_PIN_LCD_DATA9(MUX_MODE2, flags)
+#define AM33XX_LCD_DATA9_MCASP0_FSX(flags)				AM33XX_PIN_LCD_DATA9(MUX_MODE3, flags)
+#define AM33XX_LCD_DATA9_UART5_RXD(flags)				AM33XX_PIN_LCD_DATA9(MUX_MODE4, flags)
+#define AM33XX_LCD_DATA9_PR1_MII0_RXD2(flags)				AM33XX_PIN_LCD_DATA9(MUX_MODE5, flags)
+#define AM33XX_LCD_DATA9_UART2_RTSN(flags)				AM33XX_PIN_LCD_DATA9(MUX_MODE6, flags)
+#define AM33XX_LCD_DATA9_GPIO2_15(flags)				AM33XX_PIN_LCD_DATA9(MUX_MODE7, flags)
+
+#define AM33XX_LCD_DATA10(flags)					AM33XX_PIN_LCD_DATA10(MUX_MODE0, flags)
+#define AM33XX_LCD_DATA10_GPMC_A14(flags)				AM33XX_PIN_LCD_DATA10(MUX_MODE1, flags)
+#define AM33XX_LCD_DATA10_EHRPWM1A(flags)				AM33XX_PIN_LCD_DATA10(MUX_MODE2, flags)
+#define AM33XX_LCD_DATA10_MCASP0_AXR0(flags)				AM33XX_PIN_LCD_DATA10(MUX_MODE3, flags)
+#define AM33XX_LCD_DATA10_PR1_MII0_RXD1(flags)				AM33XX_PIN_LCD_DATA10(MUX_MODE5, flags)
+#define AM33XX_LCD_DATA10_UART3_CTSN(flags)				AM33XX_PIN_LCD_DATA10(MUX_MODE6, flags)
+#define AM33XX_LCD_DATA10_GPIO2_16(flags)				AM33XX_PIN_LCD_DATA10(MUX_MODE7, flags)
+
+#define AM33XX_LCD_DATA11(flags)					AM33XX_PIN_LCD_DATA11(MUX_MODE0, flags)
+#define AM33XX_LCD_DATA11_GPMC_A15(flags)				AM33XX_PIN_LCD_DATA11(MUX_MODE1, flags)
+#define AM33XX_LCD_DATA11_EHRPWM1B(flags)				AM33XX_PIN_LCD_DATA11(MUX_MODE2, flags)
+#define AM33XX_LCD_DATA11_MCASP0_AHCLKR(flags)				AM33XX_PIN_LCD_DATA11(MUX_MODE3, flags)
+#define AM33XX_LCD_DATA11_MCASP0_AXR2(flags)				AM33XX_PIN_LCD_DATA11(MUX_MODE4, flags)
+#define AM33XX_LCD_DATA11_PR1_MII0_RXD0(flags)				AM33XX_PIN_LCD_DATA11(MUX_MODE5, flags)
+#define AM33XX_LCD_DATA11_UART3_RTSN(flags)				AM33XX_PIN_LCD_DATA11(MUX_MODE6, flags)
+#define AM33XX_LCD_DATA11_GPIO2_17(flags)				AM33XX_PIN_LCD_DATA11(MUX_MODE7, flags)
+
+#define AM33XX_LCD_DATA12_LCD_DATA12(flags)				AM33XX_PIN_LCD_DATA12(MUX_MODE0, flags)
+#define AM33XX_LCD_DATA12_GPMC_A16(flags)				AM33XX_PIN_LCD_DATA12(MUX_MODE1, flags)
+#define AM33XX_LCD_DATA12_EQEP1A_IN(flags)				AM33XX_PIN_LCD_DATA12(MUX_MODE2, flags)
+#define AM33XX_LCD_DATA12_MCASP0_ACLKR(flags)				AM33XX_PIN_LCD_DATA12(MUX_MODE3, flags)
+#define AM33XX_LCD_DATA12_MCASP0_AXR2(flags)				AM33XX_PIN_LCD_DATA12(MUX_MODE4, flags)
+#define AM33XX_LCD_DATA12_PR1_MII0_RXLINK(flags)			AM33XX_PIN_LCD_DATA12(MUX_MODE5, flags)
+#define AM33XX_LCD_DATA12_UART4_CTSN(flags)				AM33XX_PIN_LCD_DATA12(MUX_MODE6, flags)
+#define AM33XX_LCD_DATA12_GPIO0_8(flags)				AM33XX_PIN_LCD_DATA12(MUX_MODE7, flags)
+
+#define AM33XX_LCD_DATA13_LCD_DATA13(flags)				AM33XX_PIN_LCD_DATA13(MUX_MODE0, flags)
+#define AM33XX_LCD_DATA13_GPMC_A17(flags)				AM33XX_PIN_LCD_DATA13(MUX_MODE1, flags)
+#define AM33XX_LCD_DATA13_EQEP1B_IN(flags)				AM33XX_PIN_LCD_DATA13(MUX_MODE2, flags)
+#define AM33XX_LCD_DATA13_MCASP0_FSR(flags)				AM33XX_PIN_LCD_DATA13(MUX_MODE3, flags)
+#define AM33XX_LCD_DATA13_MCASP0_AXR3(flags)				AM33XX_PIN_LCD_DATA13(MUX_MODE4, flags)
+#define AM33XX_LCD_DATA13_PR1_MII0_RXER(flags)				AM33XX_PIN_LCD_DATA13(MUX_MODE5, flags)
+#define AM33XX_LCD_DATA13_UART4_RTSN(flags)				AM33XX_PIN_LCD_DATA13(MUX_MODE6, flags)
+#define AM33XX_LCD_DATA13_GPIO0_9(flags)				AM33XX_PIN_LCD_DATA13(MUX_MODE7, flags)
+
+#define AM33XX_LCD_DATA14(flags)					AM33XX_PIN_LCD_DATA14(MUX_MODE0, flags)
+#define AM33XX_LCD_DATA14_GPMC_A18(flags)				AM33XX_PIN_LCD_DATA14(MUX_MODE1, flags)
+#define AM33XX_LCD_DATA14_EQEP1_INDEX(flags)				AM33XX_PIN_LCD_DATA14(MUX_MODE2, flags)
+#define AM33XX_LCD_DATA14_MCASP0_AXR1(flags)				AM33XX_PIN_LCD_DATA14(MUX_MODE3, flags)
+#define AM33XX_LCD_DATA14_UART5_RXD(flags)				AM33XX_PIN_LCD_DATA14(MUX_MODE4, flags)
+#define AM33XX_LCD_DATA14_PR1_MII_MR0_CLK(flags)			AM33XX_PIN_LCD_DATA14(MUX_MODE5, flags)
+#define AM33XX_LCD_DATA14_UART5_CTSN(flags)				AM33XX_PIN_LCD_DATA14(MUX_MODE6, flags)
+#define AM33XX_LCD_DATA14_GPIO0_10(flags)				AM33XX_PIN_LCD_DATA14(MUX_MODE7, flags)
+
+#define AM33XX_LCD_DATA15(flags)					AM33XX_PIN_LCD_DATA15(MUX_MODE0, flags)
+#define AM33XX_LCD_DATA15_GPMC_A19(flags)				AM33XX_PIN_LCD_DATA15(MUX_MODE1, flags)
+#define AM33XX_LCD_DATA15_EQEP1_STROBE(flags)				AM33XX_PIN_LCD_DATA15(MUX_MODE2, flags)
+#define AM33XX_LCD_DATA15_MCASP0_AHCLKX(flags)				AM33XX_PIN_LCD_DATA15(MUX_MODE3, flags)
+#define AM33XX_LCD_DATA15_MCASP0_AXR3(flags)				AM33XX_PIN_LCD_DATA15(MUX_MODE4, flags)
+#define AM33XX_LCD_DATA15_PR1_MII0_RXDV(flags)				AM33XX_PIN_LCD_DATA15(MUX_MODE5, flags)
+#define AM33XX_LCD_DATA15_UART5_RTSN(flags)				AM33XX_PIN_LCD_DATA15(MUX_MODE6, flags)
+#define AM33XX_LCD_DATA15_GPIO0_11(flags)				AM33XX_PIN_LCD_DATA15(MUX_MODE7, flags)
+
+#define AM33XX_LCD_HSYNC(flags)						AM33XX_PIN_LCD_HSYNC(MUX_MODE0, flags)
+#define AM33XX_LCD_HSYNC_GPMC_A9(flags)					AM33XX_PIN_LCD_HSYNC(MUX_MODE1, flags)
+#define AM33XX_LCD_HSYNC_GPMC_A2(flags)					AM33XX_PIN_LCD_HSYNC(MUX_MODE2, flags)
+#define AM33XX_LCD_HSYNC_PR1_EDIO_DATA_IN3(flags)			AM33XX_PIN_LCD_HSYNC(MUX_MODE3, flags)
+#define AM33XX_LCD_HSYNC_PR1_EDIO_DATA_OUT3(flags)			AM33XX_PIN_LCD_HSYNC(MUX_MODE4, flags)
+#define AM33XX_LCD_HSYNC_PR1_PRU1_PRU_R30_9(flags)			AM33XX_PIN_LCD_HSYNC(MUX_MODE5, flags)
+#define AM33XX_LCD_HSYNC_PR1_PRU1_PRU_R31_9(flags)			AM33XX_PIN_LCD_HSYNC(MUX_MODE6, flags)
+#define AM33XX_LCD_HSYNC_GPIO2_23(flags)				AM33XX_PIN_LCD_HSYNC(MUX_MODE7, flags)
+
+#define AM33XX_LCD_PCLK(flags)						AM33XX_PIN_LCD_PCLK(MUX_MODE0, flags)
+#define AM33XX_LCD_PCLK_GPMC_A10(flags)					AM33XX_PIN_LCD_PCLK(MUX_MODE1, flags)
+#define AM33XX_LCD_PCLK_PR1_MII0_CRS(flags)				AM33XX_PIN_LCD_PCLK(MUX_MODE2, flags)
+#define AM33XX_LCD_PCLK_PR1_EDIO_DATA_IN4(flags)			AM33XX_PIN_LCD_PCLK(MUX_MODE3, flags)
+#define AM33XX_LCD_PCLK_PR1_EDIO_DATA_OUT4(flags)			AM33XX_PIN_LCD_PCLK(MUX_MODE4, flags)
+#define AM33XX_LCD_PCLK_PR1_PRU1_PRU_R30_10(flags)			AM33XX_PIN_LCD_PCLK(MUX_MODE5, flags)
+#define AM33XX_LCD_PCLK_PR1_PRU1_PRU_R31_10(flags)			AM33XX_PIN_LCD_PCLK(MUX_MODE6, flags)
+#define AM33XX_LCD_PCLK_GPIO2_24(flags)					AM33XX_PIN_LCD_PCLK(MUX_MODE7, flags)
+
+#define AM33XXLCD_VSYNC(flags)						AM33XX_PIN_LCD_VSYNC(MUX_MODE0, flags)
+#define AM33XX_LCD_VSYNC_GPMC_A8(flags)					AM33XX_PIN_LCD_VSYNC(MUX_MODE1, flags)
+#define AM33XX_LCD_VSYNC_GPMC_A1(flags)					AM33XX_PIN_LCD_VSYNC(MUX_MODE2, flags)
+#define AM33XX_LCD_VSYNC_PR1_EDIO_DATA_IN2(flags)			AM33XX_PIN_LCD_VSYNC(MUX_MODE3, flags)
+#define AM33XX_LCD_VSYNC_PR1_EDIO_DATA_OUT2(flags)			AM33XX_PIN_LCD_VSYNC(MUX_MODE4, flags)
+#define AM33XX_LCD_VSYNC_PR1_PRU1_PRU_R30_8(flags)			AM33XX_PIN_LCD_VSYNC(MUX_MODE5, flags)
+#define AM33XX_LCD_VSYNC_PR1_PRU1_PRU_R31_8(flags)			AM33XX_PIN_LCD_VSYNC(MUX_MODE6, flags)
+#define AM33XX_LCD_VSYNC_GPIO2_22(flags)				AM33XX_PIN_LCD_VSYNC(MUX_MODE7, flags)
+
+#define AM33XX_MCASP0_FSX(flags)					AM33XX_PIN_MCASP0_FSX(MUX_MODE0, flags)
+#define AM33XX_MCASP0_FSX_EHRPWM0B(flags)				AM33XX_PIN_MCASP0_FSX(MUX_MODE1, flags)
+#define AM33XX_MCASP0_FSX_SPI1_D0(flags)				AM33XX_PIN_MCASP0_FSX(MUX_MODE3, flags)
+#define AM33XX_MCASP0_FSX_MMC1_SDCD(flags)				AM33XX_PIN_MCASP0_FSX(MUX_MODE4, flags)
+#define AM33XX_MCASP0_FSX_PR1_PRU0_PRU_R30_1(flags)			AM33XX_PIN_MCASP0_FSX(MUX_MODE5, flags)
+#define AM33XX_MCASP0_FSX_PR1_PRU0_PRU_R31_1(flags)			AM33XX_PIN_MCASP0_FSX(MUX_MODE6, flags)
+#define AM33XX_MCASP0_FSX_GPIO3_15(flags)				AM33XX_PIN_MCASP0_FSX(MUX_MODE7, flags)
+
+#define AM33XX_MCASP0_ACLKR(flags)					AM33XX_PIN_MCASP0_ACLKR(MUX_MODE0, flags)
+#define AM33XX_MCASP0_ACLKR_EQEP0A_IN(flags)				AM33XX_PIN_MCASP0_ACLKR(MUX_MODE1, flags)
+#define AM33XX_MCASP0_ACLKR_MCASP0_AXR2(flags)				AM33XX_PIN_MCASP0_ACLKR(MUX_MODE2, flags)
+#define AM33XX_MCASP0_ACLKR_MCASP1_ACLKX(flags)				AM33XX_PIN_MCASP0_ACLKR(MUX_MODE3, flags)
+#define AM33XX_MCASP0_ACLKR_MMC0_SDWP(flags)				AM33XX_PIN_MCASP0_ACLKR(MUX_MODE4, flags)
+#define AM33XX_MCASP0_ACLKR_PR1_PRU0_PRU_R30_4(flags)			AM33XX_PIN_MCASP0_ACLKR(MUX_MODE5, flags)
+#define AM33XX_MCASP0_ACLKR_PR1_PRU0_PRU_R31_4(flags)			AM33XX_PIN_MCASP0_ACLKR(MUX_MODE6, flags)
+#define AM33XX_MCASP0_ACLKR_GPIO3_18(flags)				AM33XX_PIN_MCASP0_ACLKR(MUX_MODE7, flags)
+
+#define AM33XXMCASP0_AHCLKR(flags)					AM33XX_PIN_MCASP0_AHCLKR(MUX_MODE0, flags)
+#define AM33XXMCASP0_AHCLKR_EHRPWM0_SYNCI(flags)			AM33XX_PIN_MCASP0_AHCLKR(MUX_MODE1, flags)
+#define AM33XX_MCASP0_AHCLKR_MCASP0_AXR2(flags)				AM33XX_PIN_MCASP0_AHCLKR(MUX_MODE2, flags)
+#define AM33XX_MCASP0_AHCLKR_SPI1_CS0(flags)				AM33XX_PIN_MCASP0_AHCLKR(MUX_MODE3, flags)
+#define AM33XX_MCASP0_AHCLKR_ECAP2_IN_PWM2_OUT(flags)			AM33XX_PIN_MCASP0_AHCLKR(MUX_MODE4, flags)
+#define AM33XX_MCASP0_AHCLKR_PR1_PRU0_PRU_R30_3(flags)			AM33XX_PIN_MCASP0_AHCLKR(MUX_MODE5, flags)
+#define AM33XX_MCASP0_AHCLKR_PR1_PRU0_PRU_R31_3(flags)			AM33XX_PIN_MCASP0_AHCLKR(MUX_MODE6, flags)
+#define AM33XX_MCASP0_AHCLKR_GPIO3_17(flags)				AM33XX_PIN_MCASP0_AHCLKR(MUX_MODE7, flags)
+
+#define AM33XX_MCASP0_AHCLKX(flags)					AM33XX_PIN_MCASP0_AHCLKX(MUX_MODE0, flags)
+#define AM33XX_MCASP0_AHCLKX_EQEP0_STROBE(flags)			AM33XX_PIN_MCASP0_AHCLKX(MUX_MODE1, flags)
+#define AM33XX_MCASP0_AHCLKX_MCASP0_AXR3(flags)				AM33XX_PIN_MCASP0_AHCLKX(MUX_MODE2, flags)
+#define AM33XX_MCASP0_AHCLKX_MCASP1_AXR1(flags)				AM33XX_PIN_MCASP0_AHCLKX(MUX_MODE3, flags)
+#define AM33XX_MCASP0_AHCLKX_EMU4(flags)				AM33XX_PIN_MCASP0_AHCLKX(MUX_MODE4, flags)
+#define AM33XX_MCASP0_AHCLKX_PR1_PRU0_PRU_R30_7(flags)			AM33XX_PIN_MCASP0_AHCLKX(MUX_MODE5, flags)
+#define AM33XX_MCASP0_AHCLKX_PR1_PRU0_PRU_R31_7(flags)			AM33XX_PIN_MCASP0_AHCLKX(MUX_MODE6, flags)
+#define AM33XX_MCASP0_AHCLKX_GPIO3_21(flags)				AM33XX_PIN_MCASP0_AHCLKX(MUX_MODE7, flags)
+
+#define AM33XX_MCASP0_ACLKX(flags)					AM33XX_PIN_MCASP0_ACLKX(MUX_MODE0, flags)
+#define AM33XX_MCASP0_ACLKX_EHRPWM0A(flags)				AM33XX_PIN_MCASP0_ACLKX(MUX_MODE1, flags)
+#define AM33XX_MCASP0_ACLKX_SPI1_SCLK(flags)				AM33XX_PIN_MCASP0_ACLKX(MUX_MODE3, flags)
+#define AM33XX_MCASP0_ACLKX_MMC0_SDCD(flags)				AM33XX_PIN_MCASP0_ACLKX(MUX_MODE4, flags)
+#define AM33XX_MCASP0_ACLKX_PR1_PRU0_PRU_R30_0(flags)			AM33XX_PIN_MCASP0_ACLKX(MUX_MODE5, flags)
+#define AM33XX_MCASP0_ACLKX_PR1_PRU0_PRU_R31_0(flags)			AM33XX_PIN_MCASP0_ACLKX(MUX_MODE6, flags)
+#define AM33XX_MCASP0_ACLKX_GPIO3_14(flags)				AM33XX_PIN_MCASP0_ACLKX(MUX_MODE7, flags)
+
+#define AM33XX_MCASP0_FSR(flags)					AM33XX_PIN_MCASP0_FSR(MUX_MODE0, flags)
+#define AM33XX_MCASP0_FSR_EQEP0B_IN(flags)				AM33XX_PIN_MCASP0_FSR(MUX_MODE1, flags)
+#define AM33XX_MCASP0_FSR_MCASP0_AXR3(flags)				AM33XX_PIN_MCASP0_FSR(MUX_MODE2, flags)
+#define AM33XX_MCASP0_FSR_MCASP1_FSX(flags)				AM33XX_PIN_MCASP0_FSR(MUX_MODE3, flags)
+#define AM33XX_MCASP0_FSR_EMU2(flags)					AM33XX_PIN_MCASP0_FSR(MUX_MODE4, flags)
+#define AM33XX_MCASP0_FSR_PR1_PRU0_PRU_R30_5(flags)			AM33XX_PIN_MCASP0_FSR(MUX_MODE5, flags)
+#define AM33XX_MCASP0_FSR_PR1_PRU0_PRU_R31_5(flags)			AM33XX_PIN_MCASP0_FSR(MUX_MODE6, flags)
+#define AM33XX_MCASP0_FSR_GPIO3_19(flags)				AM33XX_PIN_MCASP0_FSR(MUX_MODE7, flags)
+
+#define AM33XX_MCASP0_AXR0(flags)					AM33XX_PIN_MCASP0_AXR0(MUX_MODE0, flags)
+#define AM33XX_MCASP0_AXR0_EHRPWM0_TRIPZONE_INPUT(flags)		AM33XX_PIN_MCASP0_AXR0(MUX_MODE1, flags)
+#define AM33XX_MCASP0_AXR0_SPI1_D1(flags)				AM33XX_PIN_MCASP0_AXR0(MUX_MODE3, flags)
+#define AM33XX_MCASP0_AXR0_MMC2_SDCD(flags)				AM33XX_PIN_MCASP0_AXR0(MUX_MODE4, flags)
+#define AM33XX_MCASP0_AXR0_PR1_PRU0_PRU_R30_2(flags)			AM33XX_PIN_MCASP0_AXR0(MUX_MODE5, flags)
+#define AM33XX_MCASP0_AXR0_PR1_PRU0_PRU_R31_2(flags)			AM33XX_PIN_MCASP0_AXR0(MUX_MODE6, flags)
+#define AM33XX_MCASP0_AXR0_GPIO3_16(flags)				AM33XX_PIN_MCASP0_AXR0(MUX_MODE7, flags)
+
+#define AM33XX_MCASP0_AXR1(flags)					AM33XX_PIN_MCASP0_AXR1(MUX_MODE0, flags)
+#define AM33XX_MCASP0_AXR1_EQEP0_INDEX(flags)				AM33XX_PIN_MCASP0_AXR1(MUX_MODE1, flags)
+#define AM33XX_MCASP0_AXR1_MCASP1_AXR0(flags)				AM33XX_PIN_MCASP0_AXR1(MUX_MODE3, flags)
+#define AM33XX_MCASP0_AXR1_EMU3(flags)					AM33XX_PIN_MCASP0_AXR1(MUX_MODE4, flags)
+#define AM33XX_MCASP0_AXR1_PR1_PRU0_PRU_R30_6(flags)			AM33XX_PIN_MCASP0_AXR1(MUX_MODE5, flags)
+#define AM33XX_MCASP0_AXR1_PR1_PRU0_PRU_R31_6(flags)			AM33XX_PIN_MCASP0_AXR1(MUX_MODE6, flags)
+#define AM33XX_MCASP0_AXR1_GPIO3_20(flags)				AM33XX_PIN_MCASP0_AXR1(MUX_MODE7, flags)
+
+#define AM33XX_MDC_MDIO_CLK(flags)					AM33XX_PIN_MDC(MUX_MODE0, flags)
+#define AM33XX_MDC_TIMER5(flags)					AM33XX_PIN_MDC(MUX_MODE1, flags)
+#define AM33XX_MDC_UART5_TXD(flags)					AM33XX_PIN_MDC(MUX_MODE2, flags)
+#define AM33XX_MDC_UART3_RTSN(flags)					AM33XX_PIN_MDC(MUX_MODE3, flags)
+#define AM33XX_MDC_MMC0_SDWP(flags)					AM33XX_PIN_MDC(MUX_MODE4, flags)
+#define AM33XX_MDC_MMC1_CLK(flags)					AM33XX_PIN_MDC(MUX_MODE5, flags)
+#define AM33XX_MDC_MMC2_CLK(flags)					AM33XX_PIN_MDC(MUX_MODE6, flags)
+#define AM33XX_MDC_GPIO0_1(flags)					AM33XX_PIN_MDC(MUX_MODE7, flags)
+
+#define AM33XX_MDIO_DATA(flags)						AM33XX_PIN_MDIO(MUX_MODE0, flags)
+#define AM33XX_MDIO_TIMER6(flags)					AM33XX_PIN_MDIO(MUX_MODE1, flags)
+#define AM33XX_MDIO_UART5_RXD(flags)					AM33XX_PIN_MDIO(MUX_MODE2, flags)
+#define AM33XX_MDIO_UART3_CTSN(flags)					AM33XX_PIN_MDIO(MUX_MODE3, flags)
+#define AM33XX_MDIO_MMC0_SDCD(flags)					AM33XX_PIN_MDIO(MUX_MODE4, flags)
+#define AM33XX_MDIO_MMC1_CMD(flags)					AM33XX_PIN_MDIO(MUX_MODE5, flags)
+#define AM33XX_MDIO_MMC2_CMD(flags)					AM33XX_PIN_MDIO(MUX_MODE6, flags)
+#define AM33XX_MDIO_GPIO0_0(flags)					AM33XX_PIN_MDIO(MUX_MODE7, flags)
+
+#define AM33XX_MII1_RX_DV_GMII1_RXDV(flags)				AM33XX_PIN_MII1_RX_DV(MUX_MODE0, flags)
+#define AM33XX_MII1_RX_DV_LCD_MEMORY_CLK(flags)				AM33XX_PIN_MII1_RX_DV(MUX_MODE1, flags)
+#define AM33XX_MII1_RX_DV_RGMII1_RCTL(flags)				AM33XX_PIN_MII1_RX_DV(MUX_MODE2, flags)
+#define AM33XX_MII1_RX_DV_UART5_TXD(flags)				AM33XX_PIN_MII1_RX_DV(MUX_MODE3, flags)
+#define AM33XX_MII1_RX_DV_MCASP1_ACLKX(flags)				AM33XX_PIN_MII1_RX_DV(MUX_MODE4, flags)
+#define AM33XX_MII1_RX_DV_MMC2_DAT0(flags)				AM33XX_PIN_MII1_RX_DV(MUX_MODE5, flags)
+#define AM33XX_MII1_RX_DV_MCASP0_ACLKR(flags)				AM33XX_PIN_MII1_RX_DV(MUX_MODE6, flags)
+#define AM33XX_MII1_RX_DV_GPIO3_4(flags)				AM33XX_PIN_MII1_RX_DV(MUX_MODE7, flags)
+
+#define AM33XX_MII1_TX_EN_GMII1_TXEN(flags)				AM33XX_PIN_MII1_TX_EN(MUX_MODE0, flags)
+#define AM33XX_MII1_TX_EN_RMII1_TXEN(flags)				AM33XX_PIN_MII1_TX_EN(MUX_MODE1, flags)
+#define AM33XX_MII1_TX_EN_RGMII1_TCTL(flags)				AM33XX_PIN_MII1_TX_EN(MUX_MODE2, flags)
+#define AM33XX_MII1_TX_EN_TIMER4(flags)					AM33XX_PIN_MII1_TX_EN(MUX_MODE3, flags)
+#define AM33XX_MII1_TX_EN_MCASP1_AXR0(flags)				AM33XX_PIN_MII1_TX_EN(MUX_MODE4, flags)
+#define AM33XX_MII1_TX_EN_EQEP0_INDEX(flags)				AM33XX_PIN_MII1_TX_EN(MUX_MODE5, flags)
+#define AM33XX_MII1_TX_EN_MMC2_CMD(flags)				AM33XX_PIN_MII1_TX_EN(MUX_MODE6, flags)
+#define AM33XX_MII1_TX_EN_GPIO3_3(flags)				AM33XX_PIN_MII1_TX_EN(MUX_MODE7, flags)
+
+#define AM33XX_MII1_RX_ER_GMII1_RXERR(flags)				AM33XX_PIN_MII1_RX_ER(MUX_MODE0, flags)
+#define AM33XX_MII1_RX_ER_RMII1_RXERR(flags)				AM33XX_PIN_MII1_RX_ER(MUX_MODE1, flags)
+#define AM33XX_MII1_RX_ER_SPI1_D1(flags)				AM33XX_PIN_MII1_RX_ER(MUX_MODE2, flags)
+#define AM33XX_MII1_RX_ER_I2C1_SCL(flags)				AM33XX_PIN_MII1_RX_ER(MUX_MODE3, flags)
+#define AM33XX_MII1_RX_ER_MCASP1_FSX(flags)				AM33XX_PIN_MII1_RX_ER(MUX_MODE4, flags)
+#define AM33XX_MII1_RX_ER_UART5_RTSN(flags)				AM33XX_PIN_MII1_RX_ER(MUX_MODE5, flags)
+#define AM33XX_MII1_RX_ER_UART2_TXD(flags)				AM33XX_PIN_MII1_RX_ER(MUX_MODE6, flags)
+#define AM33XX_MII1_RX_ER_GPIO3_2(flags)				AM33XX_PIN_MII1_RX_ER(MUX_MODE7, flags)
+
+#define AM33XX_MII1_RX_CLK_GMII1_RXCLK(flags)				AM33XX_PIN_MII1_RX_CLK(MUX_MODE0, flags)
+#define AM33XX_MII1_RX_CLK_UART2_TXD(flags)				AM33XX_PIN_MII1_RX_CLK(MUX_MODE1, flags)
+#define AM33XX_MII1_RX_CLK_RGMII1_RCLK(flags)				AM33XX_PIN_MII1_RX_CLK(MUX_MODE2, flags)
+#define AM33XX_MII1_RX_CLK_MMC0_DAT6(flags)				AM33XX_PIN_MII1_RX_CLK(MUX_MODE3, flags)
+#define AM33XX_MII1_RX_CLK_MMC1_DAT1(flags)				AM33XX_PIN_MII1_RX_CLK(MUX_MODE4, flags)
+#define AM33XX_MII1_RX_CLK_UART1_DSRN(flags)				AM33XX_PIN_MII1_RX_CLK(MUX_MODE5, flags)
+#define AM33XX_MII1_RX_CLK_MCASP0_FSX(flags)				AM33XX_PIN_MII1_RX_CLK(MUX_MODE6, flags)
+#define AM33XX_MII1_RX_CLK_GPIO3_10(flags)				AM33XX_PIN_MII1_RX_CLK(MUX_MODE7, flags)
+
+#define AM33XX_MII1_TX_CLK_GMII1_TXCLK(flags)				AM33XX_PIN_MII1_TX_CLK(MUX_MODE0, flags)
+#define AM33XX_MII1_TX_CLK_UART2_RXD(flags)				AM33XX_PIN_MII1_TX_CLK(MUX_MODE1, flags)
+#define AM33XX_MII1_TX_CLK_RGMII1_TCLK(flags)				AM33XX_PIN_MII1_TX_CLK(MUX_MODE2, flags)
+#define AM33XX_MII1_TX_CLK_MMC0_DAT7(flags)				AM33XX_PIN_MII1_TX_CLK(MUX_MODE3, flags)
+#define AM33XX_MII1_TX_CLK_MMC1_DAT0(flags)				AM33XX_PIN_MII1_TX_CLK(MUX_MODE4, flags)
+#define AM33XX_MII1_TX_CLK_UART1_DCDN(flags)				AM33XX_PIN_MII1_TX_CLK(MUX_MODE5, flags)
+#define AM33XX_MII1_TX_CLK_MCASP0_ACLKX(flags)				AM33XX_PIN_MII1_TX_CLK(MUX_MODE6, flags)
+#define AM33XX_MII1_TX_CLK_GPIO3_9(flags)				AM33XX_PIN_MII1_TX_CLK(MUX_MODE7, flags)
+
+#define AM33XX_MII1_COL_GMII1_COL(flags)				AM33XX_PIN_MII1_COL(MUX_MODE0, flags)
+#define AM33XX_MII1_COL_RMII2_REFCLK(flags)				AM33XX_PIN_MII1_COL(MUX_MODE1, flags)
+#define AM33XX_MII1_COL_SPI1_SCLK(flags)				AM33XX_PIN_MII1_COL(MUX_MODE2, flags)
+#define AM33XX_MII1_COL_UART5_RXD(flags)				AM33XX_PIN_MII1_COL(MUX_MODE3, flags)
+#define AM33XX_MII1_COL_MCASP1_AXR2(flags)				AM33XX_PIN_MII1_COL(MUX_MODE4, flags)
+#define AM33XX_MII1_COL_MMC2_DAT3(flags)				AM33XX_PIN_MII1_COL(MUX_MODE5, flags)
+#define AM33XX_MII1_COL_MCASP0_AXR2(flags)				AM33XX_PIN_MII1_COL(MUX_MODE6, flags)
+#define AM33XX_MII1_COL_GPIO3_0(flags)					AM33XX_PIN_MII1_COL(MUX_MODE7, flags)
+
+#define AM33XX_MII1_CRS_GMII1_CRS(flags)				AM33XX_PIN_MII1_CRS(MUX_MODE0, flags)
+#define AM33XX_MII1_CRS_RMII1_CRS_DV(flags)				AM33XX_PIN_MII1_CRS(MUX_MODE1, flags)
+#define AM33XX_MII1_CRS_SPI1_D0(flags)					AM33XX_PIN_MII1_CRS(MUX_MODE2, flags)
+#define AM33XX_MII1_CRS_I2C1_SDA(flags)					AM33XX_PIN_MII1_CRS(MUX_MODE3, flags)
+#define AM33XX_MII1_CRS_MCASP1_ACLKX(flags)				AM33XX_PIN_MII1_CRS(MUX_MODE4, flags)
+#define AM33XX_MII1_CRS_UART5_CTSN(flags)				AM33XX_PIN_MII1_CRS(MUX_MODE5, flags)
+#define AM33XX_MII1_CRS_UART2_RXD(flags)				AM33XX_PIN_MII1_CRS(MUX_MODE6, flags)
+#define AM33XX_MII1_CRS_GPIO3_1(flags)					AM33XX_PIN_MII1_CRS(MUX_MODE7, flags)
+
+#define AM33XX_MII1_RXD0_GMII1_RXD0(flags)				AM33XX_PIN_MII1_RXD0(MUX_MODE0, flags)
+#define AM33XX_MII1_RXD0_RMII1_RXD0(flags)				AM33XX_PIN_MII1_RXD0(MUX_MODE1, flags)
+#define AM33XX_MII1_RXD0_RGMII1_RD0(flags)				AM33XX_PIN_MII1_RXD0(MUX_MODE2, flags)
+#define AM33XX_MII1_RXD0_MCASP1_AHCLKX(flags)				AM33XX_PIN_MII1_RXD0(MUX_MODE3, flags)
+#define AM33XX_MII1_RXD0_MCASP1_AHCLKR(flags)				AM33XX_PIN_MII1_RXD0(MUX_MODE4, flags)
+#define AM33XX_MII1_RXD0_MCASP1_ACLKR(flags)				AM33XX_PIN_MII1_RXD0(MUX_MODE5, flags)
+#define AM33XX_MII1_RXD0_MCASP0_AXR3(flags)				AM33XX_PIN_MII1_RXD0(MUX_MODE6, flags)
+#define AM33XX_MII1_RXD0_GPIO2_21(flags)				AM33XX_PIN_MII1_RXD0(MUX_MODE7, flags)
+
+#define AM33XX_MII1_RXD1_GMII1_RXD1(flags)				AM33XX_PIN_MII1_RXD1(MUX_MODE0, flags)
+#define AM33XX_MII1_RXD1_RMII1_RXD1(flags)				AM33XX_PIN_MII1_RXD1(MUX_MODE1, flags)
+#define AM33XX_MII1_RXD1_RGMII1_RD1(flags)				AM33XX_PIN_MII1_RXD1(MUX_MODE2, flags)
+#define AM33XX_MII1_RXD1_MCASP1_AXR3(flags)				AM33XX_PIN_MII1_RXD1(MUX_MODE3, flags)
+#define AM33XX_MII1_RXD1_MCASP1_FSR(flags)				AM33XX_PIN_MII1_RXD1(MUX_MODE4, flags)
+#define AM33XX_MII1_RXD1_EQEP0_STROBE(flags)				AM33XX_PIN_MII1_RXD1(MUX_MODE5, flags)
+#define AM33XX_MII1_RXD1_MMC2_CLK(flags)				AM33XX_PIN_MII1_RXD1(MUX_MODE6, flags)
+#define AM33XX_MII1_RXD1_GPIO2_20(flags)				AM33XX_PIN_MII1_RXD1(MUX_MODE7, flags)
+
+#define AM33XX_MII1_RXD2_GMII1_RXD2(flags)				AM33XX_PIN_MII1_RXD2(MUX_MODE0, flags)
+#define AM33XX_MII1_RXD2_UART3_TXD(flags)				AM33XX_PIN_MII1_RXD2(MUX_MODE1, flags)
+#define AM33XX_MII1_RXD2_RGMII1_RD2(flags)				AM33XX_PIN_MII1_RXD2(MUX_MODE2, flags)
+#define AM33XX_MII1_RXD2_MMC0_DAT4(flags)				AM33XX_PIN_MII1_RXD2(MUX_MODE3, flags)
+#define AM33XX_MII1_RXD2_MMC1_DAT3(flags)				AM33XX_PIN_MII1_RXD2(MUX_MODE4, flags)
+#define AM33XX_MII1_RXD2_UART1_RIN(flags)				AM33XX_PIN_MII1_RXD2(MUX_MODE5, flags)
+#define AM33XX_MII1_RXD2_MCASP0_AXR1(flags)				AM33XX_PIN_MII1_RXD2(MUX_MODE6, flags)
+#define AM33XX_MII1_RXD2_GPIO2_19(flags)				AM33XX_PIN_MII1_RXD2(MUX_MODE7, flags)
+
+#define AM33XX_MII1_RXD3_GMII1_RXD3(flags)				AM33XX_PIN_MII1_RXD3(MUX_MODE0, flags)
+#define AM33XX_MII1_RXD3_UART3_RXD(flags)				AM33XX_PIN_MII1_RXD3(MUX_MODE1, flags)
+#define AM33XX_MII1_RXD3_RGMII1_RD3(flags)				AM33XX_PIN_MII1_RXD3(MUX_MODE2, flags)
+#define AM33XX_MII1_RXD3_MMC0_DAT5(flags)				AM33XX_PIN_MII1_RXD3(MUX_MODE3, flags)
+#define AM33XX_MII1_RXD3_MMC1_DAT2(flags)				AM33XX_PIN_MII1_RXD3(MUX_MODE4, flags)
+#define AM33XX_MII1_RXD3_UART1_DTRN(flags)				AM33XX_PIN_MII1_RXD3(MUX_MODE5, flags)
+#define AM33XX_MII1_RXD3_MCASP0_AXR0(flags)				AM33XX_PIN_MII1_RXD3(MUX_MODE6, flags)
+#define AM33XX_MII1_RXD3_GPIO2_18(flags)				AM33XX_PIN_MII1_RXD3(MUX_MODE7, flags)
+
+#define AM33XX_MII1_TXD0_GMII1_TXD0(flags)				AM33XX_PIN_MII1_TXD0(MUX_MODE0, flags)
+#define AM33XX_MII1_TXD0_RMII1_TXD0(flags)				AM33XX_PIN_MII1_TXD0(MUX_MODE1, flags)
+#define AM33XX_MII1_TXD0_RGMII1_TD0(flags)				AM33XX_PIN_MII1_TXD0(MUX_MODE2, flags)
+#define AM33XX_MII1_TXD0_MCASP1_AXR2(flags)				AM33XX_PIN_MII1_TXD0(MUX_MODE3, flags)
+#define AM33XX_MII1_TXD0_MCASP1_ACLKR(flags)				AM33XX_PIN_MII1_TXD0(MUX_MODE4, flags)
+#define AM33XX_MII1_TXD0_EQEP0B_IN(flags)				AM33XX_PIN_MII1_TXD0(MUX_MODE5, flags)
+#define AM33XX_MII1_TXD0_MMC1_CLK(flags)				AM33XX_PIN_MII1_TXD0(MUX_MODE6, flags)
+#define AM33XX_MII1_TXD0_GPIO0_28(flags)				AM33XX_PIN_MII1_TXD0(MUX_MODE7, flags)
+
+#define AM33XX_MII1_TXD1_GMII1_TXD1(flags)				AM33XX_PIN_MII1_TXD1(MUX_MODE0, flags)
+#define AM33XX_MII1_TXD1_RMII1_TXD1(flags)				AM33XX_PIN_MII1_TXD1(MUX_MODE1, flags)
+#define AM33XX_MII1_TXD1_RGMII1_TD1(flags)				AM33XX_PIN_MII1_TXD1(MUX_MODE2, flags)
+#define AM33XX_MII1_TXD1_MCASP1_FSR(flags)				AM33XX_PIN_MII1_TXD1(MUX_MODE3, flags)
+#define AM33XX_MII1_TXD1_MCASP1_AXR1(flags)				AM33XX_PIN_MII1_TXD1(MUX_MODE4, flags)
+#define AM33XX_MII1_TXD1_EQEP0A_IN(flags)				AM33XX_PIN_MII1_TXD1(MUX_MODE5, flags)
+#define AM33XX_MII1_TXD1_MMC1_CMD(flags)				AM33XX_PIN_MII1_TXD1(MUX_MODE6, flags)
+#define AM33XX_MII1_TXD1_GPIO0_21(flags)				AM33XX_PIN_MII1_TXD1(MUX_MODE7, flags)
+
+#define AM33XX_MII1_TXD2_GMII1_TXD2(flags)				AM33XX_PIN_MII1_TXD2(MUX_MODE0, flags)
+#define AM33XX_MII1_TXD2_DCAN0_RX(flags)				AM33XX_PIN_MII1_TXD2(MUX_MODE1, flags)
+#define AM33XX_MII1_TXD2_RGMII1_TD2(flags)				AM33XX_PIN_MII1_TXD2(MUX_MODE2, flags)
+#define AM33XX_MII1_TXD2_UART4_TXD(flags)				AM33XX_PIN_MII1_TXD2(MUX_MODE3, flags)
+#define AM33XX_MII1_TXD2_MCASP1_AXR0(flags)				AM33XX_PIN_MII1_TXD2(MUX_MODE4, flags)
+#define AM33XX_MII1_TXD2_MMC2_DAT2(flags)				AM33XX_PIN_MII1_TXD2(MUX_MODE5, flags)
+#define AM33XX_MII1_TXD2_MCASP0_AHCLKX(flags)				AM33XX_PIN_MII1_TXD2(MUX_MODE6, flags)
+#define AM33XX_MII1_TXD2_GPIO0_17(flags)				AM33XX_PIN_MII1_TXD2(MUX_MODE7, flags)
+
+#define AM33XX_MII1_TXD3_GMII1_TXD3(flags)				AM33XX_PIN_MII1_TXD3(MUX_MODE0, flags)
+#define AM33XX_MII1_TXD3_DCAN0_TX(flags)				AM33XX_PIN_MII1_TXD3(MUX_MODE1, flags)
+#define AM33XX_MII1_TXD3_RGMII1_TD3(flags)				AM33XX_PIN_MII1_TXD3(MUX_MODE2, flags)
+#define AM33XX_MII1_TXD3_UART4_RXD(flags)				AM33XX_PIN_MII1_TXD3(MUX_MODE3, flags)
+#define AM33XX_MII1_TXD3_MCASP1_FSX(flags)				AM33XX_PIN_MII1_TXD3(MUX_MODE4, flags)
+#define AM33XX_MII1_TXD3_MMC2_DAT1(flags)				AM33XX_PIN_MII1_TXD3(MUX_MODE5, flags)
+#define AM33XX_MII1_TXD3_MCASP0_FSR(flags)				AM33XX_PIN_MII1_TXD3(MUX_MODE6, flags)
+#define AM33XX_MII1_TXD3_GPIO0_16(flags)				AM33XX_PIN_MII1_TXD3(MUX_MODE7, flags)
+
+#define AM33XX_MMC0_CMD(flags)						AM33XX_PIN_MMC0_CMD(MUX_MODE0, flags)
+#define AM33XX_MMC0_CMD_GPMC_A25(flags)					AM33XX_PIN_MMC0_CMD(MUX_MODE1, flags)
+#define AM33XX_MMC0_CMD_UART3_RTSN(flags)				AM33XX_PIN_MMC0_CMD(MUX_MODE2, flags)
+#define AM33XX_MMC0_CMD_UART2_TXD(flags)				AM33XX_PIN_MMC0_CMD(MUX_MODE3, flags)
+#define AM33XX_MMC0_CMD_DCAN1_RX(flags)					AM33XX_PIN_MMC0_CMD(MUX_MODE4, flags)
+#define AM33XX_MMC0_CMD_PR1_PRU0_PRU_R30_13(flags)			AM33XX_PIN_MMC0_CMD(MUX_MODE5, flags)
+#define AM33XX_MMC0_CMD_PR1_PRU0_PRU_R31_13(flags)			AM33XX_PIN_MMC0_CMD(MUX_MODE6, flags)
+#define AM33XX_MMC0_CMD_GPIO2_31(flags)					AM33XX_PIN_MMC0_CMD(MUX_MODE7, flags)
+
+#define AM33XX_MMC0_CLK(flags)						AM33XX_PIN_MMC0_CLK(MUX_MODE0, flags)
+#define AM33XX_MMC0_CLK_GPMC_A24(flags)					AM33XX_PIN_MMC0_CLK(MUX_MODE1, flags)
+#define AM33XX_MMC0_CLK_UART3_CTSN(flags)				AM33XX_PIN_MMC0_CLK(MUX_MODE2, flags)
+#define AM33XX_MMC0_CLK_UART2_RXD(flags)				AM33XX_PIN_MMC0_CLK(MUX_MODE3, flags)
+#define AM33XX_MMC0_CLK_DCAN1_TX(flags)					AM33XX_PIN_MMC0_CLK(MUX_MODE4, flags)
+#define AM33XX_MMC0_CLK_PR1_PRU0_PRU_R30_12(flags)			AM33XX_PIN_MMC0_CLK(MUX_MODE5, flags)
+#define AM33XX_MMC0_CLK_PR1_PRU0_PRU_R31_12(flags)			AM33XX_PIN_MMC0_CLK(MUX_MODE6, flags)
+#define AM33XX_MMC0_CLK_GPIO2_30(flags)					AM33XX_PIN_MMC0_CLK(MUX_MODE7, flags)
+
+#define AM33XX_MMC0_DAT0(flags)						AM33XX_PIN_MMC0_DAT0(MUX_MODE0, flags)
+#define AM33XX_MMC0_DAT0_GPMC_A23(flags)				AM33XX_PIN_MMC0_DAT0(MUX_MODE1, flags)
+#define AM33XX_MMC0_DAT0_UART5_RTSN(flags)				AM33XX_PIN_MMC0_DAT0(MUX_MODE2, flags)
+#define AM33XX_MMC0_DAT0_UART3_TXD(flags)				AM33XX_PIN_MMC0_DAT0(MUX_MODE3, flags)
+#define AM33XX_MMC0_DAT0_UART1_RIN(flags)				AM33XX_PIN_MMC0_DAT0(MUX_MODE4, flags)
+#define AM33XX_MMC0_DAT0_PR1_PRU0_PRU_R30_11(flags)			AM33XX_PIN_MMC0_DAT0(MUX_MODE5, flags)
+#define AM33XX_MMC0_DAT0_PR1_PRU0_PRU_R31_11(flags)			AM33XX_PIN_MMC0_DAT0(MUX_MODE6, flags)
+#define AM33XX_MMC0_DAT0_GPIO2_29(flags)				AM33XX_PIN_MMC0_DAT0(MUX_MODE7, flags)
+
+#define AM33XX_MMC0_DAT1(flags)						AM33XX_PIN_MMC0_DAT1(MUX_MODE0, flags)
+#define AM33XX_MMC0_DAT1_GPMC_A22(flags)				AM33XX_PIN_MMC0_DAT1(MUX_MODE1, flags)
+#define AM33XX_MMC0_DAT1_UART5_CTSN(flags)				AM33XX_PIN_MMC0_DAT1(MUX_MODE2, flags)
+#define AM33XX_MMC0_DAT1_UART3_RXD(flags)				AM33XX_PIN_MMC0_DAT1(MUX_MODE3, flags)
+#define AM33XX_MMC0_DAT1_UART1_DTRN(flags)				AM33XX_PIN_MMC0_DAT1(MUX_MODE4, flags)
+#define AM33XX_MMC0_DAT1_PR1_PRU0_PRU_R30_10(flags)			AM33XX_PIN_MMC0_DAT1(MUX_MODE5, flags)
+#define AM33XX_MMC0_DAT1_PR1_PRU0_PRU_R31_10(flags)			AM33XX_PIN_MMC0_DAT1(MUX_MODE6, flags)
+#define AM33XX_MMC0_DAT1_GPIO2_28(flags)				AM33XX_PIN_MMC0_DAT1(MUX_MODE7, flags)
+
+#define AM33XX_MMC0_DAT2(flags)						AM33XX_PIN_MMC0_DAT2(MUX_MODE0, flags)
+#define AM33XX_MMC0_DAT2_GPMC_A21(flags)				AM33XX_PIN_MMC0_DAT2(MUX_MODE1, flags)
+#define AM33XX_MMC0_DAT2_UART4_RTSN(flags)				AM33XX_PIN_MMC0_DAT2(MUX_MODE2, flags)
+#define AM33XX_MMC0_DAT2_TIMER6(flags)					AM33XX_PIN_MMC0_DAT2(MUX_MODE3, flags)
+#define AM33XX_MMC0_DAT2_UART1_DSRN(flags)				AM33XX_PIN_MMC0_DAT2(MUX_MODE4, flags)
+#define AM33XX_MMC0_DAT2_PR1_PRU0_PRU_R30_9(flags)			AM33XX_PIN_MMC0_DAT2(MUX_MODE5, flags)
+#define AM33XX_MMC0_DAT2_PR1_PRU0_PRU_R31_9(flags)			AM33XX_PIN_MMC0_DAT2(MUX_MODE6, flags)
+#define AM33XX_MMC0_DAT2_GPIO2_27(flags)				AM33XX_PIN_MMC0_DAT2(MUX_MODE7, flags)
+
+#define AM33XX_MMC0_DAT3(flags)						AM33XX_PIN_MMC0_DAT3(MUX_MODE0, flags)
+#define AM33XX_MMC0_DAT3_GPMC_A20(flags)				AM33XX_PIN_MMC0_DAT3(MUX_MODE1, flags)
+#define AM33XX_MMC0_DAT3_UART4_CTSN(flags)				AM33XX_PIN_MMC0_DAT3(MUX_MODE2, flags)
+#define AM33XX_MMC0_DAT3_TIMER5(flags)					AM33XX_PIN_MMC0_DAT3(MUX_MODE3, flags)
+#define AM33XX_MMC0_DAT3_UART1_DCDN(flags)				AM33XX_PIN_MMC0_DAT3(MUX_MODE4, flags)
+#define AM33XX_MMC0_DAT3_PR1_PRU0_PRU_R30_8(flags)			AM33XX_PIN_MMC0_DAT3(MUX_MODE5, flags)
+#define AM33XX_MMC0_DAT3_PR1_PRU0_PRU_R31_8(flags)			AM33XX_PIN_MMC0_DAT3(MUX_MODE6, flags)
+#define AM33XX_MMC0_DAT3_GPIO2_26(flags)				AM33XX_PIN_MMC0_DAT3(MUX_MODE7, flags)
+
+#define AM33XX_RMII1_REF_CLK(flags)					AM33XX_PIN_RMII1_REF_CLK(MUX_MODE0, flags)
+#define AM33XX_RMII1_REF_CLK_XDMA_EVENT_INTR2(flags)			AM33XX_PIN_RMII1_REF_CLK(MUX_MODE1, flags)
+#define AM33XX_RMII1_REF_CLK_SPI1_CS0(flags)				AM33XX_PIN_RMII1_REF_CLK(MUX_MODE2, flags)
+#define AM33XX_RMII1_REF_CLK_UART5_TXD(flags)				AM33XX_PIN_RMII1_REF_CLK(MUX_MODE3, flags)
+#define AM33XX_RMII1_REF_CLK_MCASP1_AXR3(flags)				AM33XX_PIN_RMII1_REF_CLK(MUX_MODE4, flags)
+#define AM33XX_RMII1_REF_CLK_MMC0_POW(flags)				AM33XX_PIN_RMII1_REF_CLK(MUX_MODE5, flags)
+#define AM33XX_RMII1_REF_CLK_MCASP1_AHCLKX(flags)			AM33XX_PIN_RMII1_REF_CLK(MUX_MODE6, flags)
+#define AM33XX_RMII1_REF_CLK_GPIO0_29(flags)				AM33XX_PIN_RMII1_REF_CLK(MUX_MODE7, flags)
+
+#define AM33XX_SPI0_SCLK(flags)						AM33XX_PIN_SPI0_SCLK(MUX_MODE0, flags)
+#define AM33XX_SPI0_SCLK_UART2_RXD(flags)				AM33XX_PIN_SPI0_SCLK(MUX_MODE1, flags)
+#define AM33XX_SPI0_SCLK_I2C2_SDA(flags)				AM33XX_PIN_SPI0_SCLK(MUX_MODE2, flags)
+#define AM33XX_SPI0_SCLK_EHRPWM0A(flags)				AM33XX_PIN_SPI0_SCLK(MUX_MODE3, flags)
+#define AM33XX_SPI0_SCLK_PR1_UART0_CTS_N(flags)				AM33XX_PIN_SPI0_SCLK(MUX_MODE4, flags)
+#define AM33XX_SPI0_SCLK_PR1_EDIO_SOF(flags)				AM33XX_PIN_SPI0_SCLK(MUX_MODE5, flags)
+#define AM33XX_SPI0_SCLK_EMU2(flags)					AM33XX_PIN_SPI0_SCLK(MUX_MODE6, flags)
+#define AM33XX_SPI0_SCLK_GPIO0_2(flags)					AM33XX_PIN_SPI0_SCLK(MUX_MODE7, flags)
+
+#define AM33XX_SPI0_CS0(flags)						AM33XX_PIN_SPI0_CS0(MUX_MODE0, flags)
+#define AM33XX_SPI0_CS0_MMC2_SDWP(flags)				AM33XX_PIN_SPI0_CS0(MUX_MODE1, flags)
+#define AM33XX_SPI0_CS0_I2C1_SCL(flags)					AM33XX_PIN_SPI0_CS0(MUX_MODE2, flags)
+#define AM33XX_SPI0_CS0_EHRPWM0_SYNCI(flags)				AM33XX_PIN_SPI0_CS0(MUX_MODE3, flags)
+#define AM33XX_SPI0_CS0_PR1_UART0_TXD(flags)				AM33XX_PIN_SPI0_CS0(MUX_MODE4, flags)
+#define AM33XX_SPI0_CS0_PR1_EDIO_DATA_IN1(flags)			AM33XX_PIN_SPI0_CS0(MUX_MODE5, flags)
+#define AM33XX_SPI0_CS0_PR1_EDIO_DATA_OUT1(flags)			AM33XX_PIN_SPI0_CS0(MUX_MODE6, flags)
+#define AM33XX_SPI0_CS0_GPIO0_5(flags)					AM33XX_PIN_SPI0_CS0(MUX_MODE7, flags)
+
+#define AM33XX_SPI0_CS1(flags)						AM33XX_PIN_SPI0_CS1(MUX_MODE0, flags)
+#define AM33XX_SPI0_CS1_UART3_RXD(flags)				AM33XX_PIN_SPI0_CS1(MUX_MODE1, flags)
+#define AM33XX_SPI0_CS1_ECAP1_IN_PWM1_OUT(flags)			AM33XX_PIN_SPI0_CS1(MUX_MODE2, flags)
+#define AM33XX_SPI0_CS1_MMC0_POW(flags)					AM33XX_PIN_SPI0_CS1(MUX_MODE3, flags)
+#define AM33XX_SPI0_CS1_XDMA_EVENT_INTR2(flags)				AM33XX_PIN_SPI0_CS1(MUX_MODE4, flags)
+#define AM33XX_SPI0_CS1_MMC0_SDCD(flags)				AM33XX_PIN_SPI0_CS1(MUX_MODE5, flags)
+#define AM33XX_SPI0_CS1_EMU4(flags)					AM33XX_PIN_SPI0_CS1(MUX_MODE6, flags)
+#define AM33XX_SPI0_CS1_GPIO0_6(flags)					AM33XX_PIN_SPI0_CS1(MUX_MODE7, flags)
+
+#define AM33XX_SPI0_D0(flags)						AM33XX_PIN_SPI0_D0(MUX_MODE0, flags)
+#define AM33XX_SPI0_D0_UART2_TXD(flags)					AM33XX_PIN_SPI0_D0(MUX_MODE1, flags)
+#define AM33XX_SPI0_D0_I2C2_SCL(flags)					AM33XX_PIN_SPI0_D0(MUX_MODE2, flags)
+#define AM33XX_SPI0_D0_EHRPWM0B(flags)					AM33XX_PIN_SPI0_D0(MUX_MODE3, flags)
+#define AM33XX_SPI0_D0_PR1_UART0_RTS_N(flags)				AM33XX_PIN_SPI0_D0(MUX_MODE4, flags)
+#define AM33XX_SPI0_D0_PR1_EDIO_LATCH_IN(flags)				AM33XX_PIN_SPI0_D0(MUX_MODE5, flags)
+#define AM33XX_SPI0_D0_EMU3(flags)					AM33XX_PIN_SPI0_D0(MUX_MODE6, flags)
+#define AM33XX_SPI0_D0_GPIO0_3(flags)					AM33XX_PIN_SPI0_D0(MUX_MODE7, flags)
+
+#define AM33XX_SPI0_D1(flags)						AM33XX_PIN_SPI0_D1(MUX_MODE0, flags)
+#define AM33XX_SPI0_D1_MMC1_SDWP(flags)					AM33XX_PIN_SPI0_D1(MUX_MODE1, flags)
+#define AM33XX_SPI0_D1_I2C1_SDA(flags)					AM33XX_PIN_SPI0_D1(MUX_MODE2, flags)
+#define AM33XX_SPI0_D1_EHRPWM0_TRIPZONE_INPUT(flags)			AM33XX_PIN_SPI0_D1(MUX_MODE3, flags)
+#define AM33XX_SPI0_D1_PR1_UART0_RXD(flags)				AM33XX_PIN_SPI0_D1(MUX_MODE4, flags)
+#define AM33XX_SPI0_D1_PR1_EDIO_DATA_IN0(flags)				AM33XX_PIN_SPI0_D1(MUX_MODE5, flags)
+#define AM33XX_SPI0_D1_PR1_EDIO_DATA_OUT0(flags)			AM33XX_PIN_SPI0_D1(MUX_MODE6, flags)
+#define AM33XX_SPI0_D1_GPIO0_4(flags)					AM33XX_PIN_SPI0_D1(MUX_MODE7, flags)
+
+#define AM33XX_UART0_TXD(flags)						AM33XX_PIN_UART0_TXD(MUX_MODE0, flags)
+#define AM33XX_UART0_TXD_SPI1_CS1(flags)				AM33XX_PIN_UART0_TXD(MUX_MODE1, flags)
+#define AM33XX_UART0_TXD_DCAN0_RX(flags)				AM33XX_PIN_UART0_TXD(MUX_MODE2, flags)
+#define AM33XX_UART0_TXD_I2C2_SCL(flags)				AM33XX_PIN_UART0_TXD(MUX_MODE3, flags)
+#define AM33XX_UART0_TXD_ECAP1_IN_PWM1_OUT(flags)			AM33XX_PIN_UART0_TXD(MUX_MODE4, flags)
+#define AM33XX_UART0_TXD_PR1_PRU1_PRU_R30_15(flags)			AM33XX_PIN_UART0_TXD(MUX_MODE5, flags)
+#define AM33XX_UART0_TXD_PR1_PRU1_PRU_R31_15(flags)			AM33XX_PIN_UART0_TXD(MUX_MODE6, flags)
+#define AM33XX_UART0_TXD_GPIO1_11(flags)				AM33XX_PIN_UART0_TXD(MUX_MODE7, flags)
+
+#define AM33XX_UART0_CTSN(flags)					AM33XX_PIN_UART0_CTSN(MUX_MODE0, flags)
+#define AM33XX_UART0_CTSN_UART4_RXD(flags)				AM33XX_PIN_UART0_CTSN(MUX_MODE1, flags)
+#define AM33XX_UART0_CTSN_DCAN1_TX(flags)				AM33XX_PIN_UART0_CTSN(MUX_MODE2, flags)
+#define AM33XX_UART0_CTSN_I2C1_SDA(flags)				AM33XX_PIN_UART0_CTSN(MUX_MODE3, flags)
+#define AM33XX_UART0_CTSN_SPI1_D0(flags)				AM33XX_PIN_UART0_CTSN(MUX_MODE4, flags)
+#define AM33XX_UART0_CTSN_TIMER7(flags)					AM33XX_PIN_UART0_CTSN(MUX_MODE5, flags)
+#define AM33XX_UART0_CTSN_PR1_EDC_SYNC0_OUT(flags)			AM33XX_PIN_UART0_CTSN(MUX_MODE6, flags)
+#define AM33XX_UART0_CTSN_GPIO1_8(flags)				AM33XX_PIN_UART0_CTSN(MUX_MODE7, flags)
+
+#define AM33XX_UART0_RXD(flags)						AM33XX_PIN_UART0_RXD(MUX_MODE0, flags)
+#define AM33XX_UART0_RXD_SPI1_CS0(flags)				AM33XX_PIN_UART0_RXD(MUX_MODE1, flags)
+#define AM33XX_UART0_RXD_DCAN0_TX(flags)				AM33XX_PIN_UART0_RXD(MUX_MODE2, flags)
+#define AM33XX_UART0_RXD_I2C2_SDA(flags)				AM33XX_PIN_UART0_RXD(MUX_MODE3, flags)
+#define AM33XX_UART0_RXD_ECAP2_IN_PWM2_OUT(flags)			AM33XX_PIN_UART0_RXD(MUX_MODE4, flags)
+#define AM33XX_UART0_RXD_PR1_PRU1_PRU_R30_14(flags)			AM33XX_PIN_UART0_RXD(MUX_MODE5, flags)
+#define AM33XX_UART0_RXD_PR1_PRU1_PRU_R31_14(flags)			AM33XX_PIN_UART0_RXD(MUX_MODE6, flags)
+#define AM33XX_UART0_RXD_GPIO1_10(flags)				AM33XX_PIN_UART0_RXD(MUX_MODE7, flags)
+
+#define AM33XX_UART0_RTSN(flags)					AM33XX_PIN_UART0_RTSN(MUX_MODE0, flags)
+#define AM33XX_UART0_RTSN_UART4_TXD(flags)				AM33XX_PIN_UART0_RTSN(MUX_MODE1, flags)
+#define AM33XX_UART0_RTSN_DCAN1_RX(flags)				AM33XX_PIN_UART0_RTSN(MUX_MODE2, flags)
+#define AM33XX_UART0_RTSN_I2C1_SCL(flags)				AM33XX_PIN_UART0_RTSN(MUX_MODE3, flags)
+#define AM33XX_UART0_RTSN_SPI1_D1(flags)				AM33XX_PIN_UART0_RTSN(MUX_MODE4, flags)
+#define AM33XX_UART0_RTSN_SPI1_CS0(flags)				AM33XX_PIN_UART0_RTSN(MUX_MODE5, flags)
+#define AM33XX_UART0_RTSN_PR1_EDC_SYNC1_OUT(flags)			AM33XX_PIN_UART0_RTSN(MUX_MODE6, flags)
+#define AM33XX_UART0_RTSN_GPIO1_9(flags)				AM33XX_PIN_UART0_RTSN(MUX_MODE7, flags)
+
+#define AM33XX_UART1_TXD(flags)						AM33XX_PIN_UART1_TXD(MUX_MODE0, flags)
+#define AM33XX_UART1_TXD_MMC2_SDWP(flags)				AM33XX_PIN_UART1_TXD(MUX_MODE1, flags)
+#define AM33XX_UART1_TXD_DCAN1_RX(flags)				AM33XX_PIN_UART1_TXD(MUX_MODE2, flags)
+#define AM33XX_UART1_TXD_I2C1_SCL(flags)				AM33XX_PIN_UART1_TXD(MUX_MODE3, flags)
+#define AM33XX_UART1_TXD_PR1_UART0_TXD(flags)				AM33XX_PIN_UART1_TXD(MUX_MODE5, flags)
+#define AM33XX_UART1_TXD_PR1_PRU0_PRU_R31_16(flags)			AM33XX_PIN_UART1_TXD(MUX_MODE6, flags)
+#define AM33XX_UART1_TXD_GPIO0_15(flags)				AM33XX_PIN_UART1_TXD(MUX_MODE7, flags)
+
+#define AM33XX_UART1_RXD(flags)						AM33XX_PIN_UART1_RXD(MUX_MODE0, flags)
+#define AM33XX_UART1_RXD_MMC1_SDWP(flags)				AM33XX_PIN_UART1_RXD(MUX_MODE1, flags)
+#define AM33XX_UART1_RXD_DCAN1_TX(flags)				AM33XX_PIN_UART1_RXD(MUX_MODE2, flags)
+#define AM33XX_UART1_RXD_I2C1_SDA(flags)				AM33XX_PIN_UART1_RXD(MUX_MODE3, flags)
+#define AM33XX_UART1_RXD_PR1_UART0_RXD(flags)				AM33XX_PIN_UART1_RXD(MUX_MODE5, flags)
+#define AM33XX_UART1_RXD_PR1_PRU1_PRU_R31_16(flags)			AM33XX_PIN_UART1_RXD(MUX_MODE6, flags)
+#define AM33XX_UART1_RXD_GPIO0_14(flags)				AM33XX_PIN_UART1_RXD(MUX_MODE7, flags)
+
+#define AM33XX_UART1_RTSN(flags)					AM33XX_PIN_UART1_RTSN(MUX_MODE0, flags)
+#define AM33XX_UART1_RTSN_TIMER5(flags)					AM33XX_PIN_UART1_RTSN(MUX_MODE1, flags)
+#define AM33XX_UART1_RTSN_DCAN0_RX(flags)				AM33XX_PIN_UART1_RTSN(MUX_MODE2, flags)
+#define AM33XX_UART1_RTSN_I2C2_SCL(flags)				AM33XX_PIN_UART1_RTSN(MUX_MODE3, flags)
+#define AM33XX_UART1_RTSN_SPI1_CS1(flags)				AM33XX_PIN_UART1_RTSN(MUX_MODE4, flags)
+#define AM33XX_UART1_RTSN_PR1_UART0_RTS_N(flags)			AM33XX_PIN_UART1_RTSN(MUX_MODE5, flags)
+#define AM33XX_UART1_RTSN_PR1_EDC_LATCH1_IN(flags)			AM33XX_PIN_UART1_RTSN(MUX_MODE6, flags)
+#define AM33XX_UART1_RTSN_GPIO0_13(flags)				AM33XX_PIN_UART1_RTSN(MUX_MODE7, flags)
+
+#define AM33XX_UART1_CTSN(flags)					AM33XX_PIN_UART1_CTSN(MUX_MODE0, flags)
+#define AM33XX_UART1_CTSN_TIMER6(flags)					AM33XX_PIN_UART1_CTSN(MUX_MODE1, flags)
+#define AM33XX_UART1_CTSN_DCAN0_TX(flags)				AM33XX_PIN_UART1_CTSN(MUX_MODE2, flags)
+#define AM33XX_UART1_CTSN_I2C2_SDA(flags)				AM33XX_PIN_UART1_CTSN(MUX_MODE3, flags)
+#define AM33XX_UART1_CTSN_SPI1_CS0(flags)				AM33XX_PIN_UART1_CTSN(MUX_MODE4, flags)
+#define AM33XX_UART1_CTSN_PR1_UART0_CTS_N(flags)			AM33XX_PIN_UART1_CTSN(MUX_MODE5, flags)
+#define AM33XX_UART1_CTSN_PR1_EDC_LATCH0_IN(flags)			AM33XX_PIN_UART1_CTSN(MUX_MODE6, flags)
+#define AM33XX_UART1_CTSN_GPIO0_12(flags)				AM33XX_PIN_UART1_CTSN(MUX_MODE7, flags)
+
+#define AM33XX_XDMA_EVENT_INTR0(flags)					AM33XX_PIN_XDMA_EVENT_INTR0(MUX_MODE0, flags)
+#define AM33XX_XDMA_EVENT_INTR0_TIMER4(flags)				AM33XX_PIN_XDMA_EVENT_INTR0(MUX_MODE2, flags)
+#define AM33XX_XDMA_EVENT_INTR0_CLKOUT1(flags)				AM33XX_PIN_XDMA_EVENT_INTR0(MUX_MODE3, flags)
+#define AM33XX_XDMA_EVENT_INTR0_SPI1_CS1(flags)				AM33XX_PIN_XDMA_EVENT_INTR0(MUX_MODE4, flags)
+#define AM33XX_XDMA_EVENT_INTR0_PR1_PRU1_PRU_R31_16(flags)		AM33XX_PIN_XDMA_EVENT_INTR0(MUX_MODE5, flags)
+#define AM33XX_XDMA_EVENT_INTR0_EMU2(flags)				AM33XX_PIN_XDMA_EVENT_INTR0(MUX_MODE6, flags)
+#define AM33XX_XDMA_EVENT_INTR0_GPIO0_19(flags)				AM33XX_PIN_XDMA_EVENT_INTR0(MUX_MODE7, flags)
+
+#define AM33XX_XDMA_EVENT_INTR1(flags)					AM33XX_PIN_XDMA_EVENT_INTR1(MUX_MODE0, flags)
+#define AM33XX_XDMA_EVENT_INTR1_TCLKIN(flags)				AM33XX_PIN_XDMA_EVENT_INTR1(MUX_MODE2, flags)
+#define AM33XX_XDMA_EVENT_INTR1_CLKOUT2(flags)				AM33XX_PIN_XDMA_EVENT_INTR1(MUX_MODE3, flags)
+#define AM33XX_XDMA_EVENT_INTR1_TIMER7(flags)				AM33XX_PIN_XDMA_EVENT_INTR1(MUX_MODE4, flags)
+#define AM33XX_XDMA_EVENT_INTR1_PR1_PRU0_PRU_R31_16(flags)		AM33XX_PIN_XDMA_EVENT_INTR1(MUX_MODE5, flags)
+#define AM33XX_XDMA_EVENT_INTR1_EMU3(flags)				AM33XX_PIN_XDMA_EVENT_INTR1(MUX_MODE6, flags)
+#define AM33XX_XDMA_EVENT_INTR1_GPIO0_20(flags)				AM33XX_PIN_XDMA_EVENT_INTR1(MUX_MODE7, flags)
+
+#define AM33XX_NNMI(flags)						AM33XX_PIN_NNMI(MUX_MODE0, flags)
+
+#define AM33XX_TMS(flags)						AM33XX_PIN_TMS(MUX_MODE0, flags)
+#define AM33XX_TDI(flags)						AM33XX_PIN_TDI(MUX_MODE0, flags)
+#define AM33XX_TDO(flags)						AM33XX_PIN_TDO(MUX_MODE0, flags)
+#define AM33XX_TCK(flags)						AM33XX_PIN_TCK(MUX_MODE0, flags)
+#define AM33XX_TRSTN(flags)						AM33XX_PIN_TRSTN(MUX_MODE0, flags)
+
+#define AM33XX_USB0_DRVVBUS(flags)					AM33XX_PIN_USB0_DRVVBUS(MUX_MODE0, flags)
+#define AM33XX_USB0_DRVVBUS_GPIO0_18(flags)				AM33XX_PIN_USB0_DRVVBUS(MUX_MODE7, flags)
+
+#define AM33XX_USB1_DRVVBUS(flags)					AM33XX_PIN_USB1_DRVVBUS(MUX_MODE0, flags)
+#define AM33XX_USB1_DRVVBUS_GPIO3_13(flags)				AM33XX_PIN_USB1_DRVVBUS(MUX_MODE7, flags)
+
+#endif
diff --git a/arch/arm/boot/dts/am33xx.dtsi b/arch/arm/boot/dts/am33xx.dtsi
index 4c2298024137..6b6dc84c4294 100644
--- a/arch/arm/boot/dts/am33xx.dtsi
+++ b/arch/arm/boot/dts/am33xx.dtsi
@@ -40,6 +40,7 @@ aliases {
 		ethernet1 = &cpsw_emac1;
 		spi0 = &spi0;
 		spi1 = &spi1;
+		mdio-gpio0 = &bitbang_mdio0;
 	};
 
 	cpus {
diff --git a/arch/arm/boot/dts/am3505-pfc-750_8202.dts b/arch/arm/boot/dts/am3505-pfc-750_8202.dts
new file mode 100644
index 000000000000..c18b331f0532
--- /dev/null
+++ b/arch/arm/boot/dts/am3505-pfc-750_8202.dts
@@ -0,0 +1,19 @@
+/*
+ * Copyright (C) 2011 Texas Instruments Incorporated - http://www.ti.com/
+ *
+ * This program is free software; you u7 redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+/dts-v1/;
+
+#include "am3505-pfc-750_820x.dtsi"
+#include "am3505-pfc-750_820x-ksz8863.dtsi"
+#include "am3505-pfc-750_820x-uart1.dtsi"
+#include "wago-devconf.dtsi"
+
+/ {
+	model = "WAGO PFC200 750-8202";
+	compatible = "wago,am3505-pfc-750_820x-000c", "wago,am3505-pfc", "ti,am3517-evm", "ti,am3517", "ti,omap3";
+
+};
diff --git a/arch/arm/boot/dts/am3505-pfc-750_8203.dts b/arch/arm/boot/dts/am3505-pfc-750_8203.dts
new file mode 100644
index 000000000000..a61971147333
--- /dev/null
+++ b/arch/arm/boot/dts/am3505-pfc-750_8203.dts
@@ -0,0 +1,31 @@
+/*
+ * Copyright (C) 2011 Texas Instruments Incorporated - http://www.ti.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+/dts-v1/;
+
+#include "am3505-pfc-750_820x.dtsi"
+#include "am3505-pfc-750_820x-ksz8863.dtsi"
+#include "am3505-pfc-750_820x-can.dtsi"
+#include "wago-devconf.dtsi"
+
+/ {
+	model = "WAGO PFC200 750-8203";
+	compatible = "wago,am3505-pfc-750_820x-000a", "wago,am3505-pfc", "ti,am3517-evm", "ti,am3517", "ti,omap3";
+
+};
+
+&hecc {
+	status = "okay";
+};
+
+&u7_g {
+	label = "can-green";
+};
+
+&u7_r {
+	label = "can-red";
+};
diff --git a/arch/arm/boot/dts/am3505-pfc-750_8204.dts b/arch/arm/boot/dts/am3505-pfc-750_8204.dts
new file mode 100644
index 000000000000..e3eebe89e2e7
--- /dev/null
+++ b/arch/arm/boot/dts/am3505-pfc-750_8204.dts
@@ -0,0 +1,32 @@
+/*
+ * Copyright (C) 2011 Texas Instruments Incorporated - http://www.ti.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+/dts-v1/;
+
+#include "am3505-pfc-750_820x.dtsi"
+#include "am3505-pfc-750_820x-ksz8863.dtsi"
+#include "am3505-pfc-750_820x-uart1.dtsi"
+#include "am3505-pfc-750_820x-can.dtsi"
+#include "wago-devconf.dtsi"
+
+/ {
+	model = "WAGO PFC200 750-8204";
+	compatible = "wago,am3505-pfc-750_820x-000e", "wago,am3505-pfc", "ti,am3517-evm", "ti,am3517", "ti,omap3";
+
+};
+
+&hecc {
+	status = "okay";
+};
+
+&u7_g {
+	label = "can-green";
+};
+
+&u7_r {
+	label = "can-red";
+};
diff --git a/arch/arm/boot/dts/am3505-pfc-750_8206.dts b/arch/arm/boot/dts/am3505-pfc-750_8206.dts
new file mode 100644
index 000000000000..9fbec794ddcb
--- /dev/null
+++ b/arch/arm/boot/dts/am3505-pfc-750_8206.dts
@@ -0,0 +1,51 @@
+/*
+ * Copyright (C) 2011 Texas Instruments Incorporated - http://www.ti.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+/dts-v1/;
+
+#include "am3505-pfc-750_820x.dtsi"
+#include "am3505-pfc-750_820x-ksz8863.dtsi"
+#include "am3505-pfc-750_820x-uart1.dtsi"
+#include "am3505-pfc-750_820x-can.dtsi"
+#include "am3505-pfc-750_820x-dpc31.dtsi"
+#include "am3505-pfc-750_820x-musb.dtsi"
+#include "wago-devconf.dtsi"
+
+/ {
+	model = "WAGO PFC200 750-8206";
+	compatible = "wago,am3505-pfc-750_820x-000f", "wago,am3505-pfc", "ti,am3517-evm", "ti,am3517", "ti,omap3";
+
+};
+
+&hecc {
+	status = "okay";
+};
+
+&u6_g {
+	label = "bf-green";
+};
+
+&u6_r {
+	label = "bf-red";
+};
+
+&u5_g {
+	label = "dia-green";
+};
+
+&u5_r {
+	label = "dia-red";
+};
+
+
+&u7_g {
+	label = "can-green";
+};
+
+&u7_r {
+	label = "can-red";
+};
diff --git a/arch/arm/boot/dts/am3505-pfc-750_8207.dts b/arch/arm/boot/dts/am3505-pfc-750_8207.dts
new file mode 100644
index 000000000000..41870052bd23
--- /dev/null
+++ b/arch/arm/boot/dts/am3505-pfc-750_8207.dts
@@ -0,0 +1,103 @@
+/*
+ * Copyright (C) 2011 Texas Instruments Incorporated - http://www.ti.com/
+ *
+ * This program is free software; you u7 redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+/dts-v1/;
+
+#include "am3505-pfc-750_820x.dtsi"
+#include "am3505-pfc-750_820x-ksz8863.dtsi"
+#include "am3505-pfc-750_820x-uart1.dtsi"
+#include "am3505-pfc-750_820x-musb.dtsi"
+#include "wago-devconf.dtsi"
+
+/ {
+	model = "WAGO PFC200 750-8207";
+	compatible = "wago,am3505-pfc-750_820x-001c", "wago,am3505-pfc", "ti,am3517-evm", "ti,am3517", "ti,omap3";
+
+};
+
+&u7_g {
+	label = "usr-green";
+	gpios = <&gpio3 24 0>;
+	linux,default-trigger = "none";
+};
+
+&u7_r {
+	label = "usr-red";
+	gpios = <&gpio3 25 0>;
+	linux,default-trigger = "none";
+};
+
+&u1_g {
+	label = "net-green";
+	gpios = <&gpio3 22 0>;
+	linux,default-trigger = "none";
+};
+&u1_r {
+	label = "net-red";
+	gpios = <&gpio3 23 0>;
+	linux,default-trigger = "none";
+};
+
+&u2_g {
+	label = "s1-green";
+	gpios = <&gpio3 18 0>;
+	linux,default-trigger = "none";
+};
+
+&u2_r {
+	label = "s1-red";
+	gpios = <&gpio3 19 0>;
+	linux,default-trigger = "none";
+};
+
+&u3_g {
+	label = "s2-green";
+	gpios = <&gpio3 14 0>;
+	linux,default-trigger = "none";
+};
+
+&u3_r {
+	label = "s2-red";
+	gpios = <&gpio3 15 0>;
+	linux,default-trigger = "none";
+};
+
+&u4_g {
+	label = "s3-green";
+	gpios = <&gpio3 10 0>;
+	linux,default-trigger = "none";
+};
+
+&u4_r {
+	label = "s3-red";
+	gpios = <&gpio3 11 0>;
+	linux,default-trigger = "none";
+};
+
+&u5_g {
+	label = "s4-green";
+	gpios = <&gpio3 6 0>;
+	linux,default-trigger = "none";
+};
+
+&u5_r {
+	label = "s4-red";
+	gpios = <&gpio3 7 0>;
+	linux,default-trigger = "none";
+};
+
+&u6_g {
+	label = "s5-green";
+	gpios = <&gpio3 2 0>;
+	linux,default-trigger = "none";
+	};
+
+&u6_r {
+	label = "s5-red";
+	gpios = <&gpio3 3 0>;
+	linux,default-trigger = "none";
+};
diff --git a/arch/arm/boot/dts/am3505-pfc-750_820x-can.dtsi b/arch/arm/boot/dts/am3505-pfc-750_820x-can.dtsi
new file mode 100644
index 000000000000..f603b49b9ed7
--- /dev/null
+++ b/arch/arm/boot/dts/am3505-pfc-750_820x-can.dtsi
@@ -0,0 +1,32 @@
+/*
+ * Copyright (C) 2011 Texas Instruments Incorporated - http://www.ti.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+&omap3_pmx_core {
+	can_pins: pinmux_can_pins {
+		pinctrl-single,pins = <
+			/* CAN-LEDs */
+			0x154 (PIN_OUTPUT | MUX_MODE4)		          /* McBSP4_CLKX   -> CAN-RUN_green     GPIO_152 */
+			0x156 (PIN_OUTPUT | MUX_MODE4)		          /* McBSP4_DR     -> CAN-RUN_red       GPIO_153 */
+			0x158 (PIN_OUTPUT | MUX_MODE4)		          /* McBSP4_DX     -> CAN-ERR_green     GPIO_154 */
+			0x15a (PIN_OUTPUT | MUX_MODE4)		          /* McBSP4_FSX    -> CAN-ERR_red       GPIO_155 */
+			0x15c (PIN_OUTPUT | MUX_MODE4)		          /* McBSP1_CLKR   -> CAN-TX_green      GPIO_156 */
+			0x15e (PIN_OUTPUT | MUX_MODE4)		          /* McBSP1_FSR    -> CAN-TX_red        GPIO_157 */
+			0x160 (PIN_OUTPUT | MUX_MODE4)		          /* McBSP1_DX     -> CAN-RX_green      GPIO_158 */
+			0x162 (PIN_OUTPUT | MUX_MODE4)		          /* McBSP1_DR     -> CAN-RX_red        GPIO_159 */
+
+			/* CAN */
+			0x1e4 (PIN_OUTPUT | MUX_MODE0)		          /* HECC1_TXD     -> CAN_TxD */
+			0x1e6 (PIN_INPUT | MUX_MODE0)		          /* HECC1_RXD     -> CAN_RxD */
+		>;
+	};
+};
+
+&hecc {
+	pinctrl-names = "default";
+	pinctrl-0 = <&can_pins>;
+};
diff --git a/arch/arm/boot/dts/am3505-pfc-750_820x-dpc31.dtsi b/arch/arm/boot/dts/am3505-pfc-750_820x-dpc31.dtsi
new file mode 100644
index 000000000000..3610bb20747f
--- /dev/null
+++ b/arch/arm/boot/dts/am3505-pfc-750_820x-dpc31.dtsi
@@ -0,0 +1,108 @@
+/*
+ * Copyright (C) 2011 Texas Instruments Incorporated - http://www.ti.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+/ {
+	/* dpc31 is on cs1 */
+	UIO_DPC31_XINT@0x1000000 {
+		compatible = "uio_pdrv_genirq";
+		reg = <0x1000000 0x4000>; /* 8k * 2 = 16k, needed because of 16Bit addressing */
+		interrupt-parent = <&gpio3>;
+		interrupts = <1 IRQ_TYPE_EDGE_FALLING>;	/* gpio3_1 / gpio_65 */
+	};
+
+	UIO_DPC31_SYNC@0x1000000 {
+		compatible = "uio_pdrv_genirq";
+		interrupt-parent = <&gpio6>;
+		interrupts = <1 IRQ_TYPE_EDGE_RISING>;	/* gpio6_1 / gpio_161 */
+	};
+
+	UIO_DPC31_DXOUT@0x1000000 {
+		compatible = "uio_pdrv_genirq";
+		interrupt-parent = <&gpio6>;
+		interrupts = <0 IRQ_TYPE_EDGE_RISING>;	/* gpio6_0 / gpio_160 */
+	};
+};
+
+&omap3_pmx_core {
+	dpc31_pins: pinmux_dpc31_pins {
+		pinctrl-single,pins = <
+		  /* gpmc */
+		  // OMAP3_CORE1_IOPAD(0x20b0, PIN_OUTPUT | MUX_MODE4) /* gpmc_ncs1.gpmc_ncs1     -> GPMC-nCS1-FB */
+		  // OMAP3_CORE1_IOPAD(0x20ce, PIN_INPUT | MUX_MODE0)  /* gpmc_wait1.gpmc_wait1 -> GPMC-WAIT1-DPC31 */
+		  /* gpios */
+		  OMAP3_CORE1_IOPAD(0x20D2, PIN_INPUT | MUX_MODE4)  /* gpmc_wait3.gpio3_1    -> SYS_nDMAREQ1
+								     * GPIO Interrupt (GPIO_65): DCP31 XINT (PG5)
+								     * Also useable for DMA transfers.
+								     */
+		  OMAP3_CORE1_IOPAD(0x2194, PIN_INPUT | MUX_MODE4)  /* mcbsp_clks.gpio6_0    -> FB-EEPROM-LOADED (= DPC31_DXCH)
+								     * GPIO Interrupt (GPIO_160): DPC31_DXCH, DCP31 IRR14 (PB3)
+								     * Active on new output data ... (optional)
+								     */
+		  OMAP3_CORE1_IOPAD(0x2196, PIN_OUTPUT | MUX_MODE4) /* mcbsp1_fsx.gpio6_1    -> FB-nSYNC (= DPC31_SYNC)
+								     * GPIO Interrupt (GPIO_161): DPC31_SYNC, DCP31 RES (PB2)
+								     * Fieldbus Sync Signal (optional)
+								     */
+		  OMAP3_CORE1_IOPAD(0x2198, PIN_OUTPUT | MUX_MODE4) /* mcbsp1_clkx.gpio6_2   -> FB-nRST (= nExt_RESET) --> GPIO_162 */
+		>;
+	};
+};
+
+/* #define PAC200_DPC31_GPIO__INT_SYS_NDMAREQ1        65: gpio3_1 */
+/* #define PAC200_DPC31_GPIO__INT_DXCH               160: gpio6_0 */
+/* #define PAC200_DPC31_GPIO__INT_FB_NSYNC           161: gpio6_1 */
+/* #define PAC200_DPC31_GPIO__RESET                  162: gpio6_2 */
+
+&wsysinit {
+	pinctrl-names = "default";
+	pinctrl-0 = <&dpc31_pins>;
+
+	dp,reset;
+	dp,gpio-rst = <&gpio6 2 GPIO_ACTIVE_HIGH>; /* dpc31 reset */
+};
+
+&gpmc {
+	dpc31: nor@1,0 {
+		reg = <1 0 0x1000000>; /* 8k * 2 = 16k, needed because of 16Bit addressing
+					* minimum is 16M - set it here */
+		bank-width = <2>; /* 1: 8bit, 2: 16bit */
+		gpmc,sync-clk-ps = <0>;
+
+		gpmc,cs-on-ns = <42>;
+		gpmc,cs-rd-off-ns = <144>;
+		gpmc,cs-wr-off-ns = <144>;
+
+		gpmc,adv-on-ns = <12>;
+		gpmc,adv-rd-off-ns = <156>;
+		gpmc,adv-wr-off-ns = <156>;
+
+		gpmc,oe-on-ns = <42>;
+		gpmc,oe-off-ns = <144>;
+
+		gpmc,we-on-ns = <42>;
+		gpmc,we-off-ns = <114>;
+
+		gpmc,rd-cycle-ns = <168>;
+		gpmc,wr-cycle-ns = <168>;
+
+		gpmc,access-ns = <150>;
+		gpmc,wr-access-ns = <54>;
+
+		gpmc,wr-data-mux-bus-ns = <18>;
+
+		gpmc,cycle2cycle-delay-ns = <0>;
+
+		gpmc,time-para-granularity;
+		gpmc,mux-add-data = <2>; /* address-data multiplexing mode */
+
+		gpmc,clk-activation-ns = <0>;
+		gpmc,wait-monitoring-ns = <0>;
+
+		#address-cells = <1>;
+		#size-cells = <1>;
+	};
+};
diff --git a/arch/arm/boot/dts/am3505-pfc-750_820x-ksz8863.dtsi b/arch/arm/boot/dts/am3505-pfc-750_820x-ksz8863.dtsi
new file mode 100644
index 000000000000..0e6e77bbe157
--- /dev/null
+++ b/arch/arm/boot/dts/am3505-pfc-750_820x-ksz8863.dtsi
@@ -0,0 +1,76 @@
+/*
+ * Copyright (C) 2019 WAGO Kontakttechnik GmbH & Co. KG - https://www.wago.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+/ {
+	swcfg_ksz8863: swcfg_ksz8863 {
+		compatible = "swcfg,ksz8863";
+		swcfg,mii-bus = <&bitbang_mdio0>;
+		swcfg,alias = "ksz8863";
+		swcfg,cpu_port = <2>;
+		swcfg,ports = <3>;
+		swcfg,vlans = <16>;
+		swcfg,switch = <&ksz8863_switch>;
+
+		status = "okay";
+	};
+};
+
+&omap3_pmx_core {
+	ksz8863_pins: pinmux_ksz8863_pins {
+		pinctrl-single,pins = <
+			0x134 (PIN_OUTPUT | MUX_MODE4) /* mmc2_dat4.gpio5_8 (gpio_136) nrst_switch */
+			OMAP3_CORE1_IOPAD(0x2168, PIN_INPUT | MUX_MODE4) /* mmc2_dat6.gpio5_10 (gpio_138) nintswitch */
+		>;
+	};
+};
+
+&bitbang_mdio0 {
+	ksz8863_switch: switch@0 {
+		compatible = "micrel,ksz8863";
+
+		#address-cells = <1>;
+		#size-cells = <0>;
+
+		pinctrl-names = "default";
+		pinctrl-0 = <&ksz8863_pins>;
+
+		ksz,reset-gpio = <&gpio5 8 GPIO_ACTIVE_LOW>;	/* gpio2_21: RMII2.EN-PHY */
+
+		reg = <0>;
+		dsa,member = <0 0>;
+
+		ksz,reset-switch;
+
+		status = "okay";
+
+		ports {
+			#address-cells = <1>;
+			#size-cells = <0>;
+
+			port@0 {
+				reg = <2>;
+				label = "ethX1";
+			};
+
+			port@1 {
+				reg = <1>;
+				label = "ethX2";
+			};
+
+			port@2 {
+				reg = <3>;
+				label = "cpu";
+				ethernet = <&davinci_emac>;
+				fixed-link {
+					speed = <100>;
+					full-duplex;
+				};
+			};
+		};
+	};
+};
diff --git a/arch/arm/boot/dts/am3505-pfc-750_820x-musb.dtsi b/arch/arm/boot/dts/am3505-pfc-750_820x-musb.dtsi
new file mode 100644
index 000000000000..88b337744a43
--- /dev/null
+++ b/arch/arm/boot/dts/am3505-pfc-750_820x-musb.dtsi
@@ -0,0 +1,38 @@
+/*
+ * Copyright (C) 2011 Texas Instruments Incorporated - http://www.ti.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+&omap3_pmx_core {
+	usb_otg_pins: pinmux_usb_otg_pins {
+		pinctrl-single,pins = <
+			/* USB OTG Mux Config */
+			0x1e2 (PIN_INPUT_PULLDOWN | MUX_MODE0)  /* usb0_drvvbus.usb0_drvvbus */
+			0x128 (PIN_OUTPUT | MUX_MODE4)          /* mmc2_clk.gpio5_2 - gpio_130: 3G Modem Reset */
+			0x12c (PIN_OUTPUT | MUX_MODE4)          /* mmc2_dat0.gpio5_4 - gpio_132: 3G Modem Power */
+		>;
+	};
+};
+
+&wsysinit {
+	modem,type = "3G";
+	modem,reset;
+	modem,gpio-rst = <&gpio5 2 GPIO_ACTIVE_HIGH>; /* 3G Modem Reset */
+};
+
+&am35x_otg_hs {
+	status = "okay";
+	pinctrl-names = "default";
+	pinctrl-0 = <&usb_otg_pins>;
+
+	multipoint = <1>;
+	num-eps = <16>;
+	ram-bits = <12>;
+
+	interface-type = <0>; 	/* ULPI */
+	mode = <1>;		/* HOST Mode */
+	power = <250>;		/* can supply 100mA when operating in host mode */
+};
diff --git a/arch/arm/boot/dts/am3505-pfc-750_820x-uart1.dtsi b/arch/arm/boot/dts/am3505-pfc-750_820x-uart1.dtsi
new file mode 100644
index 000000000000..8e0c8919f7ab
--- /dev/null
+++ b/arch/arm/boot/dts/am3505-pfc-750_820x-uart1.dtsi
@@ -0,0 +1,33 @@
+/*
+ * Copyright (C) 2011 Texas Instruments Incorporated - http://www.ti.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+&omap3_pmx_core {
+
+	uart1_rs_pins: pinmux_uart1_rs_pins {
+		pinctrl-single,pins = <
+			OMAP3_CORE1_IOPAD(0x21d2, PIN_OUTPUT | MUX_MODE4)	/* mcspi1_cs2.gpio6_16 (gpio_176) sel_rs232/485 */
+			0x150 (PIN_INPUT | MUX_MODE0)				/* uart1_cts.uart1_cts */
+			0x14e (PIN_OUTPUT | MUX_MODE0)				/* uart1_rts.uart1_rts */
+			0x152 (WAKEUP_EN | PIN_INPUT | MUX_MODE0)		/* uart1_rx.uart1_rx */
+			0x14c (PIN_OUTPUT | MUX_MODE0)				/* uart1_tx.uart1_tx */
+		>;
+	};
+};
+
+&uart1 {
+	compatible = "ti,omap3-uart-rtu";
+	status = "okay";
+	pinctrl-names = "default";
+	pinctrl-0 = <&uart1_rs_pins>;
+
+	rs485en-gpio = <&gpio6 16 GPIO_ACTIVE_LOW>;
+	rs485-rts-active-high;
+	/* rs485-rx-during-tx; */ // we only support 2-wire mode
+	rs485-rts-delay = <1 1>;
+	/* linux,rs485-enabled-at-boot-time; */
+};
diff --git a/arch/arm/boot/dts/am3505-pfc-750_820x.dtsi b/arch/arm/boot/dts/am3505-pfc-750_820x.dtsi
new file mode 100644
index 000000000000..e498716e1fce
--- /dev/null
+++ b/arch/arm/boot/dts/am3505-pfc-750_820x.dtsi
@@ -0,0 +1,678 @@
+/*
+ * Copyright (C) 2011 Texas Instruments Incorporated - http://www.ti.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include "am3517.dtsi"
+
+/ {
+	compatible = "wago,am3505-pfc", "ti,am3517", "ti,omap3";
+
+	aliases {
+		mdio-gpio0 = &bitbang_mdio0;
+		ethernet0 = &davinci_emac;
+	};
+
+	memory {
+		device_type = "memory";
+		reg = <0x80000000 0x10000000>; /* 256 MB */
+	};
+
+	vmmc_fixed: vmmc {
+		compatible = "regulator-fixed";
+		regulator-name = "vmmc_fixed";
+		regulator-min-microvolt = <3300000>;
+		regulator-max-microvolt = <3300000>;
+	};
+
+	/* common init entries */
+	wsysinit: wsysinit_init {
+		status = "okay";
+		compatible = "wago,sysinit";
+
+		tty,service   = "ttyO2";
+		tty,rs232-485 = "ttyO0";
+
+		board,variant = "pfc200";
+
+		/* sysclock adjustments,
+		   empirical values */
+		adjtimex,tick = <10083>;
+		adjtimex,frequency = <2000000>;
+	};
+
+	bitbang_mdio0: gpio_mdio {
+		status = "okay";
+		compatible = "virtual,mdio-gpio";
+		#address-cells = <1>;
+		#size-cells = <0>;
+	};
+
+	/*
+	   this name of the gpio-keys device is a
+	   historical heritage from 3.6.11 kernel.
+	   the device-name is checked in omsd. So we
+	   need to adapt this configuration
+	*/
+	PAC-Operating-Mode-Switch {
+		compatible = "gpio-keys";
+		#address-cells = <1>;
+		#size-cells = <0>;
+		autorepeat;
+
+		run {
+			label = "RUN";
+			gpios = <&gpio3 26 GPIO_ACTIVE_LOW>; /* BAS_RUN GPIO 90 */
+			linux,code = <1>;
+			linux,input-type = <5>;  /* EV_SW */
+			debounce-interval = <1>; /* debounce ticks interval in msecs */
+		};
+
+		stop {
+			label = "STOP";
+			gpios = <&gpio3 27 GPIO_ACTIVE_LOW>; /* BAS_RUN GPIO 91 */
+			linux,code = <2>;
+			linux,input-type = <5>;  /* EV_SW */
+			debounce-interval = <1>; /* debounce ticks interval in msecs */
+		};
+
+		reset {
+			label = "RESET";
+			gpios = <&gpio3 28 GPIO_ACTIVE_LOW>; /* BAS_RUN GPIO 92 */
+			linux,code = <3>;
+			linux,input-type = <1>;  /* EV_KEY */
+			debounce-interval = <1>; /* debounce ticks interval in msecs */
+		};
+
+		reset_all {
+			label = "RESET_ALL";
+			gpios = <&gpio3 29 GPIO_ACTIVE_HIGH>; /* BAS_RUN GPIO 93 */
+			linux,code = <4>;
+			linux,input-type = <1>;  /* EV_KEY */
+			debounce-interval = <1>; /* debounce ticks interval in msecs */
+		};
+
+	};
+
+	/* nvram is on cs2 */
+	UIO_NVRAM@0x2000000 {
+		compatible = "uio_pdrv_genirq";
+		reg = <0x2000000 0x20000>; /* 128k */
+	};
+
+	leds: gpio-leds {
+		compatible = "gpio-leds";
+
+		u1_g: u1-green@0 {
+			label = "u1-green";
+			gpios = <&gpio3 22 0>;
+			linux,default-trigger = "none";
+		};
+
+		u1_r: u1-red@1 {
+			label = "u1-red";
+			gpios = <&gpio3 23 0>;
+			linux,default-trigger = "none";
+		};
+
+		u2_g: u2-green@2 {
+			label = "u2-green";
+			gpios = <&gpio3 18 0>;
+			linux,default-trigger = "none";
+		};
+
+		u2_r: u2-red@3 {
+			label = "u2-red";
+			gpios = <&gpio3 19 0>;
+			linux,default-trigger = "none";
+		};
+
+		u3_g: u3-green@4 {
+			label = "u3-green";
+			gpios = <&gpio3 14 0>;
+			linux,default-trigger = "none";
+		};
+
+		u3_r: u3-red@5 {
+			label = "u3-red";
+			gpios = <&gpio3 15 0>;
+			linux,default-trigger = "none";
+		};
+
+		u4_g: u4-green@6 {
+			label = "u4-green";
+			gpios = <&gpio3 10 0>;
+			linux,default-trigger = "none";
+		};
+
+		u4_r: u4-red@7 {
+			label = "u4-red";
+			gpios = <&gpio3 11 0>;
+			linux,default-trigger = "none";
+		};
+
+		u5_g: u5-green@8 {
+			label = "u5-green";
+			gpios = <&gpio3 6 0>;
+			linux,default-trigger = "none";
+		};
+
+		u5_r: u5-red@9 {
+			label = "u5-red";
+			gpios = <&gpio3 7 0>;
+			linux,default-trigger = "none";
+		};
+
+		u6_g: u6-green@10 {
+			label = "u6-green";
+			gpios = <&gpio3 2 0>;
+			linux,default-trigger = "none";
+			};
+
+		u6_r: u6-red@11 {
+			label = "u6-red";
+			gpios = <&gpio3 3 0>;
+			linux,default-trigger = "none";
+		};
+
+		sys_g: sys-green@12 {
+			label = "sys-green";
+			gpios = <&gpio3 4 0>;
+			linux,default-trigger = "timer";
+		};
+
+		sys_r: sys-red@13 {
+			label = "sys-red";
+			gpios = <&gpio3 5 0>;
+			linux,default-trigger = "timer";
+		};
+
+
+		run_g: run-green@14 {
+			label = "run-green";
+			gpios = <&gpio3 8 0>;
+			linux,default-trigger = "none";
+		};
+
+		run_r: run-red@15 {
+			label = "run-red";
+			gpios = <&gpio3 9 0>;
+			linux,default-trigger = "none";
+		};
+
+
+		io_g: io-green@16 {
+			label = "io-green";
+			gpios = <&gpio3 12 0>;
+			linux,default-trigger = "none";
+		};
+
+		io_r: io-red@17 {
+			label = "io-red";
+			gpios = <&gpio3 13 0>;
+			linux,default-trigger = "none";
+		};
+
+
+		ms_g: ms-green@18 {
+			label = "ms-green";
+			gpios = <&gpio3 16 0>;
+			linux,default-trigger = "none";
+		};
+
+		ms_r: ms-red@19 {
+			label = "ms-red";
+			gpios = <&gpio3 17 0>;
+			linux,default-trigger = "none";
+		};
+
+
+		ns_g: ns-green@20 {
+			label = "ns-green";
+			gpios = <&gpio3 20 0>;
+			linux,default-trigger = "none";
+		};
+
+		ns_r: ns-red@21 {
+			label = "ns-red";
+			gpios = <&gpio3 21 0>;
+			linux,default-trigger = "none";
+		};
+
+		u7_g: u7-green@22 {
+			label = "u7-green";
+			gpios = <&gpio3 24 0>;
+			linux,default-trigger = "none";
+		};
+
+		u7_r: u7-red@23 {
+			label = "u7-red";
+			gpios = <&gpio3 25 0>;
+			linux,default-trigger = "none";
+		};
+	};
+
+	watchdog: watchdog {
+		/* XC6124 */
+		compatible = "linux,wdt-gpio";
+		pinctrl-names = "default";
+		pinctrl-0 = <&watchdog_pins>;
+
+		en-gpios = <&gpio6 3 GPIO_ACTIVE_LOW>; /* 163 - EN */
+		gpios = <&gpio6 4 GPIO_ACTIVE_LOW>;   /* 164 - WDI */
+
+		hw_algo = "toggle";
+		hw_margin_ms = <1600>;
+		status = "okay";
+
+		rt-prio = <91>;
+	};
+};
+
+&omap3_pmx_core {
+	pinctrl-names = "default";
+	pinctrl-0 = <
+		&led_pins
+	>;
+
+	uart2_pins: pinmux_uart2_pins {
+		pinctrl-single,pins = <
+			OMAP3_CORE1_IOPAD(0x217a, WAKEUP_EN | PIN_INPUT | MUX_MODE0) /* uart2_rx.uart2_rx */
+			OMAP3_CORE1_IOPAD(0x2178, PIN_OUTPUT | MUX_MODE0)	     /* uart2_tx.uart2_tx */
+		>;
+	};
+
+	mcspi1_pins: pinmux_mcspi1_pins {
+		pinctrl-single,pins = <
+			OMAP3_CORE1_IOPAD(0x21c8, PIN_INPUT | MUX_MODE0) /* mcspi1_clk.mcspi1_clk */
+			OMAP3_CORE1_IOPAD(0x21ca, PIN_OUTPUT | MUX_MODE0) /* mcspi1_simo.mcspi1_simo */
+			OMAP3_CORE1_IOPAD(0x21cc, PIN_INPUT | MUX_MODE0) /* mcspi1_somi.mcspi1_somi */
+			OMAP3_CORE1_IOPAD(0x21ce, PIN_OUTPUT | MUX_MODE0) /* mcspi1_cs0.mcspi1_cs0 */
+		>;
+	};
+
+	kbus_pins: pinmux_kbus_pins {
+		pinctrl-single,pins = <
+			OMAP3_CORE1_IOPAD(0x21ea, PIN_OUTPUT | MUX_MODE4) /* ccdc_vd.gpio4_1     -> KBus-nIRQ_GPIO97 */
+
+			OMAP3_CORE1_IOPAD(0x21ec, PIN_INPUT | MUX_MODE4)  /* ccdc_wen.gpio4_2    -> KBus-ErrSt0_GPIO98 */
+			OMAP3_CORE1_IOPAD(0x21ee, PIN_INPUT | MUX_MODE4)  /* ccdc_data0.gpio4_3  -> KBus-ErrSt1_GPIO99 */
+			OMAP3_CORE1_IOPAD(0x21f0, PIN_INPUT | MUX_MODE4)  /* ccdc_data1.gpio4_4  -> KBus-ErrSt2_GPIO100 */
+			OMAP3_CORE1_IOPAD(0x21f2, PIN_INPUT | MUX_MODE4)  /* ccdc_data2.gpio4_5  -> KBus-ErrSt3_GPIO101 */
+
+			OMAP3_CORE1_IOPAD(0x21f4, PIN_INPUT | MUX_MODE4)  /* ccdc_data3.gpio4_6  -> KBus-nErr_GPIO102 */
+			OMAP3_CORE1_IOPAD(0x21f6, PIN_OUTPUT | MUX_MODE4) /* ccdc_data4.gpio4_7  -> KBus-CmdSel_GPIO103 */
+			OMAP3_CORE1_IOPAD(0x21f8, PIN_INPUT | MUX_MODE4)  /* ccdc_data5.gpio4_8  -> KBus-nRDY_GPIO104 */
+			OMAP3_CORE1_IOPAD(0x21fa, PIN_OUTPUT | MUX_MODE4) /* ccdc_data6.gpio4_9  -> KBus-nRST_GPIO105 */
+			OMAP3_CORE1_IOPAD(0x21fc, PIN_INPUT | MUX_MODE4)  /* ccdc_data7.gpio4_10 -> KBus-nSync_GPIO106 */
+		>;
+	};
+
+	led_pins: pinmux_led_pins {
+		pinctrl-single,pins = <
+			OMAP3_CORE1_IOPAD(0x20d4, PIN_OUTPUT_PULLDOWN | MUX_MODE4)  /* dss_pclk.gpio_66: led_1_1_green */
+			OMAP3_CORE1_IOPAD(0x20d6, PIN_OUTPUT_PULLDOWN | MUX_MODE4)  /* dss_hsync.gpio_67: led_1_1_red */
+			OMAP3_CORE1_IOPAD(0x20d8, PIN_OUTPUT_PULLDOWN | MUX_MODE4)  /* dss_vsync.gpio_68: led_1_2_green */
+			OMAP3_CORE1_IOPAD(0x20da, PIN_OUTPUT_PULLDOWN | MUX_MODE4)  /* dss_acbias.gpio_69: led_1_2_red */
+
+			OMAP3_CORE1_IOPAD(0x20dc, PIN_OUTPUT_PULLDOWN | MUX_MODE4)  /* dss_data0.gpio_70: led_2_1_green */
+			OMAP3_CORE1_IOPAD(0x20de, PIN_OUTPUT_PULLDOWN | MUX_MODE4)	    /* dss_data1.gpio_71: led_2_1_red */
+			OMAP3_CORE1_IOPAD(0x20e0, PIN_OUTPUT_PULLDOWN | MUX_MODE4)  /* dss_data2.gpio_72: led_2_2_green */
+			OMAP3_CORE1_IOPAD(0x20e2, PIN_OUTPUT_PULLDOWN | MUX_MODE4)  /* dss_data3.gpio_73: led_2_2_red */
+
+			OMAP3_CORE1_IOPAD(0x20e4, PIN_OUTPUT_PULLDOWN | MUX_MODE4)  /* dss_data4.gpio_74: led_3_1_green */
+			OMAP3_CORE1_IOPAD(0x20e6, PIN_OUTPUT_PULLDOWN | MUX_MODE4)  /* dss_data5.gpio_75: led_3_1_red */
+			OMAP3_CORE1_IOPAD(0x20e8, PIN_OUTPUT_PULLDOWN | MUX_MODE4)  /* dss_data6.gpio_76: led_3_2_green */
+			OMAP3_CORE1_IOPAD(0x20ea, PIN_OUTPUT_PULLDOWN | MUX_MODE4)  /* dss_data7.gpio_77: led_3_2_red */
+
+			OMAP3_CORE1_IOPAD(0x20ec, PIN_OUTPUT_PULLDOWN | MUX_MODE4)  /* dss_data8.gpio_78: led_4_1_green */
+			OMAP3_CORE1_IOPAD(0x20ee, PIN_OUTPUT_PULLDOWN | MUX_MODE4)  /* dss_data9.gpio_79: led_4_1_red */
+			OMAP3_CORE1_IOPAD(0x20f0, PIN_OUTPUT_PULLDOWN | MUX_MODE4)  /* dss_data10.gpio_80: led_4_2_green */
+			OMAP3_CORE1_IOPAD(0x20f2, PIN_OUTPUT_PULLDOWN | MUX_MODE4)  /* dss_data11.gpio_81: led_4_2_red */
+
+			OMAP3_CORE1_IOPAD(0x20f4, PIN_OUTPUT_PULLDOWN | MUX_MODE4)  /* dss_data12.gpio_82: led_5_1_green */
+			OMAP3_CORE1_IOPAD(0x20f6, PIN_OUTPUT_PULLDOWN | MUX_MODE4)  /* dss_data13.gpio_83: led_5_1_red */
+			OMAP3_CORE1_IOPAD(0x20f8, PIN_OUTPUT_PULLDOWN | MUX_MODE4)  /* dss_data14.gpio_84: led_5_2_green */
+			OMAP3_CORE1_IOPAD(0x20fa, PIN_OUTPUT_PULLDOWN | MUX_MODE4)  /* dss_data15.gpio_85: led_5_2_red */
+
+			OMAP3_CORE1_IOPAD(0x20fc, PIN_OUTPUT_PULLDOWN | MUX_MODE4)  /* dss_data16.gpio_86: led_6_1_green */
+			OMAP3_CORE1_IOPAD(0x20fe, PIN_OUTPUT_PULLDOWN | MUX_MODE4)  /* dss_data17.gpio_87: led_6_1_red */
+			OMAP3_CORE1_IOPAD(0x2100, PIN_OUTPUT_PULLDOWN | MUX_MODE4)  /* dss_data18.gpio_88: led_6_2_green */
+			OMAP3_CORE1_IOPAD(0x2102, PIN_OUTPUT_PULLDOWN | MUX_MODE4)  /* dss_data19.gpio_89: led_6_2_red */
+		>;
+	};
+
+	emac_pins: pinmux_emac_pins {
+		pinctrl-single,pins = <
+			OMAP3_CORE1_IOPAD(0x2202, PIN_INPUT | MUX_MODE0)           /* rmii_rxd0.rmii_rxd0           */
+			OMAP3_CORE1_IOPAD(0x2204, PIN_INPUT | MUX_MODE0)           /* rmii_rxd1.rmii_rxd1           */
+			OMAP3_CORE1_IOPAD(0x2206, PIN_INPUT | MUX_MODE0)           /* rmii_crs_dv.rmii_crs_dv       */
+			OMAP3_CORE1_IOPAD(0x2208, PIN_INPUT | MUX_MODE0)           /* rmii_rxer.rmii_rxer           */
+			OMAP3_CORE1_IOPAD(0x220a, PIN_OUTPUT_PULLUP | MUX_MODE0)   /* rmii_txd0.rmii_txd0           */
+			OMAP3_CORE1_IOPAD(0x220c, PIN_INPUT | MUX_MODE0)           /* rmii_txd1.rmii_txd1           */
+			OMAP3_CORE1_IOPAD(0x220e, PIN_INPUT | MUX_MODE0)           /* rmii_txen.rmii_txen           */
+			OMAP3_CORE1_IOPAD(0x2210, PIN_INPUT | MUX_MODE0)           /* rmii_50mhz_clk.rmii_50mhz_clk */
+		>;
+	};
+
+	gpio_bitbang_mdio_pins: pinmux_gpio_bitbang_mdio_pins {
+		pinctrl-single,pins = <
+			/* MDIO */
+			OMAP3_CORE1_IOPAD(0x21fe, PIN_INPUT_PULLUP  | MUX_MODE4) /* rmii_mdio_data.gpio4_11 (gpio_107) */
+			OMAP3_CORE1_IOPAD(0x2200, PIN_OUTPUT_PULLUP | MUX_MODE4) /* rmii_mdio_clk.gpio4_12 (gpio_108)  */
+		>;
+	};
+
+	gpio_bitbang_mdio_sleep_pins: pinmux_gpio_bitbang_mdio_sleep_pins {
+		pinctrl-single,pins = <
+			/* MDIO reset value */
+			OMAP3_CORE1_IOPAD(0x21fe, PIN_INPUT_PULLUP   | MUX_MODE4)  /* rmii_mdio_data.rmii_mdio_data */
+			OMAP3_CORE1_IOPAD(0x2200, PIN_INPUT_PULLDOWN | MUX_MODE4)   /* rmii_mdio_clk.rmii_mdio_clk   */
+		>;
+	};
+
+	/* For all pins: offset is 0x48002030 */
+	mmc1_pins: pinmux_mmc1_pins {
+		pinctrl-single,pins = <
+			OMAP3_CORE1_IOPAD(0x2144, PIN_INPUT_PULLUP | MUX_MODE0)	/* sdmmc1_clk.sdmmc1_clk */
+			OMAP3_CORE1_IOPAD(0x2146, PIN_INPUT_PULLUP | MUX_MODE0)	/* sdmmc1_cmd.sdmmc1_cmd */
+			OMAP3_CORE1_IOPAD(0x2148, PIN_INPUT_PULLUP | MUX_MODE0)	/* sdmmc1_dat0.sdmmc1_dat0 */
+			OMAP3_CORE1_IOPAD(0x214a, PIN_INPUT_PULLUP | MUX_MODE0)	/* sdmmc1_dat1.sdmmc1_dat1 */
+			OMAP3_CORE1_IOPAD(0x214c, PIN_INPUT_PULLUP | MUX_MODE0)	/* sdmmc1_dat2.sdmmc1_dat2 */
+			OMAP3_CORE1_IOPAD(0x214e, PIN_INPUT_PULLUP | MUX_MODE0)	/* sdmmc1_dat3.sdmmc1_dat3 */
+
+			OMAP3_CORE1_IOPAD(0x213C, PIN_INPUT_PULLUP | MUX_MODE4) /* mcbsp2_fsx.gpio116 CD */
+			OMAP3_CORE1_IOPAD(0x2140, PIN_INPUT_PULLUP | MUX_MODE4) /* mcbsp2_dr.gpio118 WP */
+		>;
+	};
+
+	gpmc_pins: pinmux_gpmc_pins {
+		pinctrl-single,pins = <
+			OMAP3_CORE1_IOPAD(0x207a, PIN_OUTPUT_PULLUP | MUX_MODE0)	/* gpmc_a1.gpmc_a1 */
+			OMAP3_CORE1_IOPAD(0x207c, PIN_OUTPUT_PULLUP | MUX_MODE0)	/* gpmc_a2.gpmc_a2 */
+			OMAP3_CORE1_IOPAD(0x207e, PIN_OUTPUT_PULLUP | MUX_MODE0)	/* gpmc_a3.gpmc_a3 */
+			OMAP3_CORE1_IOPAD(0x2080, PIN_OUTPUT_PULLUP | MUX_MODE0)	/* gpmc_a4.gpmc_a4 */
+			OMAP3_CORE1_IOPAD(0x2082, PIN_OUTPUT_PULLUP | MUX_MODE0)	/* gpmc_a5.gpmc_a5 */
+			OMAP3_CORE1_IOPAD(0x2084, PIN_OUTPUT_PULLUP | MUX_MODE0)	/* gpmc_a6.gpmc_a6 */
+			OMAP3_CORE1_IOPAD(0x2086, PIN_OUTPUT_PULLUP | MUX_MODE0)	/* gpmc_a7.gpmc_a7 */
+			OMAP3_CORE1_IOPAD(0x2088, PIN_OUTPUT_PULLUP | MUX_MODE0)	/* gpmc_a8.gpmc_a8 */
+			OMAP3_CORE1_IOPAD(0x208a, PIN_OUTPUT_PULLUP | MUX_MODE0)	/* gpmc_a9.gpmc_a9 */
+			OMAP3_CORE1_IOPAD(0x208c, PIN_OUTPUT_PULLUP | MUX_MODE0)	/* gpmc_a10.gpmc_a10 */
+
+			OMAP3_CORE1_IOPAD(0x209c, PIN_INPUT_PULLUP | MUX_MODE0)	/* gpmc_d7.gpmc_d7 */
+			OMAP3_CORE1_IOPAD(0x209e, PIN_INPUT_PULLUP | MUX_MODE0)	/* gpmc_d8.gpmc_d8 */
+			OMAP3_CORE1_IOPAD(0x20a0, PIN_INPUT_PULLUP | MUX_MODE0)	/* gpmc_d10.gpmc_d9  */
+			OMAP3_CORE1_IOPAD(0x20a2, PIN_INPUT_PULLUP | MUX_MODE0)	/* gpmc_d11.gpmc_d10 */
+			OMAP3_CORE1_IOPAD(0x20a4, PIN_INPUT_PULLUP | MUX_MODE0)	/* gpmc_d12.gpmc_d11 */
+			OMAP3_CORE1_IOPAD(0x20a6, PIN_INPUT_PULLUP | MUX_MODE0)	/* gpmc_d13.gpmc_d12 */
+			OMAP3_CORE1_IOPAD(0x20a8, PIN_INPUT_PULLUP | MUX_MODE0)	/* gpmc_d14.gpmc_d13 */
+			OMAP3_CORE1_IOPAD(0x20aa, PIN_INPUT_PULLUP | MUX_MODE0)	/* gpmc_d15.gpmc_d14 */
+
+			OMAP3_CORE1_IOPAD(0x20ae, PIN_OUTPUT_PULLUP | MUX_MODE0)	/* gpmc_ncs0.gpmc_ncs0 */
+			OMAP3_CORE1_IOPAD(0x20b0, PIN_OUTPUT_PULLUP | MUX_MODE0)	/* gpmc_ncs1.gpmc_ncs1 */
+			OMAP3_CORE1_IOPAD(0x20b2, PIN_OUTPUT_PULLUP | MUX_MODE0)	/* gpmc_ncs2.gpmc_ncs2 */
+			OMAP3_CORE1_IOPAD(0x20bc, PIN_OUTPUT_PULLUP | MUX_MODE0)	/* gpmc_clk.gpmc_clk */
+
+			OMAP3_CORE1_IOPAD(0x20c0, PIN_OUTPUT | MUX_MODE0)	/* gpmc_nadv_ale.gpmc_nadv_ale */
+			OMAP3_CORE1_IOPAD(0x20c2, PIN_OUTPUT | MUX_MODE0)	/* gpmc_noe.gpmc_noe */
+			OMAP3_CORE1_IOPAD(0x20c4, PIN_OUTPUT | MUX_MODE0)	/* gpmc_nwe */
+
+			OMAP3_CORE1_IOPAD(0x20c6, PIN_OUTPUT_PULLUP | MUX_MODE0)/* gpmc_nbe0_cle.gpmc_nbe0_cle */
+
+			OMAP3_CORE1_IOPAD(0x20c8, PIN_INPUT_PULLUP | MUX_MODE0)	/* gpmc_nbe1.gpmc_nbe1 */
+			OMAP3_CORE1_IOPAD(0x20ca, PIN_INPUT | MUX_MODE0)	/* gpmc_nwp.gpmc_nwp */
+
+			OMAP3_CORE1_IOPAD(0x20cc, PIN_INPUT_PULLUP | MUX_MODE0)	/* gpmc_wait0.gpmc_wait0 */
+			OMAP3_CORE1_IOPAD(0x20ce, PIN_INPUT_PULLUP | MUX_MODE0)	/* gpmc_wait1.gpmc_wait1 */
+			OMAP3_CORE1_IOPAD(0x20d0, PIN_INPUT_PULLUP | MUX_MODE4)	/* gpmc_wait2.gpio_64 */
+		>;
+	};
+
+	watchdog_pins: pinmux_watchdog_pins {
+		pinctrl-single,pins = <
+			OMAP3_CORE1_IOPAD(0x219a, PIN_OUTPUT | MUX_MODE4) /* uart3_cts_rctx.gpio6_3, nWDG_EN_GPIO163 */
+			OMAP3_CORE1_IOPAD(0x219c, PIN_OUTPUT | MUX_MODE4) /* uart3_rts_sd.gpio6_4, WDG_TRG_GPIO164 */
+		>;
+	};
+};
+
+&bitbang_mdio0 {
+	pinctrl-names = "default", "sleep";
+	pinctrl-0 = <&gpio_bitbang_mdio_pins>;
+	pinctrl-1 = <&gpio_bitbang_mdio_sleep_pins>;
+	gpios = <&gpio4 12 0	/* 0: mdc  */
+		&gpio4 11 0>;   /* 1: mdio */
+};
+
+&davinci_emac {
+	status = "okay";
+	fixed-link {
+		speed = <100>;
+		full-duplex;
+	};
+};
+
+&uart1 {
+	status = "disabled";
+};
+
+&uart2 { /* KBUS Firmware download */
+	status = "okay";
+	pinctrl-names = "default";
+	pinctrl-0 = <&uart2_pins>;
+};
+
+&i2c1 {
+	clock-frequency = <400000>;
+};
+
+&i2c2 {
+	clock-frequency = <400000>;
+
+	rtc_r2221t@32 {
+		compatible = "ricoh,r2221tl";
+		reg = <0x32>;
+
+		interrupt-parent = <&gpio1>;
+		interrupts = <1 IRQ_TYPE_LEVEL_LOW>;
+		trim-data = <0 0 13>;
+	};
+
+
+	eeprom: 24c512@54 {
+		compatible = "atmel,24c512";
+		reg = <0x54>;
+
+		pagesize = <128>;
+		wp-gpios = <&gpio6 10 GPIO_ACTIVE_HIGH>;
+	};
+};
+
+&i2c3 {
+	status = "disabled";
+	clock-frequency = <400000>;
+};
+
+&mmc1 {
+	status = "okay";
+	pinctrl-names = "default";
+	pinctrl-0 = <&mmc1_pins>;
+	vmmc-supply = <&vmmc_fixed>;
+	bus-width = <4>;
+	wp-gpios = <&gpio4 22 GPIO_ACTIVE_HIGH>;
+	cd-gpios = <&gpio4 20 GPIO_ACTIVE_HIGH>;
+	cd-inverted;
+	cd-debounce-delay-ms = <7>;
+};
+
+&mmc2 {
+	status = "disabled";
+};
+
+&mmc3 {
+	status = "disabled";
+};
+
+&mcspi1 {
+	status = "okay";
+	pinctrl-names = "default";
+	pinctrl-0 = <&mcspi1_pins>;
+	spi-rt;
+	spi-rt-prio = <81>;
+
+	kbus0: kbus@0 {	/* KBUS on cs0 */
+		compatible = "wago,spi-kbus";
+		status = "okay";
+		pinctrl-names = "default";
+		pinctrl-0 = <&kbus_pins>;
+		reg = <0>;
+		spi-max-frequency = <10000000>; /* FIXME */
+		kbus,dma-boost; /* this enables dma boosting */
+		kbus,dma-boost-prio   = <85>;
+		kbus,dma-boost-irq-thread = "omap-dma";
+		kbus,dma-default-prio = <50>;
+
+		kbus,reset-on-boot;
+
+		/*
+		 * announce used tty-device in /sys/class/wago/system.
+		 * is needed to link this device to /dev/ttyKbus
+		 */
+		kbus,tty-device = "ttyO1"; /* corresponds to &uart2*/
+
+
+		/* some should be active low, keep all to high for compatibility reasons */
+		kbus,nrst-gpios   = <&gpio4  9 GPIO_ACTIVE_LOW>;
+		kbus,nsync-gpios  = <&gpio4 10 GPIO_ACTIVE_HIGH>;
+		kbus,cmdsel-gpios = <&gpio4  7 GPIO_ACTIVE_HIGH>;
+		kbus,nirq-gpios   = <&gpio4  1 GPIO_ACTIVE_HIGH>;
+		kbus,nerr-gpios   = <&gpio4  6 GPIO_ACTIVE_HIGH>;
+
+		kbus,errst-gpios  = <&gpio4  2 GPIO_ACTIVE_HIGH    /* 0..2 */
+				    &gpio4  3 GPIO_ACTIVE_HIGH
+				    &gpio4  4 GPIO_ACTIVE_HIGH>; /* On pfc200 we have one more:
+								  * &gpio4  4 GPIO_ACTIVE_HIGH
+								  * but keep it in sync with pfc100
+								  */
+
+		kbus,nrdy-gpios   = <&gpio4  8 GPIO_ACTIVE_HIGH>;
+	};
+};
+
+&gpmc {
+	status = "okay";
+	pinctrl-names = "default";
+	pinctrl-0 = <&gpmc_pins>;
+	num-cs = <3>;
+	num-waitpins = <4>;
+	ranges = <
+		0 0 0x08000000 0x01000000	/* CS0: NAND, 16M */
+		1 0 0x01000000 0x01000000	/* CS1: FB/DPC31, 16M */
+		2 0 0x02000000 0x01000000	/* CS2: NVRAM, 16M */
+	>;
+
+	nand: nand@0,0 {
+		compatible = "ti,omap2-nand";
+		reg = <0 0 4>; /* CS0, offset 0, IO size 4 */
+		nand-bus-width = <8>;
+		ti,nand-ecc-opt = "bch8";
+		gpmc,device-nand = "true";
+		gpmc,device-width = <1>;
+
+		gpmc,sync-clk-ps = <0>;
+		gpmc,cs-on-ns = <0>;
+		gpmc,cs-rd-off-ns = <36>;
+		gpmc,cs-wr-off-ns = <36>;
+		gpmc,adv-on-ns = <6>;
+		gpmc,adv-rd-off-ns = <24>;
+		gpmc,adv-wr-off-ns = <36>;
+		gpmc,we-on-ns = <0>;
+		gpmc,we-off-ns = <30>;
+		gpmc,oe-on-ns = <0>;
+		gpmc,oe-off-ns = <48>;
+		gpmc,access-ns = <54>;
+		gpmc,rd-cycle-ns = <72>;
+		gpmc,wr-cycle-ns = <72>;
+
+		gpmc,wait-pin = <0>;
+		gpmc,wait-on-read;
+		gpmc,wait-on-write;
+		gpmc,bus-turnaround-ns = <0>;
+		gpmc,cycle2cycle-delay-ns = <0>;
+		gpmc,clk-activation-ns = <0>;
+		gpmc,wait-monitoring-ns = <0>;
+
+		gpmc,wr-access-ns = <30>;
+		gpmc,wr-data-mux-bus-ns = <0>;
+
+		#address-cells = <1>;
+		#size-cells = <1>;
+	};
+
+	nvram: nor@2,0 { /* NVRAM Device 128k */
+		reg = <2 0 0x01000000>;
+		gpmc,device-width = <2>;
+
+		bank-width = <2>; /* 1: 8bit, 2: 16bit */
+		gpmc,sync-clk-ps = <0>;
+
+		gpmc,cs-on-ns = <12>;
+		gpmc,cs-rd-off-ns = <54>;
+		gpmc,cs-wr-off-ns = <30>;
+
+		gpmc,adv-on-ns = <6>;
+		gpmc,adv-rd-off-ns = <54>;
+		gpmc,adv-wr-off-ns = <30>;
+
+		gpmc,oe-on-ns = <12>;
+		gpmc,oe-off-ns = <54>;
+
+		gpmc,we-on-ns = <12>;
+		gpmc,we-off-ns = <30>;
+
+		gpmc,rd-cycle-ns = <54>;
+		gpmc,wr-cycle-ns = <30>;
+
+		gpmc,access-ns = <48>;
+
+		gpmc,wr-access-ns = <18>;
+		gpmc,wr-data-mux-bus-ns = <12>;
+
+		gpmc,time-para-granularity;
+		gpmc,mux-add-data = <2>; /* address-data multiplexing mode */
+
+		gpmc,clk-activation-ns = <0>;
+		gpmc,wait-monitoring-ns = <0>;
+
+		gpmc,cycle2cycle-delay-ns = <0>;
+
+		#address-cells = <1>;
+		#size-cells = <1>;
+	};
+};
+
+&usbhshost {
+	status = "disabled";
+};
+
+&sham {
+	status = "disabled";
+};
+
+&gpio3 {
+	ti,no-reset-on-init;
+};
+
+&counter32k {
+	status = "okay";
+};
+
+&wdt2 {
+	ti,no-reset-on-init;
+	ti,no-idle;
+};
+
+/include/ "pxc-nandparts.dtsi"
diff --git a/arch/arm/boot/dts/am3517.dtsi b/arch/arm/boot/dts/am3517.dtsi
index de33c4f89f33..42937307e536 100644
--- a/arch/arm/boot/dts/am3517.dtsi
+++ b/arch/arm/boot/dts/am3517.dtsi
@@ -51,14 +51,17 @@ opp100-600000000 {
 		};
 	};
 
-	ocp@68000000 {
+	am35x_ocp: ocp@68000000 {
 		am35x_otg_hs: am35x_otg_hs@5c040000 {
-			compatible = "ti,omap3-musb";
+			compatible = "ti,musb-am35x";
 			ti,hwmods = "am35x_otg_hs";
 			status = "disabled";
 			reg = <0x5c040000 0x1000>;
 			interrupts = <71>;
 			interrupt-names = "mc";
+			multipoint = <1>;
+			num-eps = <16>;
+			ram-bits = <12>;
 		};
 
 		davinci_emac: ethernet@5c000000 {
diff --git a/arch/arm/boot/dts/imx6q-ec-752_8303.dts b/arch/arm/boot/dts/imx6q-ec-752_8303.dts
new file mode 100644
index 000000000000..10dbfbd1f96f
--- /dev/null
+++ b/arch/arm/boot/dts/imx6q-ec-752_8303.dts
@@ -0,0 +1,31 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) 2020 elrest GmbH
+ */
+
+/dts-v1/;
+
+#include "imx6q-ec.dtsi"
+
+/ {
+	compatible = "wago,imx6q-vtpctp-762_4xxx-1010", \
+				"wago,imx6q-ec-752_8303", "fsl,imx6q";
+	wsysinit_init {
+		board,variant = "EC752";
+	};
+};
+
+&hdmi {
+	pinctrl-names = "default";
+	pinctrl-0 = <&pinctrl_hdmi>;
+	status = "okay";
+};
+
+&usbotg {
+	dr_mode = "otg";
+	status = "okay";
+};
+
+&cc_logic_usb_c {
+	status = "okay";
+};
diff --git a/arch/arm/boot/dts/imx6q-ec.dtsi b/arch/arm/boot/dts/imx6q-ec.dtsi
new file mode 100644
index 000000000000..74630580acbe
--- /dev/null
+++ b/arch/arm/boot/dts/imx6q-ec.dtsi
@@ -0,0 +1,983 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) 2020 elrest GmbH
+ */
+
+#include <dt-bindings/gpio/gpio.h>
+#include <dt-bindings/input/input.h>
+
+#include "imx6q.dtsi"
+#include "imx6q-vtpctp-ksz8863.dtsi"
+
+/ {
+	model = "EC Quad Board";
+
+	memory {
+		reg = <0x10000000 0x80000000>;
+	};
+
+	aliases {
+		mxcfb0 = &mxcfb1;
+		mdio-gpio0 = &bitbang_mdio0;
+		mmc0 = &usdhc1;
+		mmc1 = &usdhc3;
+		ethernet1 = &fec;
+	};
+
+	wsysinit_init {
+		compatible = "wago,sysinit";
+		add-sysfs-entries;
+
+		tty,service   = "ttymxc0";
+		tty,rs232-485 = "ttymxc1";
+
+		board,variant = "EC";
+
+		/* sysclock adjustments, empirical values */
+		adjtimex,tick = <10000>;
+		adjtimex,frequency = <2000000>;
+	};
+
+	regulators {
+		compatible = "simple-bus";
+		#address-cells = <1>;
+		#size-cells = <0>;
+
+		reg_1p5v: 1p5v {
+			compatible = "regulator-fixed";
+			regulator-name = "1P5V";
+			regulator-min-microvolt = <1500000>;
+			regulator-max-microvolt = <1500000>;
+			regulator-always-on;
+		};
+
+		reg_1p8v: 1p8v {
+			compatible = "regulator-fixed";
+			regulator-name = "1P8V";
+			regulator-min-microvolt = <1800000>;
+			regulator-max-microvolt = <1800000>;
+			regulator-always-on;
+		};
+
+		reg_2p5v: 2p5v {
+			compatible = "regulator-fixed";
+			regulator-name = "2P5V";
+			regulator-min-microvolt = <2500000>;
+			regulator-max-microvolt = <2500000>;
+			regulator-always-on;
+		};
+
+		reg_2p8v: 2p8v {
+			compatible = "regulator-fixed";
+			regulator-name = "2P8V";
+			regulator-min-microvolt = <2800000>;
+			regulator-max-microvolt = <2800000>;
+			regulator-always-on;
+		};
+
+		reg_3p3v: 3p3v {
+			compatible = "regulator-fixed";
+			regulator-name = "3P3V";
+			regulator-min-microvolt = <3300000>;
+			regulator-max-microvolt = <3300000>;
+			regulator-always-on;
+		};
+
+		reg_usb_otg_vbus: regulator@0 {
+			compatible = "regulator-fixed";
+			reg = <0>;
+			regulator-name = "usb_otg_vbus";
+			regulator-min-microvolt = <5000000>;
+			regulator-max-microvolt = <5000000>;
+			gpio = <&gpio4 15 GPIO_ACTIVE_LOW>;
+			enable-active-high;
+		};
+
+		reg_usb_h1_vbus: regulator@1 {
+			compatible = "regulator-fixed";
+			reg = <1>;
+			regulator-name = "usb_h1_vbus";
+			regulator-min-microvolt = <5000000>;
+			regulator-max-microvolt = <5000000>;
+			regulator-always-on;
+		};
+
+	};
+
+	sound {
+		compatible = "fsl,imx6dl-vtpctp-sgtl5000",
+					 "fsl,imx-audio-sgtl5000";
+		model = "imx6dl-vtpctp-sgtl5000";
+		ssi-controller = <&ssi1>;
+		audio-codec = <&codec>;
+		audio-routing =
+		"Headphone Jack", "HP_OUT";
+		mux-int-port = <1>;
+		mux-ext-port = <4>;
+	};
+
+	mxcfb1: fb@0 {
+		compatible = "fsl,mxc_sdc_fb";
+		disp_dev = "ldb";
+		interface_pix_fmt = "RGB666";
+		default_bpp = <24>;
+		int_clk = <0>;
+		late_init = <0>;
+		status = "okay";
+	};
+
+	backlight_leds {
+		pinctrl-names = "default";
+		pinctrl-0 = <&pinctrl_reg_lcd_3v3>;
+		compatible = "gpio-leds";
+
+		rs-sel {
+			gpios = <&gpio6 9 GPIO_ACTIVE_HIGH>;
+			default-state = "off";
+		};
+
+		sys-red {
+			gpios = <&gpio2 4 GPIO_ACTIVE_HIGH>;
+			default-state = "off";
+		};
+
+		sys-green {
+			gpios = <&gpio2 5 GPIO_ACTIVE_HIGH>;
+			default-state = "off";
+		};
+
+		sys-blue {
+			gpios = <&gpio2 6 GPIO_ACTIVE_HIGH>;
+			default-state = "off";
+		};
+
+		led-rd-ovrd {
+			gpios = <&gpio5 18 GPIO_ACTIVE_HIGH>;
+			default-state = "off";
+		};
+
+		led-srvr-wd-imx6 {
+			gpios = <&gpio5 19 GPIO_ACTIVE_HIGH>;
+			default-state = "off";
+		};
+
+		en-led-rd-srvr-ndw {
+			gpios = <&gpio2 16 GPIO_ACTIVE_LOW>;
+			default-state = "off";
+		};
+		beeper {
+			gpios = <&gpio2 7 GPIO_ACTIVE_HIGH>;
+			default-state = "oneshot";
+		};
+	};
+
+	v4l2_cap_1 {
+		compatible = "fsl,imx6q-v4l2-capture";
+		ipu_id = <0>;
+		csi_id = <1>;
+		mclk_source = <0>;
+		status = "okay";
+	};
+
+	v4l2_out {
+		compatible = "fsl,mxc_v4l2_output";
+		status = "okay";
+	};
+
+	susp-signals {
+		compatible = "fsl,susp-signals";
+		gpios = <&gpio1 5 GPIO_ACTIVE_LOW>;
+	};
+
+	PAC-Operating-Mode-Switch {
+		pinctrl-names = "default";
+		pinctrl-0 = <&pinctrl_bas>;
+		compatible = "gpio-keys";
+		status = "okay";
+		#address-cells = <1>;
+		#size-cells = <0>;
+		autorepeat;
+
+		run {
+			label = "RUN";
+			gpios = <&gpio2 12 GPIO_ACTIVE_LOW>; /* GPIO 44 */
+			linux,code = <1>;
+			linux,input-type = <EV_SW>;
+			debounce-interval = <1>; /* debounce in msecs */
+		};
+
+		stop {
+			label = "STOP";
+			gpios = <&gpio2 13 GPIO_ACTIVE_LOW>; /* GPIO 45 */
+			linux,code = <2>;
+			linux,input-type = <EV_SW>;
+			debounce-interval = <1>; /* debounce in msecs */
+		};
+
+		reset {
+			label = "RESET";
+			gpios = <&gpio2 15 GPIO_ACTIVE_LOW>; /* GPIO 47 */
+			linux,code = <3>;
+			linux,input-type = <EV_KEY>;
+			debounce-interval = <1>; /* debounce in msecs */
+		};
+
+		reset_all {
+			label = "RESET_ALL";
+			gpios = <&gpio4 7 GPIO_ACTIVE_LOW>; /* GPIO 103 */
+			linux,code = <4>;
+			linux,input-type = <EV_KEY>;
+			debounce-interval = <1>; /* debounce in msecs */
+		};
+
+	};
+
+	bitbang_mdio0: gpio_mdio {
+		compatible = "virtual,mdio-gpio";
+		#address-cells = <1>;
+		#size-cells = <0>;
+		##status = "okay";
+	};
+
+	rmii_clk_ext: rmii_clk {
+		compatible = "fixed-clock";
+		#clock-cells = <0>;
+		clock-frequency = <50000000>;
+	};
+};
+
+&audmux {
+	pinctrl-names = "default";
+	pinctrl-0 = <&pinctrl_audmux>;
+	status = "okay";
+};
+
+&clks {
+	fsl,ldb-di0-parent = <&clks IMX6QDL_CLK_PLL2_PFD0_352M>;
+	fsl,ldb-di1-parent = <&clks IMX6QDL_CLK_PLL2_PFD0_352M>;
+
+	assigned-clocks = <&clks IMX6QDL_CLK_CKO2>;
+	assigned-clock-rates = <12000000>;
+};
+
+&fec {
+	pinctrl-names = "default";
+	pinctrl-0 = <&pinctrl_enet>;
+	phy-mode = "rmii";
+
+	clocks = <&clks IMX6QDL_CLK_ENET>,
+		 <&clks IMX6QDL_CLK_ENET>,
+		 <&rmii_clk_ext>;
+	clock-names = "ipg", "ahb", "ptp";
+
+	status = "okay";
+	fixed-link {
+		speed = <100>;
+		full-duplex;
+	};
+
+	mdio: mdio@0 {
+		#address-cells = <1>;
+		#size-cells = <0>;
+	};
+};
+
+&i2c2 {
+	clock-frequency = <400000>;
+	pinctrl-names = "default";
+	pinctrl-0 = <&pinctrl_i2c2>;
+	status = "okay";
+
+	pmic: pf0100@8 {
+		compatible = "fsl,pfuze100";
+		reg = <0x08>;
+		interrupt-parent = <&gpio7>;
+		interrupts = <12 8>;
+
+		regulators {
+
+			/* VDD_ARM */
+			sw1a_reg: sw1ab {
+				regulator-min-microvolt = <1400000>;
+				regulator-max-microvolt = <1875000>;
+				regulator-boot-on;
+				regulator-always-on;
+				regulator-ramp-delay = <6250>;
+			};
+
+			/* VDD_SOC */
+			sw1c_reg: sw1c {
+				regulator-min-microvolt = <1400000>;
+				regulator-max-microvolt = <1875000>;
+				regulator-boot-on;
+				regulator-always-on;
+				regulator-ramp-delay = <6250>;
+			};
+
+			 /* VDD_HIGH */
+			sw2_reg: sw2 {
+				regulator-min-microvolt = <3300000>;
+				regulator-max-microvolt = <3300000>;
+				regulator-boot-on;
+				regulator-always-on;
+			};
+
+			 /* VDD_DDR */
+			sw3a_reg: sw3a {
+				regulator-min-microvolt = <1350000>;
+				regulator-max-microvolt = <1350000>;
+				regulator-boot-on;
+				regulator-always-on;
+			};
+
+			 /* VDD_DDR */
+			sw3b_reg: sw3b {
+				regulator-min-microvolt = <1350000>;
+				regulator-max-microvolt = <1350000>;
+				regulator-boot-on;
+				regulator-always-on;
+			};
+
+			/* VDD_1P8 */
+			sw4_reg: sw4 {
+				regulator-min-microvolt = <800000>;
+				regulator-max-microvolt = <3300000>;
+				regulator-always-off;
+			};
+
+			swbst_reg: swbst {
+				regulator-min-microvolt = <5000000>;
+				regulator-max-microvolt = <5150000>;
+			};
+
+			snvs_reg: vsnvs {
+				regulator-min-microvolt = <1000000>;
+				regulator-max-microvolt = <3000000>;
+				regulator-boot-on;
+				regulator-always-on;
+			};
+
+			vref_reg: vrefddr {
+				regulator-boot-on;
+				regulator-always-on;
+			};
+
+			vgen3_reg: vgen3 {
+				regulator-min-microvolt = <2500000>;
+				regulator-max-microvolt = <3300000>;
+				regulator-always-on;
+			};
+
+			vgen4_reg: vgen4 {
+				regulator-min-microvolt = <1800000>;
+				regulator-max-microvolt = <1800000>;
+				regulator-always-on;
+			};
+
+
+			vgen5_reg: vgen5 {
+				regulator-min-microvolt = <1800000>;
+				regulator-max-microvolt = <3300000>;
+				regulator-always-on;
+			};
+
+			/* supply voltage for eMMC */
+			vgen6_reg: vgen6 {
+				regulator-min-microvolt = <1800000>;
+				regulator-max-microvolt = <1800000>;
+				regulator-boot-on;
+				regulator-always-on;
+			};
+		};
+	};
+
+	lm75: lm75@49 {
+		compatible = "national,lm75";
+		reg = <0x49>;
+	};
+
+	eeprom: m24c512@54 {
+		compatible = "st,24c512", "at24";
+		reg = <0x54>;
+	};
+
+	rtc_r2221t@32 {
+		compatible = "ricoh,r2221tl";
+		reg = <0x32>;
+
+		interrupt-parent = <&gpio1>;
+		interrupts = <4 IRQ_TYPE_LEVEL_LOW>;
+	};
+
+	pca9552@61 {
+		compatible = "nxp,pca9552";
+		status = "okay";
+		reg = <0x61>;
+		#address-cells = <1>;
+		#size-cells = <0>;
+
+		sys-red-back@3 {
+			label = "sys-red-back";
+			reg = <3>;
+			linux,default-trigger = "none";
+		};
+
+		sys-green-back@4 {
+			label = "sys-green-back";
+			reg = <4>;
+			linux,default-trigger = "none";
+		};
+
+		sys-blue-back@5 {
+			label = "sys-blue-back";
+			reg = <5>;
+			linux,default-trigger = "none";
+		};
+
+		run-green@8 {
+			label = "run-green";
+			reg = <8>;
+			linux,default-trigger = "none";
+		};
+
+		run-red@9 {
+			label = "run-red";
+			reg = <9>;
+			linux,default-trigger = "none";
+		};
+
+		can-green@10 {
+			label = "can-green";
+			reg = <10>;
+			linux,default-trigger = "none";
+		};
+
+		can-red@11 {
+			label = "can-red";
+			reg = <11>;
+			linux,default-trigger = "none";
+		};
+
+		h11-green@12 {
+			label = "h11-green";
+			reg = <12>;
+			linux,default-trigger = "none";
+		};
+
+		h11-red@13 {
+			label = "h11-red";
+			reg = <13>;
+			linux,default-trigger = "none";
+		};
+
+		h12-green@14 {
+			label = "h12-green";
+			reg = <14>;
+			linux,default-trigger = "none";
+		};
+
+		h12-red@15 {
+			label = "h12-red";
+			reg = <15>;
+			linux,default-trigger = "none";
+		};
+	};
+
+	cc_logic_usb_c: cclogic@1e {
+		compatible = "nxp,ptn5150";
+		reg = <0x1e>;
+		int-gpio = <&gpio1 3 GPIO_ACTIVE_HIGH>;
+		vbus-gpio = <&gpio1 24 GPIO_ACTIVE_HIGH>;
+		pinctrl-names = "default";
+		pinctrl-0 = <&ptn5150_pins>;
+	};
+
+};
+
+&i2c3 {
+	clock-frequency = <400000>;
+	pinctrl-names = "default";
+	pinctrl-0 = <&pinctrl_i2c3>;
+	status = "okay";
+
+	codec: sgtl5000@a {
+		compatible = "fsl,sgtl5000";
+		reg = <0x0a>;
+		clocks = <&clks IMX6QDL_CLK_CKO2>;
+		/* sysclk = <12000000>; */
+		VDDD-supply = <&reg_1p8v>;
+		VDDA-supply = <&reg_3p3v>;
+		VDDIO-supply = <&reg_3p3v>;
+	};
+};
+
+&iomuxc {
+	pinctrl-names = "default";
+	pinctrl-0 = <&pinctrl_hog>;
+
+	imx6qdl-qmx6 {
+		pinctrl_hog: hoggrp {
+			fsl,pins = <
+				/* RTC-NINT */
+				MX6QDL_PAD_GPIO_4__GPIO1_IO04		0x1b0b0
+				/* RGMII Phy Interrupt */
+				MX6QDL_PAD_EIM_D23__GPIO3_IO23	0x80000000
+				/* EEPROM WP GPIO23 */
+				MX6QDL_PAD_ENET_REF_CLK__GPIO1_IO23	0x1b0b0
+				/* PCAP-BRIDGE-NRESET */
+				MX6QDL_PAD_NANDF_RB0__GPIO6_IO10	0x1b0b0
+				/* BEEPER_ON */
+				MX6QDL_PAD_NANDF_D7__GPIO2_IO07		0x1b0b0
+				/* AIO_NREST */
+				MX6QDL_PAD_EIM_A24__GPIO5_IO04		0x0b0b1
+				/* PMIC-NINTB */
+				MX6QDL_PAD_GPIO_17__GPIO7_IO12		0x0b0b1
+			>;
+		};
+
+		pinctrl_audmux: audmuxgrp {
+			fsl,pins = <
+				MX6QDL_PAD_SD2_DAT0__AUD4_RXD		0x130b0
+				MX6QDL_PAD_SD2_DAT3__AUD4_TXC		0x00008
+				MX6QDL_PAD_SD2_DAT2__AUD4_TXD		0x00008
+				MX6QDL_PAD_SD2_DAT1__AUD4_TXFS		0x00008
+				MX6QDL_PAD_NANDF_CS2__CCM_CLKO2		0x00008
+			>;
+		};
+
+		pinctrl_ecspi4: ecspi4grp {
+			fsl,pins = <
+				MX6QDL_PAD_EIM_D22__ECSPI4_MISO	0x100b1
+				MX6QDL_PAD_EIM_D28__ECSPI4_MOSI	0x100b1
+				MX6QDL_PAD_EIM_D21__ECSPI4_SCLK	0x100b1
+				/* CS0 */
+				MX6QDL_PAD_EIM_D20__GPIO3_IO20	0x1b0b0
+				/* CS1 */
+				MX6QDL_PAD_EIM_D24__GPIO3_IO24	0x1b0b0
+			>;
+		};
+
+		pinctrl_enet: enetgrp {
+			fsl,pins = <
+			MX6QDL_PAD_ENET_RXD0__ENET_RX_DATA0	0x1b0b0
+			MX6QDL_PAD_ENET_RXD1__ENET_RX_DATA1	0x1b0b0
+			MX6QDL_PAD_ENET_CRS_DV__ENET_RX_EN	0x1b0b0
+			MX6QDL_PAD_ENET_TXD0__ENET_TX_DATA0 0x1b088
+			MX6QDL_PAD_ENET_TXD1__ENET_TX_DATA1	0x1b088
+			MX6QDL_PAD_ENET_TX_EN__ENET_TX_EN	0x1b088
+			MX6QDL_PAD_GPIO_16__ENET_REF_CLK	0x4001b0a8
+			>;
+		};
+
+		pinctrl_flexcan1: flexcan1grp {
+			fsl,pins = <
+			MX6QDL_PAD_KEY_ROW2__FLEXCAN1_RX	0x80000000
+			MX6QDL_PAD_KEY_COL2__FLEXCAN1_TX	0x80000000
+			>;
+		};
+
+		pinctrl_i2c2: i2c2grp {
+			fsl,pins = <
+				MX6QDL_PAD_KEY_COL3__I2C2_SCL	0x4001b8b1
+				MX6QDL_PAD_KEY_ROW3__I2C2_SDA	0x4001b8b1
+			>;
+		};
+
+		pinctrl_i2c3: i2c3grp {
+			fsl,pins = <
+				MX6QDL_PAD_EIM_D17__I2C3_SCL	0x4001b8b1
+				MX6QDL_PAD_EIM_D18__I2C3_SDA	0x4001b8b1
+			>;
+		};
+
+		pinctrl_uart1: uart1grp {
+			fsl,pins = <
+				MX6QDL_PAD_CSI0_DAT11__UART1_RX_DATA	0x1b0b0
+				MX6QDL_PAD_CSI0_DAT10__UART1_TX_DATA	0x1b0b0
+			>;
+		};
+
+		pinctrl_uart2: uart2grp {
+			fsl,pins = <
+				MX6QDL_PAD_EIM_D26__UART2_TX_DATA	0x1b0b0
+				MX6QDL_PAD_EIM_D27__UART2_RX_DATA	0x1b0b0
+				MX6QDL_PAD_EIM_D29__GPIO3_IO29		0x1b0b0
+				MX6QDL_PAD_SD4_DAT6__UART2_CTS_B	0x1b0b0
+			>;
+		};
+
+		pinctrl_usbotg: usbotggrp {
+			fsl,pins = <
+				/* OTG_ID */
+				MX6QDL_PAD_ENET_RX_ER__USB_OTG_ID	0x1b0b1
+				/* OTG_OC */
+				MX6QDL_PAD_KEY_COL4__USB_OTG_OC		0x1b0b1
+				/* OTG_PWR */
+				MX6QDL_PAD_KEY_ROW4__GPIO4_IO15		0x130b0
+			>;
+		};
+
+		pinctrl_usdhc1: usdhc1grp {
+			fsl,pins = <
+				MX6QDL_PAD_SD1_CMD__SD1_CMD		0x17059
+				MX6QDL_PAD_SD1_CLK__SD1_CLK		0x10059
+				MX6QDL_PAD_SD1_DAT0__SD1_DATA0	0x17059
+				MX6QDL_PAD_SD1_DAT1__SD1_DATA1	0x17059
+				MX6QDL_PAD_SD1_DAT2__SD1_DATA2	0x17059
+				MX6QDL_PAD_SD1_DAT3__SD1_DATA3	0x17059
+				/* CD */
+				MX6QDL_PAD_GPIO_1__SD1_CD_B		0x1b0b0
+				/* SD-CARD.EN */
+				MX6QDL_PAD_SD4_CMD__GPIO7_IO09	0x1b0b0
+			>;
+		};
+
+		pinctrl_usdhc3: usdhc3grp {
+			fsl,pins = <
+				MX6QDL_PAD_SD3_CMD__SD3_CMD		0x17051
+				MX6QDL_PAD_SD3_CLK__SD3_CLK		0x10051
+				MX6QDL_PAD_SD3_DAT0__SD3_DATA0		0x17051
+				MX6QDL_PAD_SD3_DAT1__SD3_DATA1		0x17051
+				MX6QDL_PAD_SD3_DAT2__SD3_DATA2		0x17051
+				MX6QDL_PAD_SD3_DAT3__SD3_DATA3		0x17051
+				MX6QDL_PAD_SD3_DAT4__SD3_DATA4		0x17051
+				MX6QDL_PAD_SD3_DAT5__SD3_DATA5		0x17051
+				MX6QDL_PAD_SD3_DAT6__SD3_DATA6		0x17051
+				MX6QDL_PAD_SD3_DAT7__SD3_DATA7		0x17051
+			>;
+		};
+
+		pinctrl_reg_lcd_3v3: lpinctrl_reg_lcd_3v3grp {
+			fsl,pins = <
+			/* RS-SEL */
+			MX6QDL_PAD_NANDF_WP_B__GPIO6_IO09	0x1b0b0
+			/* CAPTAST_NRESET */
+			MX6QDL_PAD_NANDF_D3__GPIO2_IO03		0x1b0b0
+			/* LED-RD-ON */
+			MX6QDL_PAD_NANDF_D4__GPIO2_IO04		0x1b0b0
+			/* LED-GN-ON */
+			MX6QDL_PAD_NANDF_D5__GPIO2_IO05		0x1b0b0
+			/* LED-BL-ON */
+			MX6QDL_PAD_NANDF_D6__GPIO2_IO06		0x1b0b0
+			/* EN-LED-SRVR-NWD */
+			MX6QDL_PAD_EIM_A22__GPIO2_IO16		0x1b0b0
+			/* LED-RD-OVRD */
+			MX6QDL_PAD_CSI0_PIXCLK__GPIO5_IO18	0x1b0b0
+			/* LED-SRVR-WD-IMX6 */
+			MX6QDL_PAD_CSI0_MCLK__GPIO5_IO19	0x1b0b0
+			>;
+		};
+
+		pinctrl_bas: basgrp {
+			fsl,pins = <
+				/* BAS-NRUN */
+				MX6QDL_PAD_SD4_DAT4__GPIO2_IO12	0x1b0b0
+				/* BAS-NSTOP */
+				MX6QDL_PAD_SD4_DAT5__GPIO2_IO13	0x1b0b0
+				/* BAS-NRESET */
+				MX6QDL_PAD_SD4_DAT7__GPIO2_IO15	0x1b0b0
+				/* RESET-ALL */
+				MX6QDL_PAD_KEY_ROW0__GPIO4_IO07	0x1b0b0
+			>;
+		};
+
+		pinctrl_ksz8863: pinctrl_ksz8863grp {
+			fsl,pins = <
+				/* phy-reset */
+				MX6QDL_PAD_KEY_ROW1__GPIO4_IO09	0x1B0B0
+				/* phy-irq */
+				MX6QDL_PAD_GPIO_6__GPIO1_IO06	0x000b1
+			>;
+		};
+
+		gpio_bitbang_mdio_pins: pinmux_gpio_bitbang_mdio_pins {
+			fsl,pins = <
+				/* MDIO */
+				MX6QDL_PAD_ENET_MDIO__GPIO1_IO22	0x1b0b0
+				MX6QDL_PAD_ENET_MDC__GPIO1_IO31		0x1b0b0
+			>;
+		};
+
+		gpio_bitbang_mdio_sleep_pins: pinmux_gpio_bitbang_mdio_sleep {
+			fsl,pins = <
+				/* MDIO reset value */
+				MX6QDL_PAD_ENET_MDIO__GPIO1_IO22	0x1b0b0
+				MX6QDL_PAD_ENET_MDC__GPIO1_IO31		0x1b0b0
+			>;
+		};
+
+		pinctrl_weim: weimnorgrp {
+			fsl,pins = <
+			MX6QDL_PAD_EIM_CS1__EIM_CS1_B		0x17051
+			MX6QDL_PAD_EIM_CS0__EIM_CS0_B		0x17051
+			MX6QDL_PAD_EIM_EB0__EIM_EB0_B		0x17051
+			MX6QDL_PAD_EIM_EB1__EIM_EB1_B		0x17051
+			MX6QDL_PAD_EIM_EB3__EIM_EB3_B		0x17051
+			MX6QDL_PAD_EIM_LBA__EIM_LBA_B		0x17051
+			MX6QDL_PAD_EIM_OE__EIM_OE_B			0x17051
+			MX6QDL_PAD_EIM_RW__EIM_RW			0x17051
+			MX6QDL_PAD_EIM_WAIT__GPIO5_IO00		0x17051
+			MX6QDL_PAD_EIM_BCLK__EIM_BCLK		0x17051
+			MX6QDL_PAD_CSI0_DATA_EN__EIM_DATA00	0x17049
+			MX6QDL_PAD_CSI0_VSYNC__EIM_DATA01	0x17049
+			MX6QDL_PAD_CSI0_DAT4__EIM_DATA02	0x17049
+			MX6QDL_PAD_CSI0_DAT5__EIM_DATA03	0x17049
+			MX6QDL_PAD_CSI0_DAT6__EIM_DATA04	0x17049
+			MX6QDL_PAD_CSI0_DAT7__EIM_DATA05	0x17049
+			MX6QDL_PAD_CSI0_DAT8__EIM_DATA06	0x17049
+			MX6QDL_PAD_CSI0_DAT9__EIM_DATA07	0x17049
+			MX6QDL_PAD_CSI0_DAT12__EIM_DATA08	0x17049
+			MX6QDL_PAD_CSI0_DAT13__EIM_DATA09	0x17049
+			MX6QDL_PAD_CSI0_DAT14__EIM_DATA10	0x17049
+			MX6QDL_PAD_CSI0_DAT15__EIM_DATA11	0x17049
+			MX6QDL_PAD_CSI0_DAT16__EIM_DATA12	0x17049
+			MX6QDL_PAD_CSI0_DAT17__EIM_DATA13	0x17049
+			MX6QDL_PAD_CSI0_DAT18__EIM_DATA14	0x17049
+			MX6QDL_PAD_CSI0_DAT19__EIM_DATA15	0x17049
+			MX6QDL_PAD_EIM_DA0__EIM_AD00		0x17051
+			MX6QDL_PAD_EIM_DA1__EIM_AD01		0x17051
+			MX6QDL_PAD_EIM_DA2__EIM_AD02		0x17051
+			MX6QDL_PAD_EIM_DA3__EIM_AD03		0x17051
+			MX6QDL_PAD_EIM_DA4__EIM_AD04		0x17051
+			MX6QDL_PAD_EIM_DA5__EIM_AD05		0x17051
+			MX6QDL_PAD_EIM_DA6__EIM_AD06		0x17051
+			MX6QDL_PAD_EIM_DA7__EIM_AD07		0x17051
+			MX6QDL_PAD_EIM_DA8__EIM_AD08		0x17051
+			MX6QDL_PAD_EIM_DA9__EIM_AD09		0x17051
+			MX6QDL_PAD_EIM_DA10__EIM_AD10		0x17051
+			MX6QDL_PAD_EIM_DA11__EIM_AD11		0x17051
+			MX6QDL_PAD_EIM_DA12__EIM_AD12		0x17051
+			MX6QDL_PAD_EIM_DA13__EIM_AD13		0x17051
+			MX6QDL_PAD_EIM_DA14__EIM_AD14		0x17051
+			MX6QDL_PAD_EIM_DA15__EIM_AD15		0x17051
+			MX6QDL_PAD_EIM_A16__EIM_ADDR16		0x17051
+			MX6QDL_PAD_EIM_A17__EIM_ADDR17		0x17051
+			MX6QDL_PAD_EIM_A18__EIM_ADDR18		0x17051
+			MX6QDL_PAD_EIM_A19__EIM_ADDR19		0x17051
+			MX6QDL_PAD_EIM_A20__EIM_ADDR20		0x17051
+			>;
+		};
+
+		pinctrl_dio_spi: dio_spi_grp {
+			fsl,pins = <
+				MX6QDL_PAD_EIM_D19__GPIO3_IO19	0x1b0b0
+				MX6QDL_PAD_NANDF_D0__GPIO2_IO00	0x1b0b0
+			>;
+		};
+
+		pinctrl_hdmi: hdmigrp {
+			fsl,pins = <
+				MX6QDL_PAD_EIM_A25__HDMI_TX_CEC_LINE	0x1f8b0
+				MX6QDL_PAD_EIM_EB2__HDMI_TX_DDC_SCL	\
+				0x4001b8b1
+				MX6QDL_PAD_EIM_D16__HDMI_TX_DDC_SDA	\
+				0x4001b8b1
+			>;
+		};
+
+		ptn5150_pins: pinmux_ptn5150_pins {
+			fsl,pins = <
+				/* IRQ USB-C */
+				MX6QDL_PAD_GPIO_3__GPIO1_IO03	0x1b0b0
+			>;
+		};
+
+	};
+};
+
+&audmux {
+	pinctrl-names = "default";
+	pinctrl-0 = <&pinctrl_audmux>;
+	status = "okay";
+};
+
+&can1 {
+	pinctrl-names = "default";
+	pinctrl-0 = <&pinctrl_flexcan1>;
+	status = "okay";
+};
+
+&gpc {
+	/* use ldo-bypass, u-boot will check it and configure */
+	fsl,ldo-bypass = <1>;
+	fsl,wdog-reset = <1>;
+};
+
+&dcic1 {
+	dcic_id = <0>;
+	dcic_mux = "dcic-hdmi";
+	status = "okay";
+};
+
+&dcic2 {
+	dcic_id = <1>;
+	dcic_mux = "dcic-lvds1";
+	status = "okay";
+};
+
+&snvs_poweroff {
+	status = "okay";
+};
+
+&ssi1 {
+	fsl,mode = "i2s-slave";
+	status = "okay";
+};
+
+&uart1 {
+	pinctrl-names = "default";
+	pinctrl-0 = <&pinctrl_uart1>;
+	status = "okay";
+};
+
+&uart2 {
+	pinctrl-names = "default";
+	pinctrl-0 = <&pinctrl_uart2>;
+	status = "okay";
+	rs485-rts-active-high;
+	rs485-rx-during-tx;
+	uart-has-rtscts;
+	rts-gpios = <&gpio3 29 GPIO_ACTIVE_HIGH>;
+	linux,rs485-enabled-at-boot-time;
+};
+
+&usbh1 {
+	vbus-supply = <&reg_usb_h1_vbus>;
+	status = "okay";
+};
+
+&usbotg {
+	vbus-supply = <&reg_usb_otg_vbus>;
+	pinctrl-names = "default";
+	pinctrl-0 = <&pinctrl_usbotg>;
+	disable-over-current;
+	status = "okay";
+};
+
+&usdhc1 {
+	pinctrl-names = "default";
+	pinctrl-0 = <&pinctrl_usdhc1>;
+	/* cd-gpios = <&gpio7 9 GPIO_ACTIVE_LOW>; */
+	no-1-8-v;
+	keep-power-in-suspend;
+	enable-sdio-wakeup;
+	vmmc-supply = <&reg_3p3v>;
+	status = "okay";
+};
+
+&usdhc3 {
+	pinctrl-names = "default";
+	pinctrl-0 = <&pinctrl_usdhc3>;
+	non-removable;
+	bus-width = <8>;
+	no-1-8-v;
+	keep-power-in-suspend;
+	enable-sdio-wakeup;
+	vmmc-supply = <&reg_3p3v>;
+	status = "okay";
+};
+
+&bitbang_mdio0 {
+	pinctrl-names = "default", "sleep";
+	pinctrl-0 = <&gpio_bitbang_mdio_pins>;
+	pinctrl-1 = <&gpio_bitbang_mdio_sleep_pins>;
+	gpios = <&gpio1 31 0	/* 0: mdc  */
+	&gpio1 22 0>;			/* 1: mdio */
+	status = "okay";
+};
+
+&weim {
+	status = "okay";
+	pinctrl-names = "default";
+	pinctrl-0 = <&pinctrl_weim>;
+	#address-cells = <2>;
+	#size-cells = <1>;
+	ranges = <
+		0 0 0x08000000 0x04000000
+		1 0 0x0C000000 0x04000000 >;
+	fsl,weim-cs-gpr = <&gpr>;
+
+	UIO_NVRAM@1,0 {
+		compatible = "uio_pdrv_genirq";
+		reg = <1 0x0 0x20000>;
+		fsl,weim-cs-timing = <
+			0x007184B1 0x00001010
+			0x0F780000 0x00000008
+			0x04F00040 0x00000000 >;
+	};
+};
+
+&reg_arm {
+	vin-supply = <&sw1a_reg>;
+	regulator-allow-bypass;
+	regulator-min-microvolt = <1225000>;
+};
+
+&reg_soc {
+	vin-supply = <&sw1c_reg>;
+	regulator-allow-bypass;
+	regulator-min-microvolt = <1225000>;
+};
+
+&reg_pu {
+	regulator-allow-bypass;
+	regulator-min-microvolt = <1225000>;
+};
+
+&cpu0 {
+	operating-points = <
+		/* kHz  uV */
+		1200000 1400000
+		996000  1400000
+		852000  1400000
+		792000  1400000
+		396000  1400000
+	>;
+	fsl,soc-operating-points = <
+		/* ARM kHz SOC-PU uV */
+		1200000 1400000
+		996000  1400000
+		852000  1400000
+		792000  1400000
+		396000  1400000
+	>;
+};
+
+&ecspi4 {
+	pinctrl-names = "default";
+	pinctrl-0 = <&pinctrl_ecspi4>;
+	fsl,spi-num-chipselects = <2>;
+	cs-gpios = <
+		&gpio3 20 0
+		&gpio3 24 0
+	>;
+	status = "okay";
+
+	di0_spi: dio_spi@1 {
+		pinctrl-names = "default";
+		pinctrl-0 = <&pinctrl_dio_spi>;
+		compatible = "dio_spi";
+		reg = <1>;
+		spi-cpol;
+		spi-max-frequency = <1000000>;
+		gpio-load = <&gpio3 19 GPIO_ACTIVE_HIGH>;
+		gpio-reset = <&gpio2 0 GPIO_ACTIVE_HIGH>;
+	};
+};
+
+&cpu0 {
+	arm-supply = <&sw1a_reg>;
+	soc-supply = <&sw1c_reg>;
+};
+
+&swcfg_ksz8863 {
+	status = "okay";
+};
+
+&bitbang_mdio0 {
+	status = "okay";
+};
+
+&ksz8863_switch {
+	status = "okay";
+};
diff --git a/arch/arm/boot/dts/imx6q-vtpctp-0004.dts b/arch/arm/boot/dts/imx6q-vtpctp-0004.dts
new file mode 100644
index 000000000000..e3b7836bc321
--- /dev/null
+++ b/arch/arm/boot/dts/imx6q-vtpctp-0004.dts
@@ -0,0 +1,109 @@
+/*
+ * Copyright (C) 2017 elrest GmbH
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+/dts-v1/;
+
+#include "imx6q-vtpctp.dtsi"
+
+/ {
+	compatible = "wago,imx6q-vtpctp-762_4xxx-1000", "wago,imx6q-vtpctp", "fsl,imx6q";
+
+	reg_display: regulator-display {
+		compatible = "regulator-fixed";
+		regulator-name = "display-supply";
+		regulator-min-microvolt = <3300000>;
+		regulator-max-microvolt = <3300000>;
+		regulator-always-on;
+	};
+
+	display: display@di0 {
+		compatible = "fsl,imx-parallel-display";
+		#address-cells = <1>;
+		#size-cells = <0>;
+		pinctrl-names = "default";
+		pinctrl-0 = <&pinctrl_ipu_disp>;
+		interface-pix-fmt = "rgb24";
+		status = "okay";
+
+		port@0 {
+			reg = <0>;
+
+			parallel_display_in: endpoint {
+				remote-endpoint = <&ipu1_di0_disp0>;
+			};
+		};
+
+		port@1 {
+			reg = <1>;
+
+			lcd_display_out: endpoint {
+				remote-endpoint = <&lcd_panel_in>;
+			};
+		};
+	};
+
+	lcd_panel: lcd-panel {
+		compatible = "dataimage,fg040321duswmg01";
+		backlight = <&backlight>;
+		power-supply = <&reg_display>;
+
+		port {
+			lcd_panel_in: endpoint {
+				remote-endpoint = <&lcd_display_out>;
+			};
+		};
+	};
+};
+
+&ipu1_di0_disp0 {
+	remote-endpoint = <&parallel_display_in>;
+};
+
+&ecspi4 {
+	pinctrl-names = "default";
+	pinctrl-0 = <&pinctrl_ecspi4>;
+	fsl,spi-num-chipselects = <2>;
+	cs-gpios = <
+		&gpio3 20 0
+		&gpio5 2 0
+	>;
+	status = "okay";
+
+	/* touch controller */
+	touch:	tsc2046@0 {
+		pinctrl-names = "default";
+		pinctrl-0 = <&pinctrl_tsc2046>;
+
+		compatible = "ti,tsc2046";
+		vcc-supply = <&tsc2046reg>;
+
+		reg = <0>;	/* CS0 */
+		spi-max-frequency = <250000>;
+
+		interrupt-parent = <&gpio1>;
+		interrupts = <3 0>;
+		pendown-gpio = <&gpio1 3 0>;
+
+		//ti,keep-vref-on;
+		//ti,vref-delay-usecs = <800>;
+
+		ti,x-min = /bits/ 16 <0x0>;
+		ti,x-max = /bits/ 16 <0x0ffff>;
+		ti,y-min = /bits/ 16 <0x0>;
+		ti,y-max = /bits/ 16 <0x0ffff>;
+
+		ti,x-plate-ohms = /bits/ 16 <150>;
+		ti,pressure-max = /bits/ 16 <1000>;
+
+		ti,debounce-max = /bits/ 16 <30>;
+		ti,debounce-tol = /bits/ 16 <10>;
+		ti,debounce-rep = /bits/ 16 <1>;
+
+		linux,wakeup;
+	};
+};
diff --git a/arch/arm/boot/dts/imx6q-vtpctp-0005.dts b/arch/arm/boot/dts/imx6q-vtpctp-0005.dts
new file mode 100644
index 000000000000..0207a22babc0
--- /dev/null
+++ b/arch/arm/boot/dts/imx6q-vtpctp-0005.dts
@@ -0,0 +1,88 @@
+/*
+ * Copyright (C) 2017 elrest GmbH
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+/dts-v1/;
+
+#include "imx6q-vtpctp.dtsi"
+
+/ {
+	compatible = "wago,imx6q-vtpctp-762_4xxx-1001", "wago,imx6q-vtpctp", "fsl,imx6q";
+};
+
+/* 5" Display */
+&ldb {
+	status = "okay";
+	lvds-channel@0 {
+		fsl,data-mapping = "spwg";
+		fsl,data-width = <18>;
+		primary;
+		status = "okay";
+
+		display-timings {
+			native-mode = <&timing0>;
+			timing0: hsd100pxn1 {
+				clock-frequency = <25000000>;
+				hactive = <640>;
+				vactive = <480>;
+				hback-porch = <114>;
+				hfront-porch = <16>;
+				vback-porch = <32>;
+				vfront-porch = <10>;
+				hsync-len = <30>;
+				vsync-len = <3>;
+				hsync-active = <0>;
+				vsync-active = <0>;
+				de-active    = <1>;
+			};
+		};
+	};
+};
+
+&ecspi4 {
+	pinctrl-names = "default";
+	pinctrl-0 = <&pinctrl_ecspi4>;
+	fsl,spi-num-chipselects = <2>;
+	cs-gpios = <
+		&gpio3 20 0
+		&gpio5 2 0
+	>;
+	status = "okay";
+
+	/* touch controller */
+	touch:	tsc2046@0 {
+		pinctrl-names = "default";
+		pinctrl-0 = <&pinctrl_tsc2046>;
+
+		compatible = "ti,tsc2046";
+		vcc-supply = <&tsc2046reg>;
+
+		reg = <0>;	/* CS0 */
+		spi-max-frequency = <500000>;
+
+		interrupt-parent = <&gpio1>;
+		interrupts = <3 0>;
+		pendown-gpio = <&gpio1 3 0>;
+
+		ti,keep-vref-on;
+		ti,vref-delay-usecs = <800>;
+
+		ti,x-min = /bits/ 16 <0x0>;
+		ti,x-max = /bits/ 16 <0x0ffff>;
+		ti,y-min = /bits/ 16 <0x0>;
+		ti,y-max = /bits/ 16 <0x0ffff>;
+
+		ti,x-plate-ohms = /bits/ 16 <200>;
+		ti,pressure-max = /bits/ 16 <1000>;
+
+		ti,debounce-max = /bits/ 16 <30>;
+		ti,debounce-tol = /bits/ 16 <10>;
+		ti,debounce-rep = /bits/ 16 <1>;
+
+		linux,wakeup;
+	};
+};
diff --git a/arch/arm/boot/dts/imx6q-vtpctp-0007.dts b/arch/arm/boot/dts/imx6q-vtpctp-0007.dts
new file mode 100644
index 000000000000..a7fcdbf6ae98
--- /dev/null
+++ b/arch/arm/boot/dts/imx6q-vtpctp-0007.dts
@@ -0,0 +1,90 @@
+/*
+ * Copyright (C) 2017 elrest GmbH
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+/dts-v1/;
+
+#include "imx6q-vtpctp.dtsi"
+
+/ {
+	compatible = "wago,imx6q-vtpctp-762_4xxx-1002", "wago,imx6q-vtpctp", "fsl,imx6q";
+};
+
+/* 7" Display */
+&ldb {
+	status = "okay";
+
+	lvds-channel@0 {
+		fsl,data-mapping = "spwg";
+		fsl,data-width = <24>;
+		primary;
+		status = "okay";
+
+		display-timings {
+			native-mode = <&timing0>;
+			timing0: hsd100pxn1 {
+				clock-frequency = <33260000>;
+				hactive = <800>;
+				vactive = <480>;
+				hback-porch = <205>;
+				hfront-porch = <20>;
+				vback-porch = <26>;
+				vfront-porch = <5>;
+				hsync-len = <10>;
+				vsync-len = <10>;
+				hsync-active = <0>;
+				vsync-active = <0>;
+				de-active    = <1>;
+			};
+		};
+	};
+};
+
+&ecspi4 {
+	pinctrl-names = "default";
+	pinctrl-0 = <&pinctrl_ecspi4>;
+	fsl,spi-num-chipselects = <2>;
+	cs-gpios = <
+		&gpio3 20 0
+		&gpio5 2 0
+	>;
+	status = "okay";
+
+	/* touch controller */
+	touch:	tsc2046@0 {
+		pinctrl-names = "default";
+		pinctrl-0 = <&pinctrl_tsc2046>;
+
+		compatible = "ti,tsc2046";
+		vcc-supply = <&tsc2046reg>;
+
+		reg = <0>;	/* CS0 */
+		spi-max-frequency = <125000>;
+
+		interrupt-parent = <&gpio1>;
+		interrupts = <3 0>;
+		pendown-gpio = <&gpio1 3 0>;
+
+		//ti,keep-vref-on;
+		//ti,vref-delay-usecs = <800>;
+
+		ti,x-min = /bits/ 16 <0x0>;
+		ti,x-max = /bits/ 16 <0x0ffff>;
+		ti,y-min = /bits/ 16 <0x0>;
+		ti,y-max = /bits/ 16 <0x0ffff>;
+
+		ti,x-plate-ohms = /bits/ 16 <300>;
+		ti,pressure-min = /bits/ 16 <50>;
+		ti,pressure-max = /bits/ 16 <1000>;
+
+		ti,debounce-max = /bits/ 16 <30>;
+		ti,debounce-tol = /bits/ 16 <10>;
+		ti,debounce-rep = /bits/ 16 <1>;
+
+		linux,wakeup;
+	};
+};
diff --git a/arch/arm/boot/dts/imx6q-vtpctp-0010.dts b/arch/arm/boot/dts/imx6q-vtpctp-0010.dts
new file mode 100644
index 000000000000..1158ccb669c0
--- /dev/null
+++ b/arch/arm/boot/dts/imx6q-vtpctp-0010.dts
@@ -0,0 +1,105 @@
+/*
+ * Copyright (C) 2017 elrest GmbH
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+/dts-v1/;
+
+#include "imx6q-vtpctp.dtsi"
+
+/ {
+	compatible = "wago,imx6q-vtpctp-762_4xxx-1003", "wago,imx6q-vtpctp", "fsl,imx6q";
+};
+
+&ldb {
+	status = "okay";
+
+	lvds-channel@0 {
+		fsl,data-mapping = "spwg";
+		fsl,data-width = <24>;
+		primary;
+		status = "okay";
+
+		display-timings {
+			native-mode = <&timing0>;
+			timing0: hsd100pxn1 {
+				clock-frequency = <71100000>;
+				hactive = <1280>;
+				vactive = <800>;
+				hback-porch = <160>;
+				hfront-porch = <190>;
+				vback-porch = <23>;
+				vfront-porch = <33>;
+				hsync-len = <60>;
+				vsync-len = <15>;
+				hsync-active = <0>;
+				vsync-active = <0>;
+				de-active    = <1>;
+			};
+		};
+	};
+};
+
+&ecspi4 {
+	pinctrl-names = "default";
+	pinctrl-0 = <&pinctrl_ecspi4>;
+	fsl,spi-num-chipselects = <2>;
+	cs-gpios = <
+		&gpio3 20 0
+		&gpio5 2 0
+	>;
+	status = "okay";
+
+	/* touch controller */
+	touch:	tsc2046@0 {
+		pinctrl-names = "default";
+		pinctrl-0 = <&pinctrl_tsc2046>;
+
+		compatible = "ti,tsc2046";
+		vcc-supply = <&tsc2046reg>;
+
+		reg = <0>;	/* CS0 */
+		spi-max-frequency = <250000>;
+
+		interrupt-parent = <&gpio1>;
+		interrupts = <3 0>;
+		pendown-gpio = <&gpio1 3 0>;
+
+		ti,keep-vref-on;
+		ti,vref-delay-usecs = <400>;
+
+		ti,x-min = /bits/ 16 <0x0>;
+		ti,x-max = /bits/ 16 <0x0ffff>;
+		ti,y-min = /bits/ 16 <0x0>;
+		ti,y-max = /bits/ 16 <0x0ffff>;
+
+		ti,x-plate-ohms = /bits/ 16 <600>;
+		ti,pressure-max = /bits/ 16 <1000>;
+
+		ti,debounce-max = /bits/ 16 <30>;
+		ti,debounce-tol = /bits/ 16 <10>;
+		ti,debounce-rep = /bits/ 16 <1>;
+
+		linux,wakeup;
+	};
+};
+
+&clks {
+	assigned-clocks = <&clks IMX6QDL_CLK_LDB_DI0_SEL>,
+			  <&clks IMX6QDL_CLK_LDB_DI1_SEL>,
+			  <&clks IMX6QDL_CLK_IPU1_DI0_PRE_SEL>,
+			  <&clks IMX6QDL_CLK_IPU1_DI1_PRE_SEL>,
+			  <&clks IMX6QDL_CLK_IPU2_DI0_PRE_SEL>,
+			  <&clks IMX6QDL_CLK_IPU2_DI1_PRE_SEL>,
+			  <&clks IMX6QDL_CLK_CKO2>;
+	assigned-clock-parents = <&clks IMX6QDL_CLK_PLL2_PFD2_396M>,
+				 <&clks IMX6QDL_CLK_PLL2_PFD2_396M>,
+				 <&clks IMX6QDL_CLK_PLL2_PFD2_396M>,
+				 <&clks IMX6QDL_CLK_PLL2_PFD2_396M>,
+				 <&clks IMX6QDL_CLK_PLL2_PFD2_396M>,
+				 <&clks IMX6QDL_CLK_PLL2_PFD2_396M>;
+	assigned-clock-rates =  <0>, <0>, <0>, <0>, <0>, <0>, <12000000>;
+};
diff --git a/arch/arm/boot/dts/imx6q-vtpctp-0015.dts b/arch/arm/boot/dts/imx6q-vtpctp-0015.dts
new file mode 100644
index 000000000000..64bcd5a5f9b3
--- /dev/null
+++ b/arch/arm/boot/dts/imx6q-vtpctp-0015.dts
@@ -0,0 +1,111 @@
+/*
+ * Copyright (C) 2017 elrest GmbH
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+/dts-v1/;
+
+#include "imx6q-vtpctp.dtsi"
+
+/ {
+	compatible = "wago,imx6q-vtpctp-762_4xxx-1008", "wago,imx6q-vtpctp", "fsl,imx6q";
+};
+
+/*
+ * 15,6" Display Dual-Channel LVDS Panel settings for
+ * Ampire AM-19201080FTZQW-A1215HVN05 Color TFT 1920x1080
+ */
+&ldb {
+	fsl,dual-channel;
+	status = "okay";
+
+	lvds-channel@0 {
+		fsl,data-mapping = "spwg";
+		fsl,data-width = <24>;
+		status = "okay";
+		display-timings {
+			native-mode = <&timing0>;
+			timing0: g24hw01 {
+				/* 60 Hz */
+				clock-frequency = <137313000>;
+				hactive = <1920>;
+				vactive = <1080>;
+				hback-porch = <148>;
+				hfront-porch = <0>;
+				vback-porch = <11>;
+				vfront-porch = <0>;
+				hsync-len = <22>;
+				vsync-len = <4>;
+				hsync-active = <0>;
+				vsync-active = <0>;
+				de-active = <1>;
+				pixelclk-active = <0>;
+			};
+		};
+	};
+};
+
+&ecspi4 {
+	pinctrl-names = "default";
+	pinctrl-0 = <&pinctrl_ecspi4>;
+	fsl,spi-num-chipselects = <2>;
+	cs-gpios = <
+		&gpio3 20 0
+		&gpio5 2 0
+	>;
+	status = "ok";
+
+	/* touch controller */
+	touch:	tsc2046@0 {
+		pinctrl-names = "default";
+		pinctrl-0 = <&pinctrl_tsc2046>;
+
+		compatible = "ti,tsc2046";
+		vcc-supply = <&tsc2046reg>;
+
+		reg = <0>;	/* CS0 */
+		spi-max-frequency = <200000>;
+
+		interrupt-parent = <&gpio1>;
+		interrupts = <3 0>;
+		pendown-gpio = <&gpio1 3 0>;
+
+		ti,keep-vref-on;
+		ti,vref-delay-usecs = <400>;
+
+		ti,x-min = /bits/ 16 <0x0>;
+		ti,x-max = /bits/ 16 <0x0ffff>;
+		ti,y-min = /bits/ 16 <0x0>;
+		ti,y-max = /bits/ 16 <0x0ffff>;
+
+		ti,x-plate-ohms = /bits/ 16 <500>;
+		ti,pressure-min = /bits/ 16 <50>;
+		ti,pressure-max = /bits/ 16 <1000>;
+
+		ti,debounce-max = /bits/ 16 <30>;
+		ti,debounce-tol = /bits/ 16 <10>;
+		ti,debounce-rep = /bits/ 16 <1>;
+
+		linux,wakeup;
+	};
+};
+
+&clks {
+	assigned-clocks = <&clks IMX6QDL_CLK_LDB_DI0_SEL>,
+			  <&clks IMX6QDL_CLK_LDB_DI1_SEL>,
+			  <&clks IMX6QDL_CLK_IPU1_DI0_PRE_SEL>,
+			  <&clks IMX6QDL_CLK_IPU1_DI1_PRE_SEL>,
+			  <&clks IMX6QDL_CLK_IPU2_DI0_PRE_SEL>,
+			  <&clks IMX6QDL_CLK_IPU2_DI1_PRE_SEL>,
+			  <&clks IMX6QDL_CLK_CKO2>;
+	assigned-clock-parents = <&clks IMX6QDL_CLK_PLL2_PFD2_396M>,
+				 <&clks IMX6QDL_CLK_PLL2_PFD2_396M>,
+				 <&clks IMX6QDL_CLK_PLL2_PFD2_396M>,
+				 <&clks IMX6QDL_CLK_PLL2_PFD2_396M>,
+				 <&clks IMX6QDL_CLK_PLL2_PFD2_396M>,
+				 <&clks IMX6QDL_CLK_PLL2_PFD2_396M>;
+	assigned-clock-rates =  <0>, <0>, <0>, <0>, <0>, <0>, <12000000>;
+};
diff --git a/arch/arm/boot/dts/imx6q-vtpctp-0021.dts b/arch/arm/boot/dts/imx6q-vtpctp-0021.dts
new file mode 100644
index 000000000000..85434fb82444
--- /dev/null
+++ b/arch/arm/boot/dts/imx6q-vtpctp-0021.dts
@@ -0,0 +1,110 @@
+/*
+ * Copyright (C) 2017 elrest GmbH
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+/dts-v1/;
+
+#include "imx6q-vtpctp.dtsi"
+
+/ {
+	compatible = "wago,imx6q-vtpctp-762_4xxx-1009", "wago,imx6q-vtpctp", "fsl,imx6q";
+};
+
+/*
+ * 21,5" Display Dual-Channel LVDS Panel settings for
+ * AUO T215HVN05 24-inch Color TFT 1920x1080
+ */
+&ldb {
+	fsl,dual-channel;
+	status = "okay";
+
+	lvds-channel@0 {
+		fsl,data-mapping = "spwg";
+		fsl,data-width = <24>;
+		status = "okay";
+		display-timings {
+			native-mode = <&timing0>;
+			timing0: g24hw01 {
+				clock-frequency = <136278000>;
+				hactive = <1920>;
+				vactive = <1080>;
+				hback-porch = <22>;
+				hfront-porch = <22>;
+				vback-porch = <12>;
+				vfront-porch = <12>;
+				hsync-len = <46>;
+				vsync-len = <26>;
+				hsync-active = <0>;
+				vsync-active = <0>;
+				de-active = <1>;
+				pixelclk-active = <1>;
+			};
+		};
+	};
+};
+
+&ecspi4 {
+	pinctrl-names = "default";
+	pinctrl-0 = <&pinctrl_ecspi4>;
+	fsl,spi-num-chipselects = <2>;
+	cs-gpios = <
+		&gpio3 20 0
+		&gpio5 2 0
+	>;
+	status = "okay";
+
+	/* touch controller */
+	touch:	tsc2046@0 {
+		pinctrl-names = "default";
+		pinctrl-0 = <&pinctrl_tsc2046>;
+
+		compatible = "ti,tsc2046";
+		vcc-supply = <&tsc2046reg>;
+
+		reg = <0>;	/* CS0 */
+		spi-max-frequency = <100000>;
+
+		interrupt-parent = <&gpio1>;
+		interrupts = <3 0>;
+		pendown-gpio = <&gpio1 3 0>;
+
+		ti,keep-vref-on;
+		ti,vref-delay-usecs = <400>;
+
+		ti,x-min = /bits/ 16 <0x0>;
+		ti,x-max = /bits/ 16 <0x0ffff>;
+		ti,y-min = /bits/ 16 <0x0>;
+		ti,y-max = /bits/ 16 <0x0ffff>;
+
+		ti,x-plate-ohms = /bits/ 16 <300>;
+		ti,pressure-min = /bits/ 16 <50>;
+		ti,pressure-max = /bits/ 16 <1000>;
+
+		ti,debounce-max = /bits/ 16 <30>;
+		ti,debounce-tol = /bits/ 16 <10>;
+		ti,debounce-rep = /bits/ 16 <1>;
+
+		linux,wakeup;
+	};
+};
+
+&clks {
+	assigned-clocks = <&clks IMX6QDL_CLK_LDB_DI0_SEL>,
+			  <&clks IMX6QDL_CLK_LDB_DI1_SEL>,
+			  <&clks IMX6QDL_CLK_IPU1_DI0_PRE_SEL>,
+			  <&clks IMX6QDL_CLK_IPU1_DI1_PRE_SEL>,
+			  <&clks IMX6QDL_CLK_IPU2_DI0_PRE_SEL>,
+			  <&clks IMX6QDL_CLK_IPU2_DI1_PRE_SEL>,
+			  <&clks IMX6QDL_CLK_CKO2>;
+	assigned-clock-parents = <&clks IMX6QDL_CLK_PLL2_PFD2_396M>,
+				 <&clks IMX6QDL_CLK_PLL2_PFD2_396M>,
+				 <&clks IMX6QDL_CLK_PLL2_PFD2_396M>,
+				 <&clks IMX6QDL_CLK_PLL2_PFD2_396M>,
+				 <&clks IMX6QDL_CLK_PLL2_PFD2_396M>,
+				 <&clks IMX6QDL_CLK_PLL2_PFD2_396M>;
+	assigned-clock-rates =  <0>, <0>, <0>, <0>, <0>, <0>, <12000000>;
+};
diff --git a/arch/arm/boot/dts/imx6q-vtpctp-ksz8863.dtsi b/arch/arm/boot/dts/imx6q-vtpctp-ksz8863.dtsi
new file mode 100644
index 000000000000..9c8c50d9f4fc
--- /dev/null
+++ b/arch/arm/boot/dts/imx6q-vtpctp-ksz8863.dtsi
@@ -0,0 +1,77 @@
+/*
+ * Copyright (C) 2020 WAGO Kontakttechnik GmbH & Co. KG - https://www.wago.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+/ {
+	bitbang_mdio0: gpio_mdio {
+		status = "disabled";
+		compatible = "virtual,mdio-gpio";
+		#address-cells = <1>;
+		#size-cells = <0>;
+	};
+
+	swcfg_ksz8863: swcfg_ksz8863 {
+		status = "disabled";
+		compatible = "swcfg,ksz8863";
+
+		swcfg,mii-bus = <&bitbang_mdio0>;
+		swcfg,alias = "ksz8863";
+		swcfg,cpu_port = <2>;
+		swcfg,ports = <3>;
+		swcfg,vlans = <16>;
+		swcfg,switch = <&ksz8863_switch>;
+	};
+};
+
+&bitbang_mdio0 {
+	ksz8863_switch: switch@0 {
+		status = "disabled";
+		compatible = "micrel,ksz8863";
+
+		pinctrl-names = "default";
+		pinctrl-0 = <&pinctrl_ksz8863>;
+
+		#address-cells = <1>;
+		#size-cells = <0>;
+
+		ksz,reset-gpio = <&gpio2 21 GPIO_ACTIVE_LOW>;
+
+		reg = <0>;
+		dsa,member = <0 0>;
+		dsa,enable-on-boot;
+
+		ksz,reset-switch;
+		ksz,disable-internal-ldo;
+
+		ports {
+			#address-cells = <1>;
+			#size-cells = <0>;
+
+			port@0 {
+				reg = <2>;
+				label = "ethX1";
+				phy-pwrdown;
+			};
+
+			port@1 {
+				reg = <1>;
+				label = "ethX2";
+				phy-pwrdown;
+			};
+
+			port@2 {
+				reg = <3>;
+				label = "cpu";
+				ethernet = <&fec>;
+				fixed-link {
+					speed = <100>;
+					full-duplex;
+				};
+			};
+		};
+	};
+};
diff --git a/arch/arm/boot/dts/imx6q-vtpctp.dtsi b/arch/arm/boot/dts/imx6q-vtpctp.dtsi
new file mode 100644
index 000000000000..c48b7182c7e1
--- /dev/null
+++ b/arch/arm/boot/dts/imx6q-vtpctp.dtsi
@@ -0,0 +1,1153 @@
+/*
+ * Copyright (C) 2020 WAGO Kontakttechnik GmbH & Co. KG - https://www.wago.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <dt-bindings/gpio/gpio.h>
+#include <dt-bindings/input/input.h>
+
+#include "imx6q.dtsi"
+#include "imx6q-vtpctp-ksz8863.dtsi"
+
+/ {
+	model = "VTPCTP Quad Board";
+
+	memory {
+		reg = <0x10000000 0x80000000>;
+	};
+
+	aliases {
+		mxcfb0 = &mxcfb1;
+		mdio-gpio0 = &bitbang_mdio0;
+		mmc0 = &usdhc1;
+		mmc1 = &usdhc3;
+		ethernet1 = &fec;
+	};
+
+	wsysinit_init {
+		compatible = "wago,sysinit";
+		add-sysfs-entries;
+
+		tty,service   = "ttymxc0";
+		tty,rs232-485 = "ttymxc1";
+
+		board,variant = "TP600";
+
+		/* sysclock adjustments, empirical values */
+		adjtimex,tick = <10000>;
+		adjtimex,frequency = <2000000>;
+	};
+
+	regulators {
+		compatible = "simple-bus";
+		#address-cells = <1>;
+		#size-cells = <0>;
+
+		reg_1p5v: 1p5v {
+			compatible = "regulator-fixed";
+			regulator-name = "1P5V";
+			regulator-min-microvolt = <1500000>;
+			regulator-max-microvolt = <1500000>;
+			regulator-always-on;
+		};
+
+		reg_1p8v: 1p8v {
+			compatible = "regulator-fixed";
+			regulator-name = "1P8V";
+			regulator-min-microvolt = <1800000>;
+			regulator-max-microvolt = <1800000>;
+			regulator-always-on;
+		};
+
+		reg_2p5v: 2p5v {
+			compatible = "regulator-fixed";
+			regulator-name = "2P5V";
+			regulator-min-microvolt = <2500000>;
+			regulator-max-microvolt = <2500000>;
+			regulator-always-on;
+		};
+
+		reg_2p8v: 2p8v {
+			compatible = "regulator-fixed";
+			regulator-name = "2P8V";
+			regulator-min-microvolt = <2800000>;
+			regulator-max-microvolt = <2800000>;
+			regulator-always-on;
+		};
+
+		reg_3p3v: 3p3v {
+			compatible = "regulator-fixed";
+			regulator-name = "3P3V";
+			regulator-min-microvolt = <3300000>;
+			regulator-max-microvolt = <3300000>;
+			regulator-always-on;
+		};
+
+		reg_usb_otg_vbus: regulator@0 {
+			compatible = "regulator-fixed";
+			reg = <0>;
+			regulator-name = "usb_otg_vbus";
+			regulator-min-microvolt = <5000000>;
+			regulator-max-microvolt = <5000000>;
+			gpio = <&gpio4 15 GPIO_ACTIVE_LOW>;
+			enable-active-high;
+		};
+
+		reg_usb_h1_vbus: regulator@1 {
+			compatible = "regulator-fixed";
+			reg = <1>;
+			regulator-name = "usb_h1_vbus";
+			regulator-min-microvolt = <5000000>;
+			regulator-max-microvolt = <5000000>;
+			regulator-always-on;
+		};
+
+		reg_lvds_backlight_3v3: regulator@3 {
+			compatible = "regulator-fixed";
+			regulator-name = "lvds-3v3";
+			regulator-min-microvolt = <3300000>;
+			regulator-max-microvolt = <3300000>;
+			regulator-always-on;
+		};
+
+	};
+
+	tsc2046reg: tsc2046-reg {
+		compatible = "regulator-fixed";
+		regulator-name = "tsc2046-reg";
+		regulator-min-microvolt = <3300000>;
+		regulator-max-microvolt = <3300000>;
+	};
+
+	sound {
+		compatible = "fsl,imx6dl-vtpctp-sgtl5000",
+					 "fsl,imx-audio-sgtl5000";
+		model = "imx6dl-vtpctp-sgtl5000";
+		ssi-controller = <&ssi1>;
+		audio-codec = <&codec>;
+		audio-routing =
+		"Headphone Jack", "HP_OUT";
+		mux-int-port = <1>;
+		mux-ext-port = <4>;
+	};
+
+	mxcfb1: fb@0 {
+		compatible = "fsl,mxc_sdc_fb";
+		disp_dev = "ldb";
+		interface_pix_fmt = "RGB666";
+		default_bpp = <24>;
+		int_clk = <0>;
+		late_init = <0>;
+		status = "okay";
+	};
+
+	backlight: backlight {
+		compatible = "pwm-backlight";
+		pwms = <&pwm3 0 50512>;
+		brightness-levels =
+			< 0   1   2   3   4   5   6   7   8   9
+			 10  11  12  13  14  15  16  17  18  19
+			 20  21  22  23  24  25  26  27  28  29
+			 30  31  32  33  34  35  36  37  38  39
+			 40  41  42  43  44  45  46  47  48  49
+			 50  51  52  53  54  55  56  57  58  59
+			 60  61  62  63  64  65  66  67  68  69
+			 70  71  72  73  74  75  76  77  78  79
+			 80  81  82  83  84  85  86  87  88  89
+			 90  91  92  93  94  95  96  97  98  99
+			100 101 102 103 104 105 106 107 108 109
+			110 111 112 113 114 115 116 117 118 119
+			120 121 122 123 124 125 126 127 128 129
+			130 131 132 133 134 135 136 137 138 139
+			140 141 142 143 144 145 146 147 148 149
+			150 151 152 153 154 155 156 157 158 159
+			160 161 162 163 164 165 166 167 168 169
+			170 171 172 173 174 175 176 177 178 179
+			180 181 182 183 184 185 186 187 188 189
+			190 191 192 193 194 195 196 197 198 199
+			200 201 202 203 204 205 206 207 208 209
+			210 211 212 213 214 215 216 217 218 219
+			220 221 222 223 224 225 226 227 228 229
+			230 231 232 233 234 235 236 237 238 239
+			240 241 242 243 244 245 246 247 248 249
+			250 251 252 253 254 255>;
+		default-brightness-level = <0>;
+		power-supply = <&reg_lvds_backlight_3v3>;
+		status = "okay";
+	};
+
+	backlight_leds {
+		pinctrl-names = "default";
+		pinctrl-0 = <&pinctrl_reg_lcd_3v3>;
+		compatible = "gpio-leds";
+
+		vled-on {
+			gpios = <&gpio2 11 GPIO_ACTIVE_HIGH>;
+			default-state = "on";
+		};
+
+		light-on {
+			gpios = <&gpio2 8 GPIO_ACTIVE_HIGH>;
+			default-state = "on";
+		};
+
+		vlcd-on {
+			gpios = <&gpio6 11 GPIO_ACTIVE_HIGH>;
+			default-state = "on";
+		};
+
+		captast-nreset {
+			gpios = <&gpio2 3 GPIO_ACTIVE_HIGH>;
+			default-state = "on";
+		};
+
+		pcap-bridge-nreset {
+			gpios = <&gpio6 10 GPIO_ACTIVE_HIGH>;
+			default-state = "on";
+		};
+
+		rs-sel {
+			gpios = <&gpio6 9 GPIO_ACTIVE_HIGH>;
+			default-state = "off";
+		};
+
+		sys-red {
+			gpios = <&gpio2 4 GPIO_ACTIVE_HIGH>;
+			default-state = "off";
+		};
+
+		sys-green {
+			gpios = <&gpio2 5 GPIO_ACTIVE_HIGH>;
+			default-state = "off";
+		};
+
+		sys-blue {
+			gpios = <&gpio2 6 GPIO_ACTIVE_HIGH>;
+			default-state = "off";
+		};
+
+		led-rd-ovrd {
+			gpios = <&gpio5 18 GPIO_ACTIVE_HIGH>;
+			default-state = "off";
+		};
+
+		led-srvr-wd-imx6 {
+			gpios = <&gpio5 19 GPIO_ACTIVE_HIGH>;
+			default-state = "off";
+		};
+
+		en-led-rd-srvr-ndw {
+			gpios = <&gpio2 16 GPIO_ACTIVE_LOW>;
+			default-state = "off";
+		};
+		beeper {
+			gpios = <&gpio2 7 GPIO_ACTIVE_HIGH>;
+			default-state = "oneshot";
+		};
+	};
+
+	v4l2_cap_1 {
+		compatible = "fsl,imx6q-v4l2-capture";
+		ipu_id = <0>;
+		csi_id = <1>;
+		mclk_source = <0>;
+		status = "okay";
+	};
+
+	v4l2_out {
+		compatible = "fsl,mxc_v4l2_output";
+		status = "okay";
+	};
+
+	susp-signals {
+		compatible = "fsl,susp-signals";
+		gpios = <&gpio1 5 GPIO_ACTIVE_LOW>;
+	};
+
+	PAC-Operating-Mode-Switch {
+		pinctrl-names = "default";
+		pinctrl-0 = <&pinctrl_bas>;
+		compatible = "gpio-keys";
+		status = "okay";
+		#address-cells = <1>;
+		#size-cells = <0>;
+		autorepeat;
+
+		run {
+			label = "RUN";
+			gpios = <&gpio2 12 GPIO_ACTIVE_LOW>; /* GPIO 44 */
+			linux,code = <1>;
+			linux,input-type = <EV_SW>;
+			debounce-interval = <1>; /* debounce in msecs */
+		};
+
+		stop {
+			label = "STOP";
+			gpios = <&gpio2 13 GPIO_ACTIVE_LOW>; /* GPIO 45 */
+			linux,code = <2>;
+			linux,input-type = <EV_SW>;
+			debounce-interval = <1>; /* debounce in msecs */
+		};
+
+		reset {
+			label = "RESET";
+			gpios = <&gpio2 15 GPIO_ACTIVE_LOW>; /* GPIO 47 */
+			linux,code = <3>;
+			linux,input-type = <EV_KEY>;
+			debounce-interval = <1>; /* debounce in msecs */
+		};
+
+		reset_all {
+			label = "RESET_ALL";
+			gpios = <&gpio4 7 GPIO_ACTIVE_LOW>; /* GPIO 103 */
+			linux,code = <4>;
+			linux,input-type = <EV_KEY>;
+			debounce-interval = <1>; /* debounce in msecs */
+		};
+
+	};
+
+	bitbang_mdio0: gpio_mdio {
+		compatible = "virtual,mdio-gpio";
+		#address-cells = <1>;
+		#size-cells = <0>;
+		##status = "okay";
+	};
+
+	rmii_clk_ext: rmii_clk {
+		compatible = "fixed-clock";
+		#clock-cells = <0>;
+		clock-frequency = <50000000>;
+	};
+};
+
+&audmux {
+	pinctrl-names = "default";
+	pinctrl-0 = <&pinctrl_audmux>;
+	status = "okay";
+};
+
+&clks {
+	fsl,ldb-di0-parent = <&clks IMX6QDL_CLK_PLL2_PFD0_352M>;
+	fsl,ldb-di1-parent = <&clks IMX6QDL_CLK_PLL2_PFD0_352M>;
+
+	assigned-clocks = <&clks IMX6QDL_CLK_CKO2>;
+	assigned-clock-rates = <12000000>;
+};
+
+&fec {
+	pinctrl-names = "default";
+	pinctrl-0 = <&pinctrl_enet>;
+	phy-mode = "rmii";
+
+	clocks = <&clks IMX6QDL_CLK_ENET>,
+		 <&clks IMX6QDL_CLK_ENET>,
+		 <&rmii_clk_ext>;
+	clock-names = "ipg", "ahb", "ptp";
+
+	status = "okay";
+	fixed-link {
+		speed = <100>;
+		full-duplex;
+	};
+
+	mdio: mdio@0 {
+		#address-cells = <1>;
+		#size-cells = <0>;
+	};
+};
+
+&i2c2 {
+	clock-frequency = <400000>;
+	pinctrl-names = "default";
+	pinctrl-0 = <&pinctrl_i2c2>;
+	status = "okay";
+
+	pmic: pf0100@8 {
+		compatible = "fsl,pfuze100";
+		reg = <0x08>;
+		interrupt-parent = <&gpio7>;
+		interrupts = <12 8>;
+
+		regulators {
+
+			/* VDD_ARM */
+			sw1a_reg: sw1ab {
+				regulator-min-microvolt = <1400000>;
+				regulator-max-microvolt = <1875000>;
+				regulator-boot-on;
+				regulator-always-on;
+				regulator-ramp-delay = <6250>;
+			};
+
+			/* VDD_SOC */
+			sw1c_reg: sw1c {
+				regulator-min-microvolt = <1400000>;
+				regulator-max-microvolt = <1875000>;
+				regulator-boot-on;
+				regulator-always-on;
+				regulator-ramp-delay = <6250>;
+			};
+
+			 /* VDD_HIGH */
+			sw2_reg: sw2 {
+				regulator-min-microvolt = <3300000>;
+				regulator-max-microvolt = <3300000>;
+				regulator-boot-on;
+				regulator-always-on;
+			};
+
+			 /* VDD_DDR */
+			sw3a_reg: sw3a {
+				regulator-min-microvolt = <1350000>;
+				regulator-max-microvolt = <1350000>;
+				regulator-boot-on;
+				regulator-always-on;
+			};
+
+			 /* VDD_DDR */
+			sw3b_reg: sw3b {
+				regulator-min-microvolt = <1350000>;
+				regulator-max-microvolt = <1350000>;
+				regulator-boot-on;
+				regulator-always-on;
+			};
+
+			/* VDD_1P8 */
+			sw4_reg: sw4 {
+				regulator-min-microvolt = <800000>;
+				regulator-max-microvolt = <3300000>;
+				regulator-always-off;
+			};
+
+			swbst_reg: swbst {
+				regulator-min-microvolt = <5000000>;
+				regulator-max-microvolt = <5150000>;
+			};
+
+			snvs_reg: vsnvs {
+				regulator-min-microvolt = <1000000>;
+				regulator-max-microvolt = <3000000>;
+				regulator-boot-on;
+				regulator-always-on;
+			};
+
+			vref_reg: vrefddr {
+				regulator-boot-on;
+				regulator-always-on;
+			};
+
+			vgen3_reg: vgen3 {
+				regulator-min-microvolt = <2500000>;
+				regulator-max-microvolt = <3300000>;
+				regulator-always-on;
+			};
+
+			vgen4_reg: vgen4 {
+				regulator-min-microvolt = <1800000>;
+				regulator-max-microvolt = <1800000>;
+				regulator-always-on;
+			};
+
+
+			vgen5_reg: vgen5 {
+				regulator-min-microvolt = <1800000>;
+				regulator-max-microvolt = <3300000>;
+				regulator-always-on;
+			};
+
+			/* supply voltage for eMMC */
+			vgen6_reg: vgen6 {
+				regulator-min-microvolt = <1800000>;
+				regulator-max-microvolt = <1800000>;
+				regulator-boot-on;
+				regulator-always-on;
+			};
+		};
+	};
+
+	lm75: lm75@49 {
+		compatible = "national,lm75";
+		reg = <0x49>;
+	};
+
+	eeprom: m24c512@54 {
+		compatible = "st,24c512", "at24";
+		reg = <0x54>;
+	};
+
+	rtc_r2221t@32 {
+		compatible = "ricoh,r2221tl";
+		reg = <0x32>;
+
+		interrupt-parent = <&gpio1>;
+		interrupts = <4 IRQ_TYPE_LEVEL_LOW>;
+	};
+
+	pca9552@61 {
+		compatible = "nxp,pca9552";
+		status = "okay";
+		reg = <0x61>;
+		#address-cells = <1>;
+		#size-cells = <0>;
+
+		sys-red-back@3 {
+			label = "sys-red-back";
+			reg = <3>;
+			linux,default-trigger = "none";
+		};
+
+		sys-green-back@4 {
+			label = "sys-green-back";
+			reg = <4>;
+			linux,default-trigger = "none";
+		};
+
+		sys-blue-back@5 {
+			label = "sys-blue-back";
+			reg = <5>;
+			linux,default-trigger = "none";
+		};
+
+		run-green@8 {
+			label = "run-green";
+			reg = <8>;
+			linux,default-trigger = "none";
+		};
+
+		run-red@9 {
+			label = "run-red";
+			reg = <9>;
+			linux,default-trigger = "none";
+		};
+
+		can-green@10 {
+			label = "can-green";
+			reg = <10>;
+			linux,default-trigger = "none";
+		};
+
+		can-red@11 {
+			label = "can-red";
+			reg = <11>;
+			linux,default-trigger = "none";
+		};
+
+		h11-green@12 {
+			label = "h11-green";
+			reg = <12>;
+			linux,default-trigger = "none";
+		};
+
+		h11-red@13 {
+			label = "h11-red";
+			reg = <13>;
+			linux,default-trigger = "none";
+		};
+
+		h12-green@14 {
+			label = "h12-green";
+			reg = <14>;
+			linux,default-trigger = "none";
+		};
+
+		h12-red@15 {
+			label = "h12-red";
+			reg = <15>;
+			linux,default-trigger = "none";
+		};
+	};
+
+};
+
+&i2c3 {
+	clock-frequency = <400000>;
+	pinctrl-names = "default";
+	pinctrl-0 = <&pinctrl_i2c3>;
+	status = "okay";
+
+	codec: sgtl5000@a {
+		compatible = "fsl,sgtl5000";
+		reg = <0x0a>;
+		clocks = <&clks IMX6QDL_CLK_CKO2>;
+		/* sysclk = <12000000>; */
+		VDDD-supply = <&reg_1p8v>;
+		VDDA-supply = <&reg_3p3v>;
+		VDDIO-supply = <&reg_3p3v>;
+	};
+
+	qt1070: keyboard@1b {
+		pinctrl-names = "default";
+		pinctrl-0 = <&pinctrl_qt1070_irq>;
+		compatible = "qt1070";
+		reg = <0x1b>;
+		interrupt-parent = <&gpio4>;
+		interrupts = <8 IRQ_TYPE_LEVEL_LOW>;
+		wakeup-source;
+		at,threshold = /bits/ 8 <14>;
+		at,ave_aks = /bits/ 8 <129>;
+		at,di = /bits/ 8 <12>;
+		at,active_keys = /bits/ 8 <24>;
+	};
+
+	si1142: si1142@5a {
+		pinctrl-names = "default";
+		pinctrl-0 = <&pinctrl_si1142_irq>;
+		compatible = "si1142";
+		reg = <0x5a>;
+		interrupt-parent = <&gpio1>;
+		interrupts = <0 IRQ_TYPE_LEVEL_LOW>;
+		wakeup-source;
+	};
+
+	pca9552@60 {
+		compatible = "nxp,pca9552";
+		status = "okay";
+		reg = <0x60>;
+		#address-cells = <1>;
+		#size-cells = <0>;
+
+		bright-minus@9 {
+			label = "bright-minus";
+			reg = <9>;
+			linux,default-trigger = "oneshot";
+		};
+
+		bright-plus@8 {
+			label = "bright-plus";
+			reg = <8>;
+			linux,default-trigger = "oneshot";
+		};
+	};
+};
+
+&iomuxc {
+	pinctrl-names = "default";
+	pinctrl-0 = <&pinctrl_hog>;
+
+	imx6qdl-qmx6 {
+		pinctrl_hog: hoggrp {
+			fsl,pins = <
+				/* RTC-NINT */
+				MX6QDL_PAD_GPIO_4__GPIO1_IO04		0x1b0b0
+				/* RGMII Phy Interrupt */
+				MX6QDL_PAD_EIM_D23__GPIO3_IO23	0x80000000
+				/* EEPROM WP GPIO23 */
+				MX6QDL_PAD_ENET_REF_CLK__GPIO1_IO23	0x1b0b0
+				/* PCAP-BRIDGE-NRESET */
+				MX6QDL_PAD_NANDF_RB0__GPIO6_IO10	0x1b0b0
+				/* BEEPER_ON */
+				MX6QDL_PAD_NANDF_D7__GPIO2_IO07		0x1b0b0
+				/* AIO_NREST */
+				MX6QDL_PAD_EIM_A24__GPIO5_IO04		0x0b0b1
+				/* PMIC-NINTB */
+				MX6QDL_PAD_GPIO_17__GPIO7_IO12		0x0b0b1
+			>;
+		};
+
+		pinctrl_audmux: audmuxgrp {
+			fsl,pins = <
+				MX6QDL_PAD_SD2_DAT0__AUD4_RXD		0x130b0
+				MX6QDL_PAD_SD2_DAT3__AUD4_TXC		0x00008
+				MX6QDL_PAD_SD2_DAT2__AUD4_TXD		0x00008
+				MX6QDL_PAD_SD2_DAT1__AUD4_TXFS		0x00008
+				MX6QDL_PAD_NANDF_CS2__CCM_CLKO2		0x00008
+			>;
+		};
+
+		pinctrl_ecspi4: ecspi4grp {
+			fsl,pins = <
+				MX6QDL_PAD_EIM_D22__ECSPI4_MISO	0x100b1
+				MX6QDL_PAD_EIM_D28__ECSPI4_MOSI	0x100b1
+				MX6QDL_PAD_EIM_D21__ECSPI4_SCLK	0x100b1
+				/* CS0 */
+				MX6QDL_PAD_EIM_D20__GPIO3_IO20	0x1b0b0
+				/* CS1 */
+				MX6QDL_PAD_EIM_A25__GPIO5_IO02	0x1b0b0
+			>;
+		};
+
+		pinctrl_enet: enetgrp {
+			fsl,pins = <
+			MX6QDL_PAD_ENET_RXD0__ENET_RX_DATA0	0x1b0b0
+			MX6QDL_PAD_ENET_RXD1__ENET_RX_DATA1	0x1b0b0
+			MX6QDL_PAD_ENET_CRS_DV__ENET_RX_EN	0x1b0b0
+			MX6QDL_PAD_ENET_TXD0__ENET_TX_DATA0 0x1b088
+			MX6QDL_PAD_ENET_TXD1__ENET_TX_DATA1	0x1b088
+			MX6QDL_PAD_ENET_TX_EN__ENET_TX_EN	0x1b088
+			MX6QDL_PAD_GPIO_16__ENET_REF_CLK	0x4001b0a8
+			>;
+		};
+
+		pinctrl_flexcan1: flexcan1grp {
+			fsl,pins = <
+			MX6QDL_PAD_KEY_ROW2__FLEXCAN1_RX	0x80000000
+			MX6QDL_PAD_KEY_COL2__FLEXCAN1_TX	0x80000000
+			>;
+		};
+
+		pinctrl_i2c2: i2c2grp {
+			fsl,pins = <
+				MX6QDL_PAD_KEY_COL3__I2C2_SCL	0x4001b8b1
+				MX6QDL_PAD_KEY_ROW3__I2C2_SDA	0x4001b8b1
+			>;
+		};
+
+		pinctrl_i2c3: i2c3grp {
+			fsl,pins = <
+				MX6QDL_PAD_EIM_D17__I2C3_SCL	0x4001b8b1
+				MX6QDL_PAD_EIM_D18__I2C3_SDA	0x4001b8b1
+			>;
+		};
+
+		pinctrl_uart1: uart1grp {
+			fsl,pins = <
+				MX6QDL_PAD_CSI0_DAT11__UART1_RX_DATA	0x1b0b0
+				MX6QDL_PAD_CSI0_DAT10__UART1_TX_DATA	0x1b0b0
+			>;
+		};
+
+		pinctrl_uart2: uart2grp {
+			fsl,pins = <
+				MX6QDL_PAD_EIM_D26__UART2_TX_DATA	0x1b0b0
+				MX6QDL_PAD_EIM_D27__UART2_RX_DATA	0x1b0b0
+				MX6QDL_PAD_EIM_D29__GPIO3_IO29		0x1b0b0
+				MX6QDL_PAD_SD4_DAT6__UART2_CTS_B	0x1b0b0
+			>;
+		};
+
+		pinctrl_usbotg: usbotggrp {
+			fsl,pins = <
+				/* OTG_ID */
+				MX6QDL_PAD_ENET_RX_ER__USB_OTG_ID	0x1b0b1
+				/* OTG_OC */
+				MX6QDL_PAD_KEY_COL4__USB_OTG_OC		0x1b0b1
+				/* OTG_PWR */
+				MX6QDL_PAD_KEY_ROW4__GPIO4_IO15		0x130b0
+			>;
+		};
+
+		pinctrl_usdhc1: usdhc1grp {
+			fsl,pins = <
+				MX6QDL_PAD_SD1_CMD__SD1_CMD		0x17059
+				MX6QDL_PAD_SD1_CLK__SD1_CLK		0x10059
+				MX6QDL_PAD_SD1_DAT0__SD1_DATA0	0x17059
+				MX6QDL_PAD_SD1_DAT1__SD1_DATA1	0x17059
+				MX6QDL_PAD_SD1_DAT2__SD1_DATA2	0x17059
+				MX6QDL_PAD_SD1_DAT3__SD1_DATA3	0x17059
+				/* CD */
+				MX6QDL_PAD_GPIO_1__SD1_CD_B		0x1b0b0
+				/* SD-CARD.EN */
+				MX6QDL_PAD_SD4_CMD__GPIO7_IO09	0x1b0b0
+			>;
+		};
+
+		pinctrl_usdhc3: usdhc3grp {
+			fsl,pins = <
+				MX6QDL_PAD_SD3_CMD__SD3_CMD		0x17051
+				MX6QDL_PAD_SD3_CLK__SD3_CLK		0x10051
+				MX6QDL_PAD_SD3_DAT0__SD3_DATA0		0x17051
+				MX6QDL_PAD_SD3_DAT1__SD3_DATA1		0x17051
+				MX6QDL_PAD_SD3_DAT2__SD3_DATA2		0x17051
+				MX6QDL_PAD_SD3_DAT3__SD3_DATA3		0x17051
+				MX6QDL_PAD_SD3_DAT4__SD3_DATA4		0x17051
+				MX6QDL_PAD_SD3_DAT5__SD3_DATA5		0x17051
+				MX6QDL_PAD_SD3_DAT6__SD3_DATA6		0x17051
+				MX6QDL_PAD_SD3_DAT7__SD3_DATA7		0x17051
+			>;
+		};
+
+		pinctrl_reg_lcd_3v3: lpinctrl_reg_lcd_3v3grp {
+			fsl,pins = <
+			/* LCD-CTRL.LIGHT-ON */
+			MX6QDL_PAD_SD4_DAT0__GPIO2_IO08	0x80000000
+			/* LCD-CTRL.VLED-ON */
+			MX6QDL_PAD_SD4_DAT3__GPIO2_IO11	0x80000000
+			/* LCD-CTRL.VLCD-ON */
+			MX6QDL_PAD_NANDF_CS0__GPIO6_IO11	0x80000000
+			/* RS-SEL */
+			MX6QDL_PAD_NANDF_WP_B__GPIO6_IO09	0x1b0b0
+			/* CAPTAST_NRESET */
+			MX6QDL_PAD_NANDF_D3__GPIO2_IO03		0x1b0b0
+			/* LED-RD-ON */
+			MX6QDL_PAD_NANDF_D4__GPIO2_IO04		0x1b0b0
+			/* LED-GN-ON */
+			MX6QDL_PAD_NANDF_D5__GPIO2_IO05		0x1b0b0
+			/* LED-BL-ON */
+			MX6QDL_PAD_NANDF_D6__GPIO2_IO06		0x1b0b0
+			/* EN-LED-SRVR-NWD */
+			MX6QDL_PAD_EIM_A22__GPIO2_IO16		0x1b0b0
+			/* LED-RD-OVRD */
+			MX6QDL_PAD_CSI0_PIXCLK__GPIO5_IO18	0x1b0b0
+			/* LED-SRVR-WD-IMX6 */
+			MX6QDL_PAD_CSI0_MCLK__GPIO5_IO19	0x1b0b0
+			>;
+		};
+
+		pinctrl_pwm3: pwm3grp {
+			fsl,pins = <
+				MX6QDL_PAD_SD4_DAT1__PWM3_OUT	0x1b0b0
+			>;
+		};
+
+		pinctrl_bas: basgrp {
+			fsl,pins = <
+				/* BAS-NRUN */
+				MX6QDL_PAD_SD4_DAT4__GPIO2_IO12	0x1b0b0
+				/* BAS-NSTOP */
+				MX6QDL_PAD_SD4_DAT5__GPIO2_IO13	0x1b0b0
+				/* BAS-NRESET */
+				MX6QDL_PAD_SD4_DAT7__GPIO2_IO15	0x1b0b0
+				/* RESET-ALL */
+				MX6QDL_PAD_KEY_ROW0__GPIO4_IO07	0x1b0b0
+			>;
+		};
+
+		pinctrl_ksz8863: pinctrl_ksz8863grp {
+			fsl,pins = <
+				/* phy-reset */
+				MX6QDL_PAD_KEY_ROW1__GPIO4_IO09	0x1B0B0
+				/* phy-irq */
+				MX6QDL_PAD_GPIO_6__GPIO1_IO06	0x000b1
+			>;
+		};
+
+		gpio_bitbang_mdio_pins: pinmux_gpio_bitbang_mdio_pins {
+			fsl,pins = <
+				/* MDIO */
+				MX6QDL_PAD_ENET_MDIO__GPIO1_IO22	0x1b0b0
+				MX6QDL_PAD_ENET_MDC__GPIO1_IO31		0x1b0b0
+			>;
+		};
+
+		gpio_bitbang_mdio_sleep_pins: pinmux_gpio_bitbang_mdio_sleep {
+			fsl,pins = <
+				/* MDIO reset value */
+				MX6QDL_PAD_ENET_MDIO__GPIO1_IO22	0x1b0b0
+				MX6QDL_PAD_ENET_MDC__GPIO1_IO31		0x1b0b0
+			>;
+		};
+
+		/* pins for tsc2046 pendown */
+		pinctrl_tsc2046: tsc2046grp {
+			fsl,pins = <
+				/* tsc2046 PENDOWN */
+				MX6QDL_PAD_GPIO_3__GPIO1_IO03 0x1b0b0
+			>;
+		};
+
+		pinctrl_qt1070_irq: qt1070_irqgrp {
+			fsl,pins = <
+				MX6QDL_PAD_KEY_COL1__GPIO4_IO08 0x1b0b0
+			>;
+		};
+
+		pinctrl_si1142_irq: si1142_irqgrp {
+			fsl,pins = <
+				MX6QDL_PAD_GPIO_0__GPIO1_IO00 0x1b0b0
+			>;
+		};
+
+		pinctrl_weim: weimnorgrp {
+			fsl,pins = <
+			MX6QDL_PAD_EIM_CS1__EIM_CS1_B		0x17051
+			MX6QDL_PAD_EIM_CS0__EIM_CS0_B		0x17051
+			MX6QDL_PAD_EIM_EB0__EIM_EB0_B		0x17051
+			MX6QDL_PAD_EIM_EB1__EIM_EB1_B		0x17051
+			MX6QDL_PAD_EIM_EB2__EIM_EB2_B		0x17051
+			MX6QDL_PAD_EIM_EB3__EIM_EB3_B		0x17051
+			MX6QDL_PAD_EIM_LBA__EIM_LBA_B		0x17051
+			MX6QDL_PAD_EIM_OE__EIM_OE_B			0x17051
+			MX6QDL_PAD_EIM_RW__EIM_RW			0x17051
+			MX6QDL_PAD_EIM_WAIT__GPIO5_IO00		0x17051
+			MX6QDL_PAD_EIM_BCLK__EIM_BCLK		0x17051
+			MX6QDL_PAD_CSI0_DATA_EN__EIM_DATA00	0x17049
+			MX6QDL_PAD_CSI0_VSYNC__EIM_DATA01	0x17049
+			MX6QDL_PAD_CSI0_DAT4__EIM_DATA02	0x17049
+			MX6QDL_PAD_CSI0_DAT5__EIM_DATA03	0x17049
+			MX6QDL_PAD_CSI0_DAT6__EIM_DATA04	0x17049
+			MX6QDL_PAD_CSI0_DAT7__EIM_DATA05	0x17049
+			MX6QDL_PAD_CSI0_DAT8__EIM_DATA06	0x17049
+			MX6QDL_PAD_CSI0_DAT9__EIM_DATA07	0x17049
+			MX6QDL_PAD_CSI0_DAT12__EIM_DATA08	0x17049
+			MX6QDL_PAD_CSI0_DAT13__EIM_DATA09	0x17049
+			MX6QDL_PAD_CSI0_DAT14__EIM_DATA10	0x17049
+			MX6QDL_PAD_CSI0_DAT15__EIM_DATA11	0x17049
+			MX6QDL_PAD_CSI0_DAT16__EIM_DATA12	0x17049
+			MX6QDL_PAD_CSI0_DAT17__EIM_DATA13	0x17049
+			MX6QDL_PAD_CSI0_DAT18__EIM_DATA14	0x17049
+			MX6QDL_PAD_CSI0_DAT19__EIM_DATA15	0x17049
+			MX6QDL_PAD_EIM_DA0__EIM_AD00		0x17051
+			MX6QDL_PAD_EIM_DA1__EIM_AD01		0x17051
+			MX6QDL_PAD_EIM_DA2__EIM_AD02		0x17051
+			MX6QDL_PAD_EIM_DA3__EIM_AD03		0x17051
+			MX6QDL_PAD_EIM_DA4__EIM_AD04		0x17051
+			MX6QDL_PAD_EIM_DA5__EIM_AD05		0x17051
+			MX6QDL_PAD_EIM_DA6__EIM_AD06		0x17051
+			MX6QDL_PAD_EIM_DA7__EIM_AD07		0x17051
+			MX6QDL_PAD_EIM_DA8__EIM_AD08		0x17051
+			MX6QDL_PAD_EIM_DA9__EIM_AD09		0x17051
+			MX6QDL_PAD_EIM_DA10__EIM_AD10		0x17051
+			MX6QDL_PAD_EIM_DA11__EIM_AD11		0x17051
+			MX6QDL_PAD_EIM_DA12__EIM_AD12		0x17051
+			MX6QDL_PAD_EIM_DA13__EIM_AD13		0x17051
+			MX6QDL_PAD_EIM_DA14__EIM_AD14		0x17051
+			MX6QDL_PAD_EIM_DA15__EIM_AD15		0x17051
+			MX6QDL_PAD_EIM_A16__EIM_ADDR16		0x17051
+			MX6QDL_PAD_EIM_A17__EIM_ADDR17		0x17051
+			MX6QDL_PAD_EIM_A18__EIM_ADDR18		0x17051
+			MX6QDL_PAD_EIM_A19__EIM_ADDR19		0x17051
+			MX6QDL_PAD_EIM_A20__EIM_ADDR20		0x17051
+			>;
+		};
+
+		pinctrl_ipu_disp: ipudisp1grp {
+			fsl,pins = <
+			MX6QDL_PAD_DI0_DISP_CLK__IPU1_DI0_DISP_CLK	0x00008
+			MX6QDL_PAD_DISP0_DAT0__IPU1_DISP0_DATA00	0x00008
+			MX6QDL_PAD_DISP0_DAT1__IPU1_DISP0_DATA01	0x00008
+			MX6QDL_PAD_DISP0_DAT2__IPU1_DISP0_DATA02	0x00008
+			MX6QDL_PAD_DISP0_DAT3__IPU1_DISP0_DATA03	0x00008
+			MX6QDL_PAD_DISP0_DAT4__IPU1_DISP0_DATA04	0x00008
+			MX6QDL_PAD_DISP0_DAT5__IPU1_DISP0_DATA05	0x00008
+			MX6QDL_PAD_DISP0_DAT6__IPU1_DISP0_DATA06	0x00008
+			MX6QDL_PAD_DISP0_DAT7__IPU1_DISP0_DATA07	0x00008
+			MX6QDL_PAD_DISP0_DAT8__IPU1_DISP0_DATA08	0x00008
+			MX6QDL_PAD_DISP0_DAT9__IPU1_DISP0_DATA09	0x00008
+			MX6QDL_PAD_DISP0_DAT10__IPU1_DISP0_DATA10	0x00008
+			MX6QDL_PAD_DISP0_DAT11__IPU1_DISP0_DATA11	0x00008
+			MX6QDL_PAD_DISP0_DAT12__IPU1_DISP0_DATA12	0x00008
+			MX6QDL_PAD_DISP0_DAT13__IPU1_DISP0_DATA13	0x00008
+			MX6QDL_PAD_DISP0_DAT14__IPU1_DISP0_DATA14	0x00008
+			MX6QDL_PAD_DISP0_DAT15__IPU1_DISP0_DATA15	0x00008
+			MX6QDL_PAD_DISP0_DAT16__IPU1_DISP0_DATA16	0x00008
+			MX6QDL_PAD_DISP0_DAT17__IPU1_DISP0_DATA17	0x00008
+			MX6QDL_PAD_DISP0_DAT18__IPU1_DISP0_DATA18	0x00008
+			MX6QDL_PAD_DISP0_DAT19__IPU1_DISP0_DATA19	0x00008
+			MX6QDL_PAD_DISP0_DAT20__IPU1_DISP0_DATA20	0x00008
+			MX6QDL_PAD_DISP0_DAT21__IPU1_DISP0_DATA21	0x00008
+			MX6QDL_PAD_DISP0_DAT22__IPU1_DISP0_DATA22	0x00008
+			MX6QDL_PAD_DISP0_DAT23__IPU1_DISP0_DATA23	0x00008
+			MX6QDL_PAD_DI0_PIN2__IPU1_DI0_PIN02		0x00008
+			MX6QDL_PAD_DI0_PIN3__IPU1_DI0_PIN03		0x00008
+			MX6QDL_PAD_DI0_PIN15__IPU1_DI0_PIN15	0x00008
+			>;
+		};
+
+		pinctrl_dio_spi: dio_spi_grp {
+			fsl,pins = <
+				MX6QDL_PAD_EIM_D19__GPIO3_IO19	0x1b0b0
+				MX6QDL_PAD_NANDF_D0__GPIO2_IO00	0x1b0b0
+			>;
+		};
+
+	};
+};
+
+&audmux {
+	pinctrl-names = "default";
+	pinctrl-0 = <&pinctrl_audmux>;
+	status = "okay";
+};
+
+&can1 {
+	pinctrl-names = "default";
+	pinctrl-0 = <&pinctrl_flexcan1>;
+	status = "okay";
+};
+
+&gpc {
+	/* use ldo-bypass, u-boot will check it and configure */
+	fsl,ldo-bypass = <1>;
+	fsl,wdog-reset = <1>;
+};
+
+&pwm3 {
+	pinctrl-names = "default";
+	pinctrl-0 = <&pinctrl_pwm3>;
+	#pwm-cells = <2>;
+	status = "okay";
+};
+
+&dcic1 {
+	dcic_id = <0>;
+	dcic_mux = "dcic-hdmi";
+	status = "okay";
+};
+
+&dcic2 {
+	dcic_id = <1>;
+	dcic_mux = "dcic-lvds1";
+	status = "okay";
+};
+
+&snvs_poweroff {
+	status = "okay";
+};
+
+&ssi1 {
+	fsl,mode = "i2s-slave";
+	status = "okay";
+};
+
+&uart1 {
+	pinctrl-names = "default";
+	pinctrl-0 = <&pinctrl_uart1>;
+	status = "okay";
+};
+
+&uart2 {
+	pinctrl-names = "default";
+	pinctrl-0 = <&pinctrl_uart2>;
+	status = "okay";
+	rs485-rts-active-high;
+	rs485-rx-during-tx;
+	uart-has-rtscts;
+	rts-gpios = <&gpio3 29 GPIO_ACTIVE_HIGH>;
+	linux,rs485-enabled-at-boot-time;
+};
+
+&usbh1 {
+	vbus-supply = <&reg_usb_h1_vbus>;
+	status = "okay";
+};
+
+&usbotg {
+	vbus-supply = <&reg_usb_otg_vbus>;
+	pinctrl-names = "default";
+	pinctrl-0 = <&pinctrl_usbotg>;
+	disable-over-current;
+	status = "okay";
+};
+
+&usdhc1 {
+	pinctrl-names = "default";
+	pinctrl-0 = <&pinctrl_usdhc1>;
+	/* cd-gpios = <&gpio7 9 GPIO_ACTIVE_LOW>; */
+	no-1-8-v;
+	keep-power-in-suspend;
+	enable-sdio-wakeup;
+	vmmc-supply = <&reg_3p3v>;
+	status = "okay";
+};
+
+&usdhc3 {
+	pinctrl-names = "default";
+	pinctrl-0 = <&pinctrl_usdhc3>;
+	non-removable;
+	bus-width = <8>;
+	no-1-8-v;
+	keep-power-in-suspend;
+	enable-sdio-wakeup;
+	vmmc-supply = <&reg_3p3v>;
+	status = "okay";
+};
+
+&bitbang_mdio0 {
+	pinctrl-names = "default", "sleep";
+	pinctrl-0 = <&gpio_bitbang_mdio_pins>;
+	pinctrl-1 = <&gpio_bitbang_mdio_sleep_pins>;
+	gpios = <&gpio1 31 0	/* 0: mdc  */
+	&gpio1 22 0>;			/* 1: mdio */
+	status = "okay";
+};
+
+&weim {
+	status = "okay";
+	pinctrl-names = "default";
+	pinctrl-0 = <&pinctrl_weim>;
+	#address-cells = <2>;
+	#size-cells = <1>;
+	ranges = <
+		0 0 0x08000000 0x04000000
+		1 0 0x0C000000 0x04000000 >;
+	fsl,weim-cs-gpr = <&gpr>;
+
+	UIO_NVRAM@1,0 {
+		compatible = "uio_pdrv_genirq";
+		reg = <1 0x0 0x20000>;
+		fsl,weim-cs-timing = <
+			0x007184B1 0x00001010
+			0x0F780000 0x00000008
+			0x04F00040 0x00000000 >;
+	};
+};
+
+&reg_arm {
+	vin-supply = <&sw1a_reg>;
+	regulator-allow-bypass;
+	regulator-min-microvolt = <1225000>;
+};
+
+&reg_soc {
+	vin-supply = <&sw1c_reg>;
+	regulator-allow-bypass;
+	regulator-min-microvolt = <1225000>;
+};
+
+&reg_pu {
+	regulator-allow-bypass;
+	regulator-min-microvolt = <1225000>;
+};
+
+&cpu0 {
+	operating-points = <
+		/* kHz  uV */
+		1200000 1400000
+		996000  1400000
+		852000  1400000
+		792000  1400000
+		396000  1400000
+	>;
+	fsl,soc-operating-points = <
+		/* ARM kHz SOC-PU uV */
+		1200000 1400000
+		996000  1400000
+		852000  1400000
+		792000  1400000
+		396000  1400000
+	>;
+};
+
+&ecspi4 {
+	di0_spi: dio_spi@1 {
+		pinctrl-names = "default";
+		pinctrl-0 = <&pinctrl_dio_spi>;
+		compatible = "dio_spi";
+		reg = <1>;
+		spi-cpol;
+		spi-max-frequency = <1000000>;
+		gpio-load = <&gpio3 19 GPIO_ACTIVE_HIGH>;
+		gpio-reset = <&gpio2 0 GPIO_ACTIVE_HIGH>;
+	};
+};
+
+&cpu0 {
+	arm-supply = <&sw1a_reg>;
+	soc-supply = <&sw1c_reg>;
+};
+
+&swcfg_ksz8863 {
+	status = "okay";
+};
+
+&bitbang_mdio0 {
+	status = "okay";
+};
+
+&ksz8863_switch {
+	status = "okay";
+};
+
+/delete-node/ &ipu1_di0_hdmi;
+/delete-node/ &hdmi_mux_0;
+/delete-node/ &ipu1_di1_hdmi;
+/delete-node/ &hdmi_mux_1;
+
+/delete-node/ &ipu2_di0_hdmi;
+/delete-node/ &hdmi_mux_2;
+/delete-node/ &ipu2_di1_hdmi;
+/delete-node/ &hdmi_mux_3;
diff --git a/arch/arm/boot/dts/pxc-nandparts.dtsi b/arch/arm/boot/dts/pxc-nandparts.dtsi
new file mode 100644
index 000000000000..034a880424f8
--- /dev/null
+++ b/arch/arm/boot/dts/pxc-nandparts.dtsi
@@ -0,0 +1,60 @@
+/*
+ * Copyright (C) 2012 Texas Instruments Incorporated - http://www.ti.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+&nand {
+		/* 4 x 128k MLOs */
+		partition@0 {
+			label = "mlo0";
+			reg = <0x0 0x20000>;
+		};
+
+		partition@1 {
+			label = "mlo1";
+			reg = <0x20000 0x20000>;
+		};
+
+		partition@2 {
+			label = "mlo2";
+			reg = <0x40000 0x20000>;
+		};
+
+		partition@3 {
+			label = "mlo3";
+			reg = <0x60000 0x20000>;
+		};
+
+		/* 16 x 128k: 4 x stage2 (4x128k) */
+		partition@4 {
+			label = "boot0";
+			reg = <0x80000 0x80000>;
+		};
+
+		partition@5 {
+			label = "boot1";
+			reg = <0x100000 0x80000>;
+		};
+
+		partition@6 {
+			label = "boot2";
+			reg = <0x180000 0x80000>;
+		};
+
+		partition@7 {
+			label = "boot3";
+			reg = <0x200000 0x80000>;
+		};
+
+		partition@8 {
+			label = "ubidata";
+			/*
+			 * Size 0x0 extends partition to
+			 * end of nand flash.
+			 */
+			reg = <0x280000 0x0>;
+		};
+};
diff --git a/arch/arm/boot/dts/stm32mp151-cc100-pinctrl.dtsi b/arch/arm/boot/dts/stm32mp151-cc100-pinctrl.dtsi
new file mode 100644
index 000000000000..83b48dc74211
--- /dev/null
+++ b/arch/arm/boot/dts/stm32mp151-cc100-pinctrl.dtsi
@@ -0,0 +1,490 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
+/*
+ * Copyright (C) STMicroelectronics 2017 - All Rights Reserved
+ * Author: Ludovic Barre <ludovic.barre@st.com> for STMicroelectronics.
+ */
+#include <dt-bindings/pinctrl/stm32-pinfunc.h>
+
+&pinctrl {
+	adc12_ain_pins_a: adc12-ain-0 {
+		pins {
+			pinmux = <STM32_PINMUX('C', 3, ANALOG)>, /* ADC1_INP13 -> ADC1-IN1 */
+					 <STM32_PINMUX('B', 1, ANALOG)>, /* ADC1_INP5  -> ADC1-RE1 */
+					 <STM32_PINMUX('A', 3, ANALOG)>, /* ADC1_INP15 -> ADC1-RE2 */
+					 <STM32_PINMUX('A', 6, ANALOG)>; /* ADC2_INP3  -> ADC2-IN1 */
+		};
+	};
+
+	dac_ch1_pins_a: dac-ch1-0 {
+		pins {
+			pinmux = <STM32_PINMUX('A', 4, ANALOG)>;
+		};
+	};
+
+	dac_ch2_pins_a: dac-ch2-0 {
+		pins {
+			pinmux = <STM32_PINMUX('A', 5, ANALOG)>;
+		};
+	};
+
+	eth1_pins_a: eth1_a-0 {
+		pins1 {
+			pinmux = <STM32_PINMUX('A', 1, AF11)>, /* ETH1_REF_CLK */
+					 <STM32_PINMUX('B', 13, AF11)>, /* ETH1_TXD1 */
+					 <STM32_PINMUX('G', 13, AF11)>; /* ETH1_TXD0 */
+			bias-disable;
+			drive-push-pull;
+			slew-rate = <1>;
+		};
+		pins2 {
+			pinmux = <STM32_PINMUX('A', 7, AF11)>, /* ETH1_CRS_DV */
+					 <STM32_PINMUX('C', 4, AF11)>, /* ETH1_RXD0 */
+					 <STM32_PINMUX('C', 5, AF11)>; /* ETH1_RXD1 */
+			bias-disable;
+		};
+		pins3 {
+			pinmux = <STM32_PINMUX('B', 11, AF11)>; /* ETH1_TX_EN */
+		};
+	};
+
+	eth1_sleep_pins_a: eth1_sleep_a-0 {
+		pins {
+			pinmux = <STM32_PINMUX('A', 1, ANALOG)>, /* ETH1_REF_CLK */
+					 <STM32_PINMUX('A', 7, ANALOG)>, /* ETH1_CRS_DV */
+					 <STM32_PINMUX('B', 11, ANALOG)>, /* ETH1_TX_EN */
+					 <STM32_PINMUX('B', 13, ANALOG)>, /* ETH1_TXD1 */
+					 <STM32_PINMUX('C', 4, ANALOG)>, /* ETH1_RXD0 */
+					 <STM32_PINMUX('C', 5, ANALOG)>, /* ETH1_RXD1 */
+					 <STM32_PINMUX('G', 13, ANALOG)>; /* ETH1_TXD0 */
+		};
+	};
+
+	i2c2_pins_a: i2c2-0 {
+		pins {
+			pinmux = <STM32_PINMUX('H', 4, AF4)>, /* I2C2_SCL */
+				 <STM32_PINMUX('G', 15, AF4)>; /* I2C2_SDA */
+			bias-disable;
+			drive-open-drain;
+			slew-rate = <0>;
+		};
+	};
+
+	i2c2_sleep_pins_a: i2c2-sleep-0 {
+		pins {
+			pinmux = <STM32_PINMUX('H', 4, ANALOG)>, /* I2C2_SCL */
+				 <STM32_PINMUX('G', 15, ANALOG)>; /* I2C2_SDA */
+		};
+	};
+
+	sdmmc1_b4_pins_a: sdmmc1-b4-0 {
+		pins1 {
+			pinmux = <STM32_PINMUX('C', 8, AF12)>, /* SDMMC1_D0 */
+				 <STM32_PINMUX('C', 9, AF12)>, /* SDMMC1_D1 */
+				 <STM32_PINMUX('E', 6, AF8)>, /* SDMMC1_D2 */
+				 <STM32_PINMUX('C', 11, AF12)>, /* SDMMC1_D3 */
+				 <STM32_PINMUX('D', 2, AF12)>; /* SDMMC1_CMD */
+			slew-rate = <1>;
+			drive-push-pull;
+			bias-disable;
+		};
+		pins2 {
+			pinmux = <STM32_PINMUX('C', 12, AF12)>; /* SDMMC1_CK */
+			slew-rate = <2>;
+			drive-push-pull;
+			bias-disable;
+		};
+	};
+
+	sdmmc1_b4_od_pins_a: sdmmc1-b4-od-0 {
+		pins1 {
+			pinmux = <STM32_PINMUX('C', 8, AF12)>, /* SDMMC1_D0 */
+				 <STM32_PINMUX('C', 9, AF12)>, /* SDMMC1_D1 */
+				 <STM32_PINMUX('E', 6, AF8)>, /* SDMMC1_D2 */
+				 <STM32_PINMUX('C', 11, AF12)>; /* SDMMC1_D3 */
+			slew-rate = <1>;
+			drive-push-pull;
+			bias-disable;
+		};
+		pins2 {
+			pinmux = <STM32_PINMUX('C', 12, AF12)>; /* SDMMC1_CK */
+			slew-rate = <2>;
+			drive-push-pull;
+			bias-disable;
+		};
+		pins3 {
+			pinmux = <STM32_PINMUX('D', 2, AF12)>; /* SDMMC1_CMD */
+			slew-rate = <1>;
+			drive-open-drain;
+			bias-disable;
+		};
+	};
+
+	sdmmc1_b4_sleep_pins_a: sdmmc1-b4-sleep-0 {
+		pins {
+			pinmux = <STM32_PINMUX('C', 8, ANALOG)>, /* SDMMC1_D0 */
+				 <STM32_PINMUX('C', 9, ANALOG)>, /* SDMMC1_D1 */
+				 <STM32_PINMUX('E', 6, ANALOG)>, /* SDMMC1_D2 */
+				 <STM32_PINMUX('C', 11, ANALOG)>, /* SDMMC1_D3 */
+				 <STM32_PINMUX('C', 12, ANALOG)>, /* SDMMC1_CK */
+				 <STM32_PINMUX('D', 2, ANALOG)>; /* SDMMC1_CMD */
+		};
+	};
+
+	sdmmc1_dir_pins_a: sdmmc1-dir-0 {
+		pins1 {
+			pinmux = <STM32_PINMUX('F', 2, AF11)>, /* SDMMC1_D0DIR */
+				 <STM32_PINMUX('C', 7, AF8)>, /* SDMMC1_D123DIR */
+				 <STM32_PINMUX('B', 9, AF11)>; /* SDMMC1_CDIR */
+			slew-rate = <1>;
+			drive-push-pull;
+			bias-pull-up;
+		};
+		pins2{
+			pinmux = <STM32_PINMUX('E', 4, AF8)>; /* SDMMC1_CKIN */
+			bias-pull-up;
+		};
+	};
+
+	sdmmc1_dir_sleep_pins_a: sdmmc1-dir-sleep-0 {
+		pins {
+			pinmux = <STM32_PINMUX('F', 2, ANALOG)>, /* SDMMC1_D0DIR */
+				 <STM32_PINMUX('C', 7, ANALOG)>, /* SDMMC1_D123DIR */
+				 <STM32_PINMUX('B', 9, ANALOG)>, /* SDMMC1_CDIR */
+				 <STM32_PINMUX('E', 4, ANALOG)>; /* SDMMC1_CKIN */
+		};
+	};
+
+	sdmmc2_b4_pins_a: sdmmc2-b4-0 {
+		pins1 {
+			pinmux = <STM32_PINMUX('B', 14, AF9)>, /* SDMMC2_D0 */
+				 <STM32_PINMUX('B', 15, AF9)>, /* SDMMC2_D1 */
+				 <STM32_PINMUX('B', 3, AF9)>, /* SDMMC2_D2 */
+				 <STM32_PINMUX('B', 4, AF9)>, /* SDMMC2_D3 */
+				 <STM32_PINMUX('G', 6, AF10)>; /* SDMMC2_CMD */
+			slew-rate = <1>;
+			drive-push-pull;
+			bias-pull-up;
+		};
+		pins2 {
+			pinmux = <STM32_PINMUX('E', 3, AF9)>; /* SDMMC2_CK */
+			slew-rate = <2>;
+			drive-push-pull;
+			bias-pull-up;
+		};
+	};
+
+	sdmmc2_b4_od_pins_a: sdmmc2-b4-od-0 {
+		pins1 {
+			pinmux = <STM32_PINMUX('B', 14, AF9)>, /* SDMMC2_D0 */
+				 <STM32_PINMUX('B', 15, AF9)>, /* SDMMC2_D1 */
+				 <STM32_PINMUX('B', 3, AF9)>, /* SDMMC2_D2 */
+				 <STM32_PINMUX('B', 4, AF9)>; /* SDMMC2_D3 */
+			slew-rate = <1>;
+			drive-push-pull;
+			bias-pull-up;
+		};
+		pins2 {
+			pinmux = <STM32_PINMUX('E', 3, AF9)>; /* SDMMC2_CK */
+			slew-rate = <2>;
+			drive-push-pull;
+			bias-pull-up;
+		};
+		pins3 {
+			pinmux = <STM32_PINMUX('G', 6, AF10)>; /* SDMMC2_CMD */
+			slew-rate = <1>;
+			drive-open-drain;
+			bias-pull-up;
+		};
+	};
+
+	sdmmc2_b4_sleep_pins_a: sdmmc2-b4-sleep-0 {
+		pins {
+			pinmux = <STM32_PINMUX('B', 14, ANALOG)>, /* SDMMC2_D0 */
+				 <STM32_PINMUX('B', 15, ANALOG)>, /* SDMMC2_D1 */
+				 <STM32_PINMUX('B', 3, ANALOG)>, /* SDMMC2_D2 */
+				 <STM32_PINMUX('B', 4, ANALOG)>, /* SDMMC2_D3 */
+				 <STM32_PINMUX('E', 3, ANALOG)>, /* SDMMC2_CK */
+				 <STM32_PINMUX('G', 6, ANALOG)>; /* SDMMC2_CMD */
+		};
+	};
+
+	sdmmc2_b4_pins_b: sdmmc2-b4-1 {
+		pins1 {
+			pinmux = <STM32_PINMUX('B', 14, AF9)>, /* SDMMC2_D0 */
+				 <STM32_PINMUX('B', 15, AF9)>, /* SDMMC2_D1 */
+				 <STM32_PINMUX('B', 3, AF9)>, /* SDMMC2_D2 */
+				 <STM32_PINMUX('B', 4, AF9)>, /* SDMMC2_D3 */
+				 <STM32_PINMUX('G', 6, AF10)>; /* SDMMC2_CMD */
+			slew-rate = <1>;
+			drive-push-pull;
+			bias-disable;
+		};
+		pins2 {
+			pinmux = <STM32_PINMUX('E', 3, AF9)>; /* SDMMC2_CK */
+			slew-rate = <2>;
+			drive-push-pull;
+			bias-disable;
+		};
+	};
+
+	sdmmc2_b4_od_pins_b: sdmmc2-b4-od-1 {
+		pins1 {
+			pinmux = <STM32_PINMUX('B', 14, AF9)>, /* SDMMC2_D0 */
+				 <STM32_PINMUX('B', 15, AF9)>, /* SDMMC2_D1 */
+				 <STM32_PINMUX('B', 3, AF9)>, /* SDMMC2_D2 */
+				 <STM32_PINMUX('B', 4, AF9)>; /* SDMMC2_D3 */
+			slew-rate = <1>;
+			drive-push-pull;
+			bias-disable;
+		};
+		pins2 {
+			pinmux = <STM32_PINMUX('E', 3, AF9)>; /* SDMMC2_CK */
+			slew-rate = <2>;
+			drive-push-pull;
+			bias-disable;
+		};
+		pins3 {
+			pinmux = <STM32_PINMUX('G', 6, AF10)>; /* SDMMC2_CMD */
+			slew-rate = <1>;
+			drive-open-drain;
+			bias-disable;
+		};
+	};
+
+	sdmmc2_d47_pins_a: sdmmc2-d47-0 {
+		pins {
+			pinmux = <STM32_PINMUX('E', 4, AF9)>, /* SDMMC2_D4 */
+				 <STM32_PINMUX('B', 9, AF10)>, /* SDMMC2_D5 */
+				 <STM32_PINMUX('E', 5, AF9)>, /* SDMMC2_D6 */
+				 <STM32_PINMUX('D', 3, AF9)>; /* SDMMC2_D7 */
+			slew-rate = <1>;
+			drive-push-pull;
+			bias-pull-up;
+		};
+	};
+
+	sdmmc2_d47_sleep_pins_a: sdmmc2-d47-sleep-0 {
+		pins {
+			pinmux = <STM32_PINMUX('E', 4, ANALOG)>, /* SDMMC2_D4 */
+				 <STM32_PINMUX('B', 9, ANALOG)>, /* SDMMC2_D5 */
+				 <STM32_PINMUX('E', 5, ANALOG)>, /* SDMMC2_D6 */
+				 <STM32_PINMUX('D', 3, ANALOG)>; /* SDMMC2_D7 */
+		};
+	};
+
+	spi5_pins_a: spi5-0 {
+		pins1 {
+			pinmux = <STM32_PINMUX('F', 7, AF5)>, /* SPI5_SCK */
+				 <STM32_PINMUX('J', 10, AF5)>; /* SPI5_MOSI */
+			bias-disable;
+			drive-push-pull;
+			slew-rate = <0>;
+		};
+
+		pins2 {
+			pinmux = <STM32_PINMUX('H', 7, AF5)>; /* SPI5_MISO */
+			bias-disable;
+		};
+	};
+
+	uart4_pins_a: uart4_a-0 {
+		pins1 {
+			pinmux = <STM32_PINMUX('G', 11, AF6)>; /* UART4_TX */
+			bias-disable;
+			drive-push-pull;
+			slew-rate = <0>;
+		};
+		pins2 {
+			pinmux = <STM32_PINMUX('B', 2, AF8)>; /* UART4_RX */
+			bias-disable;
+		};
+	};
+
+	uart4_idle_pins_a: uart4_idle_pins_a {
+		pins1 {
+			pinmux = <STM32_PINMUX('G', 11, AF6)>; /* UART4_TX */
+		};
+		pins2 {
+			pinmux = <STM32_PINMUX('B', 2, AF8)>; /* UART4_RX */
+			bias-disable;
+		};
+	};
+
+	uart4_sleep_pins_a: uart4_sleep_a-0 {
+		pins {
+			pinmux = <STM32_PINMUX('G', 11, ANALOG)>, /* UART4_TX */
+					 <STM32_PINMUX('B', 2, ANALOG)>; /* UART4_RX */
+		};
+	};
+
+	usbotg_hs_pins_a: usbotg-hs-0 {
+		pins {
+			pinmux = <STM32_PINMUX('A', 10, ANALOG)>; /* OTG_ID */
+		};
+	};
+
+	i2c4_pins_a: i2c4-0 {
+		pins {
+			pinmux = <STM32_PINMUX('H', 11, AF4)>, /* I2C4_SCL */
+					 <STM32_PINMUX('H', 12, AF4)>; /* I2C4_SDA */
+			bias-disable;
+			drive-open-drain;
+			slew-rate = <0>;
+		};
+	};
+
+	i2c4_pins_a_sleep: i2c4-1 {
+		pins {
+			pinmux = <STM32_PINMUX('H', 11, AF4)>, /* I2C4_SCL */
+					 <STM32_PINMUX('H', 12, AF4)>; /* I2C4_SDA */
+		};
+	};
+
+	/* FMC pin configuration inspired by arch/arm/boot/dts/stm32mp15-pinctrl.dtsi */
+	fmc_pins_a: fmc-0 {
+		pins1 {
+			pinmux = <STM32_PINMUX('D', 7, AF12)>, /* FMC.NCS */
+				<STM32_PINMUX('D', 4, AF12)>, /* FMC.NOE */
+				<STM32_PINMUX('D', 5, AF12)>, /* FMC.NRW */
+				<STM32_PINMUX('E', 0, AF12)>, /* FMC.NEB0 */
+				<STM32_PINMUX('E', 1, AF12)>, /* FMC.NEB1 */
+				<STM32_PINMUX('F', 0, AF12)>, /* FMC.A0 */
+				<STM32_PINMUX('F', 1, AF12)>, /* FMC.A1 */
+				<STM32_PINMUX('F', 2, AF12)>, /* FMC.A2 */
+				<STM32_PINMUX('F', 3, AF12)>, /* FMC.A3 */
+				<STM32_PINMUX('F', 4, AF12)>, /* FMC.A4 */
+				<STM32_PINMUX('F', 5, AF12)>, /* FMC.A5 */
+				<STM32_PINMUX('F', 12, AF12)>, /* FMC.A6 */
+				<STM32_PINMUX('F', 13, AF12)>, /* FMC.A7 */
+				<STM32_PINMUX('F', 14, AF12)>, /* FMC.A8 */
+				<STM32_PINMUX('F', 15, AF12)>, /* FMC.A9 */
+				<STM32_PINMUX('G', 0, AF12)>, /* FMC.A10 */
+				<STM32_PINMUX('G', 1, AF12)>, /* FMC.A11 */
+				<STM32_PINMUX('G', 2, AF12)>, /* FMC.A12 */
+				<STM32_PINMUX('G', 3, AF12)>, /* FMC.A13 */
+				<STM32_PINMUX('G', 4, AF12)>, /* FMC.A14 */
+				<STM32_PINMUX('G', 5, AF12)>, /* FMC.A15 */
+				<STM32_PINMUX('D', 11, AF12)>, /* FMC.A16 */
+				<STM32_PINMUX('D', 12, AF12)>, /* FMC.A17 */
+				<STM32_PINMUX('D', 13, AF12)>, /* FMC.A18 */
+				<STM32_PINMUX('D', 14, AF12)>, /* FMC.D0 */
+				<STM32_PINMUX('D', 15, AF12)>, /* FMC.D1 */
+				<STM32_PINMUX('D', 0, AF12)>, /* FMC.D2 */
+				<STM32_PINMUX('D', 1, AF12)>, /* FMC.D3 */
+				<STM32_PINMUX('E', 7, AF12)>, /* FMC.D4 */
+				<STM32_PINMUX('E', 8, AF12)>, /* FMC.D5 */
+				<STM32_PINMUX('E', 9, AF12)>, /* FMC.D6 */
+				<STM32_PINMUX('E', 10, AF12)>, /* FMC.D7 */
+				<STM32_PINMUX('E', 11, AF12)>, /* FMC.D8 */
+				<STM32_PINMUX('E', 12, AF12)>, /* FMC.D9 */
+				<STM32_PINMUX('E', 13, AF12)>, /* FMC.D10 */
+				<STM32_PINMUX('E', 14, AF12)>, /* FMC.D11 */
+				<STM32_PINMUX('E', 15, AF12)>, /* FMC.D12 */
+				<STM32_PINMUX('D', 8, AF12)>, /* FMC.D13 */
+				<STM32_PINMUX('D', 9, AF12)>, /* FMC.D14 */
+				<STM32_PINMUX('D', 10, AF12)>; /* FMC.D15 */
+			bias-disable;
+			drive-push-pull;
+			slew-rate = <1>;
+		};
+		pins2 {
+                        pinmux = <STM32_PINMUX('D', 6, AF12)>; /* FMC_NWAIT */
+                        bias-pull-up;
+                };
+	};
+
+	fmc_sleep_pins_a: fmc-sleep-0 {
+		pins1 {
+			pinmux = <STM32_PINMUX('D', 7, ANALOG)>, /* FMC.NCS */
+				<STM32_PINMUX('D', 4, ANALOG)>, /* FMC.NOE */
+				<STM32_PINMUX('D', 5, ANALOG)>, /* FMC.NRW */
+				<STM32_PINMUX('E', 0, ANALOG)>, /* FMC.NEB0 */
+				<STM32_PINMUX('E', 1, ANALOG)>, /* FMC.NEB1 */
+				<STM32_PINMUX('F', 0, ANALOG)>, /* FMC.A0 */
+				<STM32_PINMUX('F', 1, ANALOG)>, /* FMC.A1 */
+				<STM32_PINMUX('F', 2, ANALOG)>, /* FMC.A2 */
+				<STM32_PINMUX('F', 3, ANALOG)>, /* FMC.A3 */
+				<STM32_PINMUX('F', 4, ANALOG)>, /* FMC.A4 */
+				<STM32_PINMUX('F', 5, ANALOG)>, /* FMC.A5 */
+				<STM32_PINMUX('F', 12, ANALOG)>, /* FMC.A6 */
+				<STM32_PINMUX('F', 13, ANALOG)>, /* FMC.A7 */
+				<STM32_PINMUX('F', 14, ANALOG)>, /* FMC.A8 */
+				<STM32_PINMUX('F', 15, ANALOG)>, /* FMC.A9 */
+				<STM32_PINMUX('G', 0, ANALOG)>, /* FMC.A10 */
+				<STM32_PINMUX('G', 1, ANALOG)>, /* FMC.A11 */
+				<STM32_PINMUX('G', 2, ANALOG)>, /* FMC.A12 */
+				<STM32_PINMUX('G', 3, ANALOG)>, /* FMC.A13 */
+				<STM32_PINMUX('G', 4, ANALOG)>, /* FMC.A14 */
+				<STM32_PINMUX('G', 5, ANALOG)>, /* FMC.A15 */
+				<STM32_PINMUX('D', 11, ANALOG)>, /* FMC.A16 */
+				<STM32_PINMUX('D', 12, ANALOG)>, /* FMC.A17 */
+				<STM32_PINMUX('D', 13, ANALOG)>, /* FMC.A18 */
+				<STM32_PINMUX('D', 14, ANALOG)>, /* FMC.D0 */
+				<STM32_PINMUX('D', 15, ANALOG)>, /* FMC.D1 */
+				<STM32_PINMUX('D', 0, ANALOG)>, /* FMC.D2 */
+				<STM32_PINMUX('D', 1, ANALOG)>, /* FMC.D3 */
+				<STM32_PINMUX('E', 7, ANALOG)>, /* FMC.D4 */
+				<STM32_PINMUX('E', 8, ANALOG)>, /* FMC.D5 */
+				<STM32_PINMUX('E', 9, ANALOG)>, /* FMC.D6 */
+				<STM32_PINMUX('E', 10, ANALOG)>, /* FMC.D7 */
+				<STM32_PINMUX('E', 11, ANALOG)>, /* FMC.D8 */
+				<STM32_PINMUX('E', 12, ANALOG)>, /* FMC.D9 */
+				<STM32_PINMUX('E', 13, ANALOG)>, /* FMC.D10 */
+				<STM32_PINMUX('E', 14, ANALOG)>, /* FMC.D11 */
+				<STM32_PINMUX('E', 15, ANALOG)>, /* FMC.D12 */
+				<STM32_PINMUX('D', 8, ANALOG)>, /* FMC.D13 */
+				<STM32_PINMUX('D', 9, ANALOG)>, /* FMC.D14 */
+				<STM32_PINMUX('D', 10, ANALOG)>, /* FMC.D15 */
+				<STM32_PINMUX('D', 6, ANALOG)>; /* FMC_NWAIT */
+		};
+	};
+};
+
+&pinctrl_z {
+	usart1_pins_a: usart1-0 {
+		pins1 {
+			pinmux = <STM32_PINMUX('Z', 7, AF7)>; /* USART1_TX */
+			bias-disable;
+			drive-push-pull;
+			slew-rate = <0>;
+		};
+		pins2 {
+			pinmux = <STM32_PINMUX('Z', 6, AF7)>,  /* USART1_RX  */
+					 <STM32_PINMUX('Z', 5, AF7)>;  /* USART1_RTS DIR */
+			bias-disable;
+		};
+		/* The UART CTS (PZ3) is connected to RS485-NRE. The linux
+		 * drivers does not support RS485-NRE, only driver enable
+		 * (RS485-DIR) is used. The RS485-DIR is connected to both DE
+		 * and nRE -> therefore, the CTS (RS485-NRE) is forced to 0.
+		 */
+		pins3 {
+			pinmux = <STM32_PINMUX('Z', 3, GPIO)>;  /* USART1_CTS NRE */
+			output-low;
+		};
+	};
+
+	usart1_idle_pins_a: usart1_idle_pins_a {
+		pins1 {
+			pinmux = <STM32_PINMUX('Z', 7, AF7)>; /* USART1_TX */
+		};
+		pins2 {
+			pinmux = <STM32_PINMUX('Z', 6, AF7)>,  /* USART1_RX  */
+					 <STM32_PINMUX('Z', 5, AF7)>;  /* USART1_RTS DIR */
+		};
+		/* see usart1_pins_a */
+		pins3 {
+			pinmux = <STM32_PINMUX('Z', 3, GPIO)>;  /* USART1_CTS NRE */
+			output-low;
+		};
+	};
+
+	usart1_sleep_pins_a: usart1_sleep_a-0 {
+		pins {
+			pinmux = <STM32_PINMUX('Z', 7, ANALOG)>, /* USART1_TX */
+					 <STM32_PINMUX('Z', 6, ANALOG)>, /* USART1_RX */
+					 <STM32_PINMUX('Z', 3, ANALOG)>, /* USART1_CTS NRE */
+					 <STM32_PINMUX('Z', 5, ANALOG)>; /* USART1_RTS DIR */
+		};
+	};
+};
diff --git a/arch/arm/boot/dts/stm32mp151-cc100.dts b/arch/arm/boot/dts/stm32mp151-cc100.dts
new file mode 100644
index 000000000000..21516714e757
--- /dev/null
+++ b/arch/arm/boot/dts/stm32mp151-cc100.dts
@@ -0,0 +1,107 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
+/*
+ * Copyright (C) STMicroelectronics 2017 - All Rights Reserved
+ * Author: Ludovic Barre <ludovic.barre@st.com> for STMicroelectronics.
+ */
+/dts-v1/;
+
+#include "stm32mp151.dtsi"
+#include "stm32mp151-cc100.dtsi"
+#include "wago-devconf.dtsi"
+
+/ {
+	model = "CC100-751-9301";
+	compatible = "wago,stm32mp151-cc100-751_9301-1011", "wago,stm32mp151-cc100", "st,stm32mp151";
+
+	memory@c0000000 {
+		reg = <0xC0000000 0x20000000>; /* 512MB RAM */
+	};
+
+	chosen {
+		stdout-path = " serial0:115200n8";
+	};
+
+	usb_phy_tuning: usb-phy-tuning {
+		st,hs-dc-level = <2>;
+		st,fs-rftime-tuning;
+		st,hs-rftime-reduction;
+		st,hs-current-trim = <15>;
+		st,hs-impedance-trim = <1>;
+		st,squelch-level = <3>;
+		st,hs-rx-offset = <2>;
+		st,no-lsfs-sc;
+	};
+
+};
+
+&wsysinit {
+	board,variant = "CC100";
+	status = "okay";
+};
+
+&spi5 {
+	pinctrl-names = "default";
+	pinctrl-0 = <&spi5_pins_a>;
+	cs-gpios = <&gpioh 5 0>;
+	status = "okay";
+	/delete-property/dmas;
+	/delete-property/dma-names;
+
+	din0_spi: din_spi@0 {
+		compatible = "din_spi";
+		reg = <0>;
+		spi-cpol;
+		spi-max-frequency = <1000000>;
+		gpio-load = <&gpioz 1 GPIO_ACTIVE_LOW>;
+		gpio-tok = <&gpioz 0 GPIO_ACTIVE_LOW>;
+		leds-gpios = <&gpioj 13 GPIO_ACTIVE_HIGH>,
+			<&gpioj 14 GPIO_ACTIVE_HIGH>,
+			<&gpiok 3 GPIO_ACTIVE_HIGH>,
+			<&gpiok 7 GPIO_ACTIVE_HIGH>,
+			<&gpiok 6 GPIO_ACTIVE_HIGH>,
+			<&gpioi 14 GPIO_ACTIVE_HIGH>,
+			<&gpioi 12 GPIO_ACTIVE_HIGH>,
+			<&gpioi 13 GPIO_ACTIVE_HIGH>;
+	};
+};
+
+&usbotg_hs {
+        compatible = "st,stm32mp15-hsotg", "snps,dwc2";
+        pinctrl-names = "default";
+        pinctrl-0 = <&usbotg_hs_pins_a>;   /* configure OTG_ID pin */
+        phys = <&usbphyc_port1 0>;         /* 0: UTMI switch selects the OTG controller */
+        phy-names = "usb2-phy";
+        vbus-supply = <&vbus_otg>;
+        dr_mode = "peripheral";
+        status = "okay";
+};
+
+&usbphyc {
+	status = "okay";
+};
+
+&usbphyc_port0 {
+	st,phy-tuning = <&usb_phy_tuning>;
+	phy-supply = <&vdd_usb>;
+	vdda1v1-supply = <&reg11>;
+	vdda1v8-supply = <&reg18>;
+};
+
+&usbphyc_port1 {
+	st,phy-tuning = <&usb_phy_tuning>;
+	phy-supply = <&vdd_usb>;
+	vdda1v1-supply = <&reg11>;
+	vdda1v8-supply = <&reg18>;
+};
+
+&bitbang_mdio0 {
+	status = "okay";
+};
+
+&ksz8863_switch {
+	status = "okay";
+};
+
+&swcfg_ksz8863 {
+	status = "okay";
+};
diff --git a/arch/arm/boot/dts/stm32mp151-cc100.dtsi b/arch/arm/boot/dts/stm32mp151-cc100.dtsi
new file mode 100644
index 000000000000..012ef60aa7c9
--- /dev/null
+++ b/arch/arm/boot/dts/stm32mp151-cc100.dtsi
@@ -0,0 +1,663 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+/*
+ * Copyright (C) 2020 elrest GmbH
+ */
+
+#include "stm32mp151.dtsi"
+#include "stm32mp151-cc100-pinctrl.dtsi"
+#include "stm32mp15xxaa-pinctrl.dtsi"
+#include <dt-bindings/gpio/gpio.h>
+#include <dt-bindings/mfd/st,stpmic1.h>
+#include <dt-bindings/input/gpio-keys.h>
+#include <dt-bindings/input/input.h>
+
+/ {
+	aliases {
+		serial0 = &uart4;
+		serial1 = &usart1;
+		ethernet0 = &ethernet0;
+		mdio-gpio0 = &bitbang_mdio0;
+		i2c0 = &i2c4;
+		i2c1 = &i2c2;
+	};
+
+	reserved-memory {
+		#address-cells = <1>;
+		#size-cells = <1>;
+		ranges;
+
+		retram: retram@0x38000000 {
+			compatible = "shared-dma-pool";
+			reg = <0x38000000 0x10000>;
+			no-map;
+		};
+
+		mcuram: mcuram@0x30000000 {
+			compatible = "shared-dma-pool";
+			reg = <0x30000000 0x40000>;
+			no-map;
+		};
+
+		mcuram2: mcuram2@0x10000000 {
+			compatible = "shared-dma-pool";
+			reg = <0x10000000 0x40000>;
+			no-map;
+		};
+
+		vdev0vring0: vdev0vring0@10040000 {
+			compatible = "shared-dma-pool";
+			reg = <0x10040000 0x2000>;
+			no-map;
+		};
+
+		vdev0vring1: vdev0vring1@10042000 {
+			compatible = "shared-dma-pool";
+			reg = <0x10042000 0x2000>;
+			no-map;
+		};
+
+		vdev0buffer: vdev0buffer@10044000 {
+			compatible = "shared-dma-pool";
+			reg = <0x10044000 0x4000>;
+			no-map;
+		};
+
+	};
+
+	sram: sram@10050000 {
+		compatible = "mmio-sram";
+		reg = <0x10050000 0x10000>;
+		#address-cells = <1>;
+		#size-cells = <1>;
+		ranges = <0 0x10050000 0x10000>;
+
+		dma_pool: dma_pool@0 {
+			reg = <0x0 0x10000>;
+			pool;
+		};
+	};
+
+	led {
+		compatible = "gpio-leds";
+		u1-red {
+			label = "u1-red";
+			gpios = <&gpioi 10 GPIO_ACTIVE_HIGH>;
+			default-state = "off";
+		};
+		u1-green {
+			label = "u1-green";
+			gpios = <&gpioj 1 GPIO_ACTIVE_HIGH>;
+			default-state = "off";
+		};
+		sys-red {
+			label = "sys-red";
+			gpios = <&gpioa 13 GPIO_ACTIVE_LOW>;
+			linux,default-trigger = "timer";
+		};
+		sys-green {
+			label = "sys-green";
+			gpios = <&gpioa 14 GPIO_ACTIVE_LOW>;
+			linux,default-trigger = "timer";
+		};
+		run-red {
+			label = "run-red";
+			gpios = <&gpiob 12 GPIO_ACTIVE_HIGH>;
+			default-state = "off";
+		};
+		run-green {
+			label = "run-green";
+			gpios = <&gpiob 0 GPIO_ACTIVE_HIGH>;
+			default-state = "off";
+		};
+		led-mmc {
+			label = "led-mmc";
+			gpios = <&gpiog 10 GPIO_ACTIVE_HIGH>;
+			linux,default-trigger = "mmc0";
+		};
+	};
+
+	PAC-Operating-Mode-Switch {
+		compatible = "gpio-keys";
+		status = "okay";
+		#address-cells = <1>;
+		#size-cells = <0>;
+		autorepeat;
+
+		run {
+			label = "RUN";
+			gpios = <&gpioj 6 GPIO_ACTIVE_LOW>; /* GPIO J 6 */
+			linux,code = <1>;
+			linux,input-type = <EV_SW>;
+			debounce-interval = <1>; /* debounce in msecs */
+		};
+
+		stop {
+			label = "STOP";
+			gpios = <&gpioa 8 GPIO_ACTIVE_LOW>; /* GPIO A 8 */
+			linux,code = <2>;
+			linux,input-type = <EV_SW>;
+			debounce-interval = <1>; /* debounce in msecs */
+		};
+
+		reset {
+			label = "RESET";
+			gpios = <&gpioi 11 GPIO_ACTIVE_LOW>; /* GPIO I 11 */
+			linux,code = <3>;
+			linux,input-type = <1>;
+			debounce-interval = <1>; /* debounce in msecs */
+		};
+
+		reset_all {
+			label = "RESET_ALL";
+			gpios = <&gpioz 4 GPIO_ACTIVE_LOW>; /* GPIO Z 4 */
+			linux,code = <4>;
+			linux,input-type = <1>;
+			debounce-interval = <1>; /* debounce in msecs */
+		};
+	};
+
+	sd_switch: regulator-sd_switch {
+		compatible = "regulator-fixed";
+		regulator-name = "sd_switch";
+		regulator-min-microvolt = <3300000>;
+		regulator-max-microvolt = <3300000>;
+		regulator-type = "voltage";
+		regulator-always-on;
+		enable-active-high;
+
+		gpio = <&gpioi 4 GPIO_ACTIVE_HIGH>;
+	};
+
+	vrs5v: regulator-rs485-5v {
+		compatible = "regulator-fixed";
+		regulator-name = "rs485-5v";
+		regulator-min-microvolt = <5000000>;
+		regulator-max-microvolt = <5000000>;
+		regulator-type = "voltage";
+		regulator-always-on;
+		enable-active-high;
+
+		gpio = <&gpioh 14 GPIO_ACTIVE_HIGH>;
+	};
+
+	vref: regulator-vref {
+		compatible = "regulator-fixed";
+		regulator-name = "vref";
+		regulator-min-microvolt = <2500000>;
+		regulator-max-microvolt = <2500000>;
+		regulator-type = "voltage";
+		regulator-always-on;
+		vin-supply = <&vdda>;
+	};
+
+	wsysinit: wsysinit_init {
+		compatible = "wago,sysinit";
+		add-sysfs-entries;
+
+		tty,service   = "ttySTM0";
+		tty,rs232-485 = "ttySTM1";
+
+		/* sysclock adjustments, empirical values */
+		adjtimex,tick = <10000>;
+		adjtimex,frequency = <200000>;
+	};
+
+	dout_drv {
+		compatible = "elrest,dout_drv";
+		douts-gpios = <&gpioh 10 GPIO_ACTIVE_HIGH>,     /* bit 0 */
+			<&gpioi 8  GPIO_ACTIVE_HIGH>,
+			<&gpioi 2  GPIO_ACTIVE_HIGH>,
+			<&gpioi 6  GPIO_ACTIVE_HIGH>;
+		leds-gpios = <&gpioj 15 GPIO_ACTIVE_HIGH>,      /* bit 0 LED */
+			<&gpioj 12 GPIO_ACTIVE_HIGH>,
+			<&gpiok 5 GPIO_ACTIVE_HIGH>,
+			<&gpiok 4 GPIO_ACTIVE_HIGH>;
+		status = "okay";
+	};
+
+	bitbang_mdio0: gpio_mdio {
+		compatible = "virtual,mdio-gpio";
+		#address-cells = <1>;
+		#size-cells = <0>;
+		gpios = <
+			&gpioc 1 0	/* 0: mdc  */
+			&gpioa 2 0	/* 1: mdio */
+		>;
+		status = "disabled";
+	};
+
+	swcfg_ksz8863: swcfg_ksz8863 {
+		compatible = "swcfg,ksz8863";
+		status = "disabled";
+
+		swcfg,mii-bus = <&bitbang_mdio0>;
+		swcfg,alias = "ksz8863";
+		swcfg,cpu_port = <2>;
+		swcfg,ports = <3>;
+		swcfg,vlans = <16>;
+		swcfg,switch = <&ksz8863_switch>;
+	};
+
+};
+
+&bitbang_mdio0 {
+	ksz8863_switch: switch@0 {
+		compatible = "micrel,ksz8863";
+		//micrel,rmii-reference-clock-select-25mhz;
+
+		#address-cells = <1>;
+		#size-cells = <0>;
+
+		ksz,reset-gpio =  <&gpiog 7 GPIO_ACTIVE_LOW>;
+
+		reg = <0>;
+		dsa,member = <0 0>;
+		dsa,enable-on-boot;
+
+		ksz,reset-switch;
+		//ksz,disable-internal-ldo;
+
+		phy-mode = "rmii";
+
+		interrupt-parent = <&gpiog>;
+		interrupts = <12 IRQ_TYPE_LEVEL_LOW>;
+
+		status = "okay";
+
+		ports {
+			#address-cells = <1>;
+			#size-cells = <0>;
+
+			port@0 {
+				reg = <1>;
+				label = "ethX1";
+				phy-mode = "rmii";
+				phy-pwrdown;
+			};
+
+			port@1 {
+				reg = <2>;
+				label = "ethX2";
+				phy-mode = "rmii";
+				phy-pwrdown;
+			};
+
+			port@2 {
+				reg = <3>;
+				label = "cpu";
+				phy-mode = "rmii";
+				ethernet = <&ethernet0>;
+				fixed-link {
+					speed = <100>;
+					full-duplex;
+				};
+			};
+		};
+	};
+
+};
+
+&ethernet0 {
+	status = "okay";
+	pinctrl-names = "default", "sleep";
+	pinctrl-0 = <&eth1_pins_a>;
+	pinctrl-1 = <&eth1_sleep_pins_a>;
+	phy-mode = "rmii";
+	max-speed = <100>;
+
+	fixed-link {
+		speed = <100>;
+		full-duplex;
+	};
+};
+
+&adc {
+	pinctrl-names = "default";
+	pinctrl-0 = <&adc12_ain_pins_a>;
+	vdd-supply = <&vdd>;
+	vdda-supply = <&vdda>;
+	vref-supply = <&vref>;
+	status = "okay";
+
+	adc1: adc@0 {
+		st,adc-channels = <1 5 13 15>;
+		st,min-sample-time-nsecs = <10000>;
+		assigned-resolution-bits = <16>;
+		status = "okay";
+	};
+
+	adc2: adc@100 {
+		st,adc-channels = <0 3>;
+		st,min-sample-time-nsecs = <10000>;
+		assigned-resolution-bits = <16>;
+		status = "okay";
+	};
+};
+
+&dac {
+	pinctrl-names = "default";
+	pinctrl-0 = <&dac_ch1_pins_a &dac_ch2_pins_a>;
+	vref-supply = <&vref>;
+	status = "okay";
+	dac1: dac@1 {
+		status = "okay";
+	};
+	dac2: dac@2 {
+		status = "okay";
+	};
+};
+
+&cpu0{
+	cpu-supply = <&vddcore>;
+};
+
+&dma1 {
+	sram = <&dma_pool>;
+};
+
+&dma2 {
+	sram = <&dma_pool>;
+};
+
+&dts {
+	status = "okay";
+};
+
+&i2c2 {
+	pinctrl-names = "default";
+	pinctrl-0 = <&i2c2_pins_a>;
+	i2c-scl-rising-time-ns = <185>;
+	i2c-scl-falling-time-ns = <20>;
+	status = "okay";
+	/delete-property/dmas;
+	/delete-property/dma-names;
+
+	rtc_r2221t@32 {
+		compatible = "ricoh,r2221tl";
+		reg = <0x32>;
+
+		interrupt-parent = <&gpiob>;
+		interrupts = <8 IRQ_TYPE_LEVEL_LOW>;
+	};
+};
+
+&i2c4 {
+	pinctrl-names = "default", "sleep";
+	pinctrl-0 = <&i2c4_pins_a>;
+	pinctrl-1 = <&i2c4_pins_a_sleep>;
+	i2c-scl-rising-time-ns = <185>;
+	i2c-scl-falling-time-ns = <20>;
+	clock-frequency = <400000>;
+	status = "okay";
+	/* spare dmas for other usage */
+	/delete-property/dmas;
+	/delete-property/dma-names;
+
+	eeprom: m24c512@54 {
+		compatible = "st,24c512", "at24";
+		reg = <0x54>;
+	};
+
+	pmic: stpmic@33 {
+		compatible = "st,stpmic1";
+		reg = <0x33>;
+		interrupts-extended = <&gpioa 0 IRQ_TYPE_EDGE_FALLING>;
+		interrupt-controller;
+		#interrupt-cells = <2>;
+
+		regulators {
+			compatible = "st,stpmic1-regulators";
+
+			ldo1-supply = <&v3v3>;
+			ldo2-supply = <&v3v3>;
+			ldo3-supply = <&vdd_ddr>;
+			ldo5-supply = <&v3v3>;
+			ldo6-supply = <&v3v3>;
+			pwr_sw1-supply = <&bst_out>;
+			pwr_sw2-supply = <&bst_out>;
+
+			vddcore: buck1 {
+				regulator-name = "vddcore";
+				regulator-min-microvolt = <1200000>;
+				regulator-max-microvolt = <1350000>;
+				regulator-always-on;
+				regulator-initial-mode = <0>;
+				regulator-over-current-protection;
+			};
+
+			vdd_ddr: buck2 {
+				regulator-name = "vdd_ddr";
+				regulator-min-microvolt = <1350000>;
+				regulator-max-microvolt = <1350000>;
+				regulator-always-on;
+				regulator-initial-mode = <0>;
+				regulator-over-current-protection;
+			};
+
+			vdd: buck3 {
+				regulator-name = "vdd";
+				regulator-min-microvolt = <3300000>;
+				regulator-max-microvolt = <3300000>;
+				regulator-always-on;
+				st,mask-reset;
+				regulator-initial-mode = <0>;
+				regulator-over-current-protection;
+			};
+
+			v3v3: buck4 {
+				regulator-name = "v3v3";
+				regulator-min-microvolt = <3300000>;
+				regulator-max-microvolt = <3300000>;
+				regulator-always-on;
+				regulator-over-current-protection;
+				regulator-initial-mode = <0>;
+			};
+
+			vtt_ddr: ldo3 {
+				regulator-name = "vtt_ddr";
+				regulator-min-microvolt = <500000>;
+				regulator-max-microvolt = <750000>;
+				regulator-always-on;
+				regulator-over-current-protection;
+			};
+
+			vdd_usb: ldo4 {
+				regulator-name = "vdd_usb";
+				regulator-min-microvolt = <3300000>;
+				regulator-max-microvolt = <3300000>;
+				interrupts = <IT_CURLIM_LDO4 0>;
+				regulator-boot-on;
+			};
+
+			vdd_sd: ldo5 {
+				regulator-name = "vdd_sd";
+				regulator-min-microvolt = <2900000>;
+				regulator-max-microvolt = <2900000>;
+				interrupts = <IT_CURLIM_LDO5 0>;
+				regulator-boot-on;
+			};
+
+			vdda: ldo6 {
+				regulator-name = "vdda";
+				regulator-min-microvolt = <2900000>;
+				regulator-max-microvolt = <2900000>;
+				regulator-always-on;
+				interrupts = <IT_CURLIM_LDO6 0>;
+			};
+
+			vref_ddr: vref_ddr {
+				regulator-name = "vref_ddr";
+				regulator-always-on;
+				regulator-over-current-protection;
+			};
+
+			bst_out: boost {
+				regulator-name = "bst_out";
+				interrupts = <IT_OCP_BOOST 0>;
+			};
+
+			vbus_otg: pwr_sw1 {
+				regulator-name = "vbus_otg";
+				interrupts = <IT_OCP_OTG 0>;
+				regulator-active-discharge;
+			};
+
+			vbus_sw: pwr_sw2 {
+				regulator-name = "vbus_sw";
+				interrupts = <IT_OCP_SWOUT 0>;
+				regulator-active-discharge;
+			};
+		};
+
+		pmic_watchdog: watchdog {
+			compatible = "st,stpmic1-wdt";
+			status = "okay";
+		};
+	};
+
+	lm75: lm75@49 {
+		compatible = "national,lm75";
+		reg = <0x49>;
+	};
+};
+
+
+&ipcc {
+	status = "okay";
+};
+
+&iwdg2 {
+	timeout-sec = <32>;
+	status = "okay";
+};
+
+&m4_rproc {
+	memory-region = <&retram>, <&mcuram>, <&mcuram2>, <&vdev0vring0>,
+			<&vdev0vring1>, <&vdev0buffer>;
+	mboxes = <&ipcc 0>, <&ipcc 1>, <&ipcc 2>;
+	mbox-names = "vq0", "vq1", "shutdown";
+	interrupt-parent = <&exti>;
+	interrupts = <68 1>;
+	interrupt-names = "wdg";
+	wakeup-source;
+	recovery;
+	status = "okay";
+};
+
+&pwr_regulators {
+	vdd-supply = <&vdd>;
+	vdd_3v3_usbfs-supply = <&vdd_usb>;
+};
+
+&rng1 {
+	status = "okay";
+};
+
+&rtc {
+	status = "disabled";
+};
+
+&sdmmc1 {
+	arm,primecell-periphid = <0x00253180>;
+	pinctrl-names = "default", "opendrain", "sleep";
+	pinctrl-0 = <&sdmmc1_b4_pins_a>;
+	pinctrl-1 = <&sdmmc1_b4_od_pins_a>;
+	pinctrl-2 = <&sdmmc1_b4_sleep_pins_a>;
+	broken-cd;
+	disable-wp;
+	st,neg-edge;
+	bus-width = <4>;
+	vmmc-supply = <&v3v3>;
+	status = "okay";
+};
+
+&sdmmc2 {
+	pinctrl-names = "default", "opendrain", "sleep";
+	pinctrl-0 = <&sdmmc2_b4_pins_a &sdmmc2_d47_pins_a>;
+	pinctrl-1 = <&sdmmc2_b4_od_pins_a &sdmmc2_d47_pins_a>;
+	pinctrl-2 = <&sdmmc2_b4_sleep_pins_a &sdmmc2_d47_sleep_pins_a>;
+	non-removable;
+	no-sd;
+	no-sdio;
+	st,neg-edge;
+	bus-width = <8>;
+	vmmc-supply = <&v3v3>;
+	vqmmc-supply = <&v3v3>;
+	mmc-ddr-3_3v;
+	status = "okay";
+};
+
+&uart4 {
+	pinctrl-names = "default", "sleep", "idle";
+	pinctrl-0 = <&uart4_pins_a>;
+	pinctrl-1 = <&uart4_sleep_pins_a>;
+	pinctrl-2 = <&uart4_idle_pins_a>;
+	/delete-property/dmas;
+	/delete-property/dma-names;
+	//uart-has-rtscts;
+	//cts-gpios = <&gpioa 15 GPIO_ACTIVE_LOW>;
+	//rts-gpios = <&gpioa 15 GPIO_ACTIVE_LOW>;
+	status = "okay";
+};
+
+&usart1 {
+	pinctrl-names = "default", "sleep", "idle";
+	pinctrl-0 = <&usart1_pins_a>;
+	pinctrl-1 = <&usart1_idle_pins_a>;
+	pinctrl-2 = <&usart1_sleep_pins_a>;
+	/delete-property/dmas;
+	/delete-property/dma-names;
+	/* Disable uart HW control - the HW control control use CTS as input,
+	 * but the CC-100 use CTS as output RS485 NRE signal -> the HW control
+	 * and uart-has-rtscts are disabled.
+	 */
+	//uart-has-rtscts;
+	/delete-property/st,hw-flow-ctrl;
+
+        rs485-rts-active-high;
+	rs485-rts-delay = <1 1>;    /* tPZH, tPZL = 100ns */
+	/* The Wago patch sync-with-topic-rg-next-vtpctp-5.10-9b893ae5a174.patch
+	 * removes the TIOCGRS485 and TIOCSRS485 and the RS485 cannot be
+	 * therefore control by ioctls -> it is enabled by default here.
+	 */
+	linux,rs485-enabled-at-boot-time;
+
+	status = "okay";
+};
+
+&fmc {
+	pinctrl-names = "default", "sleep";
+	pinctrl-0 = <&fmc_pins_a>;
+	pinctrl-1 = <&fmc_sleep_pins_a>;
+	status = "okay";
+
+	/* nvSRAM connected to the FMC (use UIO driver as in TP600) */
+	nvsram@0,0 {
+		compatible = "uio_pdrv_genirq";
+		/* 128kB nvSRAM -> the beginning of the first subbunks of bank 1 */
+		reg = <0 0x0 0x20000>;
+		linux,uio-name = "UIO_NVRAM";
+
+		/* assynchronous SRAM mode */
+		st,fmc2-ebi-cs-transaction-type = <0>;
+
+		st,fmc2-ebi-cs-buswidth = <16>;
+
+		/* The BWTR registers are not used in assynchronous SRAM mode
+		 * (EXTMOD = 0). The timing settings are shared for read and
+		 * write. The write specific settings can be omitted (this
+		 * applies to all ebi-cs-write DTS properties).
+		 *
+		 * The address and data hold time are set to minimal value
+		 * to ensure the address/data are not released before the
+		 * read/write enable at the nvSRAM interface (due to potential
+		 * difference of signal delays between processor and nvSRAM).
+		 */
+		st,fmc2-ebi-cs-byte-lane-setup-ns = <0>;
+		st,fmc2-ebi-cs-address-setup-ns = <5>;
+		st,fmc2-ebi-cs-address-hold-ns = <1>;
+		st,fmc2-ebi-cs-data-setup-ns = <20>;
+		st,fmc2-ebi-cs-data-hold-ns = <1>;
+		st,fmc2-ebi-cs-bus-turnaround-ns = <0>;
+	};
+};
diff --git a/arch/arm/boot/dts/wago-devconf.dtsi b/arch/arm/boot/dts/wago-devconf.dtsi
new file mode 100644
index 000000000000..7966c2315582
--- /dev/null
+++ b/arch/arm/boot/dts/wago-devconf.dtsi
@@ -0,0 +1,117 @@
+/*
+ * Copyright (C) 2019 WAGO Kontakttechnik GmbH & Co. KG
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+/*
+ * The DEVCONF field is used by the bootloader to identify, verify and choose
+ * the right boot configuration for the device. See bootchoser and BLSPEC
+ * documentation in barebox for more information.
+ *
+ * The DEVCONF field is a 16-bit value stored in the EEPROM of each device as a
+ * 16-bit value.
+ *
+ * 0x1fe |---------------------------------------|
+ *       |          DEVCONF Byte 0 (Low Byte)    |
+ *       | ------------------------------------- |
+ *       |          DEVCONF Byte 1 (High Byte)   |
+ * 0x200 |---------------------------------------|
+ *
+ * NOTE:
+ * The DEVCONF field contains CONF suffix. In legacy format was this a kind of
+ * "configuration" of device. In current format it is more a device
+ * identification. The name DEVCONF of this field is not apply anymore. But we
+ * decided not to rename it, becaus this field name already established in many
+ * applications and documentations outsite of kernel.
+ *
+ * ============== CURRENT FORMAT ===============================================
+ *
+ *   15  14  13  12  11  10  9  8  7  6  5  4  3  2  1  0
+ *    |   |   |   |   |   |  |  |  |  |  |  |  |  |  |  |
+ *    y   y   y   |   z   z  z  z  z  z  z  z  z  z  z  z
+ *                |
+ *                NEW DEVCONF FORMAT (has to be set to 1)
+ *
+ *    y: hardware revision (started by 0 for each device)
+ *    z: device id (see list below)
+ *
+ * | Device ID (hex) | Device description |
+ * |-----------------|--------------------|
+ * |            1000 |   VTP  4" 762-4xxx |
+ * |            1001 |   VTP  5" 762-4xxx |
+ * |            1002 |   VTP  7" 762-4xxx |
+ * |            1003 |   VTP 10" 762-4xxx |
+ * |            1004 |       PFC 750-8211 |
+ * |            1006 |       PFC 750-8217 |
+ * |            1007 |       PFC 750-8210 |
+ * |            3005 |       PFC 763-3301 |
+ * |            1008 |   VTP 15" 762-4xxx |
+ * |            1009 |   VTP 21" 762-4xxx |
+ * |            1010 |        EC 752-8303 |
+ * |            1011 |     CC100 751-9301 |
+ * |-----------------|--------------------|
+ *
+ * ============== LEGACY FORMAT ================================================
+ *
+ * This format is deprecated and should not be used for new devices.
+ *
+ *   15  14  13  12  11  10  9  8  7  6  5  4  3  2  1  0
+ *    |   |   |   |   |   |  |  |  |  |  |  |  |  |  |  |
+ *    |   x   x   |   x   x  |  |  |  |  |  |  |  |  PROFIBUS_SL
+ *    |           |          |  |  |  |  |  |  |  |  CAN
+ *    |           |          |  |  |  |  |  |  |  RS232_485
+ *    |           |          |  |  |  |  |  |  ETHERNET_IP
+ *    |           |          |  |  |  |  |  3G_MODEM
+ *    |           |          |  |  |  |  DIP_SWITCH
+ *    |           |          |  |  |  PROFIBUS_DPM
+ *    |           |          |  |   USB
+ *    |           |          |  MARVELL_SWITCH
+ *    |           |          LWL
+ *    |           NEW DEVCONF FORMAT (for legacy format have to be set to 0)
+ *    DISPLAY (AM35XX based displays)
+ *
+ *   x: reserved for future use (has to be set to 0)
+ *
+ * Following devices are using the deprecated DEVCONF format:
+ *
+ * PFC 200 V1 (AM3505)
+ *
+ * | Device ID (hex) | Device description |
+ * |-----------------|--------------------|
+ * |            000c |           750-8202 |
+ * |            000a |           750-8203 |
+ * |            000e |           750-8204 |
+ * |            000f |           750-8206 |
+ * |            001c |           750-8207 |
+ * |-----------------|--------------------|
+ *
+ * PFC 100 (AM335X)
+ *
+ * | Device ID (hex) | Device description |
+ * |-----------------|--------------------|
+ * |            0028 |      750-8100/8101 |
+ * |            000c |           750-8102 |
+ * |-----------------|--------------------|
+ *
+ * PFC 200 v2 (AM335X)
+ *
+ * | Device ID (hex) | Device description |
+ * |-----------------|--------------------|
+ * |            004e |           750-8208 |
+ * |-----------------|--------------------|
+ *
+ * PFC 200 v3 (AM335X)
+ *
+ * | Device ID (hex) | Device description |
+ * |-----------------|--------------------|
+ * |            000c |           750-8212 |
+ * |            000a |           750-8213 |
+ * |            000e |           750-8214 |
+ * |            0182 |           750-8215 |
+ * |            000f |           750-8216 |
+ * |-----------------|--------------------|
+ *
+ */
diff --git a/arch/arm/configs/am335x_pfc_adv_generic_defconfig b/arch/arm/configs/am335x_pfc_adv_generic_defconfig
new file mode 100644
index 000000000000..743653497b0c
--- /dev/null
+++ b/arch/arm/configs/am335x_pfc_adv_generic_defconfig
@@ -0,0 +1,396 @@
+CONFIG_KERNEL_LZO=y
+# CONFIG_SWAP is not set
+CONFIG_SYSVIPC=y
+CONFIG_POSIX_MQUEUE=y
+CONFIG_HIGH_RES_TIMERS=y
+CONFIG_PREEMPT_RT=y
+CONFIG_IKCONFIG=y
+CONFIG_IKCONFIG_PROC=y
+CONFIG_CGROUPS=y
+CONFIG_MEMCG=y
+CONFIG_BLK_CGROUP=y
+CONFIG_CGROUP_SCHED=y
+CONFIG_CFS_BANDWIDTH=y
+CONFIG_CGROUP_PIDS=y
+CONFIG_CGROUP_FREEZER=y
+CONFIG_CGROUP_DEVICE=y
+CONFIG_CGROUP_CPUACCT=y
+CONFIG_CGROUP_PERF=y
+CONFIG_NAMESPACES=y
+CONFIG_USER_NS=y
+CONFIG_EXPERT=y
+CONFIG_BPF_SYSCALL=y
+CONFIG_PERF_EVENTS=y
+# CONFIG_COMPAT_BRK is not set
+CONFIG_OMAP_RESET_CLOCKS=y
+CONFIG_ARCH_OMAP3=y
+CONFIG_ARCH_OMAP4=y
+CONFIG_SOC_AM33XX=y
+CONFIG_WAGO_SYSTEM_BASED_STARTUP=y
+CONFIG_IRQ_PRIORITY_TABLE=y
+# CONFIG_SOC_OMAP3430 is not set
+# CONFIG_SOC_TI81XX is not set
+# CONFIG_MACH_OMAP3517EVM is not set
+# CONFIG_MACH_OMAP3_PANDORA is not set
+CONFIG_ARM_THUMBEE=y
+CONFIG_SWP_EMULATE=y
+CONFIG_UACCESS_WITH_MEMCPY=y
+# CONFIG_ATAGS is not set
+CONFIG_KEXEC=y
+CONFIG_CPU_FREQ=y
+CONFIG_CPUFREQ_DT=y
+# CONFIG_SUSPEND is not set
+CONFIG_MODULES=y
+CONFIG_MODULE_FORCE_LOAD=y
+CONFIG_MODULE_UNLOAD=y
+CONFIG_MODULE_FORCE_UNLOAD=y
+CONFIG_MODVERSIONS=y
+CONFIG_MODULE_SRCVERSION_ALL=y
+CONFIG_BLK_DEV_BSGLIB=y
+CONFIG_BLK_DEV_INTEGRITY=y
+CONFIG_BLK_DEV_THROTTLING=y
+CONFIG_PARTITION_ADVANCED=y
+CONFIG_MAC_PARTITION=y
+# CONFIG_MQ_IOSCHED_KYBER is not set
+CONFIG_BINFMT_MISC=y
+# CONFIG_COMPACTION is not set
+CONFIG_NET=y
+CONFIG_PACKET=y
+CONFIG_UNIX=y
+CONFIG_XFRM_USER=y
+CONFIG_NET_KEY=y
+CONFIG_NET_KEY_MIGRATE=y
+CONFIG_XDP_SOCKETS=y
+CONFIG_INET=y
+CONFIG_IP_MULTICAST=y
+CONFIG_IP_ADVANCED_ROUTER=y
+CONFIG_IP_MULTIPLE_TABLES=y
+CONFIG_IP_PNP=y
+CONFIG_IP_PNP_DHCP=y
+CONFIG_IP_PNP_BOOTP=y
+CONFIG_IP_PNP_RARP=y
+CONFIG_IP_MROUTE=y
+CONFIG_IP_MROUTE_MULTIPLE_TABLES=y
+CONFIG_IP_PIMSM_V1=y
+CONFIG_IP_PIMSM_V2=y
+CONFIG_SYN_COOKIES=y
+CONFIG_INET_AH=y
+CONFIG_INET_ESP=y
+CONFIG_INET_IPCOMP=y
+CONFIG_TCP_CONG_ADVANCED=y
+# CONFIG_TCP_CONG_BIC is not set
+# CONFIG_TCP_CONG_WESTWOOD is not set
+# CONFIG_TCP_CONG_HTCP is not set
+# CONFIG_IPV6 is not set
+CONFIG_NETFILTER=y
+CONFIG_BRIDGE_NETFILTER=y
+CONFIG_NF_CONNTRACK=y
+CONFIG_NF_CONNTRACK_MARK=y
+CONFIG_NF_CONNTRACK_FTP=y
+CONFIG_NF_CONNTRACK_SNMP=y
+CONFIG_NF_CONNTRACK_TFTP=y
+CONFIG_NF_CT_NETLINK=y
+CONFIG_NETFILTER_XT_TARGET_HL=m
+CONFIG_NETFILTER_XT_TARGET_TCPMSS=y
+CONFIG_NETFILTER_XT_MATCH_ADDRTYPE=y
+CONFIG_NETFILTER_XT_MATCH_CONNLIMIT=y
+CONFIG_NETFILTER_XT_MATCH_CONNTRACK=y
+CONFIG_NETFILTER_XT_MATCH_ESP=y
+CONFIG_NETFILTER_XT_MATCH_IPRANGE=y
+CONFIG_NETFILTER_XT_MATCH_IPVS=m
+CONFIG_NETFILTER_XT_MATCH_LIMIT=y
+CONFIG_NETFILTER_XT_MATCH_MAC=y
+CONFIG_NETFILTER_XT_MATCH_MARK=y
+CONFIG_NETFILTER_XT_MATCH_POLICY=y
+CONFIG_NETFILTER_XT_MATCH_PHYSDEV=y
+CONFIG_IP_VS=m
+CONFIG_IP_VS_PROTO_TCP=y
+CONFIG_IP_VS_PROTO_UDP=y
+CONFIG_IP_VS_RR=m
+CONFIG_IP_VS_WRR=m
+CONFIG_IP_VS_NFCT=y
+CONFIG_IP_NF_IPTABLES=y
+CONFIG_IP_NF_MATCH_AH=y
+CONFIG_IP_NF_FILTER=y
+CONFIG_IP_NF_TARGET_REJECT=y
+CONFIG_IP_NF_NAT=y
+CONFIG_IP_NF_TARGET_MASQUERADE=y
+CONFIG_IP_NF_TARGET_REDIRECT=y
+CONFIG_IP_NF_MANGLE=y
+CONFIG_IP_NF_RAW=y
+CONFIG_BRIDGE_NF_EBTABLES=y
+CONFIG_BRIDGE_EBT_BROUTE=y
+CONFIG_BRIDGE_EBT_T_FILTER=y
+CONFIG_BRIDGE_EBT_T_NAT=y
+CONFIG_BRIDGE_EBT_IP=y
+CONFIG_BRIDGE_EBT_LIMIT=y
+CONFIG_BRIDGE_EBT_LOG=y
+CONFIG_BRIDGE=y
+CONFIG_NET_SCHED=y
+CONFIG_NET_SCH_HTB=y
+CONFIG_NET_SCH_HFSC=y
+CONFIG_NET_SCH_PRIO=y
+CONFIG_NET_SCH_TBF=y
+CONFIG_NET_SCH_CODEL=y
+CONFIG_NET_SCH_FQ_CODEL=y
+CONFIG_NET_CLS_BASIC=y
+CONFIG_NET_CLS_TCINDEX=y
+CONFIG_NET_CLS_ROUTE4=y
+CONFIG_NET_CLS_FW=y
+CONFIG_NET_CLS_U32=y
+CONFIG_CLS_U32_PERF=y
+CONFIG_CLS_U32_MARK=y
+CONFIG_NET_CLS_RSVP=y
+CONFIG_NET_CLS_FLOW=y
+CONFIG_NET_CLS_CGROUP=m
+CONFIG_NET_EMATCH=y
+CONFIG_NET_EMATCH_CMP=y
+CONFIG_NET_EMATCH_NBYTE=y
+CONFIG_NET_EMATCH_U32=y
+CONFIG_NET_EMATCH_META=y
+CONFIG_NET_EMATCH_TEXT=y
+CONFIG_NET_CLS_ACT=y
+CONFIG_NET_ACT_POLICE=y
+CONFIG_NET_ACT_GACT=y
+CONFIG_GACT_PROB=y
+CONFIG_NET_ACT_MIRRED=y
+CONFIG_NET_ACT_IPT=y
+CONFIG_NET_ACT_NAT=y
+CONFIG_NET_ACT_PEDIT=y
+CONFIG_NET_ACT_SKBEDIT=y
+CONFIG_NET_ACT_CSUM=y
+CONFIG_NETLINK_DIAG=y
+CONFIG_CGROUP_NET_PRIO=y
+CONFIG_BPF_JIT=y
+CONFIG_CAN=y
+# CONFIG_CAN_GW is not set
+CONFIG_CAN_VCAN=y
+CONFIG_CAN_TI_HECC=y
+CONFIG_CAN_C_CAN=y
+CONFIG_CAN_C_CAN_PLATFORM=y
+CONFIG_AF_RXRPC=y
+# CONFIG_WIRELESS is not set
+CONFIG_DEVTMPFS=y
+CONFIG_DEVTMPFS_MOUNT=y
+CONFIG_OMAP_OCP2SCP=y
+CONFIG_CONNECTOR=y
+CONFIG_MTD=y
+CONFIG_MTD_CMDLINE_PARTS=y
+CONFIG_MTD_BLOCK=y
+CONFIG_MTD_CFI=y
+CONFIG_MTD_CFI_INTELEXT=y
+CONFIG_MTD_ANV32AA1W=y
+CONFIG_MTD_RAW_NAND=y
+CONFIG_MTD_NAND_ECC_SW_BCH=y
+CONFIG_MTD_NAND_OMAP2=y
+CONFIG_MTD_NAND_OMAP_BCH=y
+CONFIG_MTD_NAND_PLATFORM=y
+CONFIG_MTD_SPI_NOR=y
+CONFIG_MTD_UBI=y
+CONFIG_MTD_UBI_FASTMAP=y
+CONFIG_BLK_DEV_LOOP=y
+CONFIG_ENCSW=y
+CONFIG_TI_SN74LV165A=y
+CONFIG_EEPROM_AT24=y
+CONFIG_SCSI=y
+# CONFIG_SCSI_PROC_FS is not set
+CONFIG_BLK_DEV_SD=y
+CONFIG_SCSI_SCAN_ASYNC=y
+# CONFIG_SCSI_LOWLEVEL is not set
+CONFIG_NETDEVICES=y
+CONFIG_DUMMY=m
+CONFIG_MACVLAN=m
+CONFIG_IPVLAN=m
+CONFIG_VXLAN=m
+CONFIG_TUN=y
+CONFIG_VETH=m
+CONFIG_NET_DSA_KSZ8863=y
+CONFIG_NET_DSA_MV88E6XXX=y
+# CONFIG_NET_VENDOR_BROADCOM is not set
+# CONFIG_NET_VENDOR_CIRRUS is not set
+# CONFIG_NET_VENDOR_FARADAY is not set
+# CONFIG_NET_VENDOR_INTEL is not set
+# CONFIG_NET_VENDOR_MARVELL is not set
+# CONFIG_NET_VENDOR_MICROCHIP is not set
+# CONFIG_NET_VENDOR_NATSEMI is not set
+# CONFIG_NET_VENDOR_SEEQ is not set
+# CONFIG_NET_VENDOR_SMSC is not set
+# CONFIG_NET_VENDOR_STMICRO is not set
+CONFIG_TI_DAVINCI_EMAC=y
+CONFIG_TI_CPSW=y
+# CONFIG_NET_VENDOR_WIZNET is not set
+CONFIG_SWCONFIG=y
+CONFIG_INTEL_XWAY_PHY=y
+CONFIG_MARVELL_PHY=y
+CONFIG_MICREL_PHY=y
+CONFIG_SWCFG_KSZ8863=y
+CONFIG_SWCFG_MV88E6321=y
+CONFIG_MDIO_BITBANG=y
+CONFIG_MDIO_GPIO=y
+CONFIG_PPP=y
+CONFIG_PPP_DEFLATE=y
+CONFIG_PPP_FILTER=y
+CONFIG_PPP_MPPE=y
+CONFIG_PPP_MULTILINK=y
+CONFIG_PPP_ASYNC=y
+CONFIG_PPP_SYNC_TTY=y
+CONFIG_USB_USBNET=y
+# CONFIG_USB_NET_AX8817X is not set
+# CONFIG_USB_NET_AX88179_178A is not set
+# CONFIG_USB_NET_CDCETHER is not set
+# CONFIG_USB_NET_CDC_NCM is not set
+# CONFIG_USB_NET_NET1080 is not set
+# CONFIG_USB_NET_CDC_SUBSET is not set
+# CONFIG_USB_NET_ZAURUS is not set
+CONFIG_USB_NET_QMI_WWAN=y
+# CONFIG_WLAN is not set
+CONFIG_INPUT_EVDEV=y
+# CONFIG_KEYBOARD_ATKBD is not set
+CONFIG_KEYBOARD_GPIO=y
+CONFIG_KEYBOARD_GPIO_POLLED=y
+# CONFIG_INPUT_MOUSE is not set
+# CONFIG_SERIO is not set
+# CONFIG_LEGACY_PTYS is not set
+CONFIG_SERIAL_OMAP=y
+CONFIG_SERIAL_OMAP_RTU=y
+CONFIG_SERIAL_OMAP_CONSOLE=y
+CONFIG_HW_RANDOM=y
+CONFIG_RMD=y
+# CONFIG_I2C_COMPAT is not set
+CONFIG_I2C_CHARDEV=y
+CONFIG_I2C_MUX=y
+CONFIG_I2C_MUX_PCA9541=y
+CONFIG_I2C_MUX_PCA954x=y
+CONFIG_SPI=y
+CONFIG_SPI_OMAP24XX=y
+CONFIG_PINCTRL_SINGLE=y
+CONFIG_GPIO_SYSFS=y
+CONFIG_GPIO_PCA953X=y
+CONFIG_GPIO_PCA953X_IRQ=y
+CONFIG_GPIO_PCF857X=y
+CONFIG_GPIO_TWL4030=y
+# CONFIG_HWMON is not set
+CONFIG_WATCHDOG=y
+CONFIG_GPIO_WATCHDOG=y
+CONFIG_OMAP_WATCHDOG=y
+CONFIG_MFD_TI_AM335X_TSCADC=y
+CONFIG_MFD_TPS65218=y
+CONFIG_MFD_TPS65910=y
+CONFIG_REGULATOR_PBIAS=y
+CONFIG_REGULATOR_TPS65218=y
+CONFIG_REGULATOR_TPS65910=y
+# CONFIG_HID_GENERIC is not set
+# CONFIG_USB_HID is not set
+CONFIG_USB=y
+CONFIG_USB_ANNOUNCE_NEW_DEVICES=y
+CONFIG_USB_OTG=y
+CONFIG_USB_EHCI_HCD=y
+CONFIG_USB_EHCI_ROOT_HUB_TT=y
+CONFIG_USB_OHCI_HCD=y
+CONFIG_USB_STORAGE=y
+CONFIG_USB_UAS=y
+CONFIG_USB_MUSB_HDRC=y
+CONFIG_USB_MUSB_OMAP2PLUS=y
+CONFIG_USB_MUSB_AM35X=y
+CONFIG_USB_MUSB_DSPS=y
+CONFIG_USB_INVENTRA_DMA=y
+CONFIG_USB_TI_CPPI41_DMA=y
+CONFIG_USB_SERIAL=y
+CONFIG_USB_SERIAL_QUALCOMM=y
+CONFIG_USB_SERIAL_OPTION=y
+CONFIG_NOP_USB_XCEIV=y
+CONFIG_AM335X_PHY_USB=y
+CONFIG_USB_ULPI=y
+CONFIG_USB_GADGET=y
+CONFIG_USB_CONFIGFS=m
+CONFIG_USB_CONFIGFS_ACM=y
+CONFIG_USB_CONFIGFS_NCM=y
+CONFIG_USB_CONFIGFS_ECM=y
+CONFIG_USB_CONFIGFS_RNDIS=y
+CONFIG_USB_CONFIGFS_MASS_STORAGE=y
+CONFIG_MMC=y
+CONFIG_MMC_SDHCI=y
+CONFIG_MMC_SDHCI_PLTFM=y
+CONFIG_MMC_OMAP_HS=y
+CONFIG_MMC_SDHCI_OMAP=y
+CONFIG_NEW_LEDS=y
+CONFIG_LEDS_CLASS=y
+CONFIG_LEDS_GPIO=y
+CONFIG_LEDS_PCA955X=y
+CONFIG_LEDS_TRIGGERS=y
+CONFIG_LEDS_TRIGGER_TIMER=y
+CONFIG_LEDS_TRIGGER_ONESHOT=y
+CONFIG_LEDS_TRIGGER_HEARTBEAT=y
+CONFIG_LEDS_TRIGGER_GPIO=y
+CONFIG_LEDS_TRIGGER_DEFAULT_ON=y
+CONFIG_LEDS_TRIGGER_TRANSIENT=y
+CONFIG_RTC_CLASS=y
+CONFIG_RTC_DRV_RS5C372=y
+CONFIG_RTC_DRV_ISL1208=y
+CONFIG_DMADEVICES=y
+CONFIG_ASYNC_TX_DMA=y
+CONFIG_DMATEST=m
+CONFIG_UIO=y
+CONFIG_UIO_PDRV_GENIRQ=y
+# CONFIG_VIRTIO_MENU is not set
+# CONFIG_VHOST_MENU is not set
+# CONFIG_IOMMU_SUPPORT is not set
+CONFIG_EXTCON_PTN5150=m
+CONFIG_PWM=y
+CONFIG_PWM_TIECAP=y
+CONFIG_PWM_TIEHRPWM=y
+CONFIG_OMAP_USB2=y
+CONFIG_EXT4_FS=y
+CONFIG_EXT4_FS_POSIX_ACL=y
+CONFIG_EXT4_FS_SECURITY=y
+CONFIG_FANOTIFY=y
+CONFIG_FUSE_FS=y
+CONFIG_OVERLAY_FS=y
+CONFIG_MSDOS_FS=y
+CONFIG_VFAT_FS=y
+CONFIG_NTFS_FS=y
+CONFIG_NTFS_RW=y
+CONFIG_TMPFS=y
+CONFIG_TMPFS_POSIX_ACL=y
+CONFIG_CONFIGFS_FS=y
+CONFIG_UBIFS_FS=y
+CONFIG_SQUASHFS=y
+CONFIG_NFS_FS=y
+CONFIG_NFS_V3_ACL=y
+CONFIG_NFS_V4=y
+CONFIG_ROOT_NFS=y
+CONFIG_NLS_CODEPAGE_437=y
+CONFIG_NLS_ISO8859_1=y
+CONFIG_NLS_UTF8=y
+CONFIG_CRYPTO_USER=y
+CONFIG_CRYPTO_CCM=y
+CONFIG_CRYPTO_XCBC=y
+CONFIG_CRYPTO_XXHASH=y
+CONFIG_CRYPTO_BLAKE2B=y
+CONFIG_CRYPTO_MICHAEL_MIC=y
+CONFIG_CRYPTO_BLOWFISH=y
+CONFIG_CRYPTO_CAMELLIA=y
+CONFIG_CRYPTO_CAST5=y
+CONFIG_CRYPTO_CAST6=y
+CONFIG_CRYPTO_SERPENT=y
+CONFIG_CRYPTO_TWOFISH=y
+CONFIG_CRYPTO_ANSI_CPRNG=y
+CONFIG_CRYPTO_USER_API_HASH=y
+CONFIG_CRYPTO_USER_API_SKCIPHER=y
+CONFIG_CRYPTO_DEV_OMAP=y
+CONFIG_CRYPTO_DEV_OMAP_SHAM=y
+CONFIG_CRYPTO_DEV_OMAP_AES=y
+CONFIG_CRYPTO_DEV_OMAP_DES=y
+CONFIG_ASYMMETRIC_KEY_TYPE=y
+CONFIG_ASYMMETRIC_PUBLIC_KEY_SUBTYPE=y
+CONFIG_X509_CERTIFICATE_PARSER=y
+CONFIG_CRC_ITU_T=y
+CONFIG_CRC7=y
+CONFIG_XZ_DEC=y
+CONFIG_PRINTK_TIME=y
+CONFIG_DYNAMIC_DEBUG=y
+CONFIG_MAGIC_SYSRQ=y
+CONFIG_DEBUG_FS=y
+CONFIG_FUNCTION_TRACER=y
+# CONFIG_UPROBE_EVENTS is not set
+# CONFIG_RUNTIME_TESTING_MENU is not set
diff --git a/arch/arm/configs/am3xxx_pfc_generic_defconfig b/arch/arm/configs/am3xxx_pfc_generic_defconfig
new file mode 100644
index 000000000000..bfa7b84f9ebe
--- /dev/null
+++ b/arch/arm/configs/am3xxx_pfc_generic_defconfig
@@ -0,0 +1,390 @@
+CONFIG_KERNEL_LZO=y
+# CONFIG_SWAP is not set
+CONFIG_SYSVIPC=y
+CONFIG_POSIX_MQUEUE=y
+CONFIG_HIGH_RES_TIMERS=y
+CONFIG_PREEMPT_RT=y
+CONFIG_IKCONFIG=y
+CONFIG_IKCONFIG_PROC=y
+CONFIG_CGROUPS=y
+CONFIG_MEMCG=y
+CONFIG_BLK_CGROUP=y
+CONFIG_CGROUP_SCHED=y
+CONFIG_CFS_BANDWIDTH=y
+CONFIG_CGROUP_PIDS=y
+CONFIG_CGROUP_FREEZER=y
+CONFIG_CGROUP_DEVICE=y
+CONFIG_CGROUP_CPUACCT=y
+CONFIG_CGROUP_PERF=y
+CONFIG_NAMESPACES=y
+CONFIG_USER_NS=y
+CONFIG_EXPERT=y
+CONFIG_BPF_SYSCALL=y
+CONFIG_PERF_EVENTS=y
+# CONFIG_COMPAT_BRK is not set
+CONFIG_OMAP_RESET_CLOCKS=y
+CONFIG_ARCH_OMAP3=y
+CONFIG_ARCH_OMAP4=y
+CONFIG_SOC_AM33XX=y
+CONFIG_WAGO_SYSTEM_BASED_STARTUP=y
+CONFIG_IRQ_PRIORITY_TABLE=y
+# CONFIG_SOC_OMAP3430 is not set
+# CONFIG_SOC_TI81XX is not set
+# CONFIG_MACH_OMAP3517EVM is not set
+# CONFIG_MACH_OMAP3_PANDORA is not set
+CONFIG_ARM_THUMBEE=y
+CONFIG_SWP_EMULATE=y
+CONFIG_UACCESS_WITH_MEMCPY=y
+# CONFIG_ATAGS is not set
+CONFIG_KEXEC=y
+CONFIG_CPU_FREQ=y
+CONFIG_CPUFREQ_DT=y
+# CONFIG_SUSPEND is not set
+CONFIG_MODULES=y
+CONFIG_MODULE_FORCE_LOAD=y
+CONFIG_MODULE_UNLOAD=y
+CONFIG_MODULE_FORCE_UNLOAD=y
+CONFIG_MODVERSIONS=y
+CONFIG_MODULE_SRCVERSION_ALL=y
+CONFIG_BLK_DEV_BSGLIB=y
+CONFIG_BLK_DEV_INTEGRITY=y
+CONFIG_BLK_DEV_THROTTLING=y
+CONFIG_PARTITION_ADVANCED=y
+CONFIG_MAC_PARTITION=y
+# CONFIG_MQ_IOSCHED_KYBER is not set
+CONFIG_BINFMT_MISC=y
+# CONFIG_COMPACTION is not set
+CONFIG_NET=y
+CONFIG_PACKET=y
+CONFIG_UNIX=y
+CONFIG_XFRM_USER=y
+CONFIG_NET_KEY=y
+CONFIG_NET_KEY_MIGRATE=y
+CONFIG_XDP_SOCKETS=y
+CONFIG_INET=y
+CONFIG_IP_MULTICAST=y
+CONFIG_IP_ADVANCED_ROUTER=y
+CONFIG_IP_MULTIPLE_TABLES=y
+CONFIG_IP_PNP=y
+CONFIG_IP_PNP_DHCP=y
+CONFIG_IP_PNP_BOOTP=y
+CONFIG_IP_PNP_RARP=y
+CONFIG_IP_MROUTE=y
+CONFIG_IP_MROUTE_MULTIPLE_TABLES=y
+CONFIG_IP_PIMSM_V1=y
+CONFIG_IP_PIMSM_V2=y
+CONFIG_SYN_COOKIES=y
+CONFIG_INET_AH=y
+CONFIG_INET_ESP=y
+CONFIG_INET_IPCOMP=y
+CONFIG_TCP_CONG_ADVANCED=y
+# CONFIG_TCP_CONG_BIC is not set
+# CONFIG_TCP_CONG_WESTWOOD is not set
+# CONFIG_TCP_CONG_HTCP is not set
+# CONFIG_IPV6 is not set
+CONFIG_NETFILTER=y
+CONFIG_BRIDGE_NETFILTER=y
+CONFIG_NF_CONNTRACK=y
+CONFIG_NF_CONNTRACK_MARK=y
+CONFIG_NF_CONNTRACK_FTP=y
+CONFIG_NF_CONNTRACK_SNMP=y
+CONFIG_NF_CONNTRACK_TFTP=y
+CONFIG_NF_CT_NETLINK=y
+CONFIG_NETFILTER_XT_TARGET_HL=m
+CONFIG_NETFILTER_XT_TARGET_TCPMSS=y
+CONFIG_NETFILTER_XT_MATCH_ADDRTYPE=y
+CONFIG_NETFILTER_XT_MATCH_CONNLIMIT=y
+CONFIG_NETFILTER_XT_MATCH_CONNTRACK=y
+CONFIG_NETFILTER_XT_MATCH_ESP=y
+CONFIG_NETFILTER_XT_MATCH_IPRANGE=y
+CONFIG_NETFILTER_XT_MATCH_IPVS=m
+CONFIG_NETFILTER_XT_MATCH_LIMIT=y
+CONFIG_NETFILTER_XT_MATCH_MAC=y
+CONFIG_NETFILTER_XT_MATCH_MARK=y
+CONFIG_NETFILTER_XT_MATCH_POLICY=y
+CONFIG_NETFILTER_XT_MATCH_PHYSDEV=y
+CONFIG_IP_VS=m
+CONFIG_IP_VS_PROTO_TCP=y
+CONFIG_IP_VS_PROTO_UDP=y
+CONFIG_IP_VS_RR=m
+CONFIG_IP_VS_WRR=m
+CONFIG_IP_VS_NFCT=y
+CONFIG_IP_NF_IPTABLES=y
+CONFIG_IP_NF_MATCH_AH=y
+CONFIG_IP_NF_FILTER=y
+CONFIG_IP_NF_TARGET_REJECT=y
+CONFIG_IP_NF_NAT=y
+CONFIG_IP_NF_TARGET_MASQUERADE=y
+CONFIG_IP_NF_TARGET_REDIRECT=y
+CONFIG_IP_NF_MANGLE=y
+CONFIG_IP_NF_RAW=y
+CONFIG_BRIDGE_NF_EBTABLES=y
+CONFIG_BRIDGE_EBT_BROUTE=y
+CONFIG_BRIDGE_EBT_T_FILTER=y
+CONFIG_BRIDGE_EBT_T_NAT=y
+CONFIG_BRIDGE_EBT_IP=y
+CONFIG_BRIDGE_EBT_LIMIT=y
+CONFIG_BRIDGE_EBT_LOG=y
+CONFIG_BRIDGE=y
+CONFIG_BRIDGE_VLAN_FILTERING=y
+CONFIG_VLAN_8021Q=m
+CONFIG_VLAN_8021Q_GVRP=y
+CONFIG_NET_SCHED=y
+CONFIG_NET_SCH_HTB=y
+CONFIG_NET_SCH_HFSC=y
+CONFIG_NET_SCH_PRIO=y
+CONFIG_NET_SCH_TBF=y
+CONFIG_NET_SCH_CODEL=y
+CONFIG_NET_SCH_FQ_CODEL=y
+CONFIG_NET_CLS_BASIC=y
+CONFIG_NET_CLS_TCINDEX=y
+CONFIG_NET_CLS_ROUTE4=y
+CONFIG_NET_CLS_FW=y
+CONFIG_NET_CLS_U32=y
+CONFIG_CLS_U32_PERF=y
+CONFIG_CLS_U32_MARK=y
+CONFIG_NET_CLS_RSVP=y
+CONFIG_NET_CLS_FLOW=y
+CONFIG_NET_CLS_CGROUP=m
+CONFIG_NET_EMATCH=y
+CONFIG_NET_EMATCH_CMP=y
+CONFIG_NET_EMATCH_NBYTE=y
+CONFIG_NET_EMATCH_U32=y
+CONFIG_NET_EMATCH_META=y
+CONFIG_NET_EMATCH_TEXT=y
+CONFIG_NET_CLS_ACT=y
+CONFIG_NET_ACT_POLICE=y
+CONFIG_NET_ACT_GACT=y
+CONFIG_GACT_PROB=y
+CONFIG_NET_ACT_MIRRED=y
+CONFIG_NET_ACT_IPT=y
+CONFIG_NET_ACT_NAT=y
+CONFIG_NET_ACT_PEDIT=y
+CONFIG_NET_ACT_SKBEDIT=y
+CONFIG_NET_ACT_CSUM=y
+CONFIG_NETLINK_DIAG=y
+CONFIG_CGROUP_NET_PRIO=y
+CONFIG_BPF_JIT=y
+CONFIG_CAN=y
+# CONFIG_CAN_GW is not set
+CONFIG_CAN_VCAN=y
+CONFIG_CAN_TI_HECC=y
+CONFIG_CAN_C_CAN=y
+CONFIG_CAN_C_CAN_PLATFORM=y
+CONFIG_AF_RXRPC=y
+# CONFIG_WIRELESS is not set
+CONFIG_DEVTMPFS=y
+CONFIG_DEVTMPFS_MOUNT=y
+CONFIG_OMAP_OCP2SCP=y
+CONFIG_CONNECTOR=y
+CONFIG_MTD=y
+CONFIG_MTD_CMDLINE_PARTS=y
+CONFIG_MTD_BLOCK=y
+CONFIG_MTD_CFI=y
+CONFIG_MTD_CFI_INTELEXT=y
+CONFIG_MTD_RAW_NAND=y
+CONFIG_MTD_NAND_ECC_SW_BCH=y
+CONFIG_MTD_NAND_OMAP2=y
+CONFIG_MTD_NAND_OMAP_BCH=y
+CONFIG_MTD_NAND_PLATFORM=y
+CONFIG_MTD_UBI=y
+CONFIG_MTD_UBI_FASTMAP=y
+CONFIG_BLK_DEV_LOOP=y
+CONFIG_TI_SN74LV165A=y
+CONFIG_EEPROM_AT24=y
+CONFIG_SCSI=y
+# CONFIG_SCSI_PROC_FS is not set
+CONFIG_BLK_DEV_SD=y
+CONFIG_SCSI_SCAN_ASYNC=y
+# CONFIG_SCSI_LOWLEVEL is not set
+CONFIG_NETDEVICES=y
+CONFIG_DUMMY=m
+CONFIG_MACVLAN=m
+CONFIG_IPVLAN=m
+CONFIG_VXLAN=m
+CONFIG_TUN=y
+CONFIG_VETH=m
+CONFIG_NET_DSA_KSZ8863=y
+CONFIG_NET_DSA_MV88E6XXX=y
+# CONFIG_NET_VENDOR_BROADCOM is not set
+# CONFIG_NET_VENDOR_CIRRUS is not set
+# CONFIG_NET_VENDOR_FARADAY is not set
+# CONFIG_NET_VENDOR_INTEL is not set
+# CONFIG_NET_VENDOR_MARVELL is not set
+# CONFIG_NET_VENDOR_MICROCHIP is not set
+# CONFIG_NET_VENDOR_NATSEMI is not set
+# CONFIG_NET_VENDOR_SEEQ is not set
+# CONFIG_NET_VENDOR_SMSC is not set
+# CONFIG_NET_VENDOR_STMICRO is not set
+CONFIG_TI_DAVINCI_EMAC=y
+CONFIG_TI_CPSW=y
+# CONFIG_NET_VENDOR_WIZNET is not set
+CONFIG_SWCONFIG=y
+CONFIG_INTEL_XWAY_PHY=y
+CONFIG_MARVELL_PHY=y
+CONFIG_MICREL_PHY=y
+CONFIG_SWCFG_KSZ8863=y
+CONFIG_SWCFG_MV88E6321=y
+CONFIG_MDIO_BITBANG=y
+CONFIG_MDIO_GPIO=y
+CONFIG_PPP=y
+CONFIG_PPP_DEFLATE=y
+CONFIG_PPP_FILTER=y
+CONFIG_PPP_MPPE=y
+CONFIG_PPP_MULTILINK=y
+CONFIG_PPP_ASYNC=y
+CONFIG_PPP_SYNC_TTY=y
+CONFIG_USB_USBNET=y
+# CONFIG_USB_NET_AX8817X is not set
+# CONFIG_USB_NET_AX88179_178A is not set
+# CONFIG_USB_NET_CDCETHER is not set
+# CONFIG_USB_NET_CDC_NCM is not set
+# CONFIG_USB_NET_NET1080 is not set
+# CONFIG_USB_NET_CDC_SUBSET is not set
+# CONFIG_USB_NET_ZAURUS is not set
+CONFIG_USB_NET_QMI_WWAN=y
+# CONFIG_WLAN is not set
+CONFIG_INPUT_EVDEV=y
+# CONFIG_KEYBOARD_ATKBD is not set
+CONFIG_KEYBOARD_GPIO=y
+# CONFIG_INPUT_MOUSE is not set
+# CONFIG_SERIO is not set
+# CONFIG_LEGACY_PTYS is not set
+CONFIG_SERIAL_OMAP=y
+CONFIG_SERIAL_OMAP_RTU=y
+CONFIG_SERIAL_OMAP_CONSOLE=y
+CONFIG_HW_RANDOM=y
+# CONFIG_I2C_COMPAT is not set
+CONFIG_I2C_CHARDEV=y
+CONFIG_I2C_MUX=y
+CONFIG_I2C_MUX_PCA9541=y
+CONFIG_I2C_MUX_PCA954x=y
+CONFIG_SPI=y
+CONFIG_SPI_OMAP24XX=y
+CONFIG_SPI_KBUS=y
+CONFIG_SPI_SPIDEV=y
+CONFIG_PINCTRL_SINGLE=y
+CONFIG_GPIO_SYSFS=y
+CONFIG_GPIO_PCA953X=y
+CONFIG_GPIO_PCA953X_IRQ=y
+CONFIG_GPIO_PCF857X=y
+CONFIG_GPIO_TWL4030=y
+# CONFIG_HWMON is not set
+CONFIG_WATCHDOG=y
+CONFIG_GPIO_WATCHDOG=y
+CONFIG_OMAP_WATCHDOG=y
+CONFIG_MFD_TI_AM335X_TSCADC=y
+CONFIG_MFD_TPS65218=y
+CONFIG_MFD_TPS65910=y
+CONFIG_REGULATOR_PBIAS=y
+CONFIG_REGULATOR_TPS65218=y
+CONFIG_REGULATOR_TPS65910=y
+# CONFIG_HID_GENERIC is not set
+# CONFIG_USB_HID is not set
+CONFIG_USB=y
+CONFIG_USB_ANNOUNCE_NEW_DEVICES=y
+CONFIG_USB_EHCI_HCD=y
+CONFIG_USB_EHCI_ROOT_HUB_TT=y
+CONFIG_USB_OHCI_HCD=y
+CONFIG_USB_STORAGE=y
+CONFIG_USB_UAS=y
+CONFIG_USB_MUSB_HDRC=y
+CONFIG_USB_MUSB_HOST=y
+CONFIG_USB_MUSB_OMAP2PLUS=y
+CONFIG_USB_MUSB_AM35X=y
+CONFIG_USB_MUSB_DSPS=y
+CONFIG_USB_INVENTRA_DMA=y
+CONFIG_USB_TI_CPPI41_DMA=y
+CONFIG_USB_SERIAL=y
+CONFIG_USB_SERIAL_QUALCOMM=y
+CONFIG_USB_SERIAL_OPTION=y
+CONFIG_NOP_USB_XCEIV=y
+CONFIG_AM335X_PHY_USB=y
+CONFIG_USB_ULPI=y
+CONFIG_USB_GADGET=y
+CONFIG_MMC=y
+CONFIG_MMC_SDHCI=y
+CONFIG_MMC_SDHCI_PLTFM=y
+CONFIG_MMC_OMAP_HS=y
+CONFIG_MMC_SDHCI_OMAP=y
+CONFIG_NEW_LEDS=y
+CONFIG_LEDS_CLASS=y
+CONFIG_LEDS_GPIO=y
+CONFIG_LEDS_PCA955X=y
+CONFIG_LEDS_TRIGGERS=y
+CONFIG_LEDS_TRIGGER_TIMER=y
+CONFIG_LEDS_TRIGGER_ONESHOT=y
+CONFIG_LEDS_TRIGGER_HEARTBEAT=y
+CONFIG_LEDS_TRIGGER_GPIO=y
+CONFIG_LEDS_TRIGGER_DEFAULT_ON=y
+CONFIG_LEDS_TRIGGER_TRANSIENT=y
+CONFIG_RTC_CLASS=y
+CONFIG_RTC_DRV_RS5C372=y
+CONFIG_RTC_DRV_ISL1208=y
+CONFIG_DMADEVICES=y
+CONFIG_ASYNC_TX_DMA=y
+CONFIG_UIO=y
+CONFIG_UIO_PDRV_GENIRQ=y
+# CONFIG_VIRTIO_MENU is not set
+# CONFIG_VHOST_MENU is not set
+# CONFIG_IOMMU_SUPPORT is not set
+CONFIG_PWM=y
+CONFIG_PWM_TIECAP=y
+CONFIG_PWM_TIEHRPWM=y
+CONFIG_OMAP_USB2=y
+CONFIG_EXT4_FS=y
+CONFIG_EXT4_FS_POSIX_ACL=y
+CONFIG_EXT4_FS_SECURITY=y
+CONFIG_FANOTIFY=y
+CONFIG_FUSE_FS=y
+CONFIG_CUSE=m
+CONFIG_OVERLAY_FS=y
+CONFIG_MSDOS_FS=y
+CONFIG_VFAT_FS=y
+CONFIG_NTFS_FS=y
+CONFIG_NTFS_RW=y
+CONFIG_TMPFS=y
+CONFIG_TMPFS_POSIX_ACL=y
+CONFIG_CONFIGFS_FS=y
+CONFIG_UBIFS_FS=y
+CONFIG_SQUASHFS=y
+CONFIG_NFS_FS=y
+CONFIG_NFS_V3_ACL=y
+CONFIG_NFS_V4=y
+CONFIG_ROOT_NFS=y
+CONFIG_NLS_CODEPAGE_437=y
+CONFIG_NLS_ISO8859_1=y
+CONFIG_NLS_UTF8=y
+CONFIG_CRYPTO_USER=y
+CONFIG_CRYPTO_CCM=y
+CONFIG_CRYPTO_XCBC=y
+CONFIG_CRYPTO_XXHASH=y
+CONFIG_CRYPTO_BLAKE2B=y
+CONFIG_CRYPTO_MICHAEL_MIC=y
+CONFIG_CRYPTO_BLOWFISH=y
+CONFIG_CRYPTO_CAMELLIA=y
+CONFIG_CRYPTO_CAST5=y
+CONFIG_CRYPTO_CAST6=y
+CONFIG_CRYPTO_SERPENT=y
+CONFIG_CRYPTO_TWOFISH=y
+CONFIG_CRYPTO_ANSI_CPRNG=y
+CONFIG_CRYPTO_USER_API_HASH=y
+CONFIG_CRYPTO_USER_API_SKCIPHER=y
+CONFIG_CRYPTO_DEV_OMAP=y
+CONFIG_CRYPTO_DEV_OMAP_SHAM=y
+CONFIG_CRYPTO_DEV_OMAP_AES=y
+CONFIG_CRYPTO_DEV_OMAP_DES=y
+CONFIG_ASYMMETRIC_KEY_TYPE=y
+CONFIG_ASYMMETRIC_PUBLIC_KEY_SUBTYPE=y
+CONFIG_X509_CERTIFICATE_PARSER=y
+CONFIG_CRC_ITU_T=y
+CONFIG_CRC7=y
+CONFIG_XZ_DEC=y
+CONFIG_PRINTK_TIME=y
+CONFIG_DYNAMIC_DEBUG=y
+CONFIG_DEBUG_INFO=y
+CONFIG_MAGIC_SYSRQ=y
+CONFIG_DEBUG_FS=y
+CONFIG_FUNCTION_TRACER=y
+# CONFIG_UPROBE_EVENTS is not set
+# CONFIG_RUNTIME_TESTING_MENU is not set
diff --git a/arch/arm/configs/imx6_vtpctp_defconfig b/arch/arm/configs/imx6_vtpctp_defconfig
new file mode 100644
index 000000000000..d008d5791eba
--- /dev/null
+++ b/arch/arm/configs/imx6_vtpctp_defconfig
@@ -0,0 +1,458 @@
+CONFIG_KERNEL_LZO=y
+# CONFIG_SWAP is not set
+CONFIG_SYSVIPC=y
+CONFIG_POSIX_MQUEUE=y
+CONFIG_HIGH_RES_TIMERS=y
+CONFIG_PREEMPT_RT=y
+CONFIG_IKCONFIG=y
+CONFIG_IKCONFIG_PROC=y
+CONFIG_CGROUPS=y
+CONFIG_MEMCG=y
+CONFIG_BLK_CGROUP=y
+CONFIG_CGROUP_SCHED=y
+CONFIG_CFS_BANDWIDTH=y
+CONFIG_CGROUP_PIDS=y
+CONFIG_CGROUP_FREEZER=y
+CONFIG_CGROUP_DEVICE=y
+CONFIG_CGROUP_CPUACCT=y
+CONFIG_CGROUP_PERF=y
+CONFIG_NAMESPACES=y
+CONFIG_USER_NS=y
+CONFIG_EXPERT=y
+CONFIG_BPF_SYSCALL=y
+# CONFIG_SLUB_DEBUG is not set
+# CONFIG_COMPAT_BRK is not set
+CONFIG_PROFILING=y
+CONFIG_ARCH_MXC=y
+CONFIG_SOC_IMX6Q=y
+CONFIG_SOC_IMX6SL=y
+CONFIG_SOC_IMX6SX=y
+CONFIG_WAGO_SYSTEM_BASED_STARTUP=y
+CONFIG_IRQ_PRIORITY_TABLE=y
+CONFIG_ARM_THUMBEE=y
+# CONFIG_ARM_ERRATA_643719 is not set
+CONFIG_SMP=y
+CONFIG_VMSPLIT_2G=y
+CONFIG_HIGHMEM=y
+# CONFIG_HIGHPTE is not set
+CONFIG_FORCE_MAX_ZONEORDER=14
+CONFIG_UACCESS_WITH_MEMCPY=y
+# CONFIG_ATAGS is not set
+CONFIG_CPU_FREQ=y
+CONFIG_CPUFREQ_DT=y
+CONFIG_VFP=y
+CONFIG_NEON=y
+CONFIG_KERNEL_MODE_NEON=y
+CONFIG_KPROBES=y
+CONFIG_MODULES=y
+CONFIG_MODULE_FORCE_LOAD=y
+CONFIG_MODULE_UNLOAD=y
+CONFIG_MODULE_FORCE_UNLOAD=y
+CONFIG_MODVERSIONS=y
+CONFIG_MODULE_SRCVERSION_ALL=y
+CONFIG_BLK_DEV_BSGLIB=y
+CONFIG_BLK_DEV_INTEGRITY=y
+CONFIG_BLK_DEV_THROTTLING=y
+CONFIG_PARTITION_ADVANCED=y
+CONFIG_MAC_PARTITION=y
+CONFIG_BINFMT_MISC=y
+CONFIG_NET=y
+CONFIG_PACKET=y
+CONFIG_UNIX=y
+CONFIG_XFRM_USER=y
+CONFIG_NET_KEY=y
+CONFIG_NET_KEY_MIGRATE=y
+CONFIG_XDP_SOCKETS=y
+CONFIG_INET=y
+CONFIG_IP_MULTICAST=y
+CONFIG_IP_ADVANCED_ROUTER=y
+CONFIG_IP_MULTIPLE_TABLES=y
+CONFIG_IP_PNP=y
+CONFIG_IP_PNP_DHCP=y
+CONFIG_IP_PNP_BOOTP=y
+CONFIG_IP_PNP_RARP=y
+CONFIG_IP_MROUTE=y
+CONFIG_IP_MROUTE_MULTIPLE_TABLES=y
+CONFIG_IP_PIMSM_V1=y
+CONFIG_IP_PIMSM_V2=y
+CONFIG_SYN_COOKIES=y
+CONFIG_INET_AH=y
+CONFIG_INET_ESP=y
+CONFIG_INET_IPCOMP=y
+CONFIG_TCP_CONG_ADVANCED=y
+# CONFIG_TCP_CONG_BIC is not set
+# CONFIG_TCP_CONG_WESTWOOD is not set
+# CONFIG_TCP_CONG_HTCP is not set
+CONFIG_IPV6_ROUTER_PREF=y
+CONFIG_IPV6_ROUTE_INFO=y
+CONFIG_IPV6_OPTIMISTIC_DAD=y
+CONFIG_INET6_AH=m
+CONFIG_INET6_ESP=m
+CONFIG_INET6_IPCOMP=m
+CONFIG_IPV6_MIP6=m
+CONFIG_IPV6_SIT=m
+CONFIG_IPV6_MULTIPLE_TABLES=y
+CONFIG_IPV6_SUBTREES=y
+CONFIG_IPV6_MROUTE=y
+CONFIG_IPV6_MROUTE_MULTIPLE_TABLES=y
+CONFIG_IPV6_PIMSM_V2=y
+CONFIG_NETFILTER=y
+CONFIG_BRIDGE_NETFILTER=y
+CONFIG_NF_CONNTRACK=y
+CONFIG_NF_CONNTRACK_MARK=y
+CONFIG_NF_CONNTRACK_FTP=y
+CONFIG_NF_CONNTRACK_SNMP=y
+CONFIG_NF_CONNTRACK_TFTP=y
+CONFIG_NF_CT_NETLINK=y
+CONFIG_NETFILTER_XT_TARGET_HL=m
+CONFIG_NETFILTER_XT_TARGET_TCPMSS=y
+CONFIG_NETFILTER_XT_MATCH_ADDRTYPE=y
+CONFIG_NETFILTER_XT_MATCH_CONNLIMIT=y
+CONFIG_NETFILTER_XT_MATCH_CONNTRACK=y
+CONFIG_NETFILTER_XT_MATCH_ESP=y
+CONFIG_NETFILTER_XT_MATCH_IPRANGE=y
+CONFIG_NETFILTER_XT_MATCH_IPVS=m
+CONFIG_NETFILTER_XT_MATCH_LIMIT=y
+CONFIG_NETFILTER_XT_MATCH_MAC=y
+CONFIG_NETFILTER_XT_MATCH_MARK=y
+CONFIG_NETFILTER_XT_MATCH_POLICY=y
+CONFIG_NETFILTER_XT_MATCH_PHYSDEV=y
+CONFIG_IP_VS=m
+CONFIG_IP_VS_PROTO_TCP=y
+CONFIG_IP_VS_PROTO_UDP=y
+CONFIG_IP_VS_RR=m
+CONFIG_IP_VS_WRR=m
+CONFIG_IP_VS_NFCT=y
+CONFIG_IP_NF_IPTABLES=y
+CONFIG_IP_NF_MATCH_AH=y
+CONFIG_IP_NF_FILTER=y
+CONFIG_IP_NF_TARGET_REJECT=y
+CONFIG_IP_NF_NAT=y
+CONFIG_IP_NF_TARGET_MASQUERADE=y
+CONFIG_IP_NF_TARGET_REDIRECT=y
+CONFIG_IP_NF_MANGLE=y
+CONFIG_IP_NF_RAW=y
+CONFIG_BRIDGE_NF_EBTABLES=y
+CONFIG_BRIDGE_EBT_BROUTE=y
+CONFIG_BRIDGE_EBT_T_FILTER=y
+CONFIG_BRIDGE_EBT_T_NAT=y
+CONFIG_BRIDGE_EBT_IP=y
+CONFIG_BRIDGE_EBT_LIMIT=y
+CONFIG_BRIDGE_EBT_LOG=y
+CONFIG_BRIDGE=y
+CONFIG_NET_SCHED=y
+CONFIG_NET_SCH_HTB=y
+CONFIG_NET_SCH_HFSC=y
+CONFIG_NET_SCH_PRIO=y
+CONFIG_NET_SCH_TBF=y
+CONFIG_NET_SCH_CODEL=y
+CONFIG_NET_SCH_FQ_CODEL=y
+CONFIG_NET_CLS_BASIC=y
+CONFIG_NET_CLS_TCINDEX=y
+CONFIG_NET_CLS_ROUTE4=y
+CONFIG_NET_CLS_FW=y
+CONFIG_NET_CLS_U32=y
+CONFIG_CLS_U32_PERF=y
+CONFIG_CLS_U32_MARK=y
+CONFIG_NET_CLS_RSVP=y
+CONFIG_NET_CLS_FLOW=y
+CONFIG_NET_CLS_CGROUP=m
+CONFIG_NET_EMATCH=y
+CONFIG_NET_EMATCH_CMP=y
+CONFIG_NET_EMATCH_NBYTE=y
+CONFIG_NET_EMATCH_U32=y
+CONFIG_NET_EMATCH_META=y
+CONFIG_NET_EMATCH_TEXT=y
+CONFIG_NET_CLS_ACT=y
+CONFIG_NET_ACT_POLICE=y
+CONFIG_NET_ACT_GACT=y
+CONFIG_GACT_PROB=y
+CONFIG_NET_ACT_MIRRED=y
+CONFIG_NET_ACT_IPT=y
+CONFIG_NET_ACT_NAT=y
+CONFIG_NET_ACT_PEDIT=y
+CONFIG_NET_ACT_SKBEDIT=y
+CONFIG_NET_ACT_CSUM=y
+CONFIG_NETLINK_DIAG=y
+CONFIG_CGROUP_NET_PRIO=y
+CONFIG_BPF_JIT=y
+CONFIG_CAN=y
+CONFIG_CAN_FLEXCAN=y
+CONFIG_CAN_M_CAN=y
+# CONFIG_WIRELESS is not set
+CONFIG_DEVTMPFS=y
+CONFIG_DEVTMPFS_MOUNT=y
+# CONFIG_STANDALONE is not set
+CONFIG_IMX_WEIM=y
+CONFIG_CONNECTOR=y
+CONFIG_BLK_DEV_LOOP=y
+CONFIG_BLK_DEV_RAM=y
+CONFIG_BLK_DEV_RAM_SIZE=65536
+# CONFIG_ENCSW is not set
+CONFIG_EEPROM_AT24=y
+CONFIG_EEPROM_AT25=y
+CONFIG_SCSI=y
+# CONFIG_SCSI_PROC_FS is not set
+CONFIG_BLK_DEV_SD=y
+# CONFIG_SCSI_LOWLEVEL is not set
+CONFIG_NETDEVICES=y
+CONFIG_DUMMY=m
+CONFIG_MACVLAN=m
+CONFIG_IPVLAN=m
+CONFIG_VXLAN=m
+CONFIG_TUN=y
+CONFIG_VETH=m
+CONFIG_NET_DSA_KSZ8863=y
+# CONFIG_NET_VENDOR_AMAZON is not set
+# CONFIG_NET_VENDOR_ARC is not set
+# CONFIG_NET_VENDOR_BROADCOM is not set
+# CONFIG_NET_VENDOR_CIRRUS is not set
+# CONFIG_NET_VENDOR_EZCHIP is not set
+# CONFIG_NET_VENDOR_FARADAY is not set
+CONFIG_FSL_PQ_MDIO=y
+CONFIG_FSL_XGMAC_MDIO=y
+# CONFIG_NET_VENDOR_HISILICON is not set
+# CONFIG_NET_VENDOR_INTEL is not set
+# CONFIG_NET_VENDOR_MARVELL is not set
+# CONFIG_NET_VENDOR_MICREL is not set
+# CONFIG_NET_VENDOR_MICROCHIP is not set
+# CONFIG_NET_VENDOR_NATSEMI is not set
+# CONFIG_NET_VENDOR_NETRONOME is not set
+# CONFIG_NET_VENDOR_QUALCOMM is not set
+# CONFIG_NET_VENDOR_RENESAS is not set
+# CONFIG_NET_VENDOR_ROCKER is not set
+# CONFIG_NET_VENDOR_SAMSUNG is not set
+# CONFIG_NET_VENDOR_SEEQ is not set
+# CONFIG_NET_VENDOR_SMSC is not set
+# CONFIG_NET_VENDOR_STMICRO is not set
+# CONFIG_NET_VENDOR_SYNOPSYS is not set
+# CONFIG_NET_VENDOR_VIA is not set
+# CONFIG_NET_VENDOR_WIZNET is not set
+CONFIG_SWCONFIG=y
+CONFIG_MICREL_PHY=y
+CONFIG_SWCFG_KSZ8863=y
+CONFIG_MDIO_BITBANG=y
+CONFIG_MDIO_GPIO=y
+CONFIG_PPP=y
+CONFIG_PPP_DEFLATE=y
+CONFIG_PPP_FILTER=y
+CONFIG_PPP_MPPE=y
+CONFIG_PPP_MULTILINK=y
+CONFIG_PPP_ASYNC=y
+CONFIG_PPP_SYNC_TTY=y
+CONFIG_USB_USBNET=y
+# CONFIG_USB_NET_AX8817X is not set
+# CONFIG_USB_NET_AX88179_178A is not set
+# CONFIG_USB_NET_CDC_NCM is not set
+# CONFIG_USB_NET_NET1080 is not set
+CONFIG_USB_NET_RNDIS_HOST=y
+# CONFIG_USB_NET_CDC_SUBSET is not set
+# CONFIG_USB_NET_ZAURUS is not set
+# CONFIG_WLAN is not set
+CONFIG_INPUT_MOUSEDEV=y
+CONFIG_INPUT_EVDEV=y
+CONFIG_KEYBOARD_QT1070=m
+CONFIG_KEYBOARD_GPIO=y
+CONFIG_MOUSE_PS2_ELANTECH=y
+CONFIG_INPUT_TOUCHSCREEN=y
+CONFIG_TOUCHSCREEN_ADS7846=y
+CONFIG_TOUCHSCREEN_PIXCIR=y
+CONFIG_TOUCHSCREEN_USB_COMPOSITE=y
+# CONFIG_TOUCHSCREEN_USB_EGALAX is not set
+# CONFIG_TOUCHSCREEN_USB_PANJIT is not set
+# CONFIG_TOUCHSCREEN_USB_3M is not set
+# CONFIG_TOUCHSCREEN_USB_ITM is not set
+# CONFIG_TOUCHSCREEN_USB_ETURBO is not set
+# CONFIG_TOUCHSCREEN_USB_GUNZE is not set
+# CONFIG_TOUCHSCREEN_USB_DMC_TSC10 is not set
+# CONFIG_TOUCHSCREEN_USB_IRTOUCH is not set
+# CONFIG_TOUCHSCREEN_USB_IDEALTEK is not set
+# CONFIG_TOUCHSCREEN_USB_GENERAL_TOUCH is not set
+# CONFIG_TOUCHSCREEN_USB_GOTOP is not set
+# CONFIG_TOUCHSCREEN_USB_JASTEC is not set
+# CONFIG_TOUCHSCREEN_USB_ELO is not set
+# CONFIG_TOUCHSCREEN_USB_E2I is not set
+# CONFIG_TOUCHSCREEN_USB_ZYTRONIC is not set
+# CONFIG_TOUCHSCREEN_USB_ETT_TC45USB is not set
+# CONFIG_TOUCHSCREEN_USB_NEXIO is not set
+# CONFIG_SERIO_SERPORT is not set
+CONFIG_VT_HW_CONSOLE_BINDING=y
+# CONFIG_LEGACY_PTYS is not set
+CONFIG_SERIAL_IMX=y
+CONFIG_SERIAL_IMX_CONSOLE=y
+# CONFIG_SERIAL_OMAP_MODBUS is not set
+# CONFIG_I2C_COMPAT is not set
+CONFIG_I2C_CHARDEV=y
+# CONFIG_I2C_HELPER_AUTO is not set
+CONFIG_I2C_IMX=y
+CONFIG_SPI=y
+CONFIG_SPI_IMX=y
+CONFIG_SPI_SPIDEV=y
+CONFIG_GPIO_SYSFS=y
+CONFIG_GPIO_MXC=y
+CONFIG_GPIO_PCA953X=y
+CONFIG_GPIO_PCA953X_IRQ=y
+CONFIG_POWER_SUPPLY=y
+CONFIG_SENSORS_LM75=y
+CONFIG_CPU_THERMAL=y
+CONFIG_IMX_THERMAL=y
+CONFIG_WATCHDOG=y
+CONFIG_WATCHDOG_SYSFS=y
+CONFIG_GPIO_WATCHDOG=y
+CONFIG_IMX2_WDT=y
+CONFIG_REGULATOR=y
+CONFIG_REGULATOR_FIXED_VOLTAGE=y
+CONFIG_REGULATOR_ANATOP=y
+CONFIG_REGULATOR_PFUZE100=y
+CONFIG_IMX_IPUV3_CORE=y
+CONFIG_DRM=y
+CONFIG_DRM_PANEL_SIMPLE=y
+CONFIG_DRM_IMX=y
+CONFIG_DRM_IMX_PARALLEL_DISPLAY=y
+CONFIG_DRM_IMX_LDB=y
+CONFIG_DRM_IMX_HDMI=y
+CONFIG_DRM_ETNAVIV=y
+CONFIG_FB_MODE_HELPERS=y
+CONFIG_FB_TILEBLITTING=y
+# CONFIG_FB_MX3 is not set
+CONFIG_LCD_CLASS_DEVICE=y
+CONFIG_LCD_PLATFORM=y
+CONFIG_BACKLIGHT_CLASS_DEVICE=y
+CONFIG_BACKLIGHT_PWM=y
+CONFIG_SOUND=y
+CONFIG_SND=y
+# CONFIG_SND_SPI is not set
+# CONFIG_SND_USB is not set
+CONFIG_SND_SOC=y
+CONFIG_SND_IMX_SOC=y
+CONFIG_SND_SOC_IMX_SGTL5000=y
+CONFIG_HIDRAW=y
+CONFIG_HID_MULTITOUCH=y
+CONFIG_HID_MULTITOUCH_DISABLE_SINGLETOUCH_EVENTS=y
+CONFIG_I2C_HID=y
+CONFIG_USB_OTG=y
+CONFIG_USB_EHCI_HCD=y
+CONFIG_USB_EHCI_MXC=y
+CONFIG_USB_OHCI_HCD=y
+CONFIG_USB_STORAGE=y
+CONFIG_USB_CHIPIDEA=y
+CONFIG_USB_CHIPIDEA_UDC=y
+CONFIG_USB_CHIPIDEA_HOST=y
+CONFIG_USB_SERIAL=y
+CONFIG_USB_SERIAL_QUALCOMM=y
+CONFIG_USB_SERIAL_OPTION=y
+CONFIG_NOP_USB_XCEIV=y
+CONFIG_USB_MXS_PHY=y
+CONFIG_USB_ULPI=y
+CONFIG_USB_GADGET=y
+CONFIG_USB_FSL_USB2=y
+CONFIG_USB_CONFIGFS=y
+CONFIG_USB_CONFIGFS_ACM=y
+CONFIG_USB_CONFIGFS_NCM=y
+CONFIG_USB_CONFIGFS_ECM=y
+CONFIG_USB_CONFIGFS_RNDIS=y
+CONFIG_USB_CONFIGFS_MASS_STORAGE=y
+CONFIG_USB_ETH=y
+CONFIG_USB_G_NCM=y
+CONFIG_USB_FUNCTIONFS=m
+CONFIG_USB_FUNCTIONFS_ETH=y
+CONFIG_USB_FUNCTIONFS_RNDIS=y
+CONFIG_USB_G_MULTI=m
+CONFIG_USB_G_MULTI_CDC=y
+CONFIG_MMC=y
+CONFIG_MMC_SDHCI=y
+CONFIG_MMC_SDHCI_PLTFM=y
+CONFIG_MMC_SDHCI_ESDHC_IMX=y
+CONFIG_NEW_LEDS=y
+CONFIG_LEDS_CLASS=y
+CONFIG_LEDS_GPIO=y
+CONFIG_LEDS_PCA955X=y
+CONFIG_LEDS_TRIGGERS=y
+CONFIG_LEDS_TRIGGER_TIMER=y
+CONFIG_LEDS_TRIGGER_ONESHOT=y
+CONFIG_LEDS_TRIGGER_HEARTBEAT=y
+CONFIG_LEDS_TRIGGER_BACKLIGHT=y
+CONFIG_LEDS_TRIGGER_GPIO=y
+CONFIG_LEDS_TRIGGER_DEFAULT_ON=y
+CONFIG_LEDS_TRIGGER_TRANSIENT=y
+CONFIG_RTC_CLASS=y
+CONFIG_RTC_DRV_RS5C372=y
+CONFIG_DMADEVICES=y
+CONFIG_IMX_SDMA=y
+CONFIG_MXS_DMA=y
+CONFIG_UIO=y
+CONFIG_UIO_PDRV_GENIRQ=y
+CONFIG_UIO_DMEM_GENIRQ=y
+CONFIG_UIO_PRUSS=y
+CONFIG_STAGING=y
+CONFIG_STAGING_MEDIA=y
+CONFIG_EXTCON_PTN5150=m
+CONFIG_IIO=y
+CONFIG_IIO_BUFFER_CB=y
+CONFIG_IIO_TRIGGERED_BUFFER=y
+CONFIG_IIO_SW_DEVICE=y
+CONFIG_IIO_SW_TRIGGER=y
+CONFIG_SI1145=m
+CONFIG_IIO_HRTIMER_TRIGGER=y
+CONFIG_PWM=y
+CONFIG_PWM_IMX27=y
+CONFIG_EXT4_FS=y
+CONFIG_EXT4_FS_POSIX_ACL=y
+CONFIG_EXT4_FS_SECURITY=y
+CONFIG_FANOTIFY=y
+CONFIG_FUSE_FS=y
+CONFIG_OVERLAY_FS=y
+CONFIG_MSDOS_FS=y
+CONFIG_VFAT_FS=y
+CONFIG_NTFS_FS=y
+CONFIG_NTFS_RW=y
+CONFIG_TMPFS_POSIX_ACL=y
+CONFIG_SQUASHFS=y
+CONFIG_NFS_FS=y
+CONFIG_NFS_V3_ACL=y
+CONFIG_NFS_V4=y
+CONFIG_ROOT_NFS=y
+CONFIG_NLS_DEFAULT="437"
+CONFIG_NLS_CODEPAGE_437=y
+CONFIG_NLS_ISO8859_1=y
+CONFIG_NLS_UTF8=y
+CONFIG_SECURITY=y
+CONFIG_SECURITYFS=y
+CONFIG_CRYPTO_USER=y
+CONFIG_CRYPTO_CCM=y
+CONFIG_CRYPTO_XCBC=y
+CONFIG_CRYPTO_XXHASH=y
+CONFIG_CRYPTO_BLAKE2B=y
+CONFIG_CRYPTO_MICHAEL_MIC=y
+CONFIG_CRYPTO_SHA512=y
+CONFIG_CRYPTO_BLOWFISH=y
+CONFIG_CRYPTO_CAMELLIA=y
+CONFIG_CRYPTO_CAST5=y
+CONFIG_CRYPTO_CAST6=y
+CONFIG_CRYPTO_SERPENT=y
+CONFIG_CRYPTO_TWOFISH=y
+CONFIG_CRYPTO_ANSI_CPRNG=y
+CONFIG_CRYPTO_USER_API_HASH=y
+CONFIG_CRYPTO_USER_API_SKCIPHER=y
+CONFIG_CRYPTO_DEV_FSL_CAAM=y
+CONFIG_CRYPTO_DEV_SAHARA=y
+CONFIG_ASYMMETRIC_KEY_TYPE=y
+CONFIG_ASYMMETRIC_PUBLIC_KEY_SUBTYPE=y
+CONFIG_X509_CERTIFICATE_PARSER=y
+CONFIG_CRC_ITU_T=y
+CONFIG_CRC7=y
+CONFIG_XZ_DEC=y
+# CONFIG_XZ_DEC_X86 is not set
+# CONFIG_XZ_DEC_POWERPC is not set
+# CONFIG_XZ_DEC_IA64 is not set
+# CONFIG_XZ_DEC_ARM is not set
+# CONFIG_XZ_DEC_ARMTHUMB is not set
+# CONFIG_XZ_DEC_SPARC is not set
+CONFIG_CMA_SIZE_MBYTES=320
+CONFIG_PRINTK_TIME=y
+CONFIG_DYNAMIC_DEBUG=y
+# CONFIG_DEBUG_BUGVERBOSE is not set
+CONFIG_MAGIC_SYSRQ=y
+CONFIG_DEBUG_FS=y
+# CONFIG_SCHED_DEBUG is not set
+CONFIG_FUNCTION_TRACER=y
diff --git a/arch/arm/configs/stm32mp1_defconfig b/arch/arm/configs/stm32mp1_defconfig
new file mode 100644
index 000000000000..d582707c1d7f
--- /dev/null
+++ b/arch/arm/configs/stm32mp1_defconfig
@@ -0,0 +1,554 @@
+CONFIG_KERNEL_LZO=y
+CONFIG_SYSVIPC=y
+CONFIG_POSIX_MQUEUE=y
+CONFIG_NO_HZ=y
+CONFIG_HIGH_RES_TIMERS=y
+CONFIG_PREEMPT_RT=y
+CONFIG_BSD_PROCESS_ACCT=y
+CONFIG_BSD_PROCESS_ACCT_V3=y
+CONFIG_TASKSTATS=y
+CONFIG_TASK_DELAY_ACCT=y
+CONFIG_TASK_XACCT=y
+CONFIG_TASK_IO_ACCOUNTING=y
+CONFIG_IKCONFIG=y
+CONFIG_IKCONFIG_PROC=y
+CONFIG_LOG_BUF_SHIFT=18
+CONFIG_CGROUPS=y
+CONFIG_MEMCG=y
+CONFIG_BLK_CGROUP=y
+CONFIG_CGROUP_SCHED=y
+CONFIG_CFS_BANDWIDTH=y
+CONFIG_CGROUP_PIDS=y
+CONFIG_CGROUP_FREEZER=y
+CONFIG_CPUSETS=y
+CONFIG_CGROUP_DEVICE=y
+CONFIG_CGROUP_CPUACCT=y
+CONFIG_CGROUP_PERF=y
+CONFIG_NAMESPACES=y
+CONFIG_RELAY=y
+CONFIG_BOOT_CONFIG=y
+CONFIG_CC_OPTIMIZE_FOR_SIZE=y
+CONFIG_EXPERT=y
+CONFIG_KALLSYMS_ALL=y
+# CONFIG_SLUB_DEBUG is not set
+# CONFIG_COMPAT_BRK is not set
+CONFIG_PROFILING=y
+CONFIG_WAGO_SYSTEM_BASED_STARTUP=y
+CONFIG_IRQ_PRIORITY_TABLE=y
+CONFIG_ARCH_STM32=y
+CONFIG_ARM_THUMBEE=y
+CONFIG_PL310_ERRATA_588369=y
+CONFIG_PL310_ERRATA_727915=y
+CONFIG_PL310_ERRATA_753970=y
+CONFIG_PL310_ERRATA_769419=y
+CONFIG_ARM_ERRATA_430973=y
+CONFIG_ARM_ERRATA_720789=y
+CONFIG_ARM_ERRATA_754322=y
+CONFIG_ARM_ERRATA_754327=y
+CONFIG_ARM_ERRATA_764369=y
+CONFIG_ARM_ERRATA_775420=y
+CONFIG_ARM_ERRATA_798181=y
+CONFIG_SMP=y
+CONFIG_MCPM=y
+CONFIG_NR_CPUS=16
+CONFIG_HOTPLUG_CPU=y
+CONFIG_HIGHMEM=y
+CONFIG_FORCE_MAX_ZONEORDER=12
+CONFIG_DEPRECATED_PARAM_STRUCT=y
+CONFIG_ARM_APPENDED_DTB=y
+CONFIG_ARM_ATAG_DTB_COMPAT=y
+CONFIG_CPU_FREQ=y
+CONFIG_CPU_FREQ_STAT=y
+CONFIG_CPU_FREQ_DEFAULT_GOV_USERSPACE=y
+CONFIG_CPUFREQ_DT=y
+CONFIG_CPU_IDLE=y
+CONFIG_ARM_CPUIDLE=y
+CONFIG_VFP=y
+CONFIG_NEON=y
+CONFIG_KERNEL_MODE_NEON=y
+# CONFIG_SUSPEND is not set
+CONFIG_PM=y
+CONFIG_PM_DEBUG=y
+CONFIG_ARM_CRYPTO=y
+CONFIG_CRYPTO_SHA1_ARM_NEON=y
+CONFIG_CRYPTO_SHA1_ARM_CE=m
+CONFIG_CRYPTO_SHA2_ARM_CE=m
+CONFIG_CRYPTO_SHA256_ARM=y
+CONFIG_CRYPTO_SHA512_ARM=y
+CONFIG_CRYPTO_AES_ARM=m
+CONFIG_CRYPTO_AES_ARM_BS=y
+CONFIG_CRYPTO_AES_ARM_CE=m
+CONFIG_CRYPTO_GHASH_ARM_CE=m
+CONFIG_CRYPTO_CRC32_ARM_CE=m
+CONFIG_CRYPTO_CHACHA20_NEON=m
+CONFIG_KPROBES=y
+CONFIG_MODULES=y
+CONFIG_MODULE_FORCE_LOAD=y
+CONFIG_MODULE_UNLOAD=y
+CONFIG_MODULE_FORCE_UNLOAD=y
+CONFIG_MODVERSIONS=y
+CONFIG_MODULE_SRCVERSION_ALL=y
+CONFIG_BLK_DEV_BSGLIB=y
+CONFIG_BLK_DEV_INTEGRITY=y
+CONFIG_BLK_DEV_THROTTLING=y
+CONFIG_PARTITION_ADVANCED=y
+CONFIG_MAC_PARTITION=y
+CONFIG_CMDLINE_PARTITION=y
+CONFIG_CMA=y
+CONFIG_NET=y
+CONFIG_PACKET=y
+CONFIG_PACKET_DIAG=m
+CONFIG_UNIX=y
+CONFIG_UNIX_DIAG=m
+CONFIG_XFRM_USER=y
+CONFIG_NET_KEY=y
+CONFIG_NET_KEY_MIGRATE=y
+CONFIG_INET=y
+CONFIG_IP_MULTICAST=y
+CONFIG_IP_ADVANCED_ROUTER=y
+CONFIG_IP_MULTIPLE_TABLES=y
+CONFIG_IP_PNP=y
+CONFIG_IP_PNP_DHCP=y
+CONFIG_IP_PNP_BOOTP=y
+CONFIG_IP_PNP_RARP=y
+CONFIG_NET_IPIP=m
+CONFIG_NET_IPGRE_DEMUX=m
+CONFIG_NET_IPGRE=m
+CONFIG_IP_MROUTE=y
+CONFIG_IP_MROUTE_MULTIPLE_TABLES=y
+CONFIG_IP_PIMSM_V1=y
+CONFIG_IP_PIMSM_V2=y
+CONFIG_SYN_COOKIES=y
+CONFIG_INET_AH=y
+CONFIG_INET_ESP=y
+CONFIG_INET_IPCOMP=y
+CONFIG_INET_UDP_DIAG=m
+CONFIG_TCP_CONG_ADVANCED=y
+# CONFIG_TCP_CONG_BIC is not set
+# CONFIG_TCP_CONG_CUBIC is not set
+# CONFIG_TCP_CONG_WESTWOOD is not set
+# CONFIG_TCP_CONG_HTCP is not set
+CONFIG_TCP_CONG_HSTCP=m
+CONFIG_TCP_CONG_HYBLA=m
+CONFIG_TCP_CONG_SCALABLE=m
+CONFIG_TCP_CONG_LP=m
+CONFIG_TCP_CONG_VENO=m
+CONFIG_TCP_CONG_YEAH=m
+CONFIG_TCP_CONG_ILLINOIS=m
+CONFIG_IPV6_ROUTER_PREF=y
+CONFIG_IPV6_ROUTE_INFO=y
+CONFIG_IPV6_OPTIMISTIC_DAD=y
+CONFIG_INET6_AH=m
+CONFIG_INET6_ESP=m
+CONFIG_INET6_IPCOMP=m
+CONFIG_IPV6_MIP6=m
+CONFIG_IPV6_SIT=m
+CONFIG_IPV6_GRE=m
+CONFIG_IPV6_MULTIPLE_TABLES=y
+CONFIG_IPV6_SUBTREES=y
+CONFIG_IPV6_MROUTE=y
+CONFIG_IPV6_MROUTE_MULTIPLE_TABLES=y
+CONFIG_IPV6_PIMSM_V2=y
+CONFIG_NETFILTER=y
+CONFIG_BRIDGE_NETFILTER=y
+CONFIG_NF_CONNTRACK=y
+CONFIG_NF_CONNTRACK_FTP=y
+CONFIG_NF_CONNTRACK_SNMP=y
+CONFIG_NF_CONNTRACK_TFTP=y
+CONFIG_NF_CT_NETLINK=y
+CONFIG_NETFILTER_XT_TARGET_CONNMARK=m
+CONFIG_NETFILTER_XT_TARGET_HL=y
+CONFIG_NETFILTER_XT_TARGET_TCPMSS=y
+CONFIG_NETFILTER_XT_MATCH_ADDRTYPE=m
+CONFIG_NETFILTER_XT_MATCH_CONNLIMIT=y
+CONFIG_NETFILTER_XT_MATCH_CONNMARK=m
+CONFIG_NETFILTER_XT_MATCH_CONNTRACK=y
+CONFIG_NETFILTER_XT_MATCH_ECN=y
+CONFIG_NETFILTER_XT_MATCH_ESP=y
+CONFIG_NETFILTER_XT_MATCH_HL=y
+CONFIG_NETFILTER_XT_MATCH_IPRANGE=y
+CONFIG_NETFILTER_XT_MATCH_IPVS=m
+# CONFIG_NETFILTER_XT_MATCH_L2TP is not set
+CONFIG_NETFILTER_XT_MATCH_LIMIT=y
+CONFIG_NETFILTER_XT_MATCH_MAC=y
+CONFIG_NETFILTER_XT_MATCH_MARK=y
+CONFIG_NETFILTER_XT_MATCH_POLICY=y
+CONFIG_NETFILTER_XT_MATCH_PHYSDEV=y
+CONFIG_IP_VS=m
+CONFIG_IP_VS_PROTO_TCP=y
+CONFIG_IP_VS_PROTO_UDP=y
+CONFIG_IP_VS_RR=m
+CONFIG_IP_VS_WRR=m
+CONFIG_IP_VS_NFCT=y
+CONFIG_IP_NF_IPTABLES=y
+CONFIG_IP_NF_MATCH_AH=y
+CONFIG_IP_NF_MATCH_ECN=m
+CONFIG_IP_NF_MATCH_RPFILTER=m
+CONFIG_IP_NF_MATCH_TTL=m
+CONFIG_IP_NF_FILTER=y
+CONFIG_IP_NF_TARGET_REJECT=y
+CONFIG_IP_NF_NAT=y
+CONFIG_IP_NF_TARGET_MASQUERADE=y
+CONFIG_IP_NF_TARGET_REDIRECT=m
+CONFIG_IP_NF_MANGLE=y
+CONFIG_IP_NF_TARGET_ECN=m
+CONFIG_IP_NF_TARGET_TTL=m
+CONFIG_IP_NF_RAW=y
+CONFIG_IP_NF_SECURITY=m
+CONFIG_IP_NF_ARPTABLES=m
+CONFIG_IP_NF_ARPFILTER=m
+CONFIG_IP_NF_ARP_MANGLE=m
+CONFIG_IP6_NF_IPTABLES=m
+CONFIG_IP6_NF_MATCH_AH=m
+CONFIG_IP6_NF_MATCH_EUI64=m
+CONFIG_IP6_NF_MATCH_FRAG=m
+CONFIG_IP6_NF_MATCH_OPTS=m
+CONFIG_IP6_NF_MATCH_HL=m
+CONFIG_IP6_NF_MATCH_IPV6HEADER=m
+CONFIG_IP6_NF_MATCH_MH=m
+CONFIG_IP6_NF_MATCH_RPFILTER=m
+CONFIG_IP6_NF_MATCH_RT=m
+CONFIG_IP6_NF_TARGET_HL=m
+CONFIG_IP6_NF_FILTER=m
+CONFIG_IP6_NF_TARGET_REJECT=m
+CONFIG_IP6_NF_MANGLE=m
+CONFIG_IP6_NF_RAW=m
+CONFIG_IP6_NF_SECURITY=m
+CONFIG_BRIDGE_NF_EBTABLES=y
+CONFIG_BRIDGE_EBT_BROUTE=y
+CONFIG_BRIDGE_EBT_T_FILTER=y
+CONFIG_BRIDGE_EBT_T_NAT=y
+CONFIG_BRIDGE_EBT_IP=y
+CONFIG_BRIDGE_EBT_LIMIT=y
+CONFIG_BRIDGE_EBT_LOG=y
+CONFIG_L2TP=m
+CONFIG_BRIDGE=y
+CONFIG_BRIDGE_VLAN_FILTERING=y
+CONFIG_NET_DSA_TAG_TRAILER=y
+CONFIG_VLAN_8021Q=m
+CONFIG_VLAN_8021Q_GVRP=y
+CONFIG_NET_SCHED=y
+CONFIG_NET_SCH_HTB=y
+CONFIG_NET_SCH_HFSC=y
+CONFIG_NET_SCH_PRIO=y
+CONFIG_NET_SCH_TBF=y
+CONFIG_NET_SCH_CODEL=y
+CONFIG_NET_SCH_FQ_CODEL=y
+CONFIG_NET_CLS_BASIC=y
+CONFIG_NET_CLS_TCINDEX=y
+CONFIG_NET_CLS_ROUTE4=y
+CONFIG_NET_CLS_FW=y
+CONFIG_NET_CLS_U32=y
+CONFIG_CLS_U32_PERF=y
+CONFIG_CLS_U32_MARK=y
+CONFIG_NET_CLS_RSVP=y
+CONFIG_NET_CLS_FLOW=y
+CONFIG_NET_CLS_CGROUP=m
+CONFIG_NET_EMATCH=y
+CONFIG_NET_EMATCH_CMP=y
+CONFIG_NET_EMATCH_NBYTE=y
+CONFIG_NET_EMATCH_U32=y
+CONFIG_NET_EMATCH_META=y
+CONFIG_NET_EMATCH_TEXT=y
+CONFIG_NET_CLS_ACT=y
+CONFIG_NET_ACT_POLICE=y
+CONFIG_NET_ACT_GACT=y
+CONFIG_GACT_PROB=y
+CONFIG_NET_ACT_MIRRED=y
+CONFIG_NET_ACT_IPT=y
+CONFIG_NET_ACT_NAT=y
+CONFIG_NET_ACT_PEDIT=y
+CONFIG_NET_ACT_SKBEDIT=y
+CONFIG_NET_ACT_CSUM=y
+CONFIG_NETLINK_DIAG=y
+CONFIG_NET_L3_MASTER_DEV=y
+CONFIG_CGROUP_NET_PRIO=y
+CONFIG_BPF_JIT=y
+# CONFIG_WIRELESS is not set
+CONFIG_DEVTMPFS=y
+CONFIG_DEVTMPFS_MOUNT=y
+CONFIG_SIMPLE_PM_BUS=y
+CONFIG_MTD=y
+CONFIG_MTD_CMDLINE_PARTS=y
+CONFIG_MTD_BLOCK=y
+CONFIG_MTD_SPI_NOR=y
+CONFIG_MTD_UBI=y
+CONFIG_OF_OVERLAY=y
+CONFIG_BLK_DEV_LOOP=y
+CONFIG_BLK_DEV_RAM=y
+CONFIG_BLK_DEV_RAM_SIZE=65536
+CONFIG_VIRTIO_BLK=y
+CONFIG_ICS932S401=y
+CONFIG_SRAM=y
+# CONFIG_ENCSW is not set
+CONFIG_EEPROM_AT24=y
+CONFIG_EEPROM_93CX6=y
+CONFIG_SCSI=y
+# CONFIG_SCSI_PROC_FS is not set
+# CONFIG_SCSI_LOWLEVEL is not set
+CONFIG_NETDEVICES=y
+CONFIG_TUN=y
+CONFIG_VETH=y
+CONFIG_NET_DSA_KSZ8863=y
+# CONFIG_NET_VENDOR_ALACRITECH is not set
+# CONFIG_NET_VENDOR_AMAZON is not set
+# CONFIG_NET_VENDOR_AQUANTIA is not set
+# CONFIG_NET_VENDOR_ARC is not set
+# CONFIG_NET_VENDOR_AURORA is not set
+# CONFIG_NET_VENDOR_BROADCOM is not set
+# CONFIG_NET_VENDOR_CADENCE is not set
+# CONFIG_NET_VENDOR_CAVIUM is not set
+# CONFIG_NET_VENDOR_CIRRUS is not set
+# CONFIG_NET_VENDOR_CORTINA is not set
+# CONFIG_NET_VENDOR_EZCHIP is not set
+# CONFIG_NET_VENDOR_FARADAY is not set
+# CONFIG_NET_VENDOR_GOOGLE is not set
+# CONFIG_NET_VENDOR_HISILICON is not set
+# CONFIG_NET_VENDOR_HUAWEI is not set
+# CONFIG_NET_VENDOR_INTEL is not set
+# CONFIG_NET_VENDOR_MARVELL is not set
+# CONFIG_NET_VENDOR_MELLANOX is not set
+# CONFIG_NET_VENDOR_MICREL is not set
+# CONFIG_NET_VENDOR_MICROCHIP is not set
+# CONFIG_NET_VENDOR_MICROSEMI is not set
+# CONFIG_NET_VENDOR_NATSEMI is not set
+# CONFIG_NET_VENDOR_NETRONOME is not set
+# CONFIG_NET_VENDOR_NI is not set
+# CONFIG_NET_VENDOR_PENSANDO is not set
+# CONFIG_NET_VENDOR_QUALCOMM is not set
+# CONFIG_NET_VENDOR_RENESAS is not set
+# CONFIG_NET_VENDOR_ROCKER is not set
+# CONFIG_NET_VENDOR_SAMSUNG is not set
+# CONFIG_NET_VENDOR_SEEQ is not set
+# CONFIG_NET_VENDOR_SOLARFLARE is not set
+# CONFIG_NET_VENDOR_SMSC is not set
+# CONFIG_NET_VENDOR_SOCIONEXT is not set
+CONFIG_STMMAC_ETH=y
+CONFIG_DWMAC_DWC_QOS_ETH=y
+# CONFIG_NET_VENDOR_SYNOPSYS is not set
+# CONFIG_NET_VENDOR_VIA is not set
+# CONFIG_NET_VENDOR_WIZNET is not set
+# CONFIG_NET_VENDOR_XILINX is not set
+CONFIG_SWCONFIG=y
+CONFIG_LED_TRIGGER_PHY=y
+CONFIG_MICREL_PHY=y
+CONFIG_MICROCHIP_PHY=y
+CONFIG_SWCFG_KSZ8863=y
+CONFIG_MDIO_BITBANG=y
+CONFIG_MDIO_GPIO=y
+# CONFIG_USB_NET_DRIVERS is not set
+# CONFIG_WLAN is not set
+CONFIG_NET_FAILOVER=y
+CONFIG_INPUT_MATRIXKMAP=y
+CONFIG_INPUT_EVDEV=y
+# CONFIG_KEYBOARD_ATKBD is not set
+CONFIG_KEYBOARD_GPIO=y
+# CONFIG_INPUT_MOUSE is not set
+CONFIG_SERIO_AMBAKMI=y
+CONFIG_SERIO_LIBPS2=y
+CONFIG_VT_HW_CONSOLE_BINDING=y
+CONFIG_SERIAL_8250=y
+CONFIG_SERIAL_8250_CONSOLE=y
+CONFIG_SERIAL_8250_EXTENDED=y
+CONFIG_SERIAL_8250_SHARE_IRQ=y
+CONFIG_SERIAL_OF_PLATFORM=y
+# CONFIG_SERIAL_OMAP_MODBUS is not set
+CONFIG_SERIAL_STM32=y
+CONFIG_SERIAL_STM32_CONSOLE=y
+CONFIG_SERIAL_DEV_BUS=y
+CONFIG_TTY_PRINTK=y
+CONFIG_VIRTIO_CONSOLE=y
+CONFIG_HW_RANDOM=y
+CONFIG_I2C=y
+CONFIG_I2C_CHARDEV=y
+CONFIG_I2C_ARB_GPIO_CHALLENGE=m
+CONFIG_I2C_MUX_PCA954x=y
+CONFIG_I2C_MUX_PINCTRL=y
+CONFIG_I2C_DEMUX_PINCTRL=y
+CONFIG_I2C_NOMADIK=y
+CONFIG_I2C_STM32F7=y
+CONFIG_I2C_SLAVE_EEPROM=y
+CONFIG_SPI=y
+CONFIG_SPI_STM32=y
+CONFIG_SPI_STM32_QSPI=y
+CONFIG_SPI_SPIDEV=y
+CONFIG_SPMI=y
+CONFIG_PINCTRL_SINGLE=y
+CONFIG_GPIO_SYSFS=y
+CONFIG_GPIO_DWAPB=y
+CONFIG_GPIO_SYSCON=y
+CONFIG_POWER_RESET=y
+CONFIG_POWER_RESET_BRCMKONA=y
+CONFIG_POWER_RESET_BRCMSTB=y
+CONFIG_POWER_RESET_GPIO=y
+CONFIG_POWER_RESET_GPIO_RESTART=y
+CONFIG_POWER_RESET_SYSCON=y
+CONFIG_POWER_RESET_SYSCON_POWEROFF=y
+CONFIG_BATTERY_SBS=y
+CONFIG_BATTERY_MAX17040=m
+CONFIG_BATTERY_MAX17042=m
+CONFIG_SENSORS_IIO_HWMON=y
+CONFIG_SENSORS_LM90=y
+CONFIG_SENSORS_LM95245=y
+CONFIG_SENSORS_NTC_THERMISTOR=m
+CONFIG_SENSORS_PWM_FAN=m
+CONFIG_SENSORS_INA2XX=m
+CONFIG_THERMAL=y
+CONFIG_CPU_THERMAL=y
+CONFIG_ST_THERMAL_MEMMAP=y
+CONFIG_WATCHDOG=y
+CONFIG_XILINX_WATCHDOG=y
+CONFIG_ARM_SP805_WATCHDOG=y
+CONFIG_DW_WATCHDOG=y
+CONFIG_STPMIC1_WATCHDOG=y
+CONFIG_BCMA=y
+CONFIG_BCMA_HOST_SOC=y
+CONFIG_BCMA_DRIVER_GMAC_CMN=y
+CONFIG_BCMA_DRIVER_GPIO=y
+CONFIG_MFD_STM32_LPTIMER=y
+CONFIG_MFD_STPMIC1=y
+CONFIG_REGULATOR=y
+CONFIG_REGULATOR_DEBUG=y
+CONFIG_REGULATOR_FIXED_VOLTAGE=y
+CONFIG_REGULATOR_VIRTUAL_CONSUMER=y
+CONFIG_REGULATOR_USERSPACE_CONSUMER=y
+CONFIG_REGULATOR_GPIO=y
+CONFIG_REGULATOR_PWM=y
+CONFIG_REGULATOR_STM32_BOOSTER=y
+CONFIG_REGULATOR_STM32_VREFBUF=y
+CONFIG_REGULATOR_STM32_PWR=y
+CONFIG_REGULATOR_STPMIC1=y
+CONFIG_REGULATOR_VCTRL=y
+# CONFIG_HID_GENERIC is not set
+CONFIG_USB=y
+CONFIG_USB_DWC2=y
+CONFIG_USB_DWC2_PERIPHERAL=y
+CONFIG_NOP_USB_XCEIV=y
+CONFIG_USB_GPIO_VBUS=y
+CONFIG_USB_GADGET=y
+CONFIG_USB_CONFIGFS=m
+CONFIG_USB_CONFIGFS_NCM=y
+CONFIG_USB_CONFIGFS_ECM=y
+CONFIG_USB_CONFIGFS_ECM_SUBSET=y
+CONFIG_USB_CONFIGFS_RNDIS=y
+CONFIG_USB_CONFIGFS_EEM=y
+CONFIG_USB_ETH=m
+CONFIG_TYPEC=y
+CONFIG_TYPEC_TCPM=y
+CONFIG_TYPEC_TCPCI=y
+CONFIG_TYPEC_UCSI=y
+CONFIG_MMC=y
+CONFIG_MMC_BLOCK_MINORS=16
+CONFIG_MMC_ARMMMCI=y
+CONFIG_MMC_SDHCI=y
+CONFIG_MMC_SDHCI_PLTFM=y
+CONFIG_MMC_CQHCI=y
+CONFIG_NEW_LEDS=y
+CONFIG_LEDS_CLASS=y
+CONFIG_LEDS_GPIO=y
+CONFIG_LEDS_PWM=y
+CONFIG_LEDS_TRIGGERS=y
+CONFIG_LEDS_TRIGGER_TIMER=y
+CONFIG_LEDS_TRIGGER_ONESHOT=y
+CONFIG_LEDS_TRIGGER_HEARTBEAT=y
+CONFIG_LEDS_TRIGGER_GPIO=y
+CONFIG_LEDS_TRIGGER_DEFAULT_ON=y
+CONFIG_LEDS_TRIGGER_TRANSIENT=y
+CONFIG_LEDS_TRIGGER_PANIC=y
+CONFIG_RTC_CLASS=y
+CONFIG_RTC_DRV_RS5C372=y
+CONFIG_RTC_DRV_STM32=y
+CONFIG_DMADEVICES=y
+CONFIG_FSL_EDMA=y
+CONFIG_PL330_DMA=y
+CONFIG_STM32_DMA=y
+CONFIG_STM32_DMAMUX=y
+CONFIG_STM32_MDMA=y
+CONFIG_DW_DMAC=y
+CONFIG_SYNC_FILE=y
+CONFIG_UIO=y
+CONFIG_UIO_PDRV_GENIRQ=y
+CONFIG_VIRTIO_MMIO=y
+CONFIG_STAGING=y
+CONFIG_STAGING_BOARD=y
+CONFIG_MAILBOX=y
+CONFIG_PL320_MBOX=y
+CONFIG_RPMSG_VIRTIO=m
+CONFIG_SOC_BRCMSTB=y
+CONFIG_PM_DEVFREQ=y
+CONFIG_DEVFREQ_GOV_SIMPLE_ONDEMAND=m
+CONFIG_MEMORY=y
+CONFIG_STM32_FMC2_EBI=y
+CONFIG_IIO=y
+CONFIG_IIO_BUFFER_CB=y
+CONFIG_IIO_SW_DEVICE=y
+CONFIG_IIO_SW_TRIGGER=y
+CONFIG_STM32_ADC_CORE=y
+CONFIG_STM32_ADC=y
+CONFIG_STM32_DAC=y
+CONFIG_MPU3050_I2C=y
+CONFIG_IIO_HRTIMER_TRIGGER=y
+CONFIG_PWM=y
+CONFIG_PHY_STM32_USBPHYC=y
+CONFIG_NVMEM_STM32_ROMEM=y
+CONFIG_EXT4_FS=y
+CONFIG_EXT4_FS_POSIX_ACL=y
+CONFIG_EXT4_FS_SECURITY=y
+CONFIG_FANOTIFY=y
+CONFIG_QUOTA=y
+# CONFIG_PRINT_QUOTA_WARNING is not set
+CONFIG_QFMT_V2=y
+CONFIG_AUTOFS4_FS=m
+CONFIG_FUSE_FS=y
+CONFIG_CUSE=m
+CONFIG_OVERLAY_FS=y
+CONFIG_MSDOS_FS=y
+CONFIG_VFAT_FS=y
+CONFIG_NTFS_FS=y
+CONFIG_NTFS_RW=y
+CONFIG_TMPFS=y
+CONFIG_TMPFS_POSIX_ACL=y
+CONFIG_SQUASHFS=y
+CONFIG_NFS_FS=y
+CONFIG_NFS_V3_ACL=y
+CONFIG_NFS_V4=y
+CONFIG_NFS_SWAP=y
+CONFIG_ROOT_NFS=y
+CONFIG_NLS_DEFAULT="437"
+CONFIG_NLS_CODEPAGE_437=y
+CONFIG_NLS_ISO8859_1=y
+CONFIG_NLS_UTF8=y
+CONFIG_SECURITY=y
+CONFIG_SECURITYFS=y
+CONFIG_CRYPTO_USER=y
+CONFIG_CRYPTO_CCM=y
+CONFIG_CRYPTO_XCBC=y
+CONFIG_CRYPTO_MICHAEL_MIC=y
+CONFIG_CRYPTO_SHA512=y
+CONFIG_CRYPTO_BLOWFISH=y
+CONFIG_CRYPTO_CAMELLIA=y
+CONFIG_CRYPTO_CAST5=y
+CONFIG_CRYPTO_CAST6=y
+CONFIG_CRYPTO_SERPENT=y
+CONFIG_CRYPTO_TWOFISH=y
+CONFIG_CRYPTO_ANSI_CPRNG=y
+CONFIG_CRYPTO_USER_API_HASH=y
+CONFIG_CRYPTO_USER_API_SKCIPHER=y
+CONFIG_ASYMMETRIC_KEY_TYPE=y
+CONFIG_ASYMMETRIC_PUBLIC_KEY_SUBTYPE=y
+CONFIG_X509_CERTIFICATE_PARSER=y
+CONFIG_CRC_T10DIF=y
+CONFIG_CRC_ITU_T=y
+CONFIG_CRC7=y
+CONFIG_PRINTK_TIME=y
+CONFIG_DYNAMIC_DEBUG=y
+# CONFIG_DEBUG_BUGVERBOSE is not set
+CONFIG_MAGIC_SYSRQ=y
+CONFIG_DEBUG_FS=y
+# CONFIG_SCHED_DEBUG is not set
+CONFIG_SCHEDSTATS=y
+CONFIG_STACKTRACE=y
+# CONFIG_FTRACE is not set
+CONFIG_UNWINDER_FRAME_POINTER=y
+CONFIG_DEBUG_LL=y
+CONFIG_EARLY_PRINTK=y
diff --git a/arch/arm/include/asm/atomic.h b/arch/arm/include/asm/atomic.h
index 455eb19a5ac1..6f5bb2d58d4b 100644
--- a/arch/arm/include/asm/atomic.h
+++ b/arch/arm/include/asm/atomic.h
@@ -25,7 +25,7 @@
 #define atomic_read(v)	READ_ONCE((v)->counter)
 #define atomic_set(v,i)	WRITE_ONCE(((v)->counter), (i))
 
-#if __LINUX_ARM_ARCH__ >= 6
+#if __LINUX_ARM_ARCH__ >= 6 && !defined(CONFIG_ARCH_OMAP3)
 
 /*
  * ARMv6 UP and SMP safe atomic ops.  We use load exclusive and
diff --git a/arch/arm/include/asm/cmpxchg.h b/arch/arm/include/asm/cmpxchg.h
index 8b701f8e175c..98539bd8d8e6 100644
--- a/arch/arm/include/asm/cmpxchg.h
+++ b/arch/arm/include/asm/cmpxchg.h
@@ -6,7 +6,8 @@
 #include <linux/prefetch.h>
 #include <asm/barrier.h>
 
-#if defined(CONFIG_CPU_SA1100) || defined(CONFIG_CPU_SA110)
+#if defined(CONFIG_CPU_SA1100) || defined(CONFIG_CPU_SA110) || \
+	defined(CONFIG_ARCH_OMAP3)
 /*
  * On the StrongARM, "swp" is terminally broken since it bypasses the
  * cache totally.  This means that the cache becomes inconsistent, and,
@@ -32,14 +33,14 @@ static inline unsigned long __xchg(unsigned long x, volatile void *ptr, int size
 #ifdef swp_is_buggy
 	unsigned long flags;
 #endif
-#if __LINUX_ARM_ARCH__ >= 6
+#if __LINUX_ARM_ARCH__ >= 6 && !defined(CONFIG_ARCH_OMAP3)
 	unsigned int tmp;
 #endif
 
 	prefetchw((const void *)ptr);
 
 	switch (size) {
-#if __LINUX_ARM_ARCH__ >= 6
+#if __LINUX_ARM_ARCH__ >= 6 && !defined(CONFIG_ARCH_OMAP3)
 #ifndef CONFIG_CPU_V6 /* MIN ARCH >= V6K */
 	case 1:
 		asm volatile("@	__xchg1\n"
@@ -121,7 +122,7 @@ static inline unsigned long __xchg(unsigned long x, volatile void *ptr, int size
 
 #include <asm-generic/cmpxchg-local.h>
 
-#if __LINUX_ARM_ARCH__ < 6
+#if __LINUX_ARM_ARCH__ < 6 || defined(CONFIG_ARCH_OMAP3)
 /* min ARCH < ARMv6 */
 
 #ifdef CONFIG_SMP
diff --git a/arch/arm/include/asm/fixmap.h b/arch/arm/include/asm/fixmap.h
index fc56fc3e1931..c279a8a463a2 100644
--- a/arch/arm/include/asm/fixmap.h
+++ b/arch/arm/include/asm/fixmap.h
@@ -7,14 +7,14 @@
 #define FIXADDR_TOP		(FIXADDR_END - PAGE_SIZE)
 
 #include <linux/pgtable.h>
-#include <asm/kmap_types.h>
+#include <asm/kmap_size.h>
 
 enum fixed_addresses {
 	FIX_EARLYCON_MEM_BASE,
 	__end_of_permanent_fixed_addresses,
 
 	FIX_KMAP_BEGIN = __end_of_permanent_fixed_addresses,
-	FIX_KMAP_END = FIX_KMAP_BEGIN + (KM_TYPE_NR * NR_CPUS) - 1,
+	FIX_KMAP_END = FIX_KMAP_BEGIN + (KM_MAX_IDX * NR_CPUS) - 1,
 
 	/* Support writing RO kernel text via kprobes, jump labels, etc. */
 	FIX_TEXT_POKE0,
diff --git a/arch/arm/include/asm/hardirq.h b/arch/arm/include/asm/hardirq.h
index b95848ed2bc7..706efafbf972 100644
--- a/arch/arm/include/asm/hardirq.h
+++ b/arch/arm/include/asm/hardirq.h
@@ -2,16 +2,11 @@
 #ifndef __ASM_HARDIRQ_H
 #define __ASM_HARDIRQ_H
 
-#include <linux/cache.h>
-#include <linux/threads.h>
 #include <asm/irq.h>
 
-typedef struct {
-	unsigned int __softirq_pending;
-} ____cacheline_aligned irq_cpustat_t;
-
-#include <linux/irq_cpustat.h>	/* Standard mappings for irq_cpustat_t above */
-
 #define __ARCH_IRQ_EXIT_IRQS_DISABLED	1
+#define ack_bad_irq ack_bad_irq
+
+#include <asm-generic/hardirq.h>
 
 #endif /* __ASM_HARDIRQ_H */
diff --git a/arch/arm/include/asm/highmem.h b/arch/arm/include/asm/highmem.h
index 31811be38d78..b22dffa8c7eb 100644
--- a/arch/arm/include/asm/highmem.h
+++ b/arch/arm/include/asm/highmem.h
@@ -2,7 +2,8 @@
 #ifndef _ASM_HIGHMEM_H
 #define _ASM_HIGHMEM_H
 
-#include <asm/kmap_types.h>
+#include <asm/kmap_size.h>
+#include <asm/fixmap.h>
 
 #define PKMAP_BASE		(PAGE_OFFSET - PMD_SIZE)
 #define LAST_PKMAP		PTRS_PER_PTE
@@ -46,19 +47,32 @@ extern pte_t *pkmap_page_table;
 
 #ifdef ARCH_NEEDS_KMAP_HIGH_GET
 extern void *kmap_high_get(struct page *page);
-#else
+
+static inline void *arch_kmap_local_high_get(struct page *page)
+{
+	if (IS_ENABLED(CONFIG_DEBUG_HIGHMEM) && !cache_is_vivt())
+		return NULL;
+	return kmap_high_get(page);
+}
+#define arch_kmap_local_high_get arch_kmap_local_high_get
+
+#else /* ARCH_NEEDS_KMAP_HIGH_GET */
 static inline void *kmap_high_get(struct page *page)
 {
 	return NULL;
 }
-#endif
+#endif /* !ARCH_NEEDS_KMAP_HIGH_GET */
 
-/*
- * The following functions are already defined by <linux/highmem.h>
- * when CONFIG_HIGHMEM is not set.
- */
-#ifdef CONFIG_HIGHMEM
-extern void *kmap_atomic_pfn(unsigned long pfn);
-#endif
+#define arch_kmap_local_post_map(vaddr, pteval)				\
+	local_flush_tlb_kernel_page(vaddr)
+
+#define arch_kmap_local_pre_unmap(vaddr)				\
+do {									\
+	if (cache_is_vivt())						\
+		__cpuc_flush_dcache_area((void *)vaddr, PAGE_SIZE);	\
+} while (0)
+
+#define arch_kmap_local_post_unmap(vaddr)				\
+	local_flush_tlb_kernel_page(vaddr)
 
 #endif
diff --git a/arch/arm/include/asm/irq.h b/arch/arm/include/asm/irq.h
index 46d41140df27..1cbcc462b07e 100644
--- a/arch/arm/include/asm/irq.h
+++ b/arch/arm/include/asm/irq.h
@@ -31,6 +31,8 @@ void handle_IRQ(unsigned int, struct pt_regs *);
 void init_IRQ(void);
 
 #ifdef CONFIG_SMP
+#include <linux/cpumask.h>
+
 extern void arch_trigger_cpumask_backtrace(const cpumask_t *mask,
 					   bool exclude_self);
 #define arch_trigger_cpumask_backtrace arch_trigger_cpumask_backtrace
diff --git a/arch/arm/include/asm/kmap_types.h b/arch/arm/include/asm/kmap_types.h
deleted file mode 100644
index 5590940ee43d..000000000000
--- a/arch/arm/include/asm/kmap_types.h
+++ /dev/null
@@ -1,10 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0 */
-#ifndef __ARM_KMAP_TYPES_H
-#define __ARM_KMAP_TYPES_H
-
-/*
- * This is the "bare minimum".  AIO seems to require this.
- */
-#define KM_TYPE_NR 16
-
-#endif
diff --git a/arch/arm/include/asm/spinlock.h b/arch/arm/include/asm/spinlock.h
index 8f009e788ad4..d21372986528 100644
--- a/arch/arm/include/asm/spinlock.h
+++ b/arch/arm/include/asm/spinlock.h
@@ -6,6 +6,10 @@
 #error SMP not supported on pre-ARMv6 CPUs
 #endif
 
+#if defined(CONFIG_ARCH_OMAP3)
+#error Cant use spinlocks
+#endif
+
 #include <linux/prefetch.h>
 #include <asm/barrier.h>
 #include <asm/processor.h>
diff --git a/arch/arm/include/asm/spinlock_types.h b/arch/arm/include/asm/spinlock_types.h
index 5976958647fe..a37c0803954b 100644
--- a/arch/arm/include/asm/spinlock_types.h
+++ b/arch/arm/include/asm/spinlock_types.h
@@ -2,10 +2,6 @@
 #ifndef __ASM_SPINLOCK_TYPES_H
 #define __ASM_SPINLOCK_TYPES_H
 
-#ifndef __LINUX_SPINLOCK_TYPES_H
-# error "please don't include this file directly"
-#endif
-
 #define TICKET_SHIFT	16
 
 typedef struct {
diff --git a/arch/arm/include/asm/thread_info.h b/arch/arm/include/asm/thread_info.h
index 536b6b979f63..875aaf9af946 100644
--- a/arch/arm/include/asm/thread_info.h
+++ b/arch/arm/include/asm/thread_info.h
@@ -46,6 +46,7 @@ struct cpu_context_save {
 struct thread_info {
 	unsigned long		flags;		/* low level flags */
 	int			preempt_count;	/* 0 => preemptable, <0 => bug */
+	int			preempt_lazy_count; /* 0 => preemptable, <0 => bug */
 	mm_segment_t		addr_limit;	/* address limit */
 	struct task_struct	*task;		/* main task structure */
 	__u32			cpu;		/* cpu */
@@ -134,7 +135,8 @@ extern int vfp_restore_user_hwstate(struct user_vfp *,
 #define TIF_SYSCALL_TRACE	4	/* syscall trace active */
 #define TIF_SYSCALL_AUDIT	5	/* syscall auditing active */
 #define TIF_SYSCALL_TRACEPOINT	6	/* syscall tracepoint instrumentation */
-#define TIF_SECCOMP		7	/* seccomp syscall filtering active */
+#define TIF_NEED_RESCHED_LAZY	7
+#define TIF_SECCOMP		8	/* seccomp syscall filtering active */
 
 #define TIF_USING_IWMMXT	17
 #define TIF_MEMDIE		18	/* is terminating due to OOM killer */
@@ -143,6 +145,7 @@ extern int vfp_restore_user_hwstate(struct user_vfp *,
 #define _TIF_SIGPENDING		(1 << TIF_SIGPENDING)
 #define _TIF_NEED_RESCHED	(1 << TIF_NEED_RESCHED)
 #define _TIF_NOTIFY_RESUME	(1 << TIF_NOTIFY_RESUME)
+#define _TIF_NEED_RESCHED_LAZY	(1 << TIF_NEED_RESCHED_LAZY)
 #define _TIF_UPROBE		(1 << TIF_UPROBE)
 #define _TIF_SYSCALL_TRACE	(1 << TIF_SYSCALL_TRACE)
 #define _TIF_SYSCALL_AUDIT	(1 << TIF_SYSCALL_AUDIT)
@@ -158,7 +161,8 @@ extern int vfp_restore_user_hwstate(struct user_vfp *,
  * Change these and you break ASM code in entry-common.S
  */
 #define _TIF_WORK_MASK		(_TIF_NEED_RESCHED | _TIF_SIGPENDING | \
-				 _TIF_NOTIFY_RESUME | _TIF_UPROBE)
+				 _TIF_NOTIFY_RESUME | _TIF_UPROBE | \
+				 _TIF_NEED_RESCHED_LAZY)
 
 #endif /* __KERNEL__ */
 #endif /* __ASM_ARM_THREAD_INFO_H */
diff --git a/arch/arm/include/asm/timex.h b/arch/arm/include/asm/timex.h
index 7c3b3671d6c2..7983ff4a1dac 100644
--- a/arch/arm/include/asm/timex.h
+++ b/arch/arm/include/asm/timex.h
@@ -10,6 +10,49 @@
 #define _ASMARM_TIMEX_H
 
 typedef unsigned long cycles_t;
-#define get_cycles()	({ cycles_t c; read_current_timer(&c) ? 0 : c; })
+
+static inline void random_get_entropy_init(u32 values[3])
+{
+	/* in general enable all counters (including cycle counter)
+	 *  reset all counters to zero.
+	 *  reset cycle counter to zero.
+	 */
+	int32_t value = 1 | 2 | 4 | 16;
+
+	asm volatile("MRC p15, 0, %0, c9, c12, 0\t\n" : "=r"(values[0])::);
+	asm volatile("MRC p15, 0, %0, c9, c12, 1\t\n" : "=r"(values[1])::);
+	asm volatile("MRC p15, 0, %0, c9, c12, 3\t\n" : "=r"(values[2])::);
+
+	// program the performance-counter control-register:
+	asm volatile("MCR p15, 0, %0, c9, c12, 0\t\n" ::"r"(value));
+
+	// enable all counters:
+	asm volatile("MCR p15, 0, %0, c9, c12, 1\t\n" ::"r"(0x8000000f));
+
+	// clear overflows:
+	asm volatile("MCR p15, 0, %0, c9, c12, 3\t\n" ::"r"(0x8000000f));
+}
+
+static inline void random_get_entropy_deinit(u32 values[3])
+{
+	asm volatile("MCR p15, 0, %0, c9, c12, 0\t\n" ::"r"(values[0]) :);
+	asm volatile("MCR p15, 0, %0, c9, c12, 1\t\n" ::"r"(values[1]) :);
+	asm volatile("MCR p15, 0, %0, c9, c12, 3\t\n" ::"r"(values[2]) :);
+}
+
+#ifndef random_get_entropy
+#define random_get_entropy()                                                   \
+	({                                                                     \
+		cycles_t c;                                                    \
+		asm volatile("MRC p15, 0, %0, c9, c13, 0\n\t" : "=r"(c)::);    \
+		c;                                                             \
+	})
+#endif
+
+#define get_cycles()                                                           \
+	({                                                                     \
+		cycles_t c;                                                    \
+		read_current_timer(&c) ? 0 : c;                                \
+	})
 
 #endif
diff --git a/arch/arm/kernel/asm-offsets.c b/arch/arm/kernel/asm-offsets.c
index be8050b0c3df..884e40a525ce 100644
--- a/arch/arm/kernel/asm-offsets.c
+++ b/arch/arm/kernel/asm-offsets.c
@@ -42,6 +42,7 @@ int main(void)
   BLANK();
   DEFINE(TI_FLAGS,		offsetof(struct thread_info, flags));
   DEFINE(TI_PREEMPT,		offsetof(struct thread_info, preempt_count));
+  DEFINE(TI_PREEMPT_LAZY,	offsetof(struct thread_info, preempt_lazy_count));
   DEFINE(TI_ADDR_LIMIT,		offsetof(struct thread_info, addr_limit));
   DEFINE(TI_TASK,		offsetof(struct thread_info, task));
   DEFINE(TI_CPU,		offsetof(struct thread_info, cpu));
diff --git a/arch/arm/kernel/entry-armv.S b/arch/arm/kernel/entry-armv.S
index 1c9e6d1452c5..6160eeeab6a8 100644
--- a/arch/arm/kernel/entry-armv.S
+++ b/arch/arm/kernel/entry-armv.S
@@ -206,11 +206,18 @@ __irq_svc:
 
 #ifdef CONFIG_PREEMPTION
 	ldr	r8, [tsk, #TI_PREEMPT]		@ get preempt count
-	ldr	r0, [tsk, #TI_FLAGS]		@ get flags
 	teq	r8, #0				@ if preempt count != 0
+	bne	1f				@ return from exeption
+	ldr	r0, [tsk, #TI_FLAGS]		@ get flags
+	tst	r0, #_TIF_NEED_RESCHED		@ if NEED_RESCHED is set
+	blne	svc_preempt			@ preempt!
+
+	ldr	r8, [tsk, #TI_PREEMPT_LAZY]	@ get preempt lazy count
+	teq	r8, #0				@ if preempt lazy count != 0
 	movne	r0, #0				@ force flags to 0
-	tst	r0, #_TIF_NEED_RESCHED
+	tst	r0, #_TIF_NEED_RESCHED_LAZY
 	blne	svc_preempt
+1:
 #endif
 
 	svc_exit r5, irq = 1			@ return from exception
@@ -225,8 +232,14 @@ svc_preempt:
 1:	bl	preempt_schedule_irq		@ irq en/disable is done inside
 	ldr	r0, [tsk, #TI_FLAGS]		@ get new tasks TI_FLAGS
 	tst	r0, #_TIF_NEED_RESCHED
+	bne	1b
+	tst	r0, #_TIF_NEED_RESCHED_LAZY
 	reteq	r8				@ go again
-	b	1b
+	ldr	r0, [tsk, #TI_PREEMPT_LAZY]	@ get preempt lazy count
+	teq	r0, #0				@ if preempt lazy count != 0
+	beq	1b
+	ret	r8				@ go again
+
 #endif
 
 __und_fault:
diff --git a/arch/arm/kernel/entry-common.S b/arch/arm/kernel/entry-common.S
index 271cb8a1eba1..fd039b1b3731 100644
--- a/arch/arm/kernel/entry-common.S
+++ b/arch/arm/kernel/entry-common.S
@@ -53,7 +53,9 @@ __ret_fast_syscall:
 	cmp	r2, #TASK_SIZE
 	blne	addr_limit_check_failed
 	ldr	r1, [tsk, #TI_FLAGS]		@ re-check for syscall tracing
-	tst	r1, #_TIF_SYSCALL_WORK | _TIF_WORK_MASK
+	tst	r1, #((_TIF_SYSCALL_WORK | _TIF_WORK_MASK) & ~_TIF_SECCOMP)
+	bne	fast_work_pending
+	tst	r1, #_TIF_SECCOMP
 	bne	fast_work_pending
 
 
@@ -90,8 +92,11 @@ __ret_fast_syscall:
 	cmp	r2, #TASK_SIZE
 	blne	addr_limit_check_failed
 	ldr	r1, [tsk, #TI_FLAGS]		@ re-check for syscall tracing
-	tst	r1, #_TIF_SYSCALL_WORK | _TIF_WORK_MASK
+	tst	r1, #((_TIF_SYSCALL_WORK | _TIF_WORK_MASK) & ~_TIF_SECCOMP)
+	bne	do_slower_path
+	tst	r1, #_TIF_SECCOMP
 	beq	no_work_pending
+do_slower_path:
  UNWIND(.fnend		)
 ENDPROC(ret_fast_syscall)
 
diff --git a/arch/arm/kernel/signal.c b/arch/arm/kernel/signal.c
index 2f81d3af5f9a..6e69f7b3d581 100644
--- a/arch/arm/kernel/signal.c
+++ b/arch/arm/kernel/signal.c
@@ -649,7 +649,8 @@ do_work_pending(struct pt_regs *regs, unsigned int thread_flags, int syscall)
 	 */
 	trace_hardirqs_off();
 	do {
-		if (likely(thread_flags & _TIF_NEED_RESCHED)) {
+		if (likely(thread_flags & (_TIF_NEED_RESCHED |
+					   _TIF_NEED_RESCHED_LAZY))) {
 			schedule();
 		} else {
 			if (unlikely(!user_mode(regs)))
diff --git a/arch/arm/kernel/smp.c b/arch/arm/kernel/smp.c
index 48099c6e1e4a..609b7d3104ea 100644
--- a/arch/arm/kernel/smp.c
+++ b/arch/arm/kernel/smp.c
@@ -672,9 +672,7 @@ static void do_handle_IPI(int ipinr)
 		break;
 
 	case IPI_CPU_BACKTRACE:
-		printk_nmi_enter();
 		nmi_cpu_backtrace(get_irq_regs());
-		printk_nmi_exit();
 		break;
 
 	default:
diff --git a/arch/arm/lib/bitops.h b/arch/arm/lib/bitops.h
index 95bd35991288..1698a0aaf487 100644
--- a/arch/arm/lib/bitops.h
+++ b/arch/arm/lib/bitops.h
@@ -2,7 +2,7 @@
 #include <asm/assembler.h>
 #include <asm/unwind.h>
 
-#if __LINUX_ARM_ARCH__ >= 6
+#if __LINUX_ARM_ARCH__ >= 6 && !defined(CONFIG_ARCH_OMAP3)
 	.macro	bitop, name, instr
 ENTRY(	\name		)
 UNWIND(	.fnstart	)
diff --git a/arch/arm/mach-imx/Makefile b/arch/arm/mach-imx/Makefile
index 9cebd360d58e..597c207c0dc7 100644
--- a/arch/arm/mach-imx/Makefile
+++ b/arch/arm/mach-imx/Makefile
@@ -68,3 +68,5 @@ obj-$(CONFIG_SOC_IMX53) += mach-imx53.o
 obj-$(CONFIG_SOC_VF610) += mach-vf610.o
 
 obj-$(CONFIG_SOC_LS1021A) += mach-ls1021a.o
+
+obj-$(CONFIG_WAGO_SYSTEM_BASED_STARTUP) += ../mach-omap2/wsysinit.o
diff --git a/arch/arm/mach-omap2/Kconfig b/arch/arm/mach-omap2/Kconfig
index 3f62a0c9450d..2733c700950d 100644
--- a/arch/arm/mach-omap2/Kconfig
+++ b/arch/arm/mach-omap2/Kconfig
@@ -115,10 +115,22 @@ config ARCH_OMAP2PLUS
 	help
 	  Systems based on OMAP2, OMAP3, OMAP4 or OMAP5
 
+config WAGO_SYSTEM_BASED_STARTUP
+	bool "Wago System based startup code"
+	help
+	 Enable Wago System based startup code
+
+config IRQ_PRIORITY_TABLE
+	bool "Enable IRQ thread priority lookup"
+	help
+	 By this feature it is possible to assign custom irq thread priorities
+	 while they are created (RT_PREEMT). The flag IRQF_THREAD_TBL_LOOKUP
+	 trigger a table lookup which may succeed or fail.
+
 config OMAP_INTERCONNECT_BARRIER
 	bool
 	select ARM_HEAVY_MB
-	
+
 
 if ARCH_OMAP2PLUS
 
diff --git a/arch/arm/mach-omap2/Makefile b/arch/arm/mach-omap2/Makefile
index 732e614c56b2..2a3910f4b0dd 100644
--- a/arch/arm/mach-omap2/Makefile
+++ b/arch/arm/mach-omap2/Makefile
@@ -238,3 +238,5 @@ targets += pm-asm-offsets.s
 clean-files += pm-asm-offsets.h
 
 obj-$(CONFIG_OMAP_IOMMU)		+= omap-iommu.o
+
+obj-$(CONFIG_WAGO_SYSTEM_BASED_STARTUP)  += wsysinit.o
diff --git a/arch/arm/mach-omap2/omap_hwmod_33xx_data.c b/arch/arm/mach-omap2/omap_hwmod_33xx_data.c
index b232f6ca6fe3..ba2c41ef83eb 100644
--- a/arch/arm/mach-omap2/omap_hwmod_33xx_data.c
+++ b/arch/arm/mach-omap2/omap_hwmod_33xx_data.c
@@ -172,6 +172,7 @@ static struct omap_hwmod am33xx_debugss_hwmod = {
 	.class		= &am33xx_debugss_hwmod_class,
 	.clkdm_name	= "l3_aon_clkdm",
 	.main_clk	= "trace_clk_div_ck",
+	.flags          =  (HWMOD_INIT_NO_IDLE | HWMOD_INIT_NO_RESET),
 	.prcm		= {
 		.omap4	= {
 			.clkctrl_offs	= AM33XX_CM_WKUP_DEBUGSS_CLKCTRL_OFFSET,
diff --git a/arch/arm/mach-omap2/pdata-quirks.c b/arch/arm/mach-omap2/pdata-quirks.c
index 2a4fe3e68b82..d0a26e5fd584 100644
--- a/arch/arm/mach-omap2/pdata-quirks.c
+++ b/arch/arm/mach-omap2/pdata-quirks.c
@@ -32,6 +32,7 @@
 #include "omap_device.h"
 #include "omap-secure.h"
 #include "soc.h"
+#include <linux/usb/musb.h>
 
 static struct omap_hsmmc_platform_data __maybe_unused mmc_pdata[2];
 
@@ -160,6 +161,58 @@ static void __init omap3_evm_legacy_init(void)
 	hsmmc2_internal_input_clk();
 }
 
+static struct omap_musb_board_data musb_board_data = {
+	.interface_type		= MUSB_INTERFACE_ULPI,
+	.mode			= MUSB_OTG,
+	.power			= 100,
+	.set_phy_power		= am35x_musb_phy_power,
+	.clear_irq		= am35x_musb_clear_irq,
+	.set_mode		= am35x_set_mode,
+	.reset			= am35x_musb_reset,
+};
+
+static struct musb_hdrc_config musb_config = {
+	.multipoint	= 1,
+	.dyn_fifo	= 1,
+	.num_eps	= 16,
+	.ram_bits	= 12,
+};
+
+static struct musb_hdrc_platform_data musb_plat = {
+	.mode		= MUSB_OTG,
+
+	/* .clock is set dynamically */
+	.config		= &musb_config,
+
+	/* REVISIT charge pump on TWL4030 can supply up to
+	 * 100 mA ... but this value is board-specific, like
+	 * "mode", and should be passed to usb_musb_init().
+	 */
+	.power		= 250,			/* up to 100 mA */
+
+	.board_data = &musb_board_data,
+	.clock = "ick",
+};
+
+static __init void am3517_evm_musb_init(void)
+{
+	u32 devconf2;
+
+	/*
+	 * Set up USB clock/mode in the DEVCONF2 register.
+	 */
+	devconf2 = omap_ctrl_readl(AM35XX_CONTROL_DEVCONF2);
+
+	/* USB2.0 PHY reference clock is 13 MHz */
+	devconf2 &= ~(CONF2_REFFREQ | CONF2_OTGMODE | CONF2_PHY_GPIOMODE);
+	devconf2 |=  CONF2_REFFREQ_13MHZ | CONF2_SESENDEN | CONF2_VBDTCTEN
+			| CONF2_DATPOL;
+
+	omap_ctrl_writel(devconf2, AM35XX_CONTROL_DEVCONF2);
+
+	pr_debug("%s: devconf2 = 0x%08x.\n", __func__, devconf2);
+}
+
 static void am35xx_enable_emac_int(void)
 {
 	u32 v;
@@ -229,6 +282,7 @@ static void __init omap3_sbc_t3517_legacy_init(void)
 static void __init am3517_evm_legacy_init(void)
 {
 	am35xx_emac_reset();
+	am3517_evm_musb_init();
 }
 
 static void __init nokia_n900_legacy_init(void)
@@ -494,6 +548,7 @@ static struct of_dev_auxdata omap_auxdata_lookup[] = {
 	OF_DEV_AUXDATA("ti,davinci_mdio", 0x5c030000, "davinci_mdio.0", NULL),
 	OF_DEV_AUXDATA("ti,am3517-emac", 0x5c000000, "davinci_emac.0",
 		       &am35xx_emac_pdata),
+	OF_DEV_AUXDATA("ti,musb-am35x", 0x5c040000, "musb-am35x.0", &musb_plat),
 	OF_DEV_AUXDATA("nokia,n900-rom-rng", 0, NULL, rx51_secure_rng_call),
 	/* McBSP modules with sidetone core */
 #if IS_ENABLED(CONFIG_SND_SOC_OMAP_MCBSP)
diff --git a/arch/arm/mach-omap2/wsysinit.c b/arch/arm/mach-omap2/wsysinit.c
new file mode 100644
index 000000000000..1f9961313546
--- /dev/null
+++ b/arch/arm/mach-omap2/wsysinit.c
@@ -0,0 +1,361 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Wago System Init Functions
+ *
+ * Copyright (C) 2014 WAGO Automation
+ *
+ * Author: Heinrich Toews
+ */
+#undef DEBUG
+
+#include <linux/irq.h>
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/err.h>
+#include <linux/module.h>
+#include <linux/platform_device.h>
+#include <linux/of.h>
+#include <linux/of_address.h>
+#include <linux/of_device.h>
+#include <linux/of_gpio.h>
+#include <linux/kobject.h>
+#include <linux/fs.h>
+#include <linux/timex.h>
+#include <asm/mach-types.h>
+#include <linux/delay.h>
+
+#ifdef CONFIG_IRQ_PRIORITY_TABLE
+#include <linux/wsysinit-prio.h>
+#endif
+
+#ifdef CONFIG_ARCH_OMAP3
+#include "soc.h"
+#endif
+
+#define	DEVICE_NAME		"wsysinit-drv"
+
+#define RESET_TIME_QUECTEL_UC20_G 200
+
+struct wsysinit_settings {
+	bool dp_reset;
+	bool modem_reset;
+	int dp_gpio_rst;
+	int modem_gpio_rst;
+	bool dp_alow;
+	const char *tty_service;
+	const char *tty_rs232_485;
+	const char *modem;
+	int drvvbus_gpio;
+	const char *board_variant;
+	u32 adjtimex_tick;
+	u32 adjtimex_frequency;
+};
+
+static struct wsysinit_settings wsysinitset = { 0,};
+
+static struct of_device_id wsysinit_dt_ids[] = {
+	{ .compatible = "wago,sysinit" },
+	{ }
+};
+MODULE_DEVICE_TABLE(of, wsysinit_dt_ids);
+
+static int dp_trig_reset(struct wsysinit_settings *ps)
+{
+	/* check if value is valid */
+	if (ps == NULL || !gpio_is_valid(ps->dp_gpio_rst))
+		return -EINVAL;
+
+	gpiod_set_value_cansleep(gpio_to_desc(ps->dp_gpio_rst), 1);
+	mdelay(100);
+	gpiod_set_value_cansleep(gpio_to_desc(ps->dp_gpio_rst), 0);
+
+	pr_info("DPx resetted (pin is active %s)!\n", ps->dp_alow ? "low" : "high");
+
+	return 0;
+}
+
+static int modem_reset(struct wsysinit_settings *ps)
+{
+	if (ps == NULL || !gpio_is_valid(ps->modem_gpio_rst))
+		return -EINVAL;
+
+	gpiod_set_value_cansleep(gpio_to_desc(ps->modem_gpio_rst), 1);
+	mdelay(RESET_TIME_QUECTEL_UC20_G);
+	gpiod_set_value_cansleep(gpio_to_desc(ps->modem_gpio_rst), 0);
+
+	return 0;
+}
+
+static ssize_t dp_sysfs_trig_reset(struct device *dev,
+				   struct device_attribute *attr,
+				   const char *buf,
+				   size_t count)
+{
+	u32 val = simple_strtoul(buf, NULL, 10);
+
+	/* check if value is valid */
+	if (val != 1)
+		return -EINVAL;
+
+	dp_trig_reset(&wsysinitset);
+
+	return count;
+}
+
+static ssize_t modem_sysfs_reset(struct device *dev,
+				struct device_attribute *attr,
+				const char *buf,
+				size_t count)
+{
+	u32 val = simple_strtoul(buf, NULL, 10);
+
+	if (val != 1)
+		return -EINVAL;
+
+	/* Return value omitted, because is of no value for the user of the command */
+	(void)modem_reset(&wsysinitset);
+
+	return count;
+}
+
+static int wsysinit_init_probe_dt(struct platform_device *pdev)
+{
+	int ret;
+	struct device_node *np = pdev->dev.of_node;
+	enum of_gpio_flags flags;
+	struct __kernel_timex tmx = {0};
+
+	wsysinitset.dp_reset = of_property_read_bool(np, "dp,reset");
+	wsysinitset.dp_gpio_rst = of_get_named_gpio_flags(np, "dp,gpio-rst", 0, &flags);
+	if (wsysinitset.dp_gpio_rst == -EPROBE_DEFER)
+		return -EPROBE_DEFER;
+
+	if (wsysinitset.dp_gpio_rst >= 0) {
+		int rqflags = 0;
+
+		wsysinitset.dp_alow = (flags & OF_GPIO_ACTIVE_LOW) ? 1 : 0;
+
+		if (wsysinitset.dp_alow) /* FLAG_ACTIVE_LOW ?? */
+			rqflags |= GPIOF_ACTIVE_LOW | GPIOF_OUT_INIT_LOW;
+		else
+			rqflags |= GPIOF_OUT_INIT_HIGH;
+
+		ret = devm_gpio_request_one(&pdev->dev, wsysinitset.dp_gpio_rst,
+					    rqflags | GPIOF_EXPORT_DIR_FIXED, /*   */
+					    "Profibus Reset Pin");
+		if (ret < 0) {
+			dev_err(&pdev->dev, "failed to claim DP reset pin\n");
+			return ret;
+		}
+
+		if (wsysinitset.dp_reset)
+			dp_trig_reset(&wsysinitset);
+
+	} else if (wsysinitset.dp_reset) {
+		dev_err(&pdev->dev, "failed to find DP gpio reset signal!\n");
+		return -1;
+	}
+
+	wsysinitset.modem_reset = of_property_read_bool(np, "modem,reset");
+	wsysinitset.modem_gpio_rst = of_get_named_gpio_flags(np, "modem,gpio-rst", 0, &flags);
+	if (wsysinitset.modem_gpio_rst == -EPROBE_DEFER)
+		return -EPROBE_DEFER;
+
+	if (wsysinitset.modem_gpio_rst >= 0) {
+		ret = devm_gpio_request_one(&pdev->dev, wsysinitset.modem_gpio_rst,
+						GPIOF_EXPORT_DIR_FIXED,
+						"Modem Reset Pin");
+		if (ret < 0) {
+			dev_err(&pdev->dev, "failed to claim modem reset pin\n");
+			return ret;
+		}
+
+		if (wsysinitset.modem_reset)
+			modem_reset(&wsysinitset);
+
+		/* devm_gpio_free(&pdev->dev, wsysinitset.dp_gpio_rst); */
+	} else if (wsysinitset.modem_reset) {
+		dev_err(&pdev->dev, "failed to find modem reset signal!\n");
+		return -1;
+	}
+
+	if (of_property_read_string(np, "modem,type", &wsysinitset.modem))
+		wsysinitset.modem = NULL;
+
+	if (of_property_read_string(np, "tty,service", &wsysinitset.tty_service))
+		wsysinitset.tty_service = NULL;
+
+	if (of_property_read_string(np, "tty,rs232-485", &wsysinitset.tty_rs232_485))
+		wsysinitset.tty_rs232_485 = NULL;
+
+	if (of_property_read_string(np, "board,variant", &wsysinitset.board_variant))
+		wsysinitset.board_variant = NULL;
+
+	if (!of_property_read_u32(np, "adjtimex,tick", &wsysinitset.adjtimex_tick)) {
+		tmx.modes |= ADJ_TICK;
+		tmx.tick = (__kernel_long_t) wsysinitset.adjtimex_tick;
+	}
+
+	if (!of_property_read_u32(np, "adjtimex,frequency", &wsysinitset.adjtimex_frequency)) {
+		tmx.modes |= ADJ_FREQUENCY;
+		tmx.freq = (__kernel_long_t) wsysinitset.adjtimex_frequency;
+	}
+
+	if (tmx.modes) {
+		do_adjtimex(&tmx);
+
+		pr_debug("%s: adjtimex: tick %u, frequency %u\n", __func__,
+			 wsysinitset.adjtimex_tick, wsysinitset.adjtimex_frequency);
+	}
+
+	return 0;
+}
+
+/* WSYSINIT Specific Kernel Parameters */
+
+char *__wsysinit_bootversion = NULL;
+core_param(bootversion, __wsysinit_bootversion, charp, 0);
+EXPORT_SYMBOL(__wsysinit_bootversion);
+
+/* WSYSINIT SYSFS Init */
+
+static dev_t wsysinit_sysfs_dev;
+struct class* wsysinit_sysfs_class = NULL;
+struct device* wsysinit_sysfs_device = NULL;
+EXPORT_SYMBOL(wsysinit_sysfs_class);
+EXPORT_SYMBOL(wsysinit_sysfs_device);
+
+ssize_t wsysinit_sysfs_bootversion_show(struct device* dev, struct device_attribute* attr, char* buf)
+{
+	if (__wsysinit_bootversion)
+		sprintf (buf, "%s\n", __wsysinit_bootversion);
+	else
+		sprintf (buf, "unknown\n");
+	return strlen(buf);
+}
+
+ssize_t wsysinit_sysfs_tty_service_show(struct device* dev, struct device_attribute* attr, char* buf)
+{
+	if (wsysinitset.tty_service)
+		sprintf(buf, "%s\n", wsysinitset.tty_service);
+	else
+		sprintf(buf, "%s\n", "unknown");
+
+	return strlen(buf);
+}
+
+ssize_t wsysinit_sysfs_tty_rs232_485_show(struct device* dev, struct device_attribute* attr, char* buf)
+{
+	if (wsysinitset.tty_rs232_485)
+		sprintf(buf, "%s\n", wsysinitset.tty_rs232_485);
+	else
+		sprintf(buf, "%s\n", "unknown");
+
+	return strlen(buf);
+}
+
+ssize_t wsysinit_sysfs_board_variant_show(struct device* dev, struct device_attribute* attr, char* buf)
+{
+	if (wsysinitset.board_variant)
+		sprintf(buf, "%s\n", wsysinitset.board_variant);
+	else
+		sprintf(buf, "%s\n", "unknown");
+
+	return strlen(buf);
+}
+
+DEVICE_ATTR (     bootversion , 0444,     wsysinit_sysfs_bootversion_show, NULL);
+DEVICE_ATTR (    tty_service  , 0444,     wsysinit_sysfs_tty_service_show, NULL);
+DEVICE_ATTR (    tty_rs232_485, 0444,   wsysinit_sysfs_tty_rs232_485_show, NULL);
+DEVICE_ATTR (    dp_trig_reset, 0200,               NULL, dp_sysfs_trig_reset);
+DEVICE_ATTR (    board_variant, 0444,   wsysinit_sysfs_board_variant_show, NULL);
+DEVICE_ATTR (      modem_reset, 0200,               NULL, modem_sysfs_reset);
+
+static void wsysinit_sysfs_init(void)
+{
+	device_create_file(wsysinit_sysfs_device, &dev_attr_bootversion);
+	device_create_file(wsysinit_sysfs_device, &dev_attr_dp_trig_reset);
+	device_create_file(wsysinit_sysfs_device, &dev_attr_board_variant);
+
+	if (wsysinitset.modem)
+		device_create_file(wsysinit_sysfs_device, &dev_attr_modem_reset);
+
+	if (wsysinitset.tty_service)
+		device_create_file(wsysinit_sysfs_device, &dev_attr_tty_service);
+
+	if (wsysinitset.tty_rs232_485)
+		device_create_file(wsysinit_sysfs_device, &dev_attr_tty_rs232_485);
+
+#ifdef CONFIG_IRQ_PRIORITY_TABLE
+	wsysinit_tbl_sysfs_init();
+#endif
+}
+
+static int wsysinit_init_probe(struct platform_device *pdev)
+{
+	int rc;
+
+	if (!pdev->dev.of_node) {
+		dev_err(&pdev->dev, "WSYSINIT Init: No DT node found!\n");
+		return -1;
+	}
+
+	rc = wsysinit_init_probe_dt(pdev);
+	if (rc < 0) {
+		dev_err(&pdev->dev, "failed to probe DT parameters\n");
+		return rc;
+	}
+
+	wsysinit_sysfs_init();
+
+	pr_info("Wago WSYSINIT Init: %s probed.\n", pdev->dev.of_node->name);
+
+	return 0;
+}
+
+static int wsysinit_init_remove(struct platform_device *pdev)
+{
+	return 0;
+}
+
+static struct platform_driver wsysinit_init_driver = {
+	.probe		= wsysinit_init_probe,
+	.remove		= wsysinit_init_remove,
+	.driver		= {
+		.name	= DEVICE_NAME,
+		.owner	= THIS_MODULE,
+		.of_match_table = of_match_ptr(wsysinit_dt_ids),
+	},
+};
+
+static __init int wsysinit_init(void)
+{
+	pr_info("WSYSINIT: create sysfs entries\n");
+
+	wsysinit_sysfs_class = class_create (THIS_MODULE, "wago");
+	if (IS_ERR(wsysinit_sysfs_class)) {
+
+		pr_err("%s: class_create: error %li\n",
+			__func__, PTR_ERR(wsysinit_sysfs_class));
+		unregister_chrdev_region(wsysinit_sysfs_dev, 1);
+		return -1;
+	}
+
+	wsysinit_sysfs_device = device_create (wsysinit_sysfs_class, NULL,
+					wsysinit_sysfs_dev, NULL, "system");
+
+	return platform_driver_register(&wsysinit_init_driver);
+}
+
+static __exit void wsysinit_exit(void)
+{
+	platform_driver_unregister(&wsysinit_init_driver);
+}
+
+#ifdef CONFIG_ARCH_OMAP3
+omap_postcore_initcall(wsysinit_init);
+#else
+postcore_initcall(wsysinit_init);
+#endif
+
+module_exit(wsysinit_exit);
diff --git a/arch/arm/mach-stm32/Makefile b/arch/arm/mach-stm32/Makefile
index c80d80c199d3..c0951426ec96 100644
--- a/arch/arm/mach-stm32/Makefile
+++ b/arch/arm/mach-stm32/Makefile
@@ -1,2 +1,3 @@
 # SPDX-License-Identifier: GPL-2.0-only
 obj-y += board-dt.o
+obj-$(CONFIG_WAGO_SYSTEM_BASED_STARTUP) += ../mach-omap2/wsysinit.o
diff --git a/arch/arm/mm/Makefile b/arch/arm/mm/Makefile
index 7cb1699fbfc4..c4ce477c5261 100644
--- a/arch/arm/mm/Makefile
+++ b/arch/arm/mm/Makefile
@@ -19,7 +19,6 @@ obj-$(CONFIG_MODULES)		+= proc-syms.o
 obj-$(CONFIG_DEBUG_VIRTUAL)	+= physaddr.o
 
 obj-$(CONFIG_ALIGNMENT_TRAP)	+= alignment.o
-obj-$(CONFIG_HIGHMEM)		+= highmem.o
 obj-$(CONFIG_HUGETLB_PAGE)	+= hugetlbpage.o
 obj-$(CONFIG_ARM_PV_FIXUP)	+= pv-fixup-asm.o
 
diff --git a/arch/arm/mm/alignment.c b/arch/arm/mm/alignment.c
index ea81e89e7740..f6e611aa6a04 100644
--- a/arch/arm/mm/alignment.c
+++ b/arch/arm/mm/alignment.c
@@ -870,6 +870,7 @@ do_alignment(unsigned long addr, unsigned int fsr, struct pt_regs *regs)
 		break;
 
 	case 0x04000000:	/* ldr or str immediate */
+	case 0x0C000000:	/* ldr | str float */
 		if (COND_BITS(instr) == 0xf0000000) /* NEON VLDn, VSTn */
 			goto bad;
 		offset.un = OFFSET_BITS(instr);
diff --git a/arch/arm/mm/cache-feroceon-l2.c b/arch/arm/mm/cache-feroceon-l2.c
index 5c1b7a7b9af6..87328766e910 100644
--- a/arch/arm/mm/cache-feroceon-l2.c
+++ b/arch/arm/mm/cache-feroceon-l2.c
@@ -49,9 +49,9 @@ static inline unsigned long l2_get_va(unsigned long paddr)
 	 * we simply install a virtual mapping for it only for the
 	 * TLB lookup to occur, hence no need to flush the untouched
 	 * memory mapping afterwards (note: a cache flush may happen
-	 * in some circumstances depending on the path taken in kunmap_atomic).
+	 * in some circumstances depending on the path taken in kunmap_local).
 	 */
-	void *vaddr = kmap_atomic_pfn(paddr >> PAGE_SHIFT);
+	void *vaddr = kmap_local_pfn(paddr >> PAGE_SHIFT);
 	return (unsigned long)vaddr + (paddr & ~PAGE_MASK);
 #else
 	return __phys_to_virt(paddr);
@@ -61,7 +61,7 @@ static inline unsigned long l2_get_va(unsigned long paddr)
 static inline void l2_put_va(unsigned long vaddr)
 {
 #ifdef CONFIG_HIGHMEM
-	kunmap_atomic((void *)vaddr);
+	kunmap_local((void *)vaddr);
 #endif
 }
 
diff --git a/arch/arm/mm/cache-xsc3l2.c b/arch/arm/mm/cache-xsc3l2.c
index d20d7af02d10..0e0a3abd8174 100644
--- a/arch/arm/mm/cache-xsc3l2.c
+++ b/arch/arm/mm/cache-xsc3l2.c
@@ -59,7 +59,7 @@ static inline void l2_unmap_va(unsigned long va)
 {
 #ifdef CONFIG_HIGHMEM
 	if (va != -1)
-		kunmap_atomic((void *)va);
+		kunmap_local((void *)va);
 #endif
 }
 
@@ -75,7 +75,7 @@ static inline unsigned long l2_map_va(unsigned long pa, unsigned long prev_va)
 		 * in place for it.
 		 */
 		l2_unmap_va(prev_va);
-		va = (unsigned long)kmap_atomic_pfn(pa >> PAGE_SHIFT);
+		va = (unsigned long)kmap_local_pfn(pa >> PAGE_SHIFT);
 	}
 	return va + (pa_offset >> (32 - PAGE_SHIFT));
 #else
diff --git a/arch/arm/mm/fault.c b/arch/arm/mm/fault.c
index efa402025031..59487ee9fd61 100644
--- a/arch/arm/mm/fault.c
+++ b/arch/arm/mm/fault.c
@@ -400,6 +400,9 @@ do_translation_fault(unsigned long addr, unsigned int fsr,
 	if (addr < TASK_SIZE)
 		return do_page_fault(addr, fsr, regs);
 
+	if (interrupts_enabled(regs))
+		local_irq_enable();
+
 	if (user_mode(regs))
 		goto bad_area;
 
@@ -470,6 +473,9 @@ do_translation_fault(unsigned long addr, unsigned int fsr,
 static int
 do_sect_fault(unsigned long addr, unsigned int fsr, struct pt_regs *regs)
 {
+	if (interrupts_enabled(regs))
+		local_irq_enable();
+
 	do_bad_area(addr, fsr, regs);
 	return 0;
 }
diff --git a/arch/arm/mm/highmem.c b/arch/arm/mm/highmem.c
deleted file mode 100644
index 187fab227b50..000000000000
--- a/arch/arm/mm/highmem.c
+++ /dev/null
@@ -1,121 +0,0 @@
-// SPDX-License-Identifier: GPL-2.0-only
-/*
- * arch/arm/mm/highmem.c -- ARM highmem support
- *
- * Author:	Nicolas Pitre
- * Created:	september 8, 2008
- * Copyright:	Marvell Semiconductors Inc.
- */
-
-#include <linux/module.h>
-#include <linux/highmem.h>
-#include <linux/interrupt.h>
-#include <asm/fixmap.h>
-#include <asm/cacheflush.h>
-#include <asm/tlbflush.h>
-#include "mm.h"
-
-static inline void set_fixmap_pte(int idx, pte_t pte)
-{
-	unsigned long vaddr = __fix_to_virt(idx);
-	pte_t *ptep = virt_to_kpte(vaddr);
-
-	set_pte_ext(ptep, pte, 0);
-	local_flush_tlb_kernel_page(vaddr);
-}
-
-static inline pte_t get_fixmap_pte(unsigned long vaddr)
-{
-	pte_t *ptep = virt_to_kpte(vaddr);
-
-	return *ptep;
-}
-
-void *kmap_atomic_high_prot(struct page *page, pgprot_t prot)
-{
-	unsigned int idx;
-	unsigned long vaddr;
-	void *kmap;
-	int type;
-
-#ifdef CONFIG_DEBUG_HIGHMEM
-	/*
-	 * There is no cache coherency issue when non VIVT, so force the
-	 * dedicated kmap usage for better debugging purposes in that case.
-	 */
-	if (!cache_is_vivt())
-		kmap = NULL;
-	else
-#endif
-		kmap = kmap_high_get(page);
-	if (kmap)
-		return kmap;
-
-	type = kmap_atomic_idx_push();
-
-	idx = FIX_KMAP_BEGIN + type + KM_TYPE_NR * smp_processor_id();
-	vaddr = __fix_to_virt(idx);
-#ifdef CONFIG_DEBUG_HIGHMEM
-	/*
-	 * With debugging enabled, kunmap_atomic forces that entry to 0.
-	 * Make sure it was indeed properly unmapped.
-	 */
-	BUG_ON(!pte_none(get_fixmap_pte(vaddr)));
-#endif
-	/*
-	 * When debugging is off, kunmap_atomic leaves the previous mapping
-	 * in place, so the contained TLB flush ensures the TLB is updated
-	 * with the new mapping.
-	 */
-	set_fixmap_pte(idx, mk_pte(page, prot));
-
-	return (void *)vaddr;
-}
-EXPORT_SYMBOL(kmap_atomic_high_prot);
-
-void kunmap_atomic_high(void *kvaddr)
-{
-	unsigned long vaddr = (unsigned long) kvaddr & PAGE_MASK;
-	int idx, type;
-
-	if (kvaddr >= (void *)FIXADDR_START) {
-		type = kmap_atomic_idx();
-		idx = FIX_KMAP_BEGIN + type + KM_TYPE_NR * smp_processor_id();
-
-		if (cache_is_vivt())
-			__cpuc_flush_dcache_area((void *)vaddr, PAGE_SIZE);
-#ifdef CONFIG_DEBUG_HIGHMEM
-		BUG_ON(vaddr != __fix_to_virt(idx));
-		set_fixmap_pte(idx, __pte(0));
-#else
-		(void) idx;  /* to kill a warning */
-#endif
-		kmap_atomic_idx_pop();
-	} else if (vaddr >= PKMAP_ADDR(0) && vaddr < PKMAP_ADDR(LAST_PKMAP)) {
-		/* this address was obtained through kmap_high_get() */
-		kunmap_high(pte_page(pkmap_page_table[PKMAP_NR(vaddr)]));
-	}
-}
-EXPORT_SYMBOL(kunmap_atomic_high);
-
-void *kmap_atomic_pfn(unsigned long pfn)
-{
-	unsigned long vaddr;
-	int idx, type;
-	struct page *page = pfn_to_page(pfn);
-
-	preempt_disable();
-	pagefault_disable();
-	if (!PageHighMem(page))
-		return page_address(page);
-
-	type = kmap_atomic_idx_push();
-	idx = FIX_KMAP_BEGIN + type + KM_TYPE_NR * smp_processor_id();
-	vaddr = __fix_to_virt(idx);
-#ifdef CONFIG_DEBUG_HIGHMEM
-	BUG_ON(!pte_none(get_fixmap_pte(vaddr)));
-#endif
-	set_fixmap_pte(idx, pfn_pte(pfn, kmap_prot));
-
-	return (void *)vaddr;
-}
diff --git a/arch/arm/plat-omap/counter_32k.c b/arch/arm/plat-omap/counter_32k.c
index 7a729ade2105..c2be57b80052 100644
--- a/arch/arm/plat-omap/counter_32k.c
+++ b/arch/arm/plat-omap/counter_32k.c
@@ -16,6 +16,7 @@
 #include <linux/io.h>
 #include <linux/clocksource.h>
 #include <linux/sched_clock.h>
+#include <linux/of.h>
 
 #include <asm/mach/time.h>
 
@@ -67,6 +68,8 @@ static void omap_read_persistent_clock64(struct timespec64 *ts)
 	*ts = persistent_ts;
 }
 
+#define OMAP2_32KSYNCNT_FREQ       32768
+
 /**
  * omap_init_clocksource_32k - setup and register counter 32k as a
  * kernel clocksource
@@ -79,7 +82,18 @@ static void omap_read_persistent_clock64(struct timespec64 *ts)
 int __init omap_init_clocksource_32k(void __iomem *vbase)
 {
 	int ret;
+	int omap2_32ksyncnt = OMAP2_32KSYNCNT_FREQ;
 
+	/*
+	 * We use internal 32k generation which is based on sys_xtal 26Mhz
+	 * external clock which is divided by 800 to provide a 32k like clock.
+	 * The theoretical value is 26000000/800 = 32500.
+	 * Empirical value is 32498.50669 ~ rounded integer would be 32499.
+	 * We use that one. This should give us a drifting of appr.
+	 * 14 ppm ~ 7.5 min/year.
+	 */
+	if (of_machine_is_compatible("wago,am3505-pfc"))
+		omap2_32ksyncnt = 32499;
 	/*
 	 * 32k sync Counter IP register offsets vary between the
 	 * highlander version and the legacy ones.
@@ -97,18 +111,18 @@ int __init omap_init_clocksource_32k(void __iomem *vbase)
 	 * __clocksource_update_freq_scale.
 	 */
 	clocks_calc_mult_shift(&persistent_mult, &persistent_shift,
-			32768, NSEC_PER_SEC, 120000);
+			omap2_32ksyncnt, NSEC_PER_SEC, 120000);
 
-	ret = clocksource_mmio_init(sync32k_cnt_reg, "32k_counter", 32768,
+	ret = clocksource_mmio_init(sync32k_cnt_reg, "32k_counter", omap2_32ksyncnt,
 				250, 32, clocksource_mmio_readl_up);
 	if (ret) {
 		pr_err("32k_counter: can't register clocksource\n");
 		return ret;
 	}
 
-	sched_clock_register(omap_32k_read_sched_clock, 32, 32768);
+	sched_clock_register(omap_32k_read_sched_clock, 32, omap2_32ksyncnt);
 	register_persistent_clock(omap_read_persistent_clock64);
-	pr_info("OMAP clocksource: 32k_counter at 32768 Hz\n");
+	pr_info("OMAP clocksource: 32k_counter at %d Hz\n", omap2_32ksyncnt);
 
 	return 0;
 }
diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig
index afe4bc55d4eb..6e9e4deb3dae 100644
--- a/arch/arm64/Kconfig
+++ b/arch/arm64/Kconfig
@@ -76,6 +76,7 @@ config ARM64
 	select ARCH_SUPPORTS_ATOMIC_RMW
 	select ARCH_SUPPORTS_INT128 if CC_HAS_INT128 && (GCC_VERSION >= 50000 || CC_IS_CLANG)
 	select ARCH_SUPPORTS_NUMA_BALANCING
+	select ARCH_SUPPORTS_RT if HAVE_POSIX_CPU_TIMERS_TASK_WORK
 	select ARCH_WANT_COMPAT_IPC_PARSE_VERSION if COMPAT
 	select ARCH_WANT_DEFAULT_BPF_JIT
 	select ARCH_WANT_DEFAULT_TOPDOWN_MMAP_LAYOUT
@@ -173,6 +174,7 @@ config ARM64
 	select HAVE_PERF_EVENTS
 	select HAVE_PERF_REGS
 	select HAVE_PERF_USER_STACK_DUMP
+	select HAVE_PREEMPT_LAZY
 	select HAVE_REGS_AND_STACK_ACCESS_API
 	select HAVE_FUNCTION_ARG_ACCESS_API
 	select HAVE_FUTEX_CMPXCHG if FUTEX
@@ -194,6 +196,7 @@ config ARM64
 	select PCI_DOMAINS_GENERIC if PCI
 	select PCI_ECAM if (ACPI && PCI)
 	select PCI_SYSCALL if PCI
+	select HAVE_POSIX_CPU_TIMERS_TASK_WORK if !KVM
 	select POWER_RESET
 	select POWER_SUPPLY
 	select SET_FS
diff --git a/arch/arm64/include/asm/hardirq.h b/arch/arm64/include/asm/hardirq.h
index 5ffa4bacdad3..cbfa7b6f2e09 100644
--- a/arch/arm64/include/asm/hardirq.h
+++ b/arch/arm64/include/asm/hardirq.h
@@ -13,11 +13,8 @@
 #include <asm/kvm_arm.h>
 #include <asm/sysreg.h>
 
-typedef struct {
-	unsigned int __softirq_pending;
-} ____cacheline_aligned irq_cpustat_t;
-
-#include <linux/irq_cpustat.h>	/* Standard mappings for irq_cpustat_t above */
+#define ack_bad_irq ack_bad_irq
+#include <asm-generic/hardirq.h>
 
 #define __ARCH_IRQ_EXIT_IRQS_DISABLED	1
 
diff --git a/arch/arm64/include/asm/preempt.h b/arch/arm64/include/asm/preempt.h
index 80e946b2abee..994f997b1572 100644
--- a/arch/arm64/include/asm/preempt.h
+++ b/arch/arm64/include/asm/preempt.h
@@ -70,17 +70,43 @@ static inline bool __preempt_count_dec_and_test(void)
 	 * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
 	 * pair.
 	 */
-	return !pc || !READ_ONCE(ti->preempt_count);
+	if (!pc || !READ_ONCE(ti->preempt_count))
+		return true;
+#ifdef CONFIG_PREEMPT_LAZY
+	if ((pc & ~PREEMPT_NEED_RESCHED))
+		return false;
+	if (current_thread_info()->preempt_lazy_count)
+		return false;
+	return test_thread_flag(TIF_NEED_RESCHED_LAZY);
+#else
+	return false;
+#endif
 }
 
 static inline bool should_resched(int preempt_offset)
 {
+#ifdef CONFIG_PREEMPT_LAZY
+	u64 pc = READ_ONCE(current_thread_info()->preempt_count);
+	if (pc == preempt_offset)
+		return true;
+
+	if ((pc & ~PREEMPT_NEED_RESCHED) != preempt_offset)
+		return false;
+
+	if (current_thread_info()->preempt_lazy_count)
+		return false;
+	return test_thread_flag(TIF_NEED_RESCHED_LAZY);
+#else
 	u64 pc = READ_ONCE(current_thread_info()->preempt_count);
 	return pc == preempt_offset;
+#endif
 }
 
 #ifdef CONFIG_PREEMPTION
 void preempt_schedule(void);
+#ifdef CONFIG_PREEMPT_RT
+void preempt_schedule_lock(void);
+#endif
 #define __preempt_schedule() preempt_schedule()
 void preempt_schedule_notrace(void);
 #define __preempt_schedule_notrace() preempt_schedule_notrace()
diff --git a/arch/arm64/include/asm/spinlock_types.h b/arch/arm64/include/asm/spinlock_types.h
index 18782f0c4721..6672b05350b4 100644
--- a/arch/arm64/include/asm/spinlock_types.h
+++ b/arch/arm64/include/asm/spinlock_types.h
@@ -5,10 +5,6 @@
 #ifndef __ASM_SPINLOCK_TYPES_H
 #define __ASM_SPINLOCK_TYPES_H
 
-#if !defined(__LINUX_SPINLOCK_TYPES_H) && !defined(__ASM_SPINLOCK_H)
-# error "please don't include this file directly"
-#endif
-
 #include <asm-generic/qspinlock_types.h>
 #include <asm-generic/qrwlock_types.h>
 
diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index 1fbab854a51b..148b53dc2840 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -29,6 +29,7 @@ struct thread_info {
 #ifdef CONFIG_ARM64_SW_TTBR0_PAN
 	u64			ttbr0;		/* saved TTBR0_EL1 */
 #endif
+	int			preempt_lazy_count;	/* 0 => preemptable, <0 => bug */
 	union {
 		u64		preempt_count;	/* 0 => preemptible, <0 => bug */
 		struct {
@@ -68,6 +69,7 @@ void arch_release_task_struct(struct task_struct *tsk);
 #define TIF_UPROBE		4	/* uprobe breakpoint or singlestep */
 #define TIF_FSCHECK		5	/* Check FS is USER_DS on return */
 #define TIF_MTE_ASYNC_FAULT	6	/* MTE Asynchronous Tag Check Fault */
+#define TIF_NEED_RESCHED_LAZY	7
 #define TIF_SYSCALL_TRACE	8	/* syscall trace active */
 #define TIF_SYSCALL_AUDIT	9	/* syscall auditing */
 #define TIF_SYSCALL_TRACEPOINT	10	/* syscall tracepoint for ftrace */
@@ -98,11 +100,14 @@ void arch_release_task_struct(struct task_struct *tsk);
 #define _TIF_32BIT		(1 << TIF_32BIT)
 #define _TIF_SVE		(1 << TIF_SVE)
 #define _TIF_MTE_ASYNC_FAULT	(1 << TIF_MTE_ASYNC_FAULT)
+#define _TIF_NEED_RESCHED_LAZY	(1 << TIF_NEED_RESCHED_LAZY)
 
 #define _TIF_WORK_MASK		(_TIF_NEED_RESCHED | _TIF_SIGPENDING | \
 				 _TIF_NOTIFY_RESUME | _TIF_FOREIGN_FPSTATE | \
-				 _TIF_UPROBE | _TIF_FSCHECK | _TIF_MTE_ASYNC_FAULT)
+				 _TIF_UPROBE | _TIF_FSCHECK | _TIF_MTE_ASYNC_FAULT | \
+				 _TIF_NEED_RESCHED_LAZY)
 
+#define _TIF_NEED_RESCHED_MASK	(_TIF_NEED_RESCHED | _TIF_NEED_RESCHED_LAZY)
 #define _TIF_SYSCALL_WORK	(_TIF_SYSCALL_TRACE | _TIF_SYSCALL_AUDIT | \
 				 _TIF_SYSCALL_TRACEPOINT | _TIF_SECCOMP | \
 				 _TIF_SYSCALL_EMU)
diff --git a/arch/arm64/kernel/asm-offsets.c b/arch/arm64/kernel/asm-offsets.c
index 7d32fc959b1a..b2f29bd2ae87 100644
--- a/arch/arm64/kernel/asm-offsets.c
+++ b/arch/arm64/kernel/asm-offsets.c
@@ -30,6 +30,7 @@ int main(void)
   BLANK();
   DEFINE(TSK_TI_FLAGS,		offsetof(struct task_struct, thread_info.flags));
   DEFINE(TSK_TI_PREEMPT,	offsetof(struct task_struct, thread_info.preempt_count));
+  DEFINE(TSK_TI_PREEMPT_LAZY,	offsetof(struct task_struct, thread_info.preempt_lazy_count));
   DEFINE(TSK_TI_ADDR_LIMIT,	offsetof(struct task_struct, thread_info.addr_limit));
 #ifdef CONFIG_ARM64_SW_TTBR0_PAN
   DEFINE(TSK_TI_TTBR0,		offsetof(struct task_struct, thread_info.ttbr0));
diff --git a/arch/arm64/kernel/entry.S b/arch/arm64/kernel/entry.S
index d72c818b019c..924cec6010ff 100644
--- a/arch/arm64/kernel/entry.S
+++ b/arch/arm64/kernel/entry.S
@@ -649,9 +649,18 @@ alternative_if ARM64_HAS_IRQ_PRIO_MASKING
 	mrs	x0, daif
 	orr	x24, x24, x0
 alternative_else_nop_endif
-	cbnz	x24, 1f				// preempt count != 0 || NMI return path
-	bl	arm64_preempt_schedule_irq	// irq en/disable is done inside
+
+	cbz	x24, 1f					// (need_resched + count) == 0
+	cbnz	w24, 2f					// count != 0
+
+	ldr	w24, [tsk, #TSK_TI_PREEMPT_LAZY]	// get preempt lazy count
+	cbnz	w24, 2f					// preempt lazy count != 0
+
+	ldr	x0, [tsk, #TSK_TI_FLAGS]		// get flags
+	tbz	x0, #TIF_NEED_RESCHED_LAZY, 2f		// needs rescheduling?
 1:
+	bl	arm64_preempt_schedule_irq		// irq en/disable is done inside
+2:
 #endif
 
 	mov	x0, sp
diff --git a/arch/arm64/kernel/fpsimd.c b/arch/arm64/kernel/fpsimd.c
index 062b21f30f94..0ea2df6554e5 100644
--- a/arch/arm64/kernel/fpsimd.c
+++ b/arch/arm64/kernel/fpsimd.c
@@ -226,6 +226,16 @@ static void sve_free(struct task_struct *task)
 	__sve_free(task);
 }
 
+static void *sve_free_atomic(struct task_struct *task)
+{
+	void *sve_state = task->thread.sve_state;
+
+	WARN_ON(test_tsk_thread_flag(task, TIF_SVE));
+
+	task->thread.sve_state = NULL;
+	return sve_state;
+}
+
 /*
  * TIF_SVE controls whether a task can use SVE without trapping while
  * in userspace, and also the way a task's FPSIMD/SVE state is stored
@@ -1022,6 +1032,7 @@ void fpsimd_thread_switch(struct task_struct *next)
 void fpsimd_flush_thread(void)
 {
 	int vl, supported_vl;
+	void *mem = NULL;
 
 	if (!system_supports_fpsimd())
 		return;
@@ -1034,7 +1045,7 @@ void fpsimd_flush_thread(void)
 
 	if (system_supports_sve()) {
 		clear_thread_flag(TIF_SVE);
-		sve_free(current);
+		mem = sve_free_atomic(current);
 
 		/*
 		 * Reset the task vector length as required.
@@ -1068,6 +1079,7 @@ void fpsimd_flush_thread(void)
 	}
 
 	put_cpu_fpsimd_context();
+	kfree(mem);
 }
 
 /*
diff --git a/arch/arm64/kernel/signal.c b/arch/arm64/kernel/signal.c
index 50852992752b..aafe59f680e8 100644
--- a/arch/arm64/kernel/signal.c
+++ b/arch/arm64/kernel/signal.c
@@ -918,7 +918,7 @@ asmlinkage void do_notify_resume(struct pt_regs *regs,
 		/* Check valid user FS if needed */
 		addr_limit_user_check();
 
-		if (thread_flags & _TIF_NEED_RESCHED) {
+		if (thread_flags & _TIF_NEED_RESCHED_MASK) {
 			/* Unmask Debug and SError for the next task */
 			local_daif_restore(DAIF_PROCCTX_NOIRQ);
 
diff --git a/arch/arm64/kvm/arm.c b/arch/arm64/kvm/arm.c
index c0ffb019ca8b..e7c39a029823 100644
--- a/arch/arm64/kvm/arm.c
+++ b/arch/arm64/kvm/arm.c
@@ -701,7 +701,7 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 		 * involves poking the GIC, which must be done in a
 		 * non-preemptible context.
 		 */
-		preempt_disable();
+		migrate_disable();
 
 		kvm_pmu_flush_hwstate(vcpu);
 
@@ -750,7 +750,7 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 				kvm_timer_sync_user(vcpu);
 			kvm_vgic_sync_hwstate(vcpu);
 			local_irq_enable();
-			preempt_enable();
+			migrate_enable();
 			continue;
 		}
 
@@ -822,7 +822,7 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 		/* Exit types that need handling before we can be preempted */
 		handle_exit_early(vcpu, ret);
 
-		preempt_enable();
+		migrate_enable();
 
 		/*
 		 * The ARMv8 architecture doesn't give the hypervisor
diff --git a/arch/csky/Kconfig b/arch/csky/Kconfig
index 268fad5f51cf..7a86481a22ff 100644
--- a/arch/csky/Kconfig
+++ b/arch/csky/Kconfig
@@ -286,6 +286,7 @@ config NR_CPUS
 config HIGHMEM
 	bool "High Memory Support"
 	depends on !CPU_CK610
+	select KMAP_LOCAL
 	default y
 
 config FORCE_MAX_ZONEORDER
diff --git a/arch/csky/include/asm/fixmap.h b/arch/csky/include/asm/fixmap.h
index 81f9477d5330..4b589cc20900 100644
--- a/arch/csky/include/asm/fixmap.h
+++ b/arch/csky/include/asm/fixmap.h
@@ -8,7 +8,7 @@
 #include <asm/memory.h>
 #ifdef CONFIG_HIGHMEM
 #include <linux/threads.h>
-#include <asm/kmap_types.h>
+#include <asm/kmap_size.h>
 #endif
 
 enum fixed_addresses {
@@ -17,7 +17,7 @@ enum fixed_addresses {
 #endif
 #ifdef CONFIG_HIGHMEM
 	FIX_KMAP_BEGIN,
-	FIX_KMAP_END = FIX_KMAP_BEGIN + (KM_TYPE_NR * NR_CPUS) - 1,
+	FIX_KMAP_END = FIX_KMAP_BEGIN + (KM_MAX_IDX * NR_CPUS) - 1,
 #endif
 	__end_of_fixed_addresses
 };
diff --git a/arch/csky/include/asm/highmem.h b/arch/csky/include/asm/highmem.h
index 14645e3d5cd5..1f4ed3f4c0d9 100644
--- a/arch/csky/include/asm/highmem.h
+++ b/arch/csky/include/asm/highmem.h
@@ -9,7 +9,7 @@
 #include <linux/init.h>
 #include <linux/interrupt.h>
 #include <linux/uaccess.h>
-#include <asm/kmap_types.h>
+#include <asm/kmap_size.h>
 #include <asm/cache.h>
 
 /* undef for production */
@@ -32,10 +32,12 @@ extern pte_t *pkmap_page_table;
 
 #define ARCH_HAS_KMAP_FLUSH_TLB
 extern void kmap_flush_tlb(unsigned long addr);
-extern void *kmap_atomic_pfn(unsigned long pfn);
 
 #define flush_cache_kmaps() do {} while (0)
 
+#define arch_kmap_local_post_map(vaddr, pteval)	kmap_flush_tlb(vaddr)
+#define arch_kmap_local_post_unmap(vaddr)	kmap_flush_tlb(vaddr)
+
 extern void kmap_init(void);
 
 #endif /* __KERNEL__ */
diff --git a/arch/csky/mm/highmem.c b/arch/csky/mm/highmem.c
index 89c10800a002..4161df3c6c15 100644
--- a/arch/csky/mm/highmem.c
+++ b/arch/csky/mm/highmem.c
@@ -9,8 +9,6 @@
 #include <asm/tlbflush.h>
 #include <asm/cacheflush.h>
 
-static pte_t *kmap_pte;
-
 unsigned long highstart_pfn, highend_pfn;
 
 void kmap_flush_tlb(unsigned long addr)
@@ -19,67 +17,7 @@ void kmap_flush_tlb(unsigned long addr)
 }
 EXPORT_SYMBOL(kmap_flush_tlb);
 
-void *kmap_atomic_high_prot(struct page *page, pgprot_t prot)
-{
-	unsigned long vaddr;
-	int idx, type;
-
-	type = kmap_atomic_idx_push();
-	idx = type + KM_TYPE_NR*smp_processor_id();
-	vaddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);
-#ifdef CONFIG_DEBUG_HIGHMEM
-	BUG_ON(!pte_none(*(kmap_pte - idx)));
-#endif
-	set_pte(kmap_pte-idx, mk_pte(page, prot));
-	flush_tlb_one((unsigned long)vaddr);
-
-	return (void *)vaddr;
-}
-EXPORT_SYMBOL(kmap_atomic_high_prot);
-
-void kunmap_atomic_high(void *kvaddr)
-{
-	unsigned long vaddr = (unsigned long) kvaddr & PAGE_MASK;
-	int idx;
-
-	if (vaddr < FIXADDR_START)
-		return;
-
-#ifdef CONFIG_DEBUG_HIGHMEM
-	idx = KM_TYPE_NR*smp_processor_id() + kmap_atomic_idx();
-
-	BUG_ON(vaddr != __fix_to_virt(FIX_KMAP_BEGIN + idx));
-
-	pte_clear(&init_mm, vaddr, kmap_pte - idx);
-	flush_tlb_one(vaddr);
-#else
-	(void) idx; /* to kill a warning */
-#endif
-	kmap_atomic_idx_pop();
-}
-EXPORT_SYMBOL(kunmap_atomic_high);
-
-/*
- * This is the same as kmap_atomic() but can map memory that doesn't
- * have a struct page associated with it.
- */
-void *kmap_atomic_pfn(unsigned long pfn)
-{
-	unsigned long vaddr;
-	int idx, type;
-
-	pagefault_disable();
-
-	type = kmap_atomic_idx_push();
-	idx = type + KM_TYPE_NR*smp_processor_id();
-	vaddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);
-	set_pte(kmap_pte-idx, pfn_pte(pfn, PAGE_KERNEL));
-	flush_tlb_one(vaddr);
-
-	return (void *) vaddr;
-}
-
-static void __init kmap_pages_init(void)
+void __init kmap_init(void)
 {
 	unsigned long vaddr;
 	pgd_t *pgd;
@@ -96,14 +34,3 @@ static void __init kmap_pages_init(void)
 	pte = pte_offset_kernel(pmd, vaddr);
 	pkmap_page_table = pte;
 }
-
-void __init kmap_init(void)
-{
-	unsigned long vaddr;
-
-	kmap_pages_init();
-
-	vaddr = __fix_to_virt(FIX_KMAP_BEGIN);
-
-	kmap_pte = pte_offset_kernel((pmd_t *)pgd_offset_k(vaddr), vaddr);
-}
diff --git a/arch/hexagon/include/asm/spinlock_types.h b/arch/hexagon/include/asm/spinlock_types.h
index 19d233497ba5..de72fb23016d 100644
--- a/arch/hexagon/include/asm/spinlock_types.h
+++ b/arch/hexagon/include/asm/spinlock_types.h
@@ -8,10 +8,6 @@
 #ifndef _ASM_SPINLOCK_TYPES_H
 #define _ASM_SPINLOCK_TYPES_H
 
-#ifndef __LINUX_SPINLOCK_TYPES_H
-# error "please don't include this file directly"
-#endif
-
 typedef struct {
 	volatile unsigned int lock;
 } arch_spinlock_t;
diff --git a/arch/ia64/include/asm/kmap_types.h b/arch/ia64/include/asm/kmap_types.h
deleted file mode 100644
index 5c268cf7c2bd..000000000000
--- a/arch/ia64/include/asm/kmap_types.h
+++ /dev/null
@@ -1,13 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0 */
-#ifndef _ASM_IA64_KMAP_TYPES_H
-#define _ASM_IA64_KMAP_TYPES_H
-
-#ifdef CONFIG_DEBUG_HIGHMEM
-#define  __WITH_KM_FENCE
-#endif
-
-#include <asm-generic/kmap_types.h>
-
-#undef __WITH_KM_FENCE
-
-#endif /* _ASM_IA64_KMAP_TYPES_H */
diff --git a/arch/ia64/include/asm/spinlock_types.h b/arch/ia64/include/asm/spinlock_types.h
index 6e345fefcdca..681408d6816f 100644
--- a/arch/ia64/include/asm/spinlock_types.h
+++ b/arch/ia64/include/asm/spinlock_types.h
@@ -2,10 +2,6 @@
 #ifndef _ASM_IA64_SPINLOCK_TYPES_H
 #define _ASM_IA64_SPINLOCK_TYPES_H
 
-#ifndef __LINUX_SPINLOCK_TYPES_H
-# error "please don't include this file directly"
-#endif
-
 typedef struct {
 	volatile unsigned int lock;
 } arch_spinlock_t;
diff --git a/arch/ia64/kernel/time.c b/arch/ia64/kernel/time.c
index 7abc5f37bfaf..733e0e3324b8 100644
--- a/arch/ia64/kernel/time.c
+++ b/arch/ia64/kernel/time.c
@@ -138,12 +138,8 @@ void vtime_account_kernel(struct task_struct *tsk)
 	struct thread_info *ti = task_thread_info(tsk);
 	__u64 stime = vtime_delta(tsk);
 
-	if ((tsk->flags & PF_VCPU) && !irq_count())
+	if (tsk->flags & PF_VCPU)
 		ti->gtime += stime;
-	else if (hardirq_count())
-		ti->hardirq_time += stime;
-	else if (in_serving_softirq())
-		ti->softirq_time += stime;
 	else
 		ti->stime += stime;
 }
@@ -156,6 +152,20 @@ void vtime_account_idle(struct task_struct *tsk)
 	ti->idle_time += vtime_delta(tsk);
 }
 
+void vtime_account_softirq(struct task_struct *tsk)
+{
+	struct thread_info *ti = task_thread_info(tsk);
+
+	ti->softirq_time += vtime_delta(tsk);
+}
+
+void vtime_account_hardirq(struct task_struct *tsk)
+{
+	struct thread_info *ti = task_thread_info(tsk);
+
+	ti->hardirq_time += vtime_delta(tsk);
+}
+
 #endif /* CONFIG_VIRT_CPU_ACCOUNTING_NATIVE */
 
 static irqreturn_t
diff --git a/arch/microblaze/Kconfig b/arch/microblaze/Kconfig
index 33925ffed68f..7f6ca0ab4f81 100644
--- a/arch/microblaze/Kconfig
+++ b/arch/microblaze/Kconfig
@@ -155,6 +155,7 @@ config XILINX_UNCACHED_SHADOW
 config HIGHMEM
 	bool "High memory support"
 	depends on MMU
+	select KMAP_LOCAL
 	help
 	  The address space of Microblaze processors is only 4 Gigabytes large
 	  and it has to accommodate user address space, kernel address
diff --git a/arch/microblaze/include/asm/fixmap.h b/arch/microblaze/include/asm/fixmap.h
index 0379ce5229e3..e6e9288bff76 100644
--- a/arch/microblaze/include/asm/fixmap.h
+++ b/arch/microblaze/include/asm/fixmap.h
@@ -20,7 +20,7 @@
 #include <asm/page.h>
 #ifdef CONFIG_HIGHMEM
 #include <linux/threads.h>
-#include <asm/kmap_types.h>
+#include <asm/kmap_size.h>
 #endif
 
 #define FIXADDR_TOP	((unsigned long)(-PAGE_SIZE))
@@ -47,7 +47,7 @@ enum fixed_addresses {
 	FIX_HOLE,
 #ifdef CONFIG_HIGHMEM
 	FIX_KMAP_BEGIN,	/* reserved pte's for temporary kernel mappings */
-	FIX_KMAP_END = FIX_KMAP_BEGIN + (KM_TYPE_NR * num_possible_cpus()) - 1,
+	FIX_KMAP_END = FIX_KMAP_BEGIN + (KM_MAX_IDX * num_possible_cpus()) - 1,
 #endif
 	__end_of_fixed_addresses
 };
diff --git a/arch/microblaze/include/asm/highmem.h b/arch/microblaze/include/asm/highmem.h
index 284ca8fb54c1..4418633fb163 100644
--- a/arch/microblaze/include/asm/highmem.h
+++ b/arch/microblaze/include/asm/highmem.h
@@ -25,7 +25,6 @@
 #include <linux/uaccess.h>
 #include <asm/fixmap.h>
 
-extern pte_t *kmap_pte;
 extern pte_t *pkmap_page_table;
 
 /*
@@ -52,6 +51,11 @@ extern pte_t *pkmap_page_table;
 
 #define flush_cache_kmaps()	{ flush_icache(); flush_dcache(); }
 
+#define arch_kmap_local_post_map(vaddr, pteval)	\
+	local_flush_tlb_page(NULL, vaddr);
+#define arch_kmap_local_post_unmap(vaddr)	\
+	local_flush_tlb_page(NULL, vaddr);
+
 #endif /* __KERNEL__ */
 
 #endif /* _ASM_HIGHMEM_H */
diff --git a/arch/microblaze/mm/Makefile b/arch/microblaze/mm/Makefile
index 1b16875cea70..8ced71100047 100644
--- a/arch/microblaze/mm/Makefile
+++ b/arch/microblaze/mm/Makefile
@@ -6,4 +6,3 @@
 obj-y := consistent.o init.o
 
 obj-$(CONFIG_MMU) += pgtable.o mmu_context.o fault.o
-obj-$(CONFIG_HIGHMEM) += highmem.o
diff --git a/arch/microblaze/mm/highmem.c b/arch/microblaze/mm/highmem.c
deleted file mode 100644
index 92e0890416c9..000000000000
--- a/arch/microblaze/mm/highmem.c
+++ /dev/null
@@ -1,78 +0,0 @@
-// SPDX-License-Identifier: GPL-2.0
-/*
- * highmem.c: virtual kernel memory mappings for high memory
- *
- * PowerPC version, stolen from the i386 version.
- *
- * Used in CONFIG_HIGHMEM systems for memory pages which
- * are not addressable by direct kernel virtual addresses.
- *
- * Copyright (C) 1999 Gerhard Wichert, Siemens AG
- *		      Gerhard.Wichert@pdb.siemens.de
- *
- *
- * Redesigned the x86 32-bit VM architecture to deal with
- * up to 16 Terrabyte physical memory. With current x86 CPUs
- * we now support up to 64 Gigabytes physical RAM.
- *
- * Copyright (C) 1999 Ingo Molnar <mingo@redhat.com>
- *
- * Reworked for PowerPC by various contributors. Moved from
- * highmem.h by Benjamin Herrenschmidt (c) 2009 IBM Corp.
- */
-
-#include <linux/export.h>
-#include <linux/highmem.h>
-
-/*
- * The use of kmap_atomic/kunmap_atomic is discouraged - kmap/kunmap
- * gives a more generic (and caching) interface. But kmap_atomic can
- * be used in IRQ contexts, so in some (very limited) cases we need
- * it.
- */
-#include <asm/tlbflush.h>
-
-void *kmap_atomic_high_prot(struct page *page, pgprot_t prot)
-{
-
-	unsigned long vaddr;
-	int idx, type;
-
-	type = kmap_atomic_idx_push();
-	idx = type + KM_TYPE_NR*smp_processor_id();
-	vaddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);
-#ifdef CONFIG_DEBUG_HIGHMEM
-	BUG_ON(!pte_none(*(kmap_pte-idx)));
-#endif
-	set_pte_at(&init_mm, vaddr, kmap_pte-idx, mk_pte(page, prot));
-	local_flush_tlb_page(NULL, vaddr);
-
-	return (void *) vaddr;
-}
-EXPORT_SYMBOL(kmap_atomic_high_prot);
-
-void kunmap_atomic_high(void *kvaddr)
-{
-	unsigned long vaddr = (unsigned long) kvaddr & PAGE_MASK;
-	int type;
-	unsigned int idx;
-
-	if (vaddr < __fix_to_virt(FIX_KMAP_END))
-		return;
-
-	type = kmap_atomic_idx();
-
-	idx = type + KM_TYPE_NR * smp_processor_id();
-#ifdef CONFIG_DEBUG_HIGHMEM
-	BUG_ON(vaddr != __fix_to_virt(FIX_KMAP_BEGIN + idx));
-#endif
-	/*
-	 * force other mappings to Oops if they'll try to access
-	 * this pte without first remap it
-	 */
-	pte_clear(&init_mm, vaddr, kmap_pte-idx);
-	local_flush_tlb_page(NULL, vaddr);
-
-	kmap_atomic_idx_pop();
-}
-EXPORT_SYMBOL(kunmap_atomic_high);
diff --git a/arch/microblaze/mm/init.c b/arch/microblaze/mm/init.c
index 45da639bd22c..1f4b5b34e600 100644
--- a/arch/microblaze/mm/init.c
+++ b/arch/microblaze/mm/init.c
@@ -49,17 +49,11 @@ unsigned long lowmem_size;
 EXPORT_SYMBOL(min_low_pfn);
 EXPORT_SYMBOL(max_low_pfn);
 
-#ifdef CONFIG_HIGHMEM
-pte_t *kmap_pte;
-EXPORT_SYMBOL(kmap_pte);
-
 static void __init highmem_init(void)
 {
 	pr_debug("%x\n", (u32)PKMAP_BASE);
 	map_page(PKMAP_BASE, 0, 0);	/* XXX gross */
 	pkmap_page_table = virt_to_kpte(PKMAP_BASE);
-
-	kmap_pte = virt_to_kpte(__fix_to_virt(FIX_KMAP_BEGIN));
 }
 
 static void highmem_setup(void)
diff --git a/arch/mips/Kconfig b/arch/mips/Kconfig
index 2000bb2b0220..6b762bebff33 100644
--- a/arch/mips/Kconfig
+++ b/arch/mips/Kconfig
@@ -2719,6 +2719,7 @@ config WAR_MIPS34K_MISSED_ITLB
 config HIGHMEM
 	bool "High Memory Support"
 	depends on 32BIT && CPU_SUPPORTS_HIGHMEM && SYS_SUPPORTS_HIGHMEM && !CPU_MIPS32_3_5_EVA
+	select KMAP_LOCAL
 
 config CPU_SUPPORTS_HIGHMEM
 	bool
diff --git a/arch/mips/include/asm/fixmap.h b/arch/mips/include/asm/fixmap.h
index 743535be7528..beea14761cef 100644
--- a/arch/mips/include/asm/fixmap.h
+++ b/arch/mips/include/asm/fixmap.h
@@ -17,7 +17,7 @@
 #include <spaces.h>
 #ifdef CONFIG_HIGHMEM
 #include <linux/threads.h>
-#include <asm/kmap_types.h>
+#include <asm/kmap_size.h>
 #endif
 
 /*
@@ -52,7 +52,7 @@ enum fixed_addresses {
 #ifdef CONFIG_HIGHMEM
 	/* reserved pte's for temporary kernel mappings */
 	FIX_KMAP_BEGIN = FIX_CMAP_END + 1,
-	FIX_KMAP_END = FIX_KMAP_BEGIN+(KM_TYPE_NR*NR_CPUS)-1,
+	FIX_KMAP_END = FIX_KMAP_BEGIN + (KM_MAX_IDX * NR_CPUS) - 1,
 #endif
 	__end_of_fixed_addresses
 };
diff --git a/arch/mips/include/asm/highmem.h b/arch/mips/include/asm/highmem.h
index f1f788b57166..19edf8e69971 100644
--- a/arch/mips/include/asm/highmem.h
+++ b/arch/mips/include/asm/highmem.h
@@ -24,7 +24,7 @@
 #include <linux/interrupt.h>
 #include <linux/uaccess.h>
 #include <asm/cpu-features.h>
-#include <asm/kmap_types.h>
+#include <asm/kmap_size.h>
 
 /* declarations for highmem.c */
 extern unsigned long highstart_pfn, highend_pfn;
@@ -48,11 +48,11 @@ extern pte_t *pkmap_page_table;
 
 #define ARCH_HAS_KMAP_FLUSH_TLB
 extern void kmap_flush_tlb(unsigned long addr);
-extern void *kmap_atomic_pfn(unsigned long pfn);
 
 #define flush_cache_kmaps()	BUG_ON(cpu_has_dc_aliases)
 
-extern void kmap_init(void);
+#define arch_kmap_local_post_map(vaddr, pteval)	local_flush_tlb_one(vaddr)
+#define arch_kmap_local_post_unmap(vaddr)	local_flush_tlb_one(vaddr)
 
 #endif /* __KERNEL__ */
 
diff --git a/arch/mips/include/asm/kmap_types.h b/arch/mips/include/asm/kmap_types.h
deleted file mode 100644
index 16665dc2431b..000000000000
--- a/arch/mips/include/asm/kmap_types.h
+++ /dev/null
@@ -1,13 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0 */
-#ifndef _ASM_KMAP_TYPES_H
-#define _ASM_KMAP_TYPES_H
-
-#ifdef CONFIG_DEBUG_HIGHMEM
-#define	 __WITH_KM_FENCE
-#endif
-
-#include <asm-generic/kmap_types.h>
-
-#undef __WITH_KM_FENCE
-
-#endif
diff --git a/arch/mips/kernel/crash_dump.c b/arch/mips/kernel/crash_dump.c
index 01b2bd95ba1f..9aba83e1eeb4 100644
--- a/arch/mips/kernel/crash_dump.c
+++ b/arch/mips/kernel/crash_dump.c
@@ -5,8 +5,6 @@
 #include <linux/uaccess.h>
 #include <linux/slab.h>
 
-static void *kdump_buf_page;
-
 /**
  * copy_oldmem_page - copy one page from "oldmem"
  * @pfn: page frame number to be copied
@@ -17,51 +15,25 @@ static void *kdump_buf_page;
  * @userbuf: if set, @buf is in user address space, use copy_to_user(),
  *	otherwise @buf is in kernel address space, use memcpy().
  *
- * Copy a page from "oldmem". For this page, there is no pte mapped
+ * Copy a page from "oldmem". For this page, there might be no pte mapped
  * in the current kernel.
- *
- * Calling copy_to_user() in atomic context is not desirable. Hence first
- * copying the data to a pre-allocated kernel page and then copying to user
- * space in non-atomic context.
  */
-ssize_t copy_oldmem_page(unsigned long pfn, char *buf,
-			 size_t csize, unsigned long offset, int userbuf)
+ssize_t copy_oldmem_page(unsigned long pfn, char *buf, size_t csize,
+			 unsigned long offset, int userbuf)
 {
 	void  *vaddr;
 
 	if (!csize)
 		return 0;
 
-	vaddr = kmap_atomic_pfn(pfn);
+	vaddr = kmap_local_pfn(pfn);
 
 	if (!userbuf) {
-		memcpy(buf, (vaddr + offset), csize);
-		kunmap_atomic(vaddr);
+		memcpy(buf, vaddr + offset, csize);
 	} else {
-		if (!kdump_buf_page) {
-			pr_warn("Kdump: Kdump buffer page not allocated\n");
-
-			return -EFAULT;
-		}
-		copy_page(kdump_buf_page, vaddr);
-		kunmap_atomic(vaddr);
-		if (copy_to_user(buf, (kdump_buf_page + offset), csize))
-			return -EFAULT;
+		if (copy_to_user(buf, vaddr + offset, csize))
+			csize = -EFAULT;
 	}
 
 	return csize;
 }
-
-static int __init kdump_buf_page_init(void)
-{
-	int ret = 0;
-
-	kdump_buf_page = kmalloc(PAGE_SIZE, GFP_KERNEL);
-	if (!kdump_buf_page) {
-		pr_warn("Kdump: Failed to allocate kdump buffer page\n");
-		ret = -ENOMEM;
-	}
-
-	return ret;
-}
-arch_initcall(kdump_buf_page_init);
diff --git a/arch/mips/mm/highmem.c b/arch/mips/mm/highmem.c
index 5fec7f45d79a..57e2f08f00d0 100644
--- a/arch/mips/mm/highmem.c
+++ b/arch/mips/mm/highmem.c
@@ -8,8 +8,6 @@
 #include <asm/fixmap.h>
 #include <asm/tlbflush.h>
 
-static pte_t *kmap_pte;
-
 unsigned long highstart_pfn, highend_pfn;
 
 void kmap_flush_tlb(unsigned long addr)
@@ -17,78 +15,3 @@ void kmap_flush_tlb(unsigned long addr)
 	flush_tlb_one(addr);
 }
 EXPORT_SYMBOL(kmap_flush_tlb);
-
-void *kmap_atomic_high_prot(struct page *page, pgprot_t prot)
-{
-	unsigned long vaddr;
-	int idx, type;
-
-	type = kmap_atomic_idx_push();
-	idx = type + KM_TYPE_NR*smp_processor_id();
-	vaddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);
-#ifdef CONFIG_DEBUG_HIGHMEM
-	BUG_ON(!pte_none(*(kmap_pte - idx)));
-#endif
-	set_pte(kmap_pte-idx, mk_pte(page, prot));
-	local_flush_tlb_one((unsigned long)vaddr);
-
-	return (void*) vaddr;
-}
-EXPORT_SYMBOL(kmap_atomic_high_prot);
-
-void kunmap_atomic_high(void *kvaddr)
-{
-	unsigned long vaddr = (unsigned long) kvaddr & PAGE_MASK;
-	int type __maybe_unused;
-
-	if (vaddr < FIXADDR_START)
-		return;
-
-	type = kmap_atomic_idx();
-#ifdef CONFIG_DEBUG_HIGHMEM
-	{
-		int idx = type + KM_TYPE_NR * smp_processor_id();
-
-		BUG_ON(vaddr != __fix_to_virt(FIX_KMAP_BEGIN + idx));
-
-		/*
-		 * force other mappings to Oops if they'll try to access
-		 * this pte without first remap it
-		 */
-		pte_clear(&init_mm, vaddr, kmap_pte-idx);
-		local_flush_tlb_one(vaddr);
-	}
-#endif
-	kmap_atomic_idx_pop();
-}
-EXPORT_SYMBOL(kunmap_atomic_high);
-
-/*
- * This is the same as kmap_atomic() but can map memory that doesn't
- * have a struct page associated with it.
- */
-void *kmap_atomic_pfn(unsigned long pfn)
-{
-	unsigned long vaddr;
-	int idx, type;
-
-	preempt_disable();
-	pagefault_disable();
-
-	type = kmap_atomic_idx_push();
-	idx = type + KM_TYPE_NR*smp_processor_id();
-	vaddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);
-	set_pte(kmap_pte-idx, pfn_pte(pfn, PAGE_KERNEL));
-	flush_tlb_one(vaddr);
-
-	return (void*) vaddr;
-}
-
-void __init kmap_init(void)
-{
-	unsigned long kmap_vstart;
-
-	/* cache the first kmap pte */
-	kmap_vstart = __fix_to_virt(FIX_KMAP_BEGIN);
-	kmap_pte = virt_to_kpte(kmap_vstart);
-}
diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 07e84a774938..bc80893e5c0f 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -36,7 +36,6 @@
 #include <asm/cachectl.h>
 #include <asm/cpu.h>
 #include <asm/dma.h>
-#include <asm/kmap_types.h>
 #include <asm/maar.h>
 #include <asm/mmu_context.h>
 #include <asm/sections.h>
@@ -402,9 +401,6 @@ void __init paging_init(void)
 
 	pagetable_init();
 
-#ifdef CONFIG_HIGHMEM
-	kmap_init();
-#endif
 #ifdef CONFIG_ZONE_DMA
 	max_zone_pfns[ZONE_DMA] = MAX_DMA_PFN;
 #endif
diff --git a/arch/nds32/Kconfig.cpu b/arch/nds32/Kconfig.cpu
index f88a12fdf0f3..c10759952485 100644
--- a/arch/nds32/Kconfig.cpu
+++ b/arch/nds32/Kconfig.cpu
@@ -157,6 +157,7 @@ config HW_SUPPORT_UNALIGNMENT_ACCESS
 config HIGHMEM
 	bool "High Memory Support"
 	depends on MMU && !CPU_CACHE_ALIASING
+	select KMAP_LOCAL
 	help
 	  The address space of Andes processors is only 4 Gigabytes large
 	  and it has to accommodate user address space, kernel address
diff --git a/arch/nds32/include/asm/fixmap.h b/arch/nds32/include/asm/fixmap.h
index 5a4bf11e5800..2fa09a2de428 100644
--- a/arch/nds32/include/asm/fixmap.h
+++ b/arch/nds32/include/asm/fixmap.h
@@ -6,7 +6,7 @@
 
 #ifdef CONFIG_HIGHMEM
 #include <linux/threads.h>
-#include <asm/kmap_types.h>
+#include <asm/kmap_size.h>
 #endif
 
 enum fixed_addresses {
@@ -14,7 +14,7 @@ enum fixed_addresses {
 	FIX_KMAP_RESERVED,
 	FIX_KMAP_BEGIN,
 #ifdef CONFIG_HIGHMEM
-	FIX_KMAP_END = FIX_KMAP_BEGIN + (KM_TYPE_NR * NR_CPUS),
+	FIX_KMAP_END = FIX_KMAP_BEGIN + (KM_MAX_IDX * NR_CPUS) - 1,
 #endif
 	FIX_EARLYCON_MEM_BASE,
 	__end_of_fixed_addresses
diff --git a/arch/nds32/include/asm/highmem.h b/arch/nds32/include/asm/highmem.h
index fe986d0e6e3f..16159a8716f2 100644
--- a/arch/nds32/include/asm/highmem.h
+++ b/arch/nds32/include/asm/highmem.h
@@ -5,7 +5,6 @@
 #define _ASM_HIGHMEM_H
 
 #include <asm/proc-fns.h>
-#include <asm/kmap_types.h>
 #include <asm/fixmap.h>
 
 /*
@@ -45,11 +44,22 @@ extern pte_t *pkmap_page_table;
 extern void kmap_init(void);
 
 /*
- * The following functions are already defined by <linux/highmem.h>
- * when CONFIG_HIGHMEM is not set.
+ * FIXME: The below looks broken vs. a kmap_atomic() in task context which
+ * is interupted and another kmap_atomic() happens in interrupt context.
+ * But what do I know about nds32. -- tglx
  */
-#ifdef CONFIG_HIGHMEM
-extern void *kmap_atomic_pfn(unsigned long pfn);
-#endif
+#define arch_kmap_local_post_map(vaddr, pteval)			\
+	do {							\
+		__nds32__tlbop_inv(vaddr);			\
+		__nds32__mtsr_dsb(vaddr, NDS32_SR_TLB_VPN);	\
+		__nds32__tlbop_rwr(pteval);			\
+		__nds32__isb();					\
+	} while (0)
+
+#define arch_kmap_local_pre_unmap(vaddr)			\
+	do {							\
+		__nds32__tlbop_inv(vaddr);			\
+		__nds32__isb();					\
+	} while (0)
 
 #endif
diff --git a/arch/nds32/mm/Makefile b/arch/nds32/mm/Makefile
index 897ecaf5cf54..14fb2e8eb036 100644
--- a/arch/nds32/mm/Makefile
+++ b/arch/nds32/mm/Makefile
@@ -3,7 +3,6 @@ obj-y				:= extable.o tlb.o fault.o init.o mmap.o \
                                    mm-nds32.o cacheflush.o proc.o
 
 obj-$(CONFIG_ALIGNMENT_TRAP)	+= alignment.o
-obj-$(CONFIG_HIGHMEM)           += highmem.o
 
 ifdef CONFIG_FUNCTION_TRACER
 CFLAGS_REMOVE_proc.o     = $(CC_FLAGS_FTRACE)
diff --git a/arch/nds32/mm/highmem.c b/arch/nds32/mm/highmem.c
deleted file mode 100644
index 4284cd59e21a..000000000000
--- a/arch/nds32/mm/highmem.c
+++ /dev/null
@@ -1,48 +0,0 @@
-// SPDX-License-Identifier: GPL-2.0
-// Copyright (C) 2005-2017 Andes Technology Corporation
-
-#include <linux/export.h>
-#include <linux/highmem.h>
-#include <linux/sched.h>
-#include <linux/smp.h>
-#include <linux/interrupt.h>
-#include <linux/memblock.h>
-#include <asm/fixmap.h>
-#include <asm/tlbflush.h>
-
-void *kmap_atomic_high_prot(struct page *page, pgprot_t prot)
-{
-	unsigned int idx;
-	unsigned long vaddr, pte;
-	int type;
-	pte_t *ptep;
-
-	type = kmap_atomic_idx_push();
-
-	idx = type + KM_TYPE_NR * smp_processor_id();
-	vaddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);
-	pte = (page_to_pfn(page) << PAGE_SHIFT) | prot;
-	ptep = pte_offset_kernel(pmd_off_k(vaddr), vaddr);
-	set_pte(ptep, pte);
-
-	__nds32__tlbop_inv(vaddr);
-	__nds32__mtsr_dsb(vaddr, NDS32_SR_TLB_VPN);
-	__nds32__tlbop_rwr(pte);
-	__nds32__isb();
-	return (void *)vaddr;
-}
-EXPORT_SYMBOL(kmap_atomic_high_prot);
-
-void kunmap_atomic_high(void *kvaddr)
-{
-	if (kvaddr >= (void *)FIXADDR_START) {
-		unsigned long vaddr = (unsigned long)kvaddr;
-		pte_t *ptep;
-		kmap_atomic_idx_pop();
-		__nds32__tlbop_inv(vaddr);
-		__nds32__isb();
-		ptep = pte_offset_kernel(pmd_off_k(vaddr), vaddr);
-		set_pte(ptep, 0);
-	}
-}
-EXPORT_SYMBOL(kunmap_atomic_high);
diff --git a/arch/openrisc/mm/init.c b/arch/openrisc/mm/init.c
index 8348feaaf46e..bf9b2310fc93 100644
--- a/arch/openrisc/mm/init.c
+++ b/arch/openrisc/mm/init.c
@@ -33,7 +33,6 @@
 #include <asm/io.h>
 #include <asm/tlb.h>
 #include <asm/mmu_context.h>
-#include <asm/kmap_types.h>
 #include <asm/fixmap.h>
 #include <asm/tlbflush.h>
 #include <asm/sections.h>
diff --git a/arch/openrisc/mm/ioremap.c b/arch/openrisc/mm/ioremap.c
index a978590d802d..5aed97a18bac 100644
--- a/arch/openrisc/mm/ioremap.c
+++ b/arch/openrisc/mm/ioremap.c
@@ -15,7 +15,6 @@
 #include <linux/io.h>
 #include <linux/pgtable.h>
 #include <asm/pgalloc.h>
-#include <asm/kmap_types.h>
 #include <asm/fixmap.h>
 #include <asm/bug.h>
 #include <linux/sched.h>
diff --git a/arch/parisc/include/asm/hardirq.h b/arch/parisc/include/asm/hardirq.h
index 7f7039516e53..fad29aa6f45f 100644
--- a/arch/parisc/include/asm/hardirq.h
+++ b/arch/parisc/include/asm/hardirq.h
@@ -32,7 +32,6 @@ typedef struct {
 DECLARE_PER_CPU_SHARED_ALIGNED(irq_cpustat_t, irq_stat);
 
 #define __ARCH_IRQ_STAT
-#define __IRQ_STAT(cpu, member) (irq_stat[cpu].member)
 #define inc_irq_stat(member)	this_cpu_inc(irq_stat.member)
 #define __inc_irq_stat(member)	__this_cpu_inc(irq_stat.member)
 #define ack_bad_irq(irq) WARN(1, "unexpected IRQ trap at vector %02x\n", irq)
diff --git a/arch/parisc/include/asm/kmap_types.h b/arch/parisc/include/asm/kmap_types.h
deleted file mode 100644
index 3e70b5cd1123..000000000000
--- a/arch/parisc/include/asm/kmap_types.h
+++ /dev/null
@@ -1,13 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0 */
-#ifndef _ASM_KMAP_TYPES_H
-#define _ASM_KMAP_TYPES_H
-
-#ifdef CONFIG_DEBUG_HIGHMEM
-#define  __WITH_KM_FENCE
-#endif
-
-#include <asm-generic/kmap_types.h>
-
-#undef __WITH_KM_FENCE
-
-#endif
diff --git a/arch/powerpc/Kconfig b/arch/powerpc/Kconfig
index 31ed8083571f..71529672b738 100644
--- a/arch/powerpc/Kconfig
+++ b/arch/powerpc/Kconfig
@@ -146,6 +146,7 @@ config PPC
 	select ARCH_MIGHT_HAVE_PC_SERIO
 	select ARCH_OPTIONAL_KERNEL_RWX		if ARCH_HAS_STRICT_KERNEL_RWX
 	select ARCH_SUPPORTS_ATOMIC_RMW
+	select ARCH_SUPPORTS_RT if HAVE_POSIX_CPU_TIMERS_TASK_WORK
 	select ARCH_USE_BUILTIN_BSWAP
 	select ARCH_USE_CMPXCHG_LOCKREF		if PPC64
 	select ARCH_USE_QUEUED_RWLOCKS		if PPC_QUEUED_SPINLOCKS
@@ -230,6 +231,7 @@ config PPC
 	select HAVE_HARDLOCKUP_DETECTOR_PERF	if PERF_EVENTS && HAVE_PERF_EVENTS_NMI && !HAVE_HARDLOCKUP_DETECTOR_ARCH
 	select HAVE_PERF_REGS
 	select HAVE_PERF_USER_STACK_DUMP
+	select HAVE_PREEMPT_LAZY
 	select MMU_GATHER_RCU_TABLE_FREE
 	select MMU_GATHER_PAGE_SIZE
 	select HAVE_REGS_AND_STACK_ACCESS_API
@@ -237,6 +239,7 @@ config PPC
 	select HAVE_SYSCALL_TRACEPOINTS
 	select HAVE_VIRT_CPU_ACCOUNTING
 	select HAVE_IRQ_TIME_ACCOUNTING
+	select HAVE_POSIX_CPU_TIMERS_TASK_WORK  if !KVM
 	select HAVE_RSEQ
 	select IOMMU_HELPER			if PPC64
 	select IRQ_DOMAIN
@@ -410,6 +413,7 @@ menu "Kernel options"
 config HIGHMEM
 	bool "High memory support"
 	depends on PPC32
+	select KMAP_LOCAL
 
 source "kernel/Kconfig.hz"
 
diff --git a/arch/powerpc/include/asm/cmpxchg.h b/arch/powerpc/include/asm/cmpxchg.h
index cf091c4c22e5..7371f7e23c35 100644
--- a/arch/powerpc/include/asm/cmpxchg.h
+++ b/arch/powerpc/include/asm/cmpxchg.h
@@ -5,7 +5,7 @@
 #ifdef __KERNEL__
 #include <linux/compiler.h>
 #include <asm/synch.h>
-#include <linux/bug.h>
+#include <linux/bits.h>
 
 #ifdef __BIG_ENDIAN
 #define BITOFF_CAL(size, off)	((sizeof(u32) - size - off) * BITS_PER_BYTE)
diff --git a/arch/powerpc/include/asm/fixmap.h b/arch/powerpc/include/asm/fixmap.h
index 6bfc87915d5d..8d03c16a3663 100644
--- a/arch/powerpc/include/asm/fixmap.h
+++ b/arch/powerpc/include/asm/fixmap.h
@@ -20,7 +20,7 @@
 #include <asm/page.h>
 #ifdef CONFIG_HIGHMEM
 #include <linux/threads.h>
-#include <asm/kmap_types.h>
+#include <asm/kmap_size.h>
 #endif
 
 #ifdef CONFIG_KASAN
@@ -55,7 +55,7 @@ enum fixed_addresses {
 	FIX_EARLY_DEBUG_BASE = FIX_EARLY_DEBUG_TOP+(ALIGN(SZ_128K, PAGE_SIZE)/PAGE_SIZE)-1,
 #ifdef CONFIG_HIGHMEM
 	FIX_KMAP_BEGIN,	/* reserved pte's for temporary kernel mappings */
-	FIX_KMAP_END = FIX_KMAP_BEGIN+(KM_TYPE_NR*NR_CPUS)-1,
+	FIX_KMAP_END = FIX_KMAP_BEGIN + (KM_MAX_IDX * NR_CPUS) - 1,
 #endif
 #ifdef CONFIG_PPC_8xx
 	/* For IMMR we need an aligned 512K area */
diff --git a/arch/powerpc/include/asm/highmem.h b/arch/powerpc/include/asm/highmem.h
index 104026f7d6bc..80a5ae771c65 100644
--- a/arch/powerpc/include/asm/highmem.h
+++ b/arch/powerpc/include/asm/highmem.h
@@ -24,12 +24,10 @@
 #ifdef __KERNEL__
 
 #include <linux/interrupt.h>
-#include <asm/kmap_types.h>
 #include <asm/cacheflush.h>
 #include <asm/page.h>
 #include <asm/fixmap.h>
 
-extern pte_t *kmap_pte;
 extern pte_t *pkmap_page_table;
 
 /*
@@ -60,6 +58,11 @@ extern pte_t *pkmap_page_table;
 
 #define flush_cache_kmaps()	flush_cache_all()
 
+#define arch_kmap_local_post_map(vaddr, pteval)	\
+	local_flush_tlb_page(NULL, vaddr)
+#define arch_kmap_local_post_unmap(vaddr)	\
+	local_flush_tlb_page(NULL, vaddr)
+
 #endif /* __KERNEL__ */
 
 #endif /* _ASM_HIGHMEM_H */
diff --git a/arch/powerpc/include/asm/kmap_types.h b/arch/powerpc/include/asm/kmap_types.h
deleted file mode 100644
index c8fa182d48c8..000000000000
--- a/arch/powerpc/include/asm/kmap_types.h
+++ /dev/null
@@ -1,13 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0-or-later */
-#ifndef _ASM_POWERPC_KMAP_TYPES_H
-#define _ASM_POWERPC_KMAP_TYPES_H
-
-#ifdef __KERNEL__
-
-/*
- */
-
-#define KM_TYPE_NR 16
-
-#endif	/* __KERNEL__ */
-#endif	/* _ASM_POWERPC_KMAP_TYPES_H */
diff --git a/arch/powerpc/include/asm/simple_spinlock_types.h b/arch/powerpc/include/asm/simple_spinlock_types.h
index 0f3cdd8faa95..d45561e9e6ba 100644
--- a/arch/powerpc/include/asm/simple_spinlock_types.h
+++ b/arch/powerpc/include/asm/simple_spinlock_types.h
@@ -2,7 +2,7 @@
 #ifndef _ASM_POWERPC_SIMPLE_SPINLOCK_TYPES_H
 #define _ASM_POWERPC_SIMPLE_SPINLOCK_TYPES_H
 
-#ifndef __LINUX_SPINLOCK_TYPES_H
+#if !defined(__LINUX_SPINLOCK_TYPES_H) && !defined(__LINUX_RT_MUTEX_H)
 # error "please don't include this file directly"
 #endif
 
diff --git a/arch/powerpc/include/asm/spinlock_types.h b/arch/powerpc/include/asm/spinlock_types.h
index c5d742f18021..cc6922a011ba 100644
--- a/arch/powerpc/include/asm/spinlock_types.h
+++ b/arch/powerpc/include/asm/spinlock_types.h
@@ -2,10 +2,6 @@
 #ifndef _ASM_POWERPC_SPINLOCK_TYPES_H
 #define _ASM_POWERPC_SPINLOCK_TYPES_H
 
-#ifndef __LINUX_SPINLOCK_TYPES_H
-# error "please don't include this file directly"
-#endif
-
 #ifdef CONFIG_PPC_QUEUED_SPINLOCKS
 #include <asm-generic/qspinlock_types.h>
 #include <asm-generic/qrwlock_types.h>
diff --git a/arch/powerpc/include/asm/stackprotector.h b/arch/powerpc/include/asm/stackprotector.h
index 1c8460e23583..b1653c160bab 100644
--- a/arch/powerpc/include/asm/stackprotector.h
+++ b/arch/powerpc/include/asm/stackprotector.h
@@ -24,7 +24,11 @@ static __always_inline void boot_init_stack_canary(void)
 	unsigned long canary;
 
 	/* Try to get a semi random initial value. */
+#ifdef CONFIG_PREEMPT_RT
+	canary = (unsigned long)&canary;
+#else
 	canary = get_random_canary();
+#endif
 	canary ^= mftb();
 	canary ^= LINUX_VERSION_CODE;
 	canary &= CANARY_MASK;
diff --git a/arch/powerpc/include/asm/thread_info.h b/arch/powerpc/include/asm/thread_info.h
index 46a210b03d2b..0e316b44b2d7 100644
--- a/arch/powerpc/include/asm/thread_info.h
+++ b/arch/powerpc/include/asm/thread_info.h
@@ -48,6 +48,8 @@
 struct thread_info {
 	int		preempt_count;		/* 0 => preemptable,
 						   <0 => BUG */
+	int             preempt_lazy_count;	/* 0 => preemptable,
+						   <0 => BUG */
 	unsigned long	local_flags;		/* private flags for thread */
 #ifdef CONFIG_LIVEPATCH
 	unsigned long *livepatch_sp;
@@ -97,11 +99,12 @@ void arch_setup_new_exec(void);
 #define TIF_SINGLESTEP		8	/* singlestepping active */
 #define TIF_NOHZ		9	/* in adaptive nohz mode */
 #define TIF_SECCOMP		10	/* secure computing */
-#define TIF_RESTOREALL		11	/* Restore all regs (implies NOERROR) */
-#define TIF_NOERROR		12	/* Force successful syscall return */
+
+#define TIF_NEED_RESCHED_LAZY	11	/* lazy rescheduling necessary */
+#define TIF_SYSCALL_TRACEPOINT	12	/* syscall tracepoint instrumentation */
+
 #define TIF_NOTIFY_RESUME	13	/* callback before returning to user */
 #define TIF_UPROBE		14	/* breakpointed or single-stepping */
-#define TIF_SYSCALL_TRACEPOINT	15	/* syscall tracepoint instrumentation */
 #define TIF_EMULATE_STACK_STORE	16	/* Is an instruction emulation
 						for stack store? */
 #define TIF_MEMDIE		17	/* is terminating due to OOM killer */
@@ -110,6 +113,9 @@ void arch_setup_new_exec(void);
 #endif
 #define TIF_POLLING_NRFLAG	19	/* true if poll_idle() is polling TIF_NEED_RESCHED */
 #define TIF_32BIT		20	/* 32 bit binary */
+#define TIF_RESTOREALL		21	/* Restore all regs (implies NOERROR) */
+#define TIF_NOERROR		22	/* Force successful syscall return */
+
 
 /* as above, but as bit values */
 #define _TIF_SYSCALL_TRACE	(1<<TIF_SYSCALL_TRACE)
@@ -129,6 +135,7 @@ void arch_setup_new_exec(void);
 #define _TIF_SYSCALL_TRACEPOINT	(1<<TIF_SYSCALL_TRACEPOINT)
 #define _TIF_EMULATE_STACK_STORE	(1<<TIF_EMULATE_STACK_STORE)
 #define _TIF_NOHZ		(1<<TIF_NOHZ)
+#define _TIF_NEED_RESCHED_LAZY	(1<<TIF_NEED_RESCHED_LAZY)
 #define _TIF_SYSCALL_EMU	(1<<TIF_SYSCALL_EMU)
 #define _TIF_SYSCALL_DOTRACE	(_TIF_SYSCALL_TRACE | _TIF_SYSCALL_AUDIT | \
 				 _TIF_SECCOMP | _TIF_SYSCALL_TRACEPOINT | \
@@ -136,8 +143,10 @@ void arch_setup_new_exec(void);
 
 #define _TIF_USER_WORK_MASK	(_TIF_SIGPENDING | _TIF_NEED_RESCHED | \
 				 _TIF_NOTIFY_RESUME | _TIF_UPROBE | \
-				 _TIF_RESTORE_TM | _TIF_PATCH_PENDING)
+				 _TIF_RESTORE_TM | _TIF_PATCH_PENDING | \
+				 _TIF_NEED_RESCHED_LAZY)
 #define _TIF_PERSYSCALL_MASK	(_TIF_RESTOREALL|_TIF_NOERROR)
+#define _TIF_NEED_RESCHED_MASK	(_TIF_NEED_RESCHED | _TIF_NEED_RESCHED_LAZY)
 
 /* Bits in local_flags */
 /* Don't move TLF_NAPPING without adjusting the code in entry_32.S */
diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index c2722ff36e98..fd5b0fbdf62c 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -189,6 +189,7 @@ int main(void)
 	OFFSET(TI_FLAGS, thread_info, flags);
 	OFFSET(TI_LOCAL_FLAGS, thread_info, local_flags);
 	OFFSET(TI_PREEMPT, thread_info, preempt_count);
+	OFFSET(TI_PREEMPT_LAZY, thread_info, preempt_lazy_count);
 
 #ifdef CONFIG_PPC64
 	OFFSET(DCACHEL1BLOCKSIZE, ppc64_caches, l1d.block_size);
diff --git a/arch/powerpc/kernel/entry_32.S b/arch/powerpc/kernel/entry_32.S
index 459f5d00b990..fc9517a97640 100644
--- a/arch/powerpc/kernel/entry_32.S
+++ b/arch/powerpc/kernel/entry_32.S
@@ -414,7 +414,9 @@ ret_from_syscall:
 	mtmsr	r10
 	lwz	r9,TI_FLAGS(r2)
 	li	r8,-MAX_ERRNO
-	andi.	r0,r9,(_TIF_SYSCALL_DOTRACE|_TIF_SINGLESTEP|_TIF_USER_WORK_MASK|_TIF_PERSYSCALL_MASK)
+	lis	r0,(_TIF_SYSCALL_DOTRACE|_TIF_SINGLESTEP|_TIF_USER_WORK_MASK|_TIF_PERSYSCALL_MASK)@h
+	ori	r0,r0, (_TIF_SYSCALL_DOTRACE|_TIF_SINGLESTEP|_TIF_USER_WORK_MASK|_TIF_PERSYSCALL_MASK)@l
+	and.	r0,r9,r0
 	bne-	syscall_exit_work
 	cmplw	0,r3,r8
 	blt+	syscall_exit_cont
@@ -530,13 +532,13 @@ syscall_dotrace:
 	b	syscall_dotrace_cont
 
 syscall_exit_work:
-	andi.	r0,r9,_TIF_RESTOREALL
+	andis.	r0,r9,_TIF_RESTOREALL@h
 	beq+	0f
 	REST_NVGPRS(r1)
 	b	2f
 0:	cmplw	0,r3,r8
 	blt+	1f
-	andi.	r0,r9,_TIF_NOERROR
+	andis.	r0,r9,_TIF_NOERROR@h
 	bne-	1f
 	lwz	r11,_CCR(r1)			/* Load CR */
 	neg	r3,r3
@@ -545,12 +547,12 @@ syscall_exit_work:
 
 1:	stw	r6,RESULT(r1)	/* Save result */
 	stw	r3,GPR3(r1)	/* Update return value */
-2:	andi.	r0,r9,(_TIF_PERSYSCALL_MASK)
+2:	andis.	r0,r9,(_TIF_PERSYSCALL_MASK)@h
 	beq	4f
 
 	/* Clear per-syscall TIF flags if any are set.  */
 
-	li	r11,_TIF_PERSYSCALL_MASK
+	lis	r11,(_TIF_PERSYSCALL_MASK)@h
 	addi	r12,r2,TI_FLAGS
 3:	lwarx	r8,0,r12
 	andc	r8,r8,r11
@@ -927,7 +929,14 @@ resume_kernel:
 	cmpwi	0,r0,0		/* if non-zero, just restore regs and return */
 	bne	restore_kuap
 	andi.	r8,r8,_TIF_NEED_RESCHED
+	bne+	1f
+	lwz	r0,TI_PREEMPT_LAZY(r2)
+	cmpwi	0,r0,0          /* if non-zero, just restore regs and return */
+	bne	restore_kuap
+	lwz	r0,TI_FLAGS(r2)
+	andi.	r0,r0,_TIF_NEED_RESCHED_LAZY
 	beq+	restore_kuap
+1:
 	lwz	r3,_MSR(r1)
 	andi.	r0,r3,MSR_EE	/* interrupts off? */
 	beq	restore_kuap	/* don't schedule if so */
@@ -1248,7 +1257,7 @@ global_dbcr0:
 #endif /* !(CONFIG_4xx || CONFIG_BOOKE) */
 
 do_work:			/* r10 contains MSR_KERNEL here */
-	andi.	r0,r9,_TIF_NEED_RESCHED
+	andi.	r0,r9,_TIF_NEED_RESCHED_MASK
 	beq	do_user_signal
 
 do_resched:			/* r10 contains MSR_KERNEL here */
@@ -1267,7 +1276,7 @@ recheck:
 	LOAD_REG_IMMEDIATE(r10,MSR_KERNEL)
 	mtmsr	r10		/* disable interrupts */
 	lwz	r9,TI_FLAGS(r2)
-	andi.	r0,r9,_TIF_NEED_RESCHED
+	andi.	r0,r9,_TIF_NEED_RESCHED_MASK
 	bne-	do_resched
 	andi.	r0,r9,_TIF_USER_WORK_MASK
 	beq	restore_user
diff --git a/arch/powerpc/kernel/exceptions-64e.S b/arch/powerpc/kernel/exceptions-64e.S
index f579ce46eef2..715ff292a8f8 100644
--- a/arch/powerpc/kernel/exceptions-64e.S
+++ b/arch/powerpc/kernel/exceptions-64e.S
@@ -1080,7 +1080,7 @@ _GLOBAL(ret_from_except_lite)
 	li	r10, -1
 	mtspr	SPRN_DBSR,r10
 	b	restore
-1:	andi.	r0,r4,_TIF_NEED_RESCHED
+1:	andi.	r0,r4,_TIF_NEED_RESCHED_MASK
 	beq	2f
 	bl	restore_interrupts
 	SCHEDULE_USER
@@ -1132,12 +1132,20 @@ resume_kernel:
 	bne-	0b
 1:
 
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPTION
 	/* Check if we need to preempt */
+	lwz	r8,TI_PREEMPT(r9)
+	cmpwi	0,r8,0		/* if non-zero, just restore regs and return */
+	bne	restore
 	andi.	r0,r4,_TIF_NEED_RESCHED
+	bne+	check_count
+
+	andi.	r0,r4,_TIF_NEED_RESCHED_LAZY
 	beq+	restore
+	lwz	r8,TI_PREEMPT_LAZY(r9)
+
 	/* Check that preempt_count() == 0 and interrupts are enabled */
-	lwz	r8,TI_PREEMPT(r9)
+check_count:
 	cmpwi	cr0,r8,0
 	bne	restore
 	ld	r0,SOFTE(r1)
@@ -1158,7 +1166,7 @@ resume_kernel:
 	 * interrupted after loading SRR0/1.
 	 */
 	wrteei	0
-#endif /* CONFIG_PREEMPT */
+#endif /* CONFIG_PREEMPTION */
 
 restore:
 	/*
diff --git a/arch/powerpc/kernel/irq.c b/arch/powerpc/kernel/irq.c
index e8a548447dd6..5ad4f27cba10 100644
--- a/arch/powerpc/kernel/irq.c
+++ b/arch/powerpc/kernel/irq.c
@@ -753,10 +753,12 @@ void *mcheckirq_ctx[NR_CPUS] __read_mostly;
 void *softirq_ctx[NR_CPUS] __read_mostly;
 void *hardirq_ctx[NR_CPUS] __read_mostly;
 
+#ifndef CONFIG_PREEMPT_RT
 void do_softirq_own_stack(void)
 {
 	call_do_softirq(softirq_ctx[smp_processor_id()]);
 }
+#endif
 
 irq_hw_number_t virq_to_hw(unsigned int virq)
 {
diff --git a/arch/powerpc/kernel/misc_32.S b/arch/powerpc/kernel/misc_32.S
index 717e658b90fd..08ee95ad6593 100644
--- a/arch/powerpc/kernel/misc_32.S
+++ b/arch/powerpc/kernel/misc_32.S
@@ -31,6 +31,7 @@
  * We store the saved ksp_limit in the unused part
  * of the STACK_FRAME_OVERHEAD
  */
+#ifndef CONFIG_PREEMPT_RT
 _GLOBAL(call_do_softirq)
 	mflr	r0
 	stw	r0,4(r1)
@@ -46,6 +47,7 @@ _GLOBAL(call_do_softirq)
 	stw	r10,THREAD+KSP_LIMIT(r2)
 	mtlr	r0
 	blr
+#endif
 
 /*
  * void call_do_irq(struct pt_regs *regs, void *sp);
diff --git a/arch/powerpc/kernel/misc_64.S b/arch/powerpc/kernel/misc_64.S
index 070465825c21..a6b33f7b3264 100644
--- a/arch/powerpc/kernel/misc_64.S
+++ b/arch/powerpc/kernel/misc_64.S
@@ -27,6 +27,7 @@
 
 	.text
 
+#ifndef CONFIG_PREEMPT_RT
 _GLOBAL(call_do_softirq)
 	mflr	r0
 	std	r0,16(r1)
@@ -37,6 +38,7 @@ _GLOBAL(call_do_softirq)
 	ld	r0,16(r1)
 	mtlr	r0
 	blr
+#endif
 
 _GLOBAL(call_do_irq)
 	mflr	r0
diff --git a/arch/powerpc/kernel/nvram_64.c b/arch/powerpc/kernel/nvram_64.c
index 532f22637783..1ef55f4b389a 100644
--- a/arch/powerpc/kernel/nvram_64.c
+++ b/arch/powerpc/kernel/nvram_64.c
@@ -73,7 +73,8 @@ static const char *nvram_os_partitions[] = {
 };
 
 static void oops_to_nvram(struct kmsg_dumper *dumper,
-			  enum kmsg_dump_reason reason);
+			  enum kmsg_dump_reason reason,
+			  struct kmsg_dumper_iter *iter);
 
 static struct kmsg_dumper nvram_kmsg_dumper = {
 	.dump = oops_to_nvram
@@ -643,7 +644,8 @@ void __init nvram_init_oops_partition(int rtas_partition_exists)
  * partition.  If that's too much, go back and capture uncompressed text.
  */
 static void oops_to_nvram(struct kmsg_dumper *dumper,
-			  enum kmsg_dump_reason reason)
+			  enum kmsg_dump_reason reason,
+			  struct kmsg_dumper_iter *iter)
 {
 	struct oops_log_info *oops_hdr = (struct oops_log_info *)oops_buf;
 	static unsigned int oops_count = 0;
@@ -681,13 +683,13 @@ static void oops_to_nvram(struct kmsg_dumper *dumper,
 		return;
 
 	if (big_oops_buf) {
-		kmsg_dump_get_buffer(dumper, false,
+		kmsg_dump_get_buffer(iter, false,
 				     big_oops_buf, big_oops_buf_sz, &text_len);
 		rc = zip_oops(text_len);
 	}
 	if (rc != 0) {
-		kmsg_dump_rewind(dumper);
-		kmsg_dump_get_buffer(dumper, false,
+		kmsg_dump_rewind(iter);
+		kmsg_dump_get_buffer(iter, false,
 				     oops_data, oops_data_sz, &text_len);
 		err_type = ERR_TYPE_KERNEL_PANIC;
 		oops_hdr->version = cpu_to_be16(OOPS_HDR_VERSION);
diff --git a/arch/powerpc/kernel/syscall_64.c b/arch/powerpc/kernel/syscall_64.c
index 310bcd768cd5..ae3212dcf562 100644
--- a/arch/powerpc/kernel/syscall_64.c
+++ b/arch/powerpc/kernel/syscall_64.c
@@ -193,7 +193,7 @@ notrace unsigned long syscall_exit_prepare(unsigned long r3,
 	ti_flags = READ_ONCE(*ti_flagsp);
 	while (unlikely(ti_flags & (_TIF_USER_WORK_MASK & ~_TIF_RESTORE_TM))) {
 		local_irq_enable();
-		if (ti_flags & _TIF_NEED_RESCHED) {
+		if (ti_flags & _TIF_NEED_RESCHED_MASK) {
 			schedule();
 		} else {
 			/*
@@ -277,7 +277,7 @@ notrace unsigned long interrupt_exit_user_prepare(struct pt_regs *regs, unsigned
 	ti_flags = READ_ONCE(*ti_flagsp);
 	while (unlikely(ti_flags & (_TIF_USER_WORK_MASK & ~_TIF_RESTORE_TM))) {
 		local_irq_enable(); /* returning to user: may enable */
-		if (ti_flags & _TIF_NEED_RESCHED) {
+		if (ti_flags & _TIF_NEED_RESCHED_MASK) {
 			schedule();
 		} else {
 			if (ti_flags & _TIF_SIGPENDING)
@@ -361,11 +361,15 @@ notrace unsigned long interrupt_exit_kernel_prepare(struct pt_regs *regs, unsign
 		/* Returning to a kernel context with local irqs enabled. */
 		WARN_ON_ONCE(!(regs->msr & MSR_EE));
 again:
-		if (IS_ENABLED(CONFIG_PREEMPT)) {
+		if (IS_ENABLED(CONFIG_PREEMPTION)) {
 			/* Return to preemptible kernel context */
 			if (unlikely(*ti_flagsp & _TIF_NEED_RESCHED)) {
 				if (preempt_count() == 0)
 					preempt_schedule_irq();
+			} else if (unlikely(*ti_flagsp & _TIF_NEED_RESCHED_LAZY)) {
+				if ((preempt_count() == 0) &&
+				    (current_thread_info()->preempt_lazy_count == 0))
+					preempt_schedule_irq();
 			}
 		}
 
diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 1d20f0f77a92..7e0a497a36ee 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -312,12 +312,11 @@ static unsigned long vtime_delta_scaled(struct cpu_accounting_data *acct,
 	return stime_scaled;
 }
 
-static unsigned long vtime_delta(struct task_struct *tsk,
+static unsigned long vtime_delta(struct cpu_accounting_data *acct,
 				 unsigned long *stime_scaled,
 				 unsigned long *steal_time)
 {
 	unsigned long now, stime;
-	struct cpu_accounting_data *acct = get_accounting(tsk);
 
 	WARN_ON_ONCE(!irqs_disabled());
 
@@ -332,29 +331,30 @@ static unsigned long vtime_delta(struct task_struct *tsk,
 	return stime;
 }
 
+static void vtime_delta_kernel(struct cpu_accounting_data *acct,
+			       unsigned long *stime, unsigned long *stime_scaled)
+{
+	unsigned long steal_time;
+
+	*stime = vtime_delta(acct, stime_scaled, &steal_time);
+	*stime -= min(*stime, steal_time);
+	acct->steal_time += steal_time;
+}
+
 void vtime_account_kernel(struct task_struct *tsk)
 {
-	unsigned long stime, stime_scaled, steal_time;
 	struct cpu_accounting_data *acct = get_accounting(tsk);
+	unsigned long stime, stime_scaled;
 
-	stime = vtime_delta(tsk, &stime_scaled, &steal_time);
-
-	stime -= min(stime, steal_time);
-	acct->steal_time += steal_time;
+	vtime_delta_kernel(acct, &stime, &stime_scaled);
 
-	if ((tsk->flags & PF_VCPU) && !irq_count()) {
+	if (tsk->flags & PF_VCPU) {
 		acct->gtime += stime;
 #ifdef CONFIG_ARCH_HAS_SCALED_CPUTIME
 		acct->utime_scaled += stime_scaled;
 #endif
 	} else {
-		if (hardirq_count())
-			acct->hardirq_time += stime;
-		else if (in_serving_softirq())
-			acct->softirq_time += stime;
-		else
-			acct->stime += stime;
-
+		acct->stime += stime;
 #ifdef CONFIG_ARCH_HAS_SCALED_CPUTIME
 		acct->stime_scaled += stime_scaled;
 #endif
@@ -367,10 +367,34 @@ void vtime_account_idle(struct task_struct *tsk)
 	unsigned long stime, stime_scaled, steal_time;
 	struct cpu_accounting_data *acct = get_accounting(tsk);
 
-	stime = vtime_delta(tsk, &stime_scaled, &steal_time);
+	stime = vtime_delta(acct, &stime_scaled, &steal_time);
 	acct->idle_time += stime + steal_time;
 }
 
+static void vtime_account_irq_field(struct cpu_accounting_data *acct,
+				    unsigned long *field)
+{
+	unsigned long stime, stime_scaled;
+
+	vtime_delta_kernel(acct, &stime, &stime_scaled);
+	*field += stime;
+#ifdef CONFIG_ARCH_HAS_SCALED_CPUTIME
+	acct->stime_scaled += stime_scaled;
+#endif
+}
+
+void vtime_account_softirq(struct task_struct *tsk)
+{
+	struct cpu_accounting_data *acct = get_accounting(tsk);
+	vtime_account_irq_field(acct, &acct->softirq_time);
+}
+
+void vtime_account_hardirq(struct task_struct *tsk)
+{
+	struct cpu_accounting_data *acct = get_accounting(tsk);
+	vtime_account_irq_field(acct, &acct->hardirq_time);
+}
+
 static void vtime_flush_scaled(struct task_struct *tsk,
 			       struct cpu_accounting_data *acct)
 {
diff --git a/arch/powerpc/kernel/traps.c b/arch/powerpc/kernel/traps.c
index 5006dcbe1d9f..13bf1b0e78b2 100644
--- a/arch/powerpc/kernel/traps.c
+++ b/arch/powerpc/kernel/traps.c
@@ -170,7 +170,6 @@ extern void panic_flush_kmsg_start(void)
 
 extern void panic_flush_kmsg_end(void)
 {
-	printk_safe_flush_on_panic();
 	kmsg_dump(KMSG_DUMP_PANIC);
 	bust_spinlocks(0);
 	debug_locks_off();
@@ -260,12 +259,17 @@ static char *get_mmu_str(void)
 
 static int __die(const char *str, struct pt_regs *regs, long err)
 {
+	const char *pr = "";
+
 	printk("Oops: %s, sig: %ld [#%d]\n", str, err, ++die_counter);
 
+	if (IS_ENABLED(CONFIG_PREEMPTION))
+		pr = IS_ENABLED(CONFIG_PREEMPT_RT) ? " PREEMPT_RT" : " PREEMPT";
+
 	printk("%s PAGE_SIZE=%luK%s%s%s%s%s%s %s\n",
 	       IS_ENABLED(CONFIG_CPU_LITTLE_ENDIAN) ? "LE" : "BE",
 	       PAGE_SIZE / 1024, get_mmu_str(),
-	       IS_ENABLED(CONFIG_PREEMPT) ? " PREEMPT" : "",
+	       pr,
 	       IS_ENABLED(CONFIG_SMP) ? " SMP" : "",
 	       IS_ENABLED(CONFIG_SMP) ? (" NR_CPUS=" __stringify(NR_CPUS)) : "",
 	       debug_pagealloc_enabled() ? " DEBUG_PAGEALLOC" : "",
diff --git a/arch/powerpc/kernel/watchdog.c b/arch/powerpc/kernel/watchdog.c
index af3c15a1d41e..8ae46c5945d0 100644
--- a/arch/powerpc/kernel/watchdog.c
+++ b/arch/powerpc/kernel/watchdog.c
@@ -181,11 +181,6 @@ static void watchdog_smp_panic(int cpu, u64 tb)
 
 	wd_smp_unlock(&flags);
 
-	printk_safe_flush();
-	/*
-	 * printk_safe_flush() seems to require another print
-	 * before anything actually goes out to console.
-	 */
 	if (sysctl_hardlockup_all_cpu_backtrace)
 		trigger_allbutself_cpu_backtrace();
 
diff --git a/arch/powerpc/kexec/crash.c b/arch/powerpc/kexec/crash.c
index c9a889880214..d488311efab1 100644
--- a/arch/powerpc/kexec/crash.c
+++ b/arch/powerpc/kexec/crash.c
@@ -311,9 +311,6 @@ void default_machine_crash_shutdown(struct pt_regs *regs)
 	unsigned int i;
 	int (*old_handler)(struct pt_regs *regs);
 
-	/* Avoid hardlocking with irresponsive CPU holding logbuf_lock */
-	printk_nmi_enter();
-
 	/*
 	 * This function is only called after the system
 	 * has panicked or is otherwise in a critical state.
diff --git a/arch/powerpc/kvm/Kconfig b/arch/powerpc/kvm/Kconfig
index 549591d9aaa2..efb5bfe93f70 100644
--- a/arch/powerpc/kvm/Kconfig
+++ b/arch/powerpc/kvm/Kconfig
@@ -178,6 +178,7 @@ config KVM_E500MC
 config KVM_MPIC
 	bool "KVM in-kernel MPIC emulation"
 	depends on KVM && E500
+	depends on !PREEMPT_RT
 	select HAVE_KVM_IRQCHIP
 	select HAVE_KVM_IRQFD
 	select HAVE_KVM_IRQ_ROUTING
diff --git a/arch/powerpc/mm/Makefile b/arch/powerpc/mm/Makefile
index 55b4a8bd408a..3b4e9e4e25ea 100644
--- a/arch/powerpc/mm/Makefile
+++ b/arch/powerpc/mm/Makefile
@@ -16,7 +16,6 @@ obj-$(CONFIG_NEED_MULTIPLE_NODES) += numa.o
 obj-$(CONFIG_PPC_MM_SLICES)	+= slice.o
 obj-$(CONFIG_HUGETLB_PAGE)	+= hugetlbpage.o
 obj-$(CONFIG_NOT_COHERENT_CACHE) += dma-noncoherent.o
-obj-$(CONFIG_HIGHMEM)		+= highmem.o
 obj-$(CONFIG_PPC_COPRO_BASE)	+= copro_fault.o
 obj-$(CONFIG_PPC_PTDUMP)	+= ptdump/
 obj-$(CONFIG_KASAN)		+= kasan/
diff --git a/arch/powerpc/mm/highmem.c b/arch/powerpc/mm/highmem.c
deleted file mode 100644
index 624b4438aff9..000000000000
--- a/arch/powerpc/mm/highmem.c
+++ /dev/null
@@ -1,67 +0,0 @@
-// SPDX-License-Identifier: GPL-2.0
-/*
- * highmem.c: virtual kernel memory mappings for high memory
- *
- * PowerPC version, stolen from the i386 version.
- *
- * Used in CONFIG_HIGHMEM systems for memory pages which
- * are not addressable by direct kernel virtual addresses.
- *
- * Copyright (C) 1999 Gerhard Wichert, Siemens AG
- *		      Gerhard.Wichert@pdb.siemens.de
- *
- *
- * Redesigned the x86 32-bit VM architecture to deal with
- * up to 16 Terrabyte physical memory. With current x86 CPUs
- * we now support up to 64 Gigabytes physical RAM.
- *
- * Copyright (C) 1999 Ingo Molnar <mingo@redhat.com>
- *
- * Reworked for PowerPC by various contributors. Moved from
- * highmem.h by Benjamin Herrenschmidt (c) 2009 IBM Corp.
- */
-
-#include <linux/highmem.h>
-#include <linux/module.h>
-
-void *kmap_atomic_high_prot(struct page *page, pgprot_t prot)
-{
-	unsigned long vaddr;
-	int idx, type;
-
-	type = kmap_atomic_idx_push();
-	idx = type + KM_TYPE_NR*smp_processor_id();
-	vaddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);
-	WARN_ON(IS_ENABLED(CONFIG_DEBUG_HIGHMEM) && !pte_none(*(kmap_pte - idx)));
-	__set_pte_at(&init_mm, vaddr, kmap_pte-idx, mk_pte(page, prot), 1);
-	local_flush_tlb_page(NULL, vaddr);
-
-	return (void*) vaddr;
-}
-EXPORT_SYMBOL(kmap_atomic_high_prot);
-
-void kunmap_atomic_high(void *kvaddr)
-{
-	unsigned long vaddr = (unsigned long) kvaddr & PAGE_MASK;
-
-	if (vaddr < __fix_to_virt(FIX_KMAP_END))
-		return;
-
-	if (IS_ENABLED(CONFIG_DEBUG_HIGHMEM)) {
-		int type = kmap_atomic_idx();
-		unsigned int idx;
-
-		idx = type + KM_TYPE_NR * smp_processor_id();
-		WARN_ON(vaddr != __fix_to_virt(FIX_KMAP_BEGIN + idx));
-
-		/*
-		 * force other mappings to Oops if they'll try to access
-		 * this pte without first remap it
-		 */
-		pte_clear(&init_mm, vaddr, kmap_pte-idx);
-		local_flush_tlb_page(NULL, vaddr);
-	}
-
-	kmap_atomic_idx_pop();
-}
-EXPORT_SYMBOL(kunmap_atomic_high);
diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 22eb1c718e62..1b74565b3e16 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -62,11 +62,6 @@
 unsigned long long memory_limit;
 bool init_mem_is_free;
 
-#ifdef CONFIG_HIGHMEM
-pte_t *kmap_pte;
-EXPORT_SYMBOL(kmap_pte);
-#endif
-
 pgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn,
 			      unsigned long size, pgprot_t vma_prot)
 {
@@ -236,8 +231,6 @@ void __init paging_init(void)
 
 	map_kernel_page(PKMAP_BASE, 0, __pgprot(0));	/* XXX gross */
 	pkmap_page_table = virt_to_kpte(PKMAP_BASE);
-
-	kmap_pte = virt_to_kpte(__fix_to_virt(FIX_KMAP_BEGIN));
 #endif /* CONFIG_HIGHMEM */
 
 	printk(KERN_DEBUG "Top of RAM: 0x%llx, Total RAM: 0x%llx\n",
diff --git a/arch/powerpc/platforms/powernv/opal-kmsg.c b/arch/powerpc/platforms/powernv/opal-kmsg.c
index 6c3bc4b4da98..ec862846bc82 100644
--- a/arch/powerpc/platforms/powernv/opal-kmsg.c
+++ b/arch/powerpc/platforms/powernv/opal-kmsg.c
@@ -20,7 +20,8 @@
  * message, it just ensures that OPAL completely flushes the console buffer.
  */
 static void kmsg_dump_opal_console_flush(struct kmsg_dumper *dumper,
-				     enum kmsg_dump_reason reason)
+					 enum kmsg_dump_reason reason,
+					 struct kmsg_dumper_iter *iter)
 {
 	/*
 	 * Outside of a panic context the pollers will continue to run,
diff --git a/arch/powerpc/platforms/pseries/iommu.c b/arch/powerpc/platforms/pseries/iommu.c
index e4198700ed1a..62bd38cd80d1 100644
--- a/arch/powerpc/platforms/pseries/iommu.c
+++ b/arch/powerpc/platforms/pseries/iommu.c
@@ -24,6 +24,7 @@
 #include <linux/of.h>
 #include <linux/iommu.h>
 #include <linux/rculist.h>
+#include <linux/local_lock.h>
 #include <asm/io.h>
 #include <asm/prom.h>
 #include <asm/rtas.h>
@@ -190,7 +191,13 @@ static int tce_build_pSeriesLP(unsigned long liobn, long tcenum, long tceshift,
 	return ret;
 }
 
-static DEFINE_PER_CPU(__be64 *, tce_page);
+struct tce_page {
+	__be64 * page;
+	local_lock_t lock;
+};
+static DEFINE_PER_CPU(struct tce_page, tce_page) = {
+	.lock = INIT_LOCAL_LOCK(lock),
+};
 
 static int tce_buildmulti_pSeriesLP(struct iommu_table *tbl, long tcenum,
 				     long npages, unsigned long uaddr,
@@ -212,9 +219,10 @@ static int tce_buildmulti_pSeriesLP(struct iommu_table *tbl, long tcenum,
 		                           direction, attrs);
 	}
 
-	local_irq_save(flags);	/* to protect tcep and the page behind it */
+	/* to protect tcep and the page behind it */
+	local_lock_irqsave(&tce_page.lock, flags);
 
-	tcep = __this_cpu_read(tce_page);
+	tcep = __this_cpu_read(tce_page.page);
 
 	/* This is safe to do since interrupts are off when we're called
 	 * from iommu_alloc{,_sg}()
@@ -223,12 +231,12 @@ static int tce_buildmulti_pSeriesLP(struct iommu_table *tbl, long tcenum,
 		tcep = (__be64 *)__get_free_page(GFP_ATOMIC);
 		/* If allocation fails, fall back to the loop implementation */
 		if (!tcep) {
-			local_irq_restore(flags);
+			local_unlock_irqrestore(&tce_page.lock, flags);
 			return tce_build_pSeriesLP(tbl->it_index, tcenum,
 					tbl->it_page_shift,
 					npages, uaddr, direction, attrs);
 		}
-		__this_cpu_write(tce_page, tcep);
+		__this_cpu_write(tce_page.page, tcep);
 	}
 
 	rpn = __pa(uaddr) >> TCE_SHIFT;
@@ -258,7 +266,7 @@ static int tce_buildmulti_pSeriesLP(struct iommu_table *tbl, long tcenum,
 		tcenum += limit;
 	} while (npages > 0 && !rc);
 
-	local_irq_restore(flags);
+	local_unlock_irqrestore(&tce_page.lock, flags);
 
 	if (unlikely(rc == H_NOT_ENOUGH_RESOURCES)) {
 		ret = (int)rc;
@@ -429,16 +437,17 @@ static int tce_setrange_multi_pSeriesLP(unsigned long start_pfn,
 				DMA_BIDIRECTIONAL, 0);
 	}
 
-	local_irq_disable();	/* to protect tcep and the page behind it */
-	tcep = __this_cpu_read(tce_page);
+	/* to protect tcep and the page behind it */
+	local_lock_irq(&tce_page.lock);
+	tcep = __this_cpu_read(tce_page.page);
 
 	if (!tcep) {
 		tcep = (__be64 *)__get_free_page(GFP_ATOMIC);
 		if (!tcep) {
-			local_irq_enable();
+			local_unlock_irq(&tce_page.lock);
 			return -ENOMEM;
 		}
-		__this_cpu_write(tce_page, tcep);
+		__this_cpu_write(tce_page.page, tcep);
 	}
 
 	proto_tce = TCE_PCI_READ | TCE_PCI_WRITE;
@@ -481,7 +490,7 @@ static int tce_setrange_multi_pSeriesLP(unsigned long start_pfn,
 
 	/* error cleanup: caller will clear whole range */
 
-	local_irq_enable();
+	local_unlock_irq(&tce_page.lock);
 	return rc;
 }
 
diff --git a/arch/powerpc/xmon/xmon.c b/arch/powerpc/xmon/xmon.c
index 5559edf36756..d62b8e053d4c 100644
--- a/arch/powerpc/xmon/xmon.c
+++ b/arch/powerpc/xmon/xmon.c
@@ -3005,7 +3005,7 @@ print_address(unsigned long addr)
 static void
 dump_log_buf(void)
 {
-	struct kmsg_dumper dumper = { .active = 1 };
+	struct kmsg_dumper_iter iter = { .active = 1 };
 	unsigned char buf[128];
 	size_t len;
 
@@ -3017,9 +3017,9 @@ dump_log_buf(void)
 	catch_memory_errors = 1;
 	sync();
 
-	kmsg_dump_rewind_nolock(&dumper);
+	kmsg_dump_rewind(&iter);
 	xmon_start_pagination();
-	while (kmsg_dump_get_line_nolock(&dumper, false, buf, sizeof(buf), &len)) {
+	while (kmsg_dump_get_line(&iter, false, buf, sizeof(buf), &len)) {
 		buf[len] = '\0';
 		printf("%s", buf);
 	}
diff --git a/arch/s390/Kconfig b/arch/s390/Kconfig
index 4a2a12be04c9..6f1fdcd3b5db 100644
--- a/arch/s390/Kconfig
+++ b/arch/s390/Kconfig
@@ -181,6 +181,7 @@ config S390
 	select HAVE_RSEQ
 	select HAVE_SYSCALL_TRACEPOINTS
 	select HAVE_VIRT_CPU_ACCOUNTING
+	select HAVE_VIRT_CPU_ACCOUNTING_IDLE
 	select IOMMU_HELPER		if PCI
 	select IOMMU_SUPPORT		if PCI
 	select MODULES_USE_ELF_RELA
diff --git a/arch/s390/include/asm/spinlock_types.h b/arch/s390/include/asm/spinlock_types.h
index cfed272e4fd5..8e28e8176ec8 100644
--- a/arch/s390/include/asm/spinlock_types.h
+++ b/arch/s390/include/asm/spinlock_types.h
@@ -2,10 +2,6 @@
 #ifndef __ASM_SPINLOCK_TYPES_H
 #define __ASM_SPINLOCK_TYPES_H
 
-#ifndef __LINUX_SPINLOCK_TYPES_H
-# error "please don't include this file directly"
-#endif
-
 typedef struct {
 	int lock;
 } __attribute__ ((aligned (4))) arch_spinlock_t;
diff --git a/arch/s390/include/asm/vtime.h b/arch/s390/include/asm/vtime.h
index 3622d4ebc73a..fac6a67988eb 100644
--- a/arch/s390/include/asm/vtime.h
+++ b/arch/s390/include/asm/vtime.h
@@ -2,7 +2,6 @@
 #ifndef _S390_VTIME_H
 #define _S390_VTIME_H
 
-#define __ARCH_HAS_VTIME_ACCOUNT
 #define __ARCH_HAS_VTIME_TASK_SWITCH
 
 #endif /* _S390_VTIME_H */
diff --git a/arch/s390/kernel/vtime.c b/arch/s390/kernel/vtime.c
index 7b3af2d6b9ba..978a35ea6081 100644
--- a/arch/s390/kernel/vtime.c
+++ b/arch/s390/kernel/vtime.c
@@ -223,35 +223,50 @@ void vtime_flush(struct task_struct *tsk)
 	S390_lowcore.avg_steal_timer = avg_steal;
 }
 
+static u64 vtime_delta(void)
+{
+	u64 timer = S390_lowcore.last_update_timer;
+
+	S390_lowcore.last_update_timer = get_vtimer();
+
+	return timer - S390_lowcore.last_update_timer;
+}
+
 /*
  * Update process times based on virtual cpu times stored by entry.S
  * to the lowcore fields user_timer, system_timer & steal_clock.
  */
-void vtime_account_irq_enter(struct task_struct *tsk)
+void vtime_account_kernel(struct task_struct *tsk)
 {
-	u64 timer;
-
-	timer = S390_lowcore.last_update_timer;
-	S390_lowcore.last_update_timer = get_vtimer();
-	timer -= S390_lowcore.last_update_timer;
+	u64 delta = vtime_delta();
 
-	if ((tsk->flags & PF_VCPU) && (irq_count() == 0))
-		S390_lowcore.guest_timer += timer;
-	else if (hardirq_count())
-		S390_lowcore.hardirq_timer += timer;
-	else if (in_serving_softirq())
-		S390_lowcore.softirq_timer += timer;
+	if (tsk->flags & PF_VCPU)
+		S390_lowcore.guest_timer += delta;
 	else
-		S390_lowcore.system_timer += timer;
+		S390_lowcore.system_timer += delta;
 
-	virt_timer_forward(timer);
+	virt_timer_forward(delta);
 }
-EXPORT_SYMBOL_GPL(vtime_account_irq_enter);
-
-void vtime_account_kernel(struct task_struct *tsk)
-__attribute__((alias("vtime_account_irq_enter")));
 EXPORT_SYMBOL_GPL(vtime_account_kernel);
 
+void vtime_account_softirq(struct task_struct *tsk)
+{
+	u64 delta = vtime_delta();
+
+	S390_lowcore.softirq_timer += delta;
+
+	virt_timer_forward(delta);
+}
+
+void vtime_account_hardirq(struct task_struct *tsk)
+{
+	u64 delta = vtime_delta();
+
+	S390_lowcore.hardirq_timer += delta;
+
+	virt_timer_forward(delta);
+}
+
 /*
  * Sorted add to a list. List is linear searched until first bigger
  * element is found.
diff --git a/arch/sh/include/asm/fixmap.h b/arch/sh/include/asm/fixmap.h
index f38adc189b83..b07fbc7f7bc6 100644
--- a/arch/sh/include/asm/fixmap.h
+++ b/arch/sh/include/asm/fixmap.h
@@ -13,9 +13,6 @@
 #include <linux/kernel.h>
 #include <linux/threads.h>
 #include <asm/page.h>
-#ifdef CONFIG_HIGHMEM
-#include <asm/kmap_types.h>
-#endif
 
 /*
  * Here we define all the compile-time 'special' virtual
@@ -53,11 +50,6 @@ enum fixed_addresses {
 	FIX_CMAP_BEGIN,
 	FIX_CMAP_END = FIX_CMAP_BEGIN + (FIX_N_COLOURS * NR_CPUS) - 1,
 
-#ifdef CONFIG_HIGHMEM
-	FIX_KMAP_BEGIN,	/* reserved pte's for temporary kernel mappings */
-	FIX_KMAP_END = FIX_KMAP_BEGIN + (KM_TYPE_NR * NR_CPUS) - 1,
-#endif
-
 #ifdef CONFIG_IOREMAP_FIXED
 	/*
 	 * FIX_IOREMAP entries are useful for mapping physical address
diff --git a/arch/sh/include/asm/hardirq.h b/arch/sh/include/asm/hardirq.h
index edaea3559a23..9fe4495a8e90 100644
--- a/arch/sh/include/asm/hardirq.h
+++ b/arch/sh/include/asm/hardirq.h
@@ -2,16 +2,10 @@
 #ifndef __ASM_SH_HARDIRQ_H
 #define __ASM_SH_HARDIRQ_H
 
-#include <linux/threads.h>
-#include <linux/irq.h>
-
-typedef struct {
-	unsigned int __softirq_pending;
-	unsigned int __nmi_count;		/* arch dependent */
-} ____cacheline_aligned irq_cpustat_t;
-
-#include <linux/irq_cpustat.h>	/* Standard mappings for irq_cpustat_t above */
-
 extern void ack_bad_irq(unsigned int irq);
+#define ack_bad_irq ack_bad_irq
+#define ARCH_WANTS_NMI_IRQSTAT
+
+#include <asm-generic/hardirq.h>
 
 #endif /* __ASM_SH_HARDIRQ_H */
diff --git a/arch/sh/include/asm/kmap_types.h b/arch/sh/include/asm/kmap_types.h
deleted file mode 100644
index b78107f923dd..000000000000
--- a/arch/sh/include/asm/kmap_types.h
+++ /dev/null
@@ -1,15 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0 */
-#ifndef __SH_KMAP_TYPES_H
-#define __SH_KMAP_TYPES_H
-
-/* Dummy header just to define km_type. */
-
-#ifdef CONFIG_DEBUG_HIGHMEM
-#define  __WITH_KM_FENCE
-#endif
-
-#include <asm-generic/kmap_types.h>
-
-#undef __WITH_KM_FENCE
-
-#endif
diff --git a/arch/sh/include/asm/spinlock_types.h b/arch/sh/include/asm/spinlock_types.h
index e82369f286a2..22ca9a98bbb8 100644
--- a/arch/sh/include/asm/spinlock_types.h
+++ b/arch/sh/include/asm/spinlock_types.h
@@ -2,10 +2,6 @@
 #ifndef __ASM_SH_SPINLOCK_TYPES_H
 #define __ASM_SH_SPINLOCK_TYPES_H
 
-#ifndef __LINUX_SPINLOCK_TYPES_H
-# error "please don't include this file directly"
-#endif
-
 typedef struct {
 	volatile unsigned int lock;
 } arch_spinlock_t;
diff --git a/arch/sh/kernel/irq.c b/arch/sh/kernel/irq.c
index 5717c7cbdd97..5db7af565dec 100644
--- a/arch/sh/kernel/irq.c
+++ b/arch/sh/kernel/irq.c
@@ -44,7 +44,7 @@ int arch_show_interrupts(struct seq_file *p, int prec)
 
 	seq_printf(p, "%*s: ", prec, "NMI");
 	for_each_online_cpu(j)
-		seq_printf(p, "%10u ", nmi_count(j));
+		seq_printf(p, "%10u ", per_cpu(irq_stat.__nmi_count, j));
 	seq_printf(p, "  Non-maskable interrupts\n");
 
 	seq_printf(p, "%*s: %10u\n", prec, "ERR", atomic_read(&irq_err_count));
@@ -148,6 +148,7 @@ void irq_ctx_exit(int cpu)
 	hardirq_ctx[cpu] = NULL;
 }
 
+#ifndef CONFIG_PREEMPT_RT
 void do_softirq_own_stack(void)
 {
 	struct thread_info *curctx;
@@ -175,6 +176,7 @@ void do_softirq_own_stack(void)
 		  "r5", "r6", "r7", "r8", "r9", "r15", "t", "pr"
 	);
 }
+#endif
 #else
 static inline void handle_one_irq(unsigned int irq)
 {
diff --git a/arch/sh/kernel/traps.c b/arch/sh/kernel/traps.c
index 9c3d32b80038..f5beecdac693 100644
--- a/arch/sh/kernel/traps.c
+++ b/arch/sh/kernel/traps.c
@@ -186,7 +186,7 @@ BUILD_TRAP_HANDLER(nmi)
 	arch_ftrace_nmi_enter();
 
 	nmi_enter();
-	nmi_count(cpu)++;
+	this_cpu_inc(irq_stat.__nmi_count);
 
 	switch (notify_die(DIE_NMI, "NMI", regs, 0, vec & 0xff, SIGINT)) {
 	case NOTIFY_OK:
diff --git a/arch/sh/mm/init.c b/arch/sh/mm/init.c
index 3348e0c4d769..0db6919af8d3 100644
--- a/arch/sh/mm/init.c
+++ b/arch/sh/mm/init.c
@@ -362,9 +362,6 @@ void __init mem_init(void)
 	mem_init_print_info(NULL);
 	pr_info("virtual kernel memory layout:\n"
 		"    fixmap  : 0x%08lx - 0x%08lx   (%4ld kB)\n"
-#ifdef CONFIG_HIGHMEM
-		"    pkmap   : 0x%08lx - 0x%08lx   (%4ld kB)\n"
-#endif
 		"    vmalloc : 0x%08lx - 0x%08lx   (%4ld MB)\n"
 		"    lowmem  : 0x%08lx - 0x%08lx   (%4ld MB) (cached)\n"
 #ifdef CONFIG_UNCACHED_MAPPING
@@ -376,11 +373,6 @@ void __init mem_init(void)
 		FIXADDR_START, FIXADDR_TOP,
 		(FIXADDR_TOP - FIXADDR_START) >> 10,
 
-#ifdef CONFIG_HIGHMEM
-		PKMAP_BASE, PKMAP_BASE+LAST_PKMAP*PAGE_SIZE,
-		(LAST_PKMAP*PAGE_SIZE) >> 10,
-#endif
-
 		(unsigned long)VMALLOC_START, VMALLOC_END,
 		(VMALLOC_END - VMALLOC_START) >> 20,
 
diff --git a/arch/sparc/Kconfig b/arch/sparc/Kconfig
index 530b7ec5d3ca..a38d00d8b783 100644
--- a/arch/sparc/Kconfig
+++ b/arch/sparc/Kconfig
@@ -139,6 +139,7 @@ config MMU
 config HIGHMEM
 	bool
 	default y if SPARC32
+	select KMAP_LOCAL
 
 config ZONE_DMA
 	bool
diff --git a/arch/sparc/include/asm/highmem.h b/arch/sparc/include/asm/highmem.h
index 6c35f0d27ee1..875116209ec1 100644
--- a/arch/sparc/include/asm/highmem.h
+++ b/arch/sparc/include/asm/highmem.h
@@ -24,7 +24,6 @@
 #include <linux/interrupt.h>
 #include <linux/pgtable.h>
 #include <asm/vaddrs.h>
-#include <asm/kmap_types.h>
 #include <asm/pgtsrmmu.h>
 
 /* declarations for highmem.c */
@@ -33,8 +32,6 @@ extern unsigned long highstart_pfn, highend_pfn;
 #define kmap_prot __pgprot(SRMMU_ET_PTE | SRMMU_PRIV | SRMMU_CACHE)
 extern pte_t *pkmap_page_table;
 
-void kmap_init(void) __init;
-
 /*
  * Right now we initialize only a single pte table. It can be extended
  * easily, subsequent pte tables have to be allocated in one physical
@@ -53,6 +50,11 @@ void kmap_init(void) __init;
 
 #define flush_cache_kmaps()	flush_cache_all()
 
+/* FIXME: Use __flush_tlb_one(vaddr) instead of flush_cache_all() -- Anton */
+#define arch_kmap_local_post_map(vaddr, pteval)	flush_cache_all()
+#define arch_kmap_local_post_unmap(vaddr)	flush_cache_all()
+
+
 #endif /* __KERNEL__ */
 
 #endif /* _ASM_HIGHMEM_H */
diff --git a/arch/sparc/include/asm/kmap_types.h b/arch/sparc/include/asm/kmap_types.h
deleted file mode 100644
index 55a99b6bd91e..000000000000
--- a/arch/sparc/include/asm/kmap_types.h
+++ /dev/null
@@ -1,11 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0 */
-#ifndef _ASM_KMAP_TYPES_H
-#define _ASM_KMAP_TYPES_H
-
-/* Dummy header just to define km_type.  None of this
- * is actually used on sparc.  -DaveM
- */
-
-#include <asm-generic/kmap_types.h>
-
-#endif
diff --git a/arch/sparc/include/asm/vaddrs.h b/arch/sparc/include/asm/vaddrs.h
index 84d054b07a6f..4fec0341e2a8 100644
--- a/arch/sparc/include/asm/vaddrs.h
+++ b/arch/sparc/include/asm/vaddrs.h
@@ -32,13 +32,13 @@
 #define SRMMU_NOCACHE_ALCRATIO	64	/* 256 pages per 64MB of system RAM */
 
 #ifndef __ASSEMBLY__
-#include <asm/kmap_types.h>
+#include <asm/kmap_size.h>
 
 enum fixed_addresses {
 	FIX_HOLE,
 #ifdef CONFIG_HIGHMEM
 	FIX_KMAP_BEGIN,
-	FIX_KMAP_END = (KM_TYPE_NR * NR_CPUS),
+	FIX_KMAP_END = (KM_MAX_IDX * NR_CPUS),
 #endif
 	__end_of_fixed_addresses
 };
diff --git a/arch/sparc/kernel/irq_64.c b/arch/sparc/kernel/irq_64.c
index 3ec9f1402aad..eb21682abfcb 100644
--- a/arch/sparc/kernel/irq_64.c
+++ b/arch/sparc/kernel/irq_64.c
@@ -854,6 +854,7 @@ void __irq_entry handler_irq(int pil, struct pt_regs *regs)
 	set_irq_regs(old_regs);
 }
 
+#ifndef CONFIG_PREEMPT_RT
 void do_softirq_own_stack(void)
 {
 	void *orig_sp, *sp = softirq_stack[smp_processor_id()];
@@ -868,6 +869,7 @@ void do_softirq_own_stack(void)
 	__asm__ __volatile__("mov %0, %%sp"
 			     : : "r" (orig_sp));
 }
+#endif
 
 #ifdef CONFIG_HOTPLUG_CPU
 void fixup_irqs(void)
diff --git a/arch/sparc/mm/Makefile b/arch/sparc/mm/Makefile
index b078205b70e0..68db1f859b02 100644
--- a/arch/sparc/mm/Makefile
+++ b/arch/sparc/mm/Makefile
@@ -15,6 +15,3 @@ obj-$(CONFIG_SPARC32)   += leon_mm.o
 
 # Only used by sparc64
 obj-$(CONFIG_HUGETLB_PAGE) += hugetlbpage.o
-
-# Only used by sparc32
-obj-$(CONFIG_HIGHMEM)   += highmem.o
diff --git a/arch/sparc/mm/highmem.c b/arch/sparc/mm/highmem.c
deleted file mode 100644
index 8f2a2afb048a..000000000000
--- a/arch/sparc/mm/highmem.c
+++ /dev/null
@@ -1,115 +0,0 @@
-// SPDX-License-Identifier: GPL-2.0
-/*
- *  highmem.c: virtual kernel memory mappings for high memory
- *
- *  Provides kernel-static versions of atomic kmap functions originally
- *  found as inlines in include/asm-sparc/highmem.h.  These became
- *  needed as kmap_atomic() and kunmap_atomic() started getting
- *  called from within modules.
- *  -- Tomas Szepe <szepe@pinerecords.com>, September 2002
- *
- *  But kmap_atomic() and kunmap_atomic() cannot be inlined in
- *  modules because they are loaded with btfixup-ped functions.
- */
-
-/*
- * The use of kmap_atomic/kunmap_atomic is discouraged - kmap/kunmap
- * gives a more generic (and caching) interface. But kmap_atomic can
- * be used in IRQ contexts, so in some (very limited) cases we need it.
- *
- * XXX This is an old text. Actually, it's good to use atomic kmaps,
- * provided you remember that they are atomic and not try to sleep
- * with a kmap taken, much like a spinlock. Non-atomic kmaps are
- * shared by CPUs, and so precious, and establishing them requires IPI.
- * Atomic kmaps are lightweight and we may have NCPUS more of them.
- */
-#include <linux/highmem.h>
-#include <linux/export.h>
-#include <linux/mm.h>
-
-#include <asm/cacheflush.h>
-#include <asm/tlbflush.h>
-#include <asm/vaddrs.h>
-
-static pte_t *kmap_pte;
-
-void __init kmap_init(void)
-{
-	unsigned long address = __fix_to_virt(FIX_KMAP_BEGIN);
-
-        /* cache the first kmap pte */
-        kmap_pte = virt_to_kpte(address);
-}
-
-void *kmap_atomic_high_prot(struct page *page, pgprot_t prot)
-{
-	unsigned long vaddr;
-	long idx, type;
-
-	type = kmap_atomic_idx_push();
-	idx = type + KM_TYPE_NR*smp_processor_id();
-	vaddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);
-
-/* XXX Fix - Anton */
-#if 0
-	__flush_cache_one(vaddr);
-#else
-	flush_cache_all();
-#endif
-
-#ifdef CONFIG_DEBUG_HIGHMEM
-	BUG_ON(!pte_none(*(kmap_pte-idx)));
-#endif
-	set_pte(kmap_pte-idx, mk_pte(page, prot));
-/* XXX Fix - Anton */
-#if 0
-	__flush_tlb_one(vaddr);
-#else
-	flush_tlb_all();
-#endif
-
-	return (void*) vaddr;
-}
-EXPORT_SYMBOL(kmap_atomic_high_prot);
-
-void kunmap_atomic_high(void *kvaddr)
-{
-	unsigned long vaddr = (unsigned long) kvaddr & PAGE_MASK;
-	int type;
-
-	if (vaddr < FIXADDR_START)
-		return;
-
-	type = kmap_atomic_idx();
-
-#ifdef CONFIG_DEBUG_HIGHMEM
-	{
-		unsigned long idx;
-
-		idx = type + KM_TYPE_NR * smp_processor_id();
-		BUG_ON(vaddr != __fix_to_virt(FIX_KMAP_BEGIN+idx));
-
-		/* XXX Fix - Anton */
-#if 0
-		__flush_cache_one(vaddr);
-#else
-		flush_cache_all();
-#endif
-
-		/*
-		 * force other mappings to Oops if they'll try to access
-		 * this pte without first remap it
-		 */
-		pte_clear(&init_mm, vaddr, kmap_pte-idx);
-		/* XXX Fix - Anton */
-#if 0
-		__flush_tlb_one(vaddr);
-#else
-		flush_tlb_all();
-#endif
-	}
-#endif
-
-	kmap_atomic_idx_pop();
-}
-EXPORT_SYMBOL(kunmap_atomic_high);
diff --git a/arch/sparc/mm/srmmu.c b/arch/sparc/mm/srmmu.c
index 0070f8b9a753..a03caa5f6628 100644
--- a/arch/sparc/mm/srmmu.c
+++ b/arch/sparc/mm/srmmu.c
@@ -971,8 +971,6 @@ void __init srmmu_paging_init(void)
 
 	sparc_context_init(num_contexts);
 
-	kmap_init();
-
 	{
 		unsigned long max_zone_pfn[MAX_NR_ZONES] = { 0 };
 
diff --git a/arch/um/include/asm/fixmap.h b/arch/um/include/asm/fixmap.h
index 2c697a145ac1..2efac5827188 100644
--- a/arch/um/include/asm/fixmap.h
+++ b/arch/um/include/asm/fixmap.h
@@ -3,7 +3,6 @@
 #define __UM_FIXMAP_H
 
 #include <asm/processor.h>
-#include <asm/kmap_types.h>
 #include <asm/archparam.h>
 #include <asm/page.h>
 #include <linux/threads.h>
diff --git a/arch/um/include/asm/hardirq.h b/arch/um/include/asm/hardirq.h
index b426796d26fd..52e2c36267a9 100644
--- a/arch/um/include/asm/hardirq.h
+++ b/arch/um/include/asm/hardirq.h
@@ -2,22 +2,7 @@
 #ifndef __ASM_UM_HARDIRQ_H
 #define __ASM_UM_HARDIRQ_H
 
-#include <linux/cache.h>
-#include <linux/threads.h>
-
-typedef struct {
-	unsigned int __softirq_pending;
-} ____cacheline_aligned irq_cpustat_t;
-
-#include <linux/irq_cpustat.h>	/* Standard mappings for irq_cpustat_t above */
-#include <linux/irq.h>
-
-#ifndef ack_bad_irq
-static inline void ack_bad_irq(unsigned int irq)
-{
-	printk(KERN_CRIT "unexpected IRQ trap at vector %02x\n", irq);
-}
-#endif
+#include <asm-generic/hardirq.h>
 
 #define __ARCH_IRQ_EXIT_IRQS_DISABLED 1
 
diff --git a/arch/um/include/asm/kmap_types.h b/arch/um/include/asm/kmap_types.h
deleted file mode 100644
index b0bd12de1d23..000000000000
--- a/arch/um/include/asm/kmap_types.h
+++ /dev/null
@@ -1,13 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0 */
-/* 
- * Copyright (C) 2002 Jeff Dike (jdike@karaya.com)
- */
-
-#ifndef __UM_KMAP_TYPES_H
-#define __UM_KMAP_TYPES_H
-
-/* No more #include "asm/arch/kmap_types.h" ! */
-
-#define KM_TYPE_NR 14
-
-#endif
diff --git a/arch/um/kernel/kmsg_dump.c b/arch/um/kernel/kmsg_dump.c
index e4abac6c9727..173999422ed8 100644
--- a/arch/um/kernel/kmsg_dump.c
+++ b/arch/um/kernel/kmsg_dump.c
@@ -1,15 +1,19 @@
 // SPDX-License-Identifier: GPL-2.0
 #include <linux/kmsg_dump.h>
+#include <linux/spinlock.h>
 #include <linux/console.h>
 #include <shared/init.h>
 #include <shared/kern.h>
 #include <os.h>
 
 static void kmsg_dumper_stdout(struct kmsg_dumper *dumper,
-				enum kmsg_dump_reason reason)
+				enum kmsg_dump_reason reason,
+				struct kmsg_dumper_iter *iter)
 {
+	static DEFINE_SPINLOCK(lock);
 	static char line[1024];
 	struct console *con;
+	unsigned long flags;
 	size_t len = 0;
 
 	/* only dump kmsg when no console is available */
@@ -24,11 +28,16 @@ static void kmsg_dumper_stdout(struct kmsg_dumper *dumper,
 	if (con)
 		return;
 
+	if (!spin_trylock_irqsave(&lock, flags))
+		return;
+
 	printf("kmsg_dump:\n");
-	while (kmsg_dump_get_line(dumper, true, line, sizeof(line), &len)) {
+	while (kmsg_dump_get_line(iter, true, line, sizeof(line), &len)) {
 		line[len] = '\0';
 		printf("%s", line);
 	}
+
+	spin_unlock_irqrestore(&lock, flags);
 }
 
 static struct kmsg_dumper kmsg_dumper = {
diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig
index 3a5ecb1039bf..a2c12ad69173 100644
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@ -15,6 +15,7 @@ config X86_32
 	select CLKSRC_I8253
 	select CLONE_BACKWARDS
 	select HAVE_DEBUG_STACKOVERFLOW
+	select KMAP_LOCAL
 	select MODULES_USE_ELF_REL
 	select OLD_SIGACTION
 	select GENERIC_VDSO_32
@@ -93,6 +94,7 @@ config X86
 	select ARCH_SUPPORTS_ACPI
 	select ARCH_SUPPORTS_ATOMIC_RMW
 	select ARCH_SUPPORTS_NUMA_BALANCING	if X86_64
+	select ARCH_SUPPORTS_RT
 	select ARCH_USE_BUILTIN_BSWAP
 	select ARCH_USE_QUEUED_RWLOCKS
 	select ARCH_USE_QUEUED_SPINLOCKS
@@ -210,6 +212,7 @@ config X86
 	select HAVE_PCI
 	select HAVE_PERF_REGS
 	select HAVE_PERF_USER_STACK_DUMP
+	select HAVE_PREEMPT_LAZY
 	select MMU_GATHER_RCU_TABLE_FREE		if PARAVIRT
 	select HAVE_POSIX_CPU_TIMERS_TASK_WORK
 	select HAVE_REGS_AND_STACK_ACCESS_API
diff --git a/arch/x86/crypto/aesni-intel_glue.c b/arch/x86/crypto/aesni-intel_glue.c
index f9a1d98e7534..81f1decfbc9b 100644
--- a/arch/x86/crypto/aesni-intel_glue.c
+++ b/arch/x86/crypto/aesni-intel_glue.c
@@ -376,14 +376,14 @@ static int ecb_encrypt(struct skcipher_request *req)
 
 	err = skcipher_walk_virt(&walk, req, true);
 
-	kernel_fpu_begin();
 	while ((nbytes = walk.nbytes)) {
+		kernel_fpu_begin();
 		aesni_ecb_enc(ctx, walk.dst.virt.addr, walk.src.virt.addr,
 			      nbytes & AES_BLOCK_MASK);
+		kernel_fpu_end();
 		nbytes &= AES_BLOCK_SIZE - 1;
 		err = skcipher_walk_done(&walk, nbytes);
 	}
-	kernel_fpu_end();
 
 	return err;
 }
@@ -398,14 +398,14 @@ static int ecb_decrypt(struct skcipher_request *req)
 
 	err = skcipher_walk_virt(&walk, req, true);
 
-	kernel_fpu_begin();
 	while ((nbytes = walk.nbytes)) {
+		kernel_fpu_begin();
 		aesni_ecb_dec(ctx, walk.dst.virt.addr, walk.src.virt.addr,
 			      nbytes & AES_BLOCK_MASK);
+		kernel_fpu_end();
 		nbytes &= AES_BLOCK_SIZE - 1;
 		err = skcipher_walk_done(&walk, nbytes);
 	}
-	kernel_fpu_end();
 
 	return err;
 }
@@ -420,14 +420,14 @@ static int cbc_encrypt(struct skcipher_request *req)
 
 	err = skcipher_walk_virt(&walk, req, true);
 
-	kernel_fpu_begin();
 	while ((nbytes = walk.nbytes)) {
+		kernel_fpu_begin();
 		aesni_cbc_enc(ctx, walk.dst.virt.addr, walk.src.virt.addr,
 			      nbytes & AES_BLOCK_MASK, walk.iv);
+		kernel_fpu_end();
 		nbytes &= AES_BLOCK_SIZE - 1;
 		err = skcipher_walk_done(&walk, nbytes);
 	}
-	kernel_fpu_end();
 
 	return err;
 }
@@ -442,14 +442,14 @@ static int cbc_decrypt(struct skcipher_request *req)
 
 	err = skcipher_walk_virt(&walk, req, true);
 
-	kernel_fpu_begin();
 	while ((nbytes = walk.nbytes)) {
+		kernel_fpu_begin();
 		aesni_cbc_dec(ctx, walk.dst.virt.addr, walk.src.virt.addr,
 			      nbytes & AES_BLOCK_MASK, walk.iv);
+		kernel_fpu_end();
 		nbytes &= AES_BLOCK_SIZE - 1;
 		err = skcipher_walk_done(&walk, nbytes);
 	}
-	kernel_fpu_end();
 
 	return err;
 }
@@ -497,18 +497,20 @@ static int ctr_crypt(struct skcipher_request *req)
 
 	err = skcipher_walk_virt(&walk, req, true);
 
-	kernel_fpu_begin();
 	while ((nbytes = walk.nbytes) >= AES_BLOCK_SIZE) {
+		kernel_fpu_begin();
 		aesni_ctr_enc_tfm(ctx, walk.dst.virt.addr, walk.src.virt.addr,
 			              nbytes & AES_BLOCK_MASK, walk.iv);
+		kernel_fpu_end();
 		nbytes &= AES_BLOCK_SIZE - 1;
 		err = skcipher_walk_done(&walk, nbytes);
 	}
 	if (walk.nbytes) {
+		kernel_fpu_begin();
 		ctr_crypt_final(ctx, &walk);
+		kernel_fpu_end();
 		err = skcipher_walk_done(&walk, 0);
 	}
-	kernel_fpu_end();
 
 	return err;
 }
diff --git a/arch/x86/crypto/cast5_avx_glue.c b/arch/x86/crypto/cast5_avx_glue.c
index 384ccb00f9e1..2f8df8ef8644 100644
--- a/arch/x86/crypto/cast5_avx_glue.c
+++ b/arch/x86/crypto/cast5_avx_glue.c
@@ -46,7 +46,7 @@ static inline void cast5_fpu_end(bool fpu_enabled)
 
 static int ecb_crypt(struct skcipher_request *req, bool enc)
 {
-	bool fpu_enabled = false;
+	bool fpu_enabled;
 	struct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);
 	struct cast5_ctx *ctx = crypto_skcipher_ctx(tfm);
 	struct skcipher_walk walk;
@@ -61,7 +61,7 @@ static int ecb_crypt(struct skcipher_request *req, bool enc)
 		u8 *wsrc = walk.src.virt.addr;
 		u8 *wdst = walk.dst.virt.addr;
 
-		fpu_enabled = cast5_fpu_begin(fpu_enabled, &walk, nbytes);
+		fpu_enabled = cast5_fpu_begin(false, &walk, nbytes);
 
 		/* Process multi-block batch */
 		if (nbytes >= bsize * CAST5_PARALLEL_BLOCKS) {
@@ -90,10 +90,9 @@ static int ecb_crypt(struct skcipher_request *req, bool enc)
 		} while (nbytes >= bsize);
 
 done:
+		cast5_fpu_end(fpu_enabled);
 		err = skcipher_walk_done(&walk, nbytes);
 	}
-
-	cast5_fpu_end(fpu_enabled);
 	return err;
 }
 
@@ -197,7 +196,7 @@ static int cbc_decrypt(struct skcipher_request *req)
 {
 	struct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);
 	struct cast5_ctx *ctx = crypto_skcipher_ctx(tfm);
-	bool fpu_enabled = false;
+	bool fpu_enabled;
 	struct skcipher_walk walk;
 	unsigned int nbytes;
 	int err;
@@ -205,12 +204,11 @@ static int cbc_decrypt(struct skcipher_request *req)
 	err = skcipher_walk_virt(&walk, req, false);
 
 	while ((nbytes = walk.nbytes)) {
-		fpu_enabled = cast5_fpu_begin(fpu_enabled, &walk, nbytes);
+		fpu_enabled = cast5_fpu_begin(false, &walk, nbytes);
 		nbytes = __cbc_decrypt(ctx, &walk);
+		cast5_fpu_end(fpu_enabled);
 		err = skcipher_walk_done(&walk, nbytes);
 	}
-
-	cast5_fpu_end(fpu_enabled);
 	return err;
 }
 
@@ -277,7 +275,7 @@ static int ctr_crypt(struct skcipher_request *req)
 {
 	struct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);
 	struct cast5_ctx *ctx = crypto_skcipher_ctx(tfm);
-	bool fpu_enabled = false;
+	bool fpu_enabled;
 	struct skcipher_walk walk;
 	unsigned int nbytes;
 	int err;
@@ -285,13 +283,12 @@ static int ctr_crypt(struct skcipher_request *req)
 	err = skcipher_walk_virt(&walk, req, false);
 
 	while ((nbytes = walk.nbytes) >= CAST5_BLOCK_SIZE) {
-		fpu_enabled = cast5_fpu_begin(fpu_enabled, &walk, nbytes);
+		fpu_enabled = cast5_fpu_begin(false, &walk, nbytes);
 		nbytes = __ctr_crypt(&walk, ctx);
+		cast5_fpu_end(fpu_enabled);
 		err = skcipher_walk_done(&walk, nbytes);
 	}
 
-	cast5_fpu_end(fpu_enabled);
-
 	if (walk.nbytes) {
 		ctr_crypt_final(&walk, ctx);
 		err = skcipher_walk_done(&walk, 0);
diff --git a/arch/x86/crypto/glue_helper.c b/arch/x86/crypto/glue_helper.c
index d3d91a0abf88..6d0774721514 100644
--- a/arch/x86/crypto/glue_helper.c
+++ b/arch/x86/crypto/glue_helper.c
@@ -24,7 +24,7 @@ int glue_ecb_req_128bit(const struct common_glue_ctx *gctx,
 	void *ctx = crypto_skcipher_ctx(crypto_skcipher_reqtfm(req));
 	const unsigned int bsize = 128 / 8;
 	struct skcipher_walk walk;
-	bool fpu_enabled = false;
+	bool fpu_enabled;
 	unsigned int nbytes;
 	int err;
 
@@ -37,7 +37,7 @@ int glue_ecb_req_128bit(const struct common_glue_ctx *gctx,
 		unsigned int i;
 
 		fpu_enabled = glue_fpu_begin(bsize, gctx->fpu_blocks_limit,
-					     &walk, fpu_enabled, nbytes);
+					     &walk, false, nbytes);
 		for (i = 0; i < gctx->num_funcs; i++) {
 			func_bytes = bsize * gctx->funcs[i].num_blocks;
 
@@ -55,10 +55,9 @@ int glue_ecb_req_128bit(const struct common_glue_ctx *gctx,
 			if (nbytes < bsize)
 				break;
 		}
+		glue_fpu_end(fpu_enabled);
 		err = skcipher_walk_done(&walk, nbytes);
 	}
-
-	glue_fpu_end(fpu_enabled);
 	return err;
 }
 EXPORT_SYMBOL_GPL(glue_ecb_req_128bit);
@@ -101,7 +100,7 @@ int glue_cbc_decrypt_req_128bit(const struct common_glue_ctx *gctx,
 	void *ctx = crypto_skcipher_ctx(crypto_skcipher_reqtfm(req));
 	const unsigned int bsize = 128 / 8;
 	struct skcipher_walk walk;
-	bool fpu_enabled = false;
+	bool fpu_enabled;
 	unsigned int nbytes;
 	int err;
 
@@ -115,7 +114,7 @@ int glue_cbc_decrypt_req_128bit(const struct common_glue_ctx *gctx,
 		u128 last_iv;
 
 		fpu_enabled = glue_fpu_begin(bsize, gctx->fpu_blocks_limit,
-					     &walk, fpu_enabled, nbytes);
+					     &walk, false, nbytes);
 		/* Start of the last block. */
 		src += nbytes / bsize - 1;
 		dst += nbytes / bsize - 1;
@@ -148,10 +147,10 @@ int glue_cbc_decrypt_req_128bit(const struct common_glue_ctx *gctx,
 done:
 		u128_xor(dst, dst, (u128 *)walk.iv);
 		*(u128 *)walk.iv = last_iv;
+		glue_fpu_end(fpu_enabled);
 		err = skcipher_walk_done(&walk, nbytes);
 	}
 
-	glue_fpu_end(fpu_enabled);
 	return err;
 }
 EXPORT_SYMBOL_GPL(glue_cbc_decrypt_req_128bit);
@@ -162,7 +161,7 @@ int glue_ctr_req_128bit(const struct common_glue_ctx *gctx,
 	void *ctx = crypto_skcipher_ctx(crypto_skcipher_reqtfm(req));
 	const unsigned int bsize = 128 / 8;
 	struct skcipher_walk walk;
-	bool fpu_enabled = false;
+	bool fpu_enabled;
 	unsigned int nbytes;
 	int err;
 
@@ -176,7 +175,7 @@ int glue_ctr_req_128bit(const struct common_glue_ctx *gctx,
 		le128 ctrblk;
 
 		fpu_enabled = glue_fpu_begin(bsize, gctx->fpu_blocks_limit,
-					     &walk, fpu_enabled, nbytes);
+					     &walk, false, nbytes);
 
 		be128_to_le128(&ctrblk, (be128 *)walk.iv);
 
@@ -202,11 +201,10 @@ int glue_ctr_req_128bit(const struct common_glue_ctx *gctx,
 		}
 
 		le128_to_be128((be128 *)walk.iv, &ctrblk);
+		glue_fpu_end(fpu_enabled);
 		err = skcipher_walk_done(&walk, nbytes);
 	}
 
-	glue_fpu_end(fpu_enabled);
-
 	if (nbytes) {
 		le128 ctrblk;
 		u128 tmp;
@@ -306,8 +304,14 @@ int glue_xts_req_128bit(const struct common_glue_ctx *gctx,
 	tweak_fn(tweak_ctx, walk.iv, walk.iv);
 
 	while (nbytes) {
+		fpu_enabled = glue_fpu_begin(bsize, gctx->fpu_blocks_limit,
+					     &walk, fpu_enabled,
+					     nbytes < bsize ? bsize : nbytes);
 		nbytes = __glue_xts_req_128bit(gctx, crypt_ctx, &walk);
 
+		glue_fpu_end(fpu_enabled);
+		fpu_enabled = false;
+
 		err = skcipher_walk_done(&walk, nbytes);
 		nbytes = walk.nbytes;
 	}
diff --git a/arch/x86/include/asm/fixmap.h b/arch/x86/include/asm/fixmap.h
index 77217bd292bd..8eba66a33e39 100644
--- a/arch/x86/include/asm/fixmap.h
+++ b/arch/x86/include/asm/fixmap.h
@@ -31,7 +31,7 @@
 #include <asm/pgtable_types.h>
 #ifdef CONFIG_X86_32
 #include <linux/threads.h>
-#include <asm/kmap_types.h>
+#include <asm/kmap_size.h>
 #else
 #include <uapi/asm/vsyscall.h>
 #endif
@@ -94,7 +94,7 @@ enum fixed_addresses {
 #endif
 #ifdef CONFIG_X86_32
 	FIX_KMAP_BEGIN,	/* reserved pte's for temporary kernel mappings */
-	FIX_KMAP_END = FIX_KMAP_BEGIN+(KM_TYPE_NR*NR_CPUS)-1,
+	FIX_KMAP_END = FIX_KMAP_BEGIN + (KM_MAX_IDX * NR_CPUS) - 1,
 #ifdef CONFIG_PCI_MMCONFIG
 	FIX_PCIE_MCFG,
 #endif
@@ -151,7 +151,6 @@ extern void reserve_top_address(unsigned long reserve);
 
 extern int fixmaps_set;
 
-extern pte_t *kmap_pte;
 extern pte_t *pkmap_page_table;
 
 void __native_set_fixmap(enum fixed_addresses idx, pte_t pte);
diff --git a/arch/x86/include/asm/fpu/api.h b/arch/x86/include/asm/fpu/api.h
index 38f4936045ab..41d3be7da969 100644
--- a/arch/x86/include/asm/fpu/api.h
+++ b/arch/x86/include/asm/fpu/api.h
@@ -28,6 +28,7 @@ extern void kernel_fpu_begin_mask(unsigned int kfpu_mask);
 extern void kernel_fpu_end(void);
 extern bool irq_fpu_usable(void);
 extern void fpregs_mark_activate(void);
+extern void kernel_fpu_resched(void);
 
 /* Code that is unaware of kernel_fpu_begin_mask() can use this */
 static inline void kernel_fpu_begin(void)
@@ -40,17 +41,32 @@ static inline void kernel_fpu_begin(void)
  * A context switch will (and softirq might) save CPU's FPU registers to
  * fpu->state and set TIF_NEED_FPU_LOAD leaving CPU's FPU registers in
  * a random state.
+ *
+ * local_bh_disable() protects against both preemption and soft interrupts
+ * on !RT kernels.
+ *
+ * On RT kernels local_bh_disable() is not sufficient because it only
+ * serializes soft interrupt related sections via a local lock, but stays
+ * preemptible. Disabling preemption is the right choice here as bottom
+ * half processing is always in thread context on RT kernels so it
+ * implicitly prevents bottom half processing as well.
+ *
+ * Disabling preemption also serializes against kernel_fpu_begin().
  */
 static inline void fpregs_lock(void)
 {
-	preempt_disable();
-	local_bh_disable();
+	if (!IS_ENABLED(CONFIG_PREEMPT_RT))
+		local_bh_disable();
+	else
+		preempt_disable();
 }
 
 static inline void fpregs_unlock(void)
 {
-	local_bh_enable();
-	preempt_enable();
+	if (!IS_ENABLED(CONFIG_PREEMPT_RT))
+		local_bh_enable();
+	else
+		preempt_enable();
 }
 
 #ifdef CONFIG_X86_DEBUG_FPU
diff --git a/arch/x86/include/asm/highmem.h b/arch/x86/include/asm/highmem.h
index 0f420b24e0fc..032e020853aa 100644
--- a/arch/x86/include/asm/highmem.h
+++ b/arch/x86/include/asm/highmem.h
@@ -23,7 +23,6 @@
 
 #include <linux/interrupt.h>
 #include <linux/threads.h>
-#include <asm/kmap_types.h>
 #include <asm/tlbflush.h>
 #include <asm/paravirt.h>
 #include <asm/fixmap.h>
@@ -58,11 +57,17 @@ extern unsigned long highstart_pfn, highend_pfn;
 #define PKMAP_NR(virt)  ((virt-PKMAP_BASE) >> PAGE_SHIFT)
 #define PKMAP_ADDR(nr)  (PKMAP_BASE + ((nr) << PAGE_SHIFT))
 
-void *kmap_atomic_pfn(unsigned long pfn);
-void *kmap_atomic_prot_pfn(unsigned long pfn, pgprot_t prot);
-
 #define flush_cache_kmaps()	do { } while (0)
 
+#define	arch_kmap_local_post_map(vaddr, pteval)		\
+	arch_flush_lazy_mmu_mode()
+
+#define	arch_kmap_local_post_unmap(vaddr)		\
+	do {						\
+		flush_tlb_one_kernel((vaddr));		\
+		arch_flush_lazy_mmu_mode();		\
+	} while (0)
+
 extern void add_highpages_with_active_regions(int nid, unsigned long start_pfn,
 					unsigned long end_pfn);
 
diff --git a/arch/x86/include/asm/iomap.h b/arch/x86/include/asm/iomap.h
index bacf68c4d70e..e2de092fc38c 100644
--- a/arch/x86/include/asm/iomap.h
+++ b/arch/x86/include/asm/iomap.h
@@ -9,19 +9,14 @@
 #include <linux/fs.h>
 #include <linux/mm.h>
 #include <linux/uaccess.h>
+#include <linux/highmem.h>
 #include <asm/cacheflush.h>
 #include <asm/tlbflush.h>
 
-void __iomem *
-iomap_atomic_prot_pfn(unsigned long pfn, pgprot_t prot);
+void __iomem *__iomap_local_pfn_prot(unsigned long pfn, pgprot_t prot);
 
-void
-iounmap_atomic(void __iomem *kvaddr);
+int iomap_create_wc(resource_size_t base, unsigned long size, pgprot_t *prot);
 
-int
-iomap_create_wc(resource_size_t base, unsigned long size, pgprot_t *prot);
-
-void
-iomap_free(resource_size_t base, unsigned long size);
+void iomap_free(resource_size_t base, unsigned long size);
 
 #endif /* _ASM_X86_IOMAP_H */
diff --git a/arch/x86/include/asm/kmap_types.h b/arch/x86/include/asm/kmap_types.h
deleted file mode 100644
index 04ab8266e347..000000000000
--- a/arch/x86/include/asm/kmap_types.h
+++ /dev/null
@@ -1,13 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0 */
-#ifndef _ASM_X86_KMAP_TYPES_H
-#define _ASM_X86_KMAP_TYPES_H
-
-#if defined(CONFIG_X86_32) && defined(CONFIG_DEBUG_HIGHMEM)
-#define  __WITH_KM_FENCE
-#endif
-
-#include <asm-generic/kmap_types.h>
-
-#undef __WITH_KM_FENCE
-
-#endif /* _ASM_X86_KMAP_TYPES_H */
diff --git a/arch/x86/include/asm/paravirt_types.h b/arch/x86/include/asm/paravirt_types.h
index 0fad9f61c76a..b6b02b7c19cc 100644
--- a/arch/x86/include/asm/paravirt_types.h
+++ b/arch/x86/include/asm/paravirt_types.h
@@ -41,7 +41,6 @@
 #ifndef __ASSEMBLY__
 
 #include <asm/desc_defs.h>
-#include <asm/kmap_types.h>
 #include <asm/pgtable_types.h>
 #include <asm/nospec-branch.h>
 
diff --git a/arch/x86/include/asm/preempt.h b/arch/x86/include/asm/preempt.h
index 69485ca13665..471dec2d78e1 100644
--- a/arch/x86/include/asm/preempt.h
+++ b/arch/x86/include/asm/preempt.h
@@ -89,20 +89,54 @@ static __always_inline void __preempt_count_sub(int val)
  * a decrement which hits zero means we have no preempt_count and should
  * reschedule.
  */
-static __always_inline bool __preempt_count_dec_and_test(void)
+static __always_inline bool ____preempt_count_dec_and_test(void)
 {
 	return GEN_UNARY_RMWcc("decl", __preempt_count, e, __percpu_arg([var]));
 }
 
+static __always_inline bool __preempt_count_dec_and_test(void)
+{
+	if (____preempt_count_dec_and_test())
+		return true;
+#ifdef CONFIG_PREEMPT_LAZY
+	if (preempt_count())
+		return false;
+	if (current_thread_info()->preempt_lazy_count)
+		return false;
+	return test_thread_flag(TIF_NEED_RESCHED_LAZY);
+#else
+	return false;
+#endif
+}
+
 /*
  * Returns true when we need to resched and can (barring IRQ state).
  */
 static __always_inline bool should_resched(int preempt_offset)
 {
+#ifdef CONFIG_PREEMPT_LAZY
+	u32 tmp;
+	tmp = raw_cpu_read_4(__preempt_count);
+	if (tmp == preempt_offset)
+		return true;
+
+	/* preempt count == 0 ? */
+	tmp &= ~PREEMPT_NEED_RESCHED;
+	if (tmp != preempt_offset)
+		return false;
+	/* XXX PREEMPT_LOCK_OFFSET */
+	if (current_thread_info()->preempt_lazy_count)
+		return false;
+	return test_thread_flag(TIF_NEED_RESCHED_LAZY);
+#else
 	return unlikely(raw_cpu_read_4(__preempt_count) == preempt_offset);
+#endif
 }
 
 #ifdef CONFIG_PREEMPTION
+#ifdef CONFIG_PREEMPT_RT
+  extern void preempt_schedule_lock(void);
+#endif
   extern asmlinkage void preempt_schedule_thunk(void);
 # define __preempt_schedule() \
 	asm volatile ("call preempt_schedule_thunk" : ASM_CALL_CONSTRAINT)
diff --git a/arch/x86/include/asm/signal.h b/arch/x86/include/asm/signal.h
index 6fd8410a3910..f3bf2f515edb 100644
--- a/arch/x86/include/asm/signal.h
+++ b/arch/x86/include/asm/signal.h
@@ -28,6 +28,19 @@ typedef struct {
 #define SA_IA32_ABI	0x02000000u
 #define SA_X32_ABI	0x01000000u
 
+/*
+ * Because some traps use the IST stack, we must keep preemption
+ * disabled while calling do_trap(), but do_trap() may call
+ * force_sig_info() which will grab the signal spin_locks for the
+ * task, which in PREEMPT_RT are mutexes.  By defining
+ * ARCH_RT_DELAYS_SIGNAL_SEND the force_sig_info() will set
+ * TIF_NOTIFY_RESUME and set up the signal to be sent on exit of the
+ * trap.
+ */
+#if defined(CONFIG_PREEMPT_RT)
+#define ARCH_RT_DELAYS_SIGNAL_SEND
+#endif
+
 #ifndef CONFIG_COMPAT
 typedef sigset_t compat_sigset_t;
 #endif
diff --git a/arch/x86/include/asm/stackprotector.h b/arch/x86/include/asm/stackprotector.h
index 7fb482f0f25b..3df0a95c9e13 100644
--- a/arch/x86/include/asm/stackprotector.h
+++ b/arch/x86/include/asm/stackprotector.h
@@ -65,7 +65,7 @@
  */
 static __always_inline void boot_init_stack_canary(void)
 {
-	u64 canary;
+	u64 canary = 0;
 	u64 tsc;
 
 #ifdef CONFIG_X86_64
@@ -76,8 +76,14 @@ static __always_inline void boot_init_stack_canary(void)
 	 * of randomness. The TSC only matters for very early init,
 	 * there it already has some randomness on most systems. Later
 	 * on during the bootup the random pool has true entropy too.
+	 * For preempt-rt we need to weaken the randomness a bit, as
+	 * we can't call into the random generator from atomic context
+	 * due to locking constraints. We just leave canary
+	 * uninitialized and use the TSC based randomness on top of it.
 	 */
+#ifndef CONFIG_PREEMPT_RT
 	get_random_bytes(&canary, sizeof(canary));
+#endif
 	tsc = rdtsc();
 	canary += tsc + (tsc << 32UL);
 	canary &= CANARY_MASK;
diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 44733a4bfc42..f16f2ea58be4 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -56,17 +56,24 @@ struct task_struct;
 struct thread_info {
 	unsigned long		flags;		/* low level flags */
 	u32			status;		/* thread synchronous flags */
+	int			preempt_lazy_count;	/* 0 => lazy preemptable
+							  <0 => BUG */
 };
 
 #define INIT_THREAD_INFO(tsk)			\
 {						\
 	.flags		= 0,			\
+	.preempt_lazy_count = 0,		\
 }
 
 #else /* !__ASSEMBLY__ */
 
 #include <asm/asm-offsets.h>
 
+#define GET_THREAD_INFO(reg) \
+	_ASM_MOV PER_CPU_VAR(cpu_current_top_of_stack),reg ; \
+	_ASM_SUB $(THREAD_SIZE),reg ;
+
 #endif
 
 /*
@@ -93,6 +100,7 @@ struct thread_info {
 #define TIF_NOTSC		16	/* TSC is not accessible in userland */
 #define TIF_IA32		17	/* IA32 compatibility process */
 #define TIF_SLD			18	/* Restore split lock detection on context switch */
+#define TIF_NEED_RESCHED_LAZY	19	/* lazy rescheduling necessary */
 #define TIF_MEMDIE		20	/* is terminating due to OOM killer */
 #define TIF_POLLING_NRFLAG	21	/* idle is polling for TIF_NEED_RESCHED */
 #define TIF_IO_BITMAP		22	/* uses I/O bitmap */
@@ -122,6 +130,7 @@ struct thread_info {
 #define _TIF_NOTSC		(1 << TIF_NOTSC)
 #define _TIF_IA32		(1 << TIF_IA32)
 #define _TIF_SLD		(1 << TIF_SLD)
+#define _TIF_NEED_RESCHED_LAZY	(1 << TIF_NEED_RESCHED_LAZY)
 #define _TIF_POLLING_NRFLAG	(1 << TIF_POLLING_NRFLAG)
 #define _TIF_IO_BITMAP		(1 << TIF_IO_BITMAP)
 #define _TIF_FORCED_TF		(1 << TIF_FORCED_TF)
@@ -154,6 +163,8 @@ struct thread_info {
 
 #define _TIF_WORK_CTXSW_NEXT	(_TIF_WORK_CTXSW)
 
+#define _TIF_NEED_RESCHED_MASK	(_TIF_NEED_RESCHED | _TIF_NEED_RESCHED_LAZY)
+
 #define STACK_WARN		(THREAD_SIZE/8)
 
 /*
diff --git a/arch/x86/kernel/cpu/mshyperv.c b/arch/x86/kernel/cpu/mshyperv.c
index 6cc50ab07bde..4055fe3ae869 100644
--- a/arch/x86/kernel/cpu/mshyperv.c
+++ b/arch/x86/kernel/cpu/mshyperv.c
@@ -80,11 +80,12 @@ EXPORT_SYMBOL_GPL(hv_remove_vmbus_irq);
 DEFINE_IDTENTRY_SYSVEC(sysvec_hyperv_stimer0)
 {
 	struct pt_regs *old_regs = set_irq_regs(regs);
+	u64 ip = regs ? instruction_pointer(regs) : 0;
 
 	inc_irq_stat(hyperv_stimer0_count);
 	if (hv_stimer0_handler)
 		hv_stimer0_handler();
-	add_interrupt_randomness(HYPERV_STIMER0_VECTOR, 0);
+	add_interrupt_randomness(HYPERV_STIMER0_VECTOR, 0, ip);
 	ack_APIC_irq();
 
 	set_irq_regs(old_regs);
diff --git a/arch/x86/kernel/crash_dump_32.c b/arch/x86/kernel/crash_dump_32.c
index 33ee47670b99..5fcac46aaf6b 100644
--- a/arch/x86/kernel/crash_dump_32.c
+++ b/arch/x86/kernel/crash_dump_32.c
@@ -13,8 +13,6 @@
 
 #include <linux/uaccess.h>
 
-static void *kdump_buf_page;
-
 static inline bool is_crashed_pfn_valid(unsigned long pfn)
 {
 #ifndef CONFIG_X86_PAE
@@ -41,15 +39,11 @@ static inline bool is_crashed_pfn_valid(unsigned long pfn)
  * @userbuf: if set, @buf is in user address space, use copy_to_user(),
  *	otherwise @buf is in kernel address space, use memcpy().
  *
- * Copy a page from "oldmem". For this page, there is no pte mapped
- * in the current kernel. We stitch up a pte, similar to kmap_atomic.
- *
- * Calling copy_to_user() in atomic context is not desirable. Hence first
- * copying the data to a pre-allocated kernel page and then copying to user
- * space in non-atomic context.
+ * Copy a page from "oldmem". For this page, there might be no pte mapped
+ * in the current kernel.
  */
-ssize_t copy_oldmem_page(unsigned long pfn, char *buf,
-                               size_t csize, unsigned long offset, int userbuf)
+ssize_t copy_oldmem_page(unsigned long pfn, char *buf, size_t csize,
+			 unsigned long offset, int userbuf)
 {
 	void  *vaddr;
 
@@ -59,38 +53,16 @@ ssize_t copy_oldmem_page(unsigned long pfn, char *buf,
 	if (!is_crashed_pfn_valid(pfn))
 		return -EFAULT;
 
-	vaddr = kmap_atomic_pfn(pfn);
+	vaddr = kmap_local_pfn(pfn);
 
 	if (!userbuf) {
-		memcpy(buf, (vaddr + offset), csize);
-		kunmap_atomic(vaddr);
+		memcpy(buf, vaddr + offset, csize);
 	} else {
-		if (!kdump_buf_page) {
-			printk(KERN_WARNING "Kdump: Kdump buffer page not"
-				" allocated\n");
-			kunmap_atomic(vaddr);
-			return -EFAULT;
-		}
-		copy_page(kdump_buf_page, vaddr);
-		kunmap_atomic(vaddr);
-		if (copy_to_user(buf, (kdump_buf_page + offset), csize))
-			return -EFAULT;
+		if (copy_to_user(buf, vaddr + offset, csize))
+			csize = -EFAULT;
 	}
 
-	return csize;
-}
+	kunmap_local(vaddr);
 
-static int __init kdump_buf_page_init(void)
-{
-	int ret = 0;
-
-	kdump_buf_page = kmalloc(PAGE_SIZE, GFP_KERNEL);
-	if (!kdump_buf_page) {
-		printk(KERN_WARNING "Kdump: Failed to allocate kdump buffer"
-			 " page\n");
-		ret = -ENOMEM;
-	}
-
-	return ret;
+	return csize;
 }
-arch_initcall(kdump_buf_page_init);
diff --git a/arch/x86/kernel/fpu/core.c b/arch/x86/kernel/fpu/core.c
index 571220ac8bea..d315d45b64fa 100644
--- a/arch/x86/kernel/fpu/core.c
+++ b/arch/x86/kernel/fpu/core.c
@@ -159,6 +159,18 @@ void kernel_fpu_end(void)
 }
 EXPORT_SYMBOL_GPL(kernel_fpu_end);
 
+void kernel_fpu_resched(void)
+{
+	WARN_ON_FPU(!this_cpu_read(in_kernel_fpu));
+
+	if (should_resched(PREEMPT_OFFSET)) {
+		kernel_fpu_end();
+		cond_resched();
+		kernel_fpu_begin();
+	}
+}
+EXPORT_SYMBOL_GPL(kernel_fpu_resched);
+
 /*
  * Save the FPU state (mark it for reload if necessary):
  *
diff --git a/arch/x86/kernel/irq_32.c b/arch/x86/kernel/irq_32.c
index 0b79efc87be5..93c6b88b382a 100644
--- a/arch/x86/kernel/irq_32.c
+++ b/arch/x86/kernel/irq_32.c
@@ -131,6 +131,7 @@ int irq_init_percpu_irqstack(unsigned int cpu)
 	return 0;
 }
 
+#ifndef CONFIG_PREEMPT_RT
 void do_softirq_own_stack(void)
 {
 	struct irq_stack *irqstk;
@@ -147,6 +148,7 @@ void do_softirq_own_stack(void)
 
 	call_on_stack(__do_softirq, isp);
 }
+#endif
 
 void __handle_irq(struct irq_desc *desc, struct pt_regs *regs)
 {
diff --git a/arch/x86/kernel/irq_64.c b/arch/x86/kernel/irq_64.c
index 440eed558558..7cfc4e6b7c94 100644
--- a/arch/x86/kernel/irq_64.c
+++ b/arch/x86/kernel/irq_64.c
@@ -72,7 +72,9 @@ int irq_init_percpu_irqstack(unsigned int cpu)
 	return map_irq_stack(cpu);
 }
 
+#ifndef CONFIG_PREEMPT_RT
 void do_softirq_own_stack(void)
 {
 	run_on_irqstack_cond(__do_softirq, NULL);
 }
+#endif
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index fa5f059c2b94..9503e0500af8 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -7884,6 +7884,14 @@ int kvm_arch_init(void *opaque)
 		goto out;
 	}
 
+#ifdef CONFIG_PREEMPT_RT
+	if (!boot_cpu_has(X86_FEATURE_CONSTANT_TSC)) {
+		pr_err("RT requires X86_FEATURE_CONSTANT_TSC\n");
+		r = -EOPNOTSUPP;
+		goto out;
+	}
+#endif
+
 	r = -ENOMEM;
 	x86_fpu_cache = kmem_cache_create("x86_fpu", sizeof(struct fpu),
 					  __alignof__(struct fpu), SLAB_ACCOUNT,
diff --git a/arch/x86/mm/highmem_32.c b/arch/x86/mm/highmem_32.c
index 075fe51317b0..2c54b76d8f84 100644
--- a/arch/x86/mm/highmem_32.c
+++ b/arch/x86/mm/highmem_32.c
@@ -4,65 +4,6 @@
 #include <linux/swap.h> /* for totalram_pages */
 #include <linux/memblock.h>
 
-void *kmap_atomic_high_prot(struct page *page, pgprot_t prot)
-{
-	unsigned long vaddr;
-	int idx, type;
-
-	type = kmap_atomic_idx_push();
-	idx = type + KM_TYPE_NR*smp_processor_id();
-	vaddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);
-	BUG_ON(!pte_none(*(kmap_pte-idx)));
-	set_pte(kmap_pte-idx, mk_pte(page, prot));
-	arch_flush_lazy_mmu_mode();
-
-	return (void *)vaddr;
-}
-EXPORT_SYMBOL(kmap_atomic_high_prot);
-
-/*
- * This is the same as kmap_atomic() but can map memory that doesn't
- * have a struct page associated with it.
- */
-void *kmap_atomic_pfn(unsigned long pfn)
-{
-	return kmap_atomic_prot_pfn(pfn, kmap_prot);
-}
-EXPORT_SYMBOL_GPL(kmap_atomic_pfn);
-
-void kunmap_atomic_high(void *kvaddr)
-{
-	unsigned long vaddr = (unsigned long) kvaddr & PAGE_MASK;
-
-	if (vaddr >= __fix_to_virt(FIX_KMAP_END) &&
-	    vaddr <= __fix_to_virt(FIX_KMAP_BEGIN)) {
-		int idx, type;
-
-		type = kmap_atomic_idx();
-		idx = type + KM_TYPE_NR * smp_processor_id();
-
-#ifdef CONFIG_DEBUG_HIGHMEM
-		WARN_ON_ONCE(vaddr != __fix_to_virt(FIX_KMAP_BEGIN + idx));
-#endif
-		/*
-		 * Force other mappings to Oops if they'll try to access this
-		 * pte without first remap it.  Keeping stale mappings around
-		 * is a bad idea also, in case the page changes cacheability
-		 * attributes or becomes a protected page in a hypervisor.
-		 */
-		kpte_clear_flush(kmap_pte-idx, vaddr);
-		kmap_atomic_idx_pop();
-		arch_flush_lazy_mmu_mode();
-	}
-#ifdef CONFIG_DEBUG_HIGHMEM
-	else {
-		BUG_ON(vaddr < PAGE_OFFSET);
-		BUG_ON(vaddr >= (unsigned long)high_memory);
-	}
-#endif
-}
-EXPORT_SYMBOL(kunmap_atomic_high);
-
 void __init set_highmem_pages_init(void)
 {
 	struct zone *zone;
diff --git a/arch/x86/mm/init_32.c b/arch/x86/mm/init_32.c
index 7c055259de3a..da31c2635ee4 100644
--- a/arch/x86/mm/init_32.c
+++ b/arch/x86/mm/init_32.c
@@ -394,19 +394,6 @@ kernel_physical_mapping_init(unsigned long start,
 	return last_map_addr;
 }
 
-pte_t *kmap_pte;
-
-static void __init kmap_init(void)
-{
-	unsigned long kmap_vstart;
-
-	/*
-	 * Cache the first kmap pte:
-	 */
-	kmap_vstart = __fix_to_virt(FIX_KMAP_BEGIN);
-	kmap_pte = virt_to_kpte(kmap_vstart);
-}
-
 #ifdef CONFIG_HIGHMEM
 static void __init permanent_kmaps_init(pgd_t *pgd_base)
 {
@@ -712,8 +699,6 @@ void __init paging_init(void)
 
 	__flush_tlb_all();
 
-	kmap_init();
-
 	/*
 	 * NOTE: at this point the bootmem allocator is fully available.
 	 */
diff --git a/arch/x86/mm/iomap_32.c b/arch/x86/mm/iomap_32.c
index f60398aeb644..9aaa756ddf21 100644
--- a/arch/x86/mm/iomap_32.c
+++ b/arch/x86/mm/iomap_32.c
@@ -44,28 +44,7 @@ void iomap_free(resource_size_t base, unsigned long size)
 }
 EXPORT_SYMBOL_GPL(iomap_free);
 
-void *kmap_atomic_prot_pfn(unsigned long pfn, pgprot_t prot)
-{
-	unsigned long vaddr;
-	int idx, type;
-
-	preempt_disable();
-	pagefault_disable();
-
-	type = kmap_atomic_idx_push();
-	idx = type + KM_TYPE_NR * smp_processor_id();
-	vaddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);
-	set_pte(kmap_pte - idx, pfn_pte(pfn, prot));
-	arch_flush_lazy_mmu_mode();
-
-	return (void *)vaddr;
-}
-
-/*
- * Map 'pfn' using protections 'prot'
- */
-void __iomem *
-iomap_atomic_prot_pfn(unsigned long pfn, pgprot_t prot)
+void __iomem *__iomap_local_pfn_prot(unsigned long pfn, pgprot_t prot)
 {
 	/*
 	 * For non-PAT systems, translate non-WB request to UC- just in
@@ -81,36 +60,6 @@ iomap_atomic_prot_pfn(unsigned long pfn, pgprot_t prot)
 	/* Filter out unsupported __PAGE_KERNEL* bits: */
 	pgprot_val(prot) &= __default_kernel_pte_mask;
 
-	return (void __force __iomem *) kmap_atomic_prot_pfn(pfn, prot);
-}
-EXPORT_SYMBOL_GPL(iomap_atomic_prot_pfn);
-
-void
-iounmap_atomic(void __iomem *kvaddr)
-{
-	unsigned long vaddr = (unsigned long) kvaddr & PAGE_MASK;
-
-	if (vaddr >= __fix_to_virt(FIX_KMAP_END) &&
-	    vaddr <= __fix_to_virt(FIX_KMAP_BEGIN)) {
-		int idx, type;
-
-		type = kmap_atomic_idx();
-		idx = type + KM_TYPE_NR * smp_processor_id();
-
-#ifdef CONFIG_DEBUG_HIGHMEM
-		WARN_ON_ONCE(vaddr != __fix_to_virt(FIX_KMAP_BEGIN + idx));
-#endif
-		/*
-		 * Force other mappings to Oops if they'll try to access this
-		 * pte without first remap it.  Keeping stale mappings around
-		 * is a bad idea also, in case the page changes cacheability
-		 * attributes or becomes a protected page in a hypervisor.
-		 */
-		kpte_clear_flush(kmap_pte-idx, vaddr);
-		kmap_atomic_idx_pop();
-	}
-
-	pagefault_enable();
-	preempt_enable();
+	return (void __force __iomem *)__kmap_local_pfn_prot(pfn, prot);
 }
-EXPORT_SYMBOL_GPL(iounmap_atomic);
+EXPORT_SYMBOL_GPL(__iomap_local_pfn_prot);
diff --git a/arch/xtensa/Kconfig b/arch/xtensa/Kconfig
index d0dfa50bd0bb..dc22ef3cf4be 100644
--- a/arch/xtensa/Kconfig
+++ b/arch/xtensa/Kconfig
@@ -666,6 +666,7 @@ endchoice
 config HIGHMEM
 	bool "High Memory Support"
 	depends on MMU
+	select KMAP_LOCAL
 	help
 	  Linux can use the full amount of RAM in the system by
 	  default. However, the default MMUv2 setup only maps the
diff --git a/arch/xtensa/include/asm/fixmap.h b/arch/xtensa/include/asm/fixmap.h
index a06ffb0c61c7..92049b61c351 100644
--- a/arch/xtensa/include/asm/fixmap.h
+++ b/arch/xtensa/include/asm/fixmap.h
@@ -16,7 +16,7 @@
 #ifdef CONFIG_HIGHMEM
 #include <linux/threads.h>
 #include <linux/pgtable.h>
-#include <asm/kmap_types.h>
+#include <asm/kmap_size.h>
 #endif
 
 /*
@@ -39,7 +39,7 @@ enum fixed_addresses {
 	/* reserved pte's for temporary kernel mappings */
 	FIX_KMAP_BEGIN,
 	FIX_KMAP_END = FIX_KMAP_BEGIN +
-		(KM_TYPE_NR * NR_CPUS * DCACHE_N_COLORS) - 1,
+		(KM_MAX_IDX * NR_CPUS * DCACHE_N_COLORS) - 1,
 #endif
 	__end_of_fixed_addresses
 };
diff --git a/arch/xtensa/include/asm/highmem.h b/arch/xtensa/include/asm/highmem.h
index eac503215f17..0fc3b1cebc56 100644
--- a/arch/xtensa/include/asm/highmem.h
+++ b/arch/xtensa/include/asm/highmem.h
@@ -16,9 +16,8 @@
 #include <linux/pgtable.h>
 #include <asm/cacheflush.h>
 #include <asm/fixmap.h>
-#include <asm/kmap_types.h>
 
-#define PKMAP_BASE		((FIXADDR_START - \
+#define PKMAP_BASE		((FIXADDR_START -			\
 				  (LAST_PKMAP + 1) * PAGE_SIZE) & PMD_MASK)
 #define LAST_PKMAP		(PTRS_PER_PTE * DCACHE_N_COLORS)
 #define LAST_PKMAP_MASK		(LAST_PKMAP - 1)
@@ -68,6 +67,15 @@ static inline void flush_cache_kmaps(void)
 	flush_cache_all();
 }
 
+enum fixed_addresses kmap_local_map_idx(int type, unsigned long pfn);
+#define arch_kmap_local_map_idx		kmap_local_map_idx
+
+enum fixed_addresses kmap_local_unmap_idx(int type, unsigned long addr);
+#define arch_kmap_local_unmap_idx	kmap_local_unmap_idx
+
+#define arch_kmap_local_post_unmap(vaddr)	\
+	local_flush_tlb_kernel_range(vaddr, vaddr + PAGE_SIZE)
+
 void kmap_init(void);
 
 #endif
diff --git a/arch/xtensa/include/asm/spinlock_types.h b/arch/xtensa/include/asm/spinlock_types.h
index 64c9389254f1..dc846323b1cd 100644
--- a/arch/xtensa/include/asm/spinlock_types.h
+++ b/arch/xtensa/include/asm/spinlock_types.h
@@ -2,10 +2,6 @@
 #ifndef __ASM_SPINLOCK_TYPES_H
 #define __ASM_SPINLOCK_TYPES_H
 
-#if !defined(__LINUX_SPINLOCK_TYPES_H) && !defined(__ASM_SPINLOCK_H)
-# error "please don't include this file directly"
-#endif
-
 #include <asm-generic/qspinlock_types.h>
 #include <asm-generic/qrwlock_types.h>
 
diff --git a/arch/xtensa/mm/highmem.c b/arch/xtensa/mm/highmem.c
index 673196fe862e..0735ca5e8f86 100644
--- a/arch/xtensa/mm/highmem.c
+++ b/arch/xtensa/mm/highmem.c
@@ -12,8 +12,6 @@
 #include <linux/highmem.h>
 #include <asm/tlbflush.h>
 
-static pte_t *kmap_pte;
-
 #if DCACHE_WAY_SIZE > PAGE_SIZE
 unsigned int last_pkmap_nr_arr[DCACHE_N_COLORS];
 wait_queue_head_t pkmap_map_wait_arr[DCACHE_N_COLORS];
@@ -33,59 +31,25 @@ static inline void kmap_waitqueues_init(void)
 
 static inline enum fixed_addresses kmap_idx(int type, unsigned long color)
 {
-	return (type + KM_TYPE_NR * smp_processor_id()) * DCACHE_N_COLORS +
+	return (type + KM_MAX_IDX * smp_processor_id()) * DCACHE_N_COLORS +
 		color;
 }
 
-void *kmap_atomic_high_prot(struct page *page, pgprot_t prot)
+enum fixed_addresses kmap_local_map_idx(int type, unsigned long pfn)
 {
-	enum fixed_addresses idx;
-	unsigned long vaddr;
-
-	idx = kmap_idx(kmap_atomic_idx_push(),
-		       DCACHE_ALIAS(page_to_phys(page)));
-	vaddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);
-#ifdef CONFIG_DEBUG_HIGHMEM
-	BUG_ON(!pte_none(*(kmap_pte + idx)));
-#endif
-	set_pte(kmap_pte + idx, mk_pte(page, prot));
-
-	return (void *)vaddr;
+	return kmap_idx(type, DCACHE_ALIAS(pfn << PAGE_SHIFT));
 }
-EXPORT_SYMBOL(kmap_atomic_high_prot);
 
-void kunmap_atomic_high(void *kvaddr)
+enum fixed_addresses kmap_local_unmap_idx(int type, unsigned long addr)
 {
-	if (kvaddr >= (void *)FIXADDR_START &&
-	    kvaddr < (void *)FIXADDR_TOP) {
-		int idx = kmap_idx(kmap_atomic_idx(),
-				   DCACHE_ALIAS((unsigned long)kvaddr));
-
-		/*
-		 * Force other mappings to Oops if they'll try to access this
-		 * pte without first remap it.  Keeping stale mappings around
-		 * is a bad idea also, in case the page changes cacheability
-		 * attributes or becomes a protected page in a hypervisor.
-		 */
-		pte_clear(&init_mm, kvaddr, kmap_pte + idx);
-		local_flush_tlb_kernel_range((unsigned long)kvaddr,
-					     (unsigned long)kvaddr + PAGE_SIZE);
-
-		kmap_atomic_idx_pop();
-	}
+	return kmap_idx(type, DCACHE_ALIAS(addr));
 }
-EXPORT_SYMBOL(kunmap_atomic_high);
 
 void __init kmap_init(void)
 {
-	unsigned long kmap_vstart;
-
 	/* Check if this memory layout is broken because PKMAP overlaps
 	 * page table.
 	 */
 	BUILD_BUG_ON(PKMAP_BASE < TLBTEMP_BASE_1 + TLBTEMP_SIZE);
-	/* cache the first kmap pte */
-	kmap_vstart = __fix_to_virt(FIX_KMAP_BEGIN);
-	kmap_pte = virt_to_kpte(kmap_vstart);
 	kmap_waitqueues_init();
 }
diff --git a/block/blk-mq.c b/block/blk-mq.c
index 2a1eff60c797..b293f74ea8ca 100644
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@ -41,7 +41,7 @@
 #include "blk-mq-sched.h"
 #include "blk-rq-qos.h"
 
-static DEFINE_PER_CPU(struct list_head, blk_cpu_done);
+static DEFINE_PER_CPU(struct llist_head, blk_cpu_done);
 
 static void blk_mq_poll_stats_start(struct request_queue *q);
 static void blk_mq_poll_stats_fn(struct blk_stat_callback *cb);
@@ -565,80 +565,29 @@ void blk_mq_end_request(struct request *rq, blk_status_t error)
 }
 EXPORT_SYMBOL(blk_mq_end_request);
 
-/*
- * Softirq action handler - move entries to local list and loop over them
- * while passing them to the queue registered handler.
- */
-static __latent_entropy void blk_done_softirq(struct softirq_action *h)
+static void blk_complete_reqs(struct llist_head *list)
 {
-	struct list_head *cpu_list, local_list;
-
-	local_irq_disable();
-	cpu_list = this_cpu_ptr(&blk_cpu_done);
-	list_replace_init(cpu_list, &local_list);
-	local_irq_enable();
-
-	while (!list_empty(&local_list)) {
-		struct request *rq;
+	struct llist_node *entry = llist_reverse_order(llist_del_all(list));
+	struct request *rq, *next;
 
-		rq = list_entry(local_list.next, struct request, ipi_list);
-		list_del_init(&rq->ipi_list);
+	llist_for_each_entry_safe(rq, next, entry, ipi_list)
 		rq->q->mq_ops->complete(rq);
-	}
 }
 
-static void blk_mq_trigger_softirq(struct request *rq)
+static __latent_entropy void blk_done_softirq(struct softirq_action *h)
 {
-	struct list_head *list;
-	unsigned long flags;
-
-	local_irq_save(flags);
-	list = this_cpu_ptr(&blk_cpu_done);
-	list_add_tail(&rq->ipi_list, list);
-
-	/*
-	 * If the list only contains our just added request, signal a raise of
-	 * the softirq.  If there are already entries there, someone already
-	 * raised the irq but it hasn't run yet.
-	 */
-	if (list->next == &rq->ipi_list)
-		raise_softirq_irqoff(BLOCK_SOFTIRQ);
-	local_irq_restore(flags);
+	blk_complete_reqs(this_cpu_ptr(&blk_cpu_done));
 }
 
 static int blk_softirq_cpu_dead(unsigned int cpu)
 {
-	/*
-	 * If a CPU goes away, splice its entries to the current CPU
-	 * and trigger a run of the softirq
-	 */
-	local_irq_disable();
-	list_splice_init(&per_cpu(blk_cpu_done, cpu),
-			 this_cpu_ptr(&blk_cpu_done));
-	raise_softirq_irqoff(BLOCK_SOFTIRQ);
-	local_irq_enable();
-
+	blk_complete_reqs(&per_cpu(blk_cpu_done, cpu));
 	return 0;
 }
 
-
 static void __blk_mq_complete_request_remote(void *data)
 {
-	struct request *rq = data;
-
-	/*
-	 * For most of single queue controllers, there is only one irq vector
-	 * for handling I/O completion, and the only irq's affinity is set
-	 * to all possible CPUs.  On most of ARCHs, this affinity means the irq
-	 * is handled on one specific CPU.
-	 *
-	 * So complete I/O requests in softirq context in case of single queue
-	 * devices to avoid degrading I/O performance due to irqsoff latency.
-	 */
-	if (rq->q->nr_hw_queues == 1)
-		blk_mq_trigger_softirq(rq);
-	else
-		rq->q->mq_ops->complete(rq);
+	__raise_softirq_irqoff(BLOCK_SOFTIRQ);
 }
 
 static inline bool blk_mq_complete_need_ipi(struct request *rq)
@@ -648,6 +597,14 @@ static inline bool blk_mq_complete_need_ipi(struct request *rq)
 	if (!IS_ENABLED(CONFIG_SMP) ||
 	    !test_bit(QUEUE_FLAG_SAME_COMP, &rq->q->queue_flags))
 		return false;
+	/*
+	 * With force threaded interrupts enabled, raising softirq from an SMP
+	 * function call will always result in waking the ksoftirqd thread.
+	 * This is probably worse than completing the request on a different
+	 * cache domain.
+	 */
+	if (force_irqthreads)
+		return false;
 
 	/* same CPU or cache domain?  Complete locally */
 	if (cpu == rq->mq_ctx->cpu ||
@@ -659,6 +616,32 @@ static inline bool blk_mq_complete_need_ipi(struct request *rq)
 	return cpu_online(rq->mq_ctx->cpu);
 }
 
+static void blk_mq_complete_send_ipi(struct request *rq)
+{
+	struct llist_head *list;
+	unsigned int cpu;
+
+	cpu = rq->mq_ctx->cpu;
+	list = &per_cpu(blk_cpu_done, cpu);
+	if (llist_add(&rq->ipi_list, list)) {
+		rq->csd.func = __blk_mq_complete_request_remote;
+		rq->csd.info = rq;
+		rq->csd.flags = 0;
+		smp_call_function_single_async(cpu, &rq->csd);
+	}
+}
+
+static void blk_mq_raise_softirq(struct request *rq)
+{
+	struct llist_head *list;
+
+	preempt_disable();
+	list = this_cpu_ptr(&blk_cpu_done);
+	if (llist_add(&rq->ipi_list, list))
+		raise_softirq(BLOCK_SOFTIRQ);
+	preempt_enable();
+}
+
 bool blk_mq_complete_request_remote(struct request *rq)
 {
 	WRITE_ONCE(rq->state, MQ_RQ_COMPLETE);
@@ -671,17 +654,15 @@ bool blk_mq_complete_request_remote(struct request *rq)
 		return false;
 
 	if (blk_mq_complete_need_ipi(rq)) {
-		rq->csd.func = __blk_mq_complete_request_remote;
-		rq->csd.info = rq;
-		rq->csd.flags = 0;
-		smp_call_function_single_async(rq->mq_ctx->cpu, &rq->csd);
-	} else {
-		if (rq->q->nr_hw_queues > 1)
-			return false;
-		blk_mq_trigger_softirq(rq);
+		blk_mq_complete_send_ipi(rq);
+		return true;
 	}
 
-	return true;
+	if (rq->q->nr_hw_queues == 1) {
+		blk_mq_raise_softirq(rq);
+		return true;
+	}
+	return false;
 }
 EXPORT_SYMBOL_GPL(blk_mq_complete_request_remote);
 
@@ -1604,14 +1585,14 @@ static void __blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async,
 		return;
 
 	if (!async && !(hctx->flags & BLK_MQ_F_BLOCKING)) {
-		int cpu = get_cpu();
+		int cpu = get_cpu_light();
 		if (cpumask_test_cpu(cpu, hctx->cpumask)) {
 			__blk_mq_run_hw_queue(hctx);
-			put_cpu();
+			put_cpu_light();
 			return;
 		}
 
-		put_cpu();
+		put_cpu_light();
 	}
 
 	kblockd_mod_delayed_work_on(blk_mq_hctx_next_cpu(hctx), &hctx->run_work,
@@ -3909,7 +3890,7 @@ static int __init blk_mq_init(void)
 	int i;
 
 	for_each_possible_cpu(i)
-		INIT_LIST_HEAD(&per_cpu(blk_cpu_done, i));
+		init_llist_head(&per_cpu(blk_cpu_done, i));
 	open_softirq(BLOCK_SOFTIRQ, blk_done_softirq);
 
 	cpuhp_setup_state_nocalls(CPUHP_BLOCK_SOFTIRQ_DEAD,
diff --git a/crypto/cryptd.c b/crypto/cryptd.c
index a1bea0f4baa8..5f8ca8c1f59c 100644
--- a/crypto/cryptd.c
+++ b/crypto/cryptd.c
@@ -36,6 +36,7 @@ static struct workqueue_struct *cryptd_wq;
 struct cryptd_cpu_queue {
 	struct crypto_queue queue;
 	struct work_struct work;
+	spinlock_t qlock;
 };
 
 struct cryptd_queue {
@@ -105,6 +106,7 @@ static int cryptd_init_queue(struct cryptd_queue *queue,
 		cpu_queue = per_cpu_ptr(queue->cpu_queue, cpu);
 		crypto_init_queue(&cpu_queue->queue, max_cpu_qlen);
 		INIT_WORK(&cpu_queue->work, cryptd_queue_worker);
+		spin_lock_init(&cpu_queue->qlock);
 	}
 	pr_info("cryptd: max_cpu_qlen set to %d\n", max_cpu_qlen);
 	return 0;
@@ -129,8 +131,10 @@ static int cryptd_enqueue_request(struct cryptd_queue *queue,
 	struct cryptd_cpu_queue *cpu_queue;
 	refcount_t *refcnt;
 
-	cpu = get_cpu();
-	cpu_queue = this_cpu_ptr(queue->cpu_queue);
+	cpu_queue = raw_cpu_ptr(queue->cpu_queue);
+	spin_lock_bh(&cpu_queue->qlock);
+	cpu = smp_processor_id();
+
 	err = crypto_enqueue_request(&cpu_queue->queue, request);
 
 	refcnt = crypto_tfm_ctx(request->tfm);
@@ -146,7 +150,7 @@ static int cryptd_enqueue_request(struct cryptd_queue *queue,
 	refcount_inc(refcnt);
 
 out_put_cpu:
-	put_cpu();
+	spin_unlock_bh(&cpu_queue->qlock);
 
 	return err;
 }
@@ -162,16 +166,11 @@ static void cryptd_queue_worker(struct work_struct *work)
 	cpu_queue = container_of(work, struct cryptd_cpu_queue, work);
 	/*
 	 * Only handle one request at a time to avoid hogging crypto workqueue.
-	 * preempt_disable/enable is used to prevent being preempted by
-	 * cryptd_enqueue_request(). local_bh_disable/enable is used to prevent
-	 * cryptd_enqueue_request() being accessed from software interrupts.
 	 */
-	local_bh_disable();
-	preempt_disable();
+	spin_lock_bh(&cpu_queue->qlock);
 	backlog = crypto_get_backlog(&cpu_queue->queue);
 	req = crypto_dequeue_request(&cpu_queue->queue);
-	preempt_enable();
-	local_bh_enable();
+	spin_unlock_bh(&cpu_queue->qlock);
 
 	if (!req)
 		return;
diff --git a/drivers/atm/eni.c b/drivers/atm/eni.c
index 316a9947541f..e96a4e8a4a10 100644
--- a/drivers/atm/eni.c
+++ b/drivers/atm/eni.c
@@ -2054,7 +2054,7 @@ static int eni_send(struct atm_vcc *vcc,struct sk_buff *skb)
 	}
 	submitted++;
 	ATM_SKB(skb)->vcc = vcc;
-	tasklet_disable(&ENI_DEV(vcc->dev)->task);
+	tasklet_disable_in_atomic(&ENI_DEV(vcc->dev)->task);
 	res = do_tx(skb);
 	tasklet_enable(&ENI_DEV(vcc->dev)->task);
 	if (res == enq_ok) return 0;
diff --git a/drivers/block/zram/zram_drv.c b/drivers/block/zram/zram_drv.c
index 711168451e9e..64ed15dac678 100644
--- a/drivers/block/zram/zram_drv.c
+++ b/drivers/block/zram/zram_drv.c
@@ -59,6 +59,40 @@ static void zram_free_page(struct zram *zram, size_t index);
 static int zram_bvec_read(struct zram *zram, struct bio_vec *bvec,
 				u32 index, int offset, struct bio *bio);
 
+#ifdef CONFIG_PREEMPT_RT
+static void zram_meta_init_table_locks(struct zram *zram, size_t num_pages)
+{
+	size_t index;
+
+	for (index = 0; index < num_pages; index++)
+		spin_lock_init(&zram->table[index].lock);
+}
+
+static int zram_slot_trylock(struct zram *zram, u32 index)
+{
+	int ret;
+
+	ret = spin_trylock(&zram->table[index].lock);
+	if (ret)
+		__set_bit(ZRAM_LOCK, &zram->table[index].flags);
+	return ret;
+}
+
+static void zram_slot_lock(struct zram *zram, u32 index)
+{
+	spin_lock(&zram->table[index].lock);
+	__set_bit(ZRAM_LOCK, &zram->table[index].flags);
+}
+
+static void zram_slot_unlock(struct zram *zram, u32 index)
+{
+	__clear_bit(ZRAM_LOCK, &zram->table[index].flags);
+	spin_unlock(&zram->table[index].lock);
+}
+
+#else
+
+static void zram_meta_init_table_locks(struct zram *zram, size_t num_pages) { }
 
 static int zram_slot_trylock(struct zram *zram, u32 index)
 {
@@ -74,6 +108,7 @@ static void zram_slot_unlock(struct zram *zram, u32 index)
 {
 	bit_spin_unlock(ZRAM_LOCK, &zram->table[index].flags);
 }
+#endif
 
 static inline bool init_done(struct zram *zram)
 {
@@ -1160,6 +1195,7 @@ static bool zram_meta_alloc(struct zram *zram, u64 disksize)
 
 	if (!huge_class_size)
 		huge_class_size = zs_huge_class_size(zram->mem_pool);
+	zram_meta_init_table_locks(zram, num_pages);
 	return true;
 }
 
diff --git a/drivers/block/zram/zram_drv.h b/drivers/block/zram/zram_drv.h
index f2fd46daa760..7e4dd447e1dd 100644
--- a/drivers/block/zram/zram_drv.h
+++ b/drivers/block/zram/zram_drv.h
@@ -63,6 +63,7 @@ struct zram_table_entry {
 		unsigned long element;
 	};
 	unsigned long flags;
+	spinlock_t lock;
 #ifdef CONFIG_ZRAM_MEMORY_TRACKING
 	ktime_t ac_time;
 #endif
diff --git a/drivers/char/Kconfig b/drivers/char/Kconfig
index d229a2d0c017..32d27df6274b 100644
--- a/drivers/char/Kconfig
+++ b/drivers/char/Kconfig
@@ -471,6 +471,15 @@ config ADI
 	  and SSM (Silicon Secured Memory).  Intended consumers of this
 	  driver include crash and makedumpfile.
 
+config RMD
+	tristate "Rocket Master Device Driver"
+	depends on OF_ADDRESS && OF_IRQ && HAS_DMA
+	help
+	  The RMD driver interfaces with a peripheral designed on
+	  a programmable logic (FPGA).
+
+	  If unsure, say N.
+
 endmenu
 
 config RANDOM_TRUST_CPU
diff --git a/drivers/char/Makefile b/drivers/char/Makefile
index ffce287ef415..a6809ff01d80 100644
--- a/drivers/char/Makefile
+++ b/drivers/char/Makefile
@@ -47,3 +47,4 @@ obj-$(CONFIG_PS3_FLASH)		+= ps3flash.o
 obj-$(CONFIG_XILLYBUS)		+= xillybus/
 obj-$(CONFIG_POWERNV_OP_PANEL)	+= powernv-op-panel.o
 obj-$(CONFIG_ADI)		+= adi.o
+obj-$(CONFIG_RMD)		+= rmd.o
diff --git a/drivers/char/random.c b/drivers/char/random.c
index f462b9d2f5a5..8b33767c1666 100644
--- a/drivers/char/random.c
+++ b/drivers/char/random.c
@@ -1252,28 +1252,27 @@ static __u32 get_reg(struct fast_pool *f, struct pt_regs *regs)
 	return *ptr;
 }
 
-void add_interrupt_randomness(int irq, int irq_flags)
+void add_interrupt_randomness(int irq, int irq_flags, __u64 ip)
 {
 	struct entropy_store	*r;
 	struct fast_pool	*fast_pool = this_cpu_ptr(&irq_randomness);
-	struct pt_regs		*regs = get_irq_regs();
 	unsigned long		now = jiffies;
 	cycles_t		cycles = random_get_entropy();
 	__u32			c_high, j_high;
-	__u64			ip;
 	unsigned long		seed;
 	int			credit = 0;
 
 	if (cycles == 0)
-		cycles = get_reg(fast_pool, regs);
+		cycles = get_reg(fast_pool, NULL);
 	c_high = (sizeof(cycles) > 4) ? cycles >> 32 : 0;
 	j_high = (sizeof(now) > 4) ? now >> 32 : 0;
 	fast_pool->pool[0] ^= cycles ^ j_high ^ irq;
 	fast_pool->pool[1] ^= now ^ c_high;
-	ip = regs ? instruction_pointer(regs) : _RET_IP_;
+	if (!ip)
+		ip = _RET_IP_;
 	fast_pool->pool[2] ^= ip;
 	fast_pool->pool[3] ^= (sizeof(ip) > 4) ? ip >> 32 :
-		get_reg(fast_pool, regs);
+		get_reg(fast_pool, NULL);
 
 	fast_mix(fast_pool);
 	add_interrupt_bench(cycles);
@@ -1606,6 +1605,14 @@ static void try_to_generate_entropy(void)
 		struct timer_list timer;
 	} stack;
 
+
+#ifdef CONFIG_ARM
+{
+	u32 values[3];
+
+	random_get_entropy_init(values);
+#endif
+
 	stack.now = random_get_entropy();
 
 	/* Slow counter - or none. Don't even bother */
@@ -1621,6 +1628,11 @@ static void try_to_generate_entropy(void)
 		stack.now = random_get_entropy();
 	}
 
+#ifdef CONFIG_ARM
+	random_get_entropy_deinit(values);
+}
+#endif
+
 	del_timer_sync(&stack.timer);
 	destroy_timer_on_stack(&stack.timer);
 	mix_pool_bytes(&input_pool, &stack.now, sizeof(stack.now));
diff --git a/drivers/char/rmd.c b/drivers/char/rmd.c
new file mode 100644
index 000000000000..e92f3e28e3af
--- /dev/null
+++ b/drivers/char/rmd.c
@@ -0,0 +1,1161 @@
+// SPDX-License-Identifier: GPL-2.0+
+
+#include <linux/module.h>
+#include <linux/device.h>
+#include <linux/platform_device.h>
+#include <linux/of.h>
+#include <linux/of_irq.h>
+#include <linux/of_address.h>
+#include <linux/of_device.h>
+#include <linux/of_platform.h>
+#include <linux/err.h>
+#include <linux/dmaengine.h>
+#include <linux/cdev.h>
+#include <linux/interrupt.h>
+#include <linux/slab.h>
+#include <linux/dma-mapping.h>
+#include <linux/uaccess.h>
+#include <linux/gpio.h>
+#include <linux/rmd.h>
+#include <linux/firmware.h>
+#include <linux/pinctrl/consumer.h>
+#include <linux/mtd/mtd.h>
+#include <linux/delay.h>
+
+#include <linux/debugfs.h>
+
+#define RMD_NAME "rmd"
+#define RMD_MAX_DEVICES 42
+#define RMD_TRIPPLE_BUF_SIZE 3
+
+#define SARDTF_OFFS 0x10
+#define FIRMWARE_TYPE "rmd-01-01-f"
+#define DEFAULT_IMAGE FIRMWARE_TYPE "_reversed_rbf.img"
+#define STAMP_FORMAT "YYYY-MM-ddThh:mm:ss"
+enum {
+	fw_hdr_version,
+	fw_hdr_type,
+	fw_hdr_stamp,
+	fw_hdr_data,
+	fw_hdr_max
+};
+
+static struct rmd_base_t {
+	int		major;
+	struct class	*class;
+	struct cdev	*cdev;
+	struct mutex	idr_lock;
+	struct idr	idr;
+} rmd_base;
+
+enum rmd_idet_cs_field {
+	COMMIT,
+	DISCARD,
+	READ_FINISH,
+	DATA_VALID
+};
+
+enum rmd_channel_direction {
+	DEV_TO_MEM,
+	MEM_TO_DEV
+};
+
+struct rmd_dma_info {
+	struct dma_chan			*chan;
+	struct dma_async_tx_descriptor	*desc;
+	dma_cookie_t			cookie;
+	dma_addr_t			buf;
+};
+
+struct rmd_channel {
+	/* is empty when in softcopy mode */
+	struct rmd_dma_info		*dma_info;
+	enum rmd_channel_direction	direction;
+
+	/* holds the current user index that indicates
+	 * the memory offset in user mapped buffer.
+	 */
+	unsigned int			user_buf_idx;
+
+	/* buffer index currently used for data transmition
+	 * to/from the FIFO. Not used in case of dma based
+	 * transmission.
+	 */
+	unsigned int			kernel_buf_idx;
+
+	/* Contains the currently free buffer index optionally
+	 * or'd with RMD_FLAG_NEW_DATA.
+	 */
+	unsigned int			tripple_buf_ctrl_offset;
+
+	unsigned char			*buf;
+	dma_addr_t			buf_dma;
+	size_t				buf_size;
+};
+
+struct rmd {
+	struct device		*dev;
+	struct completion	completion;
+	int			minor;
+	unsigned int		irq;
+	unsigned long		is_open;
+	const char		*fw_name;
+
+	struct rmd_info		*info;
+	struct gpio_desc	*nce;
+	struct gpio_desc	*nrst;
+	struct gpio_desc	*nconfig;
+
+	/* ioremaped and mapped rmd memory */
+	__iomem void		*base;
+	phys_addr_t		base_phys;
+
+	/* supported max fifo size in rmd per channel */
+	size_t			fifo_max_size;
+
+	/* holds the last measured cycle time when user waits for
+	 * next cycle
+	 */
+	ktime_t			last_call_time;
+	struct dentry		*debug;
+};
+
+struct rmd_info {
+	struct rmd		*rmd;
+	struct rmd_config	config;
+	struct rmd_channel	rx_channel;
+	struct rmd_channel	tx_channel;
+	bool			is_configured;
+	bool			is_tx_enabled;
+
+	/* control structure for pdi and pdo tripple buffer */
+	u32			*tripple_buf_ctrl;
+	dma_addr_t		tripple_buf_ctrl_dma;
+	size_t			tripple_buf_ctrl_size;
+};
+
+struct rmd_firmware {
+	u32 datetime;
+	char version[16];
+	char stamp[sizeof(STAMP_FORMAT)];
+	size_t size;
+	/* Firmware must be 64 byte aligned to match SPI FIFO size */
+	u8 data[0] __aligned(64);
+};
+
+static int rmd_set_fpga_reset(struct rmd *rmd, int val)
+{
+	if (!rmd->nce)
+		return -ENXIO;
+
+	gpiod_set_value_cansleep(rmd->nconfig, val);
+	gpiod_set_value_cansleep(rmd->nce, val);
+
+	return 0;
+}
+
+static int rmd_fpga_restart(struct rmd *rmd)
+{
+	if (!rmd->nrst) {
+		dev_err(rmd->dev, "FPGA image could not be restarted\n");
+		return -ENXIO;
+	}
+
+	/* Restart FPGA image */
+	gpiod_set_value_cansleep(rmd->nrst, 0);
+	udelay(100);
+	gpiod_set_value_cansleep(rmd->nrst, 1);
+	mdelay(1);
+
+	return 0;
+}
+
+static int rmd_disable(struct rmd *rmd, int val)
+{
+	struct pinctrl *pinctrl;
+	int ret = rmd_set_fpga_reset(rmd, val);
+
+	if (ret)
+		return ret;
+
+	pinctrl = devm_pinctrl_get_select(rmd->dev->parent,
+			val ? "active" : PINCTRL_STATE_SLEEP);
+
+	if (IS_ERR(pinctrl))
+		return PTR_ERR(pinctrl);
+
+	devm_pinctrl_put(pinctrl);
+
+	return 0;
+}
+
+#define two_digit(p, x) (((p)[0] - '0') * (x) + (p)[1] - '0')
+#define to_bcd(p) two_digit((p), 16)
+#define to_num(p) two_digit((p), 10)
+
+static u32 stamp_to_version(const char *stamp)
+{
+	return to_bcd(stamp + 14) +
+		(to_bcd(stamp + 11) << 8) +
+		(((to_num(stamp + 2) - 10) & 0xf) << 16) +
+		((to_num(stamp + 5) & 0xf) << 20) +
+		(to_bcd(stamp + 8) << 24);
+}
+
+static struct rmd_firmware *parse_fw(struct device *dev,
+					const u8 *buf, size_t len)
+{
+	int i, l;
+	struct rmd_firmware *rmd_fw;
+
+	if (!buf || len < 5 || buf[0] != fw_hdr_max)
+		return NULL;
+
+	rmd_fw = kmalloc(sizeof(*rmd_fw) + len, GFP_KERNEL);
+	if (!rmd_fw)
+		return NULL;
+
+	for (i = 0, buf++, len--; len > 0 && i < fw_hdr_max; i++) {
+		l = buf[0] + (buf[1] << 8) + (buf[2] << 16) + (buf[3] << 24);
+
+		if (len < l+4)
+			goto err_out;
+
+		buf += 4;
+		len -= 4;
+		switch (i) {
+		case fw_hdr_version:
+			if (l >= sizeof(rmd_fw->version))
+				goto err_out;
+			memcpy(rmd_fw->version, buf, l);
+			rmd_fw->version[l] = 0;
+			break;
+		case fw_hdr_type:
+			if (l != sizeof(FIRMWARE_TYPE) - 1 ||
+			    memcmp(buf, FIRMWARE_TYPE, l))
+				goto err_out;
+			break;
+		case fw_hdr_stamp:
+			if (l != sizeof(STAMP_FORMAT) -1)
+				goto err_out;
+			rmd_fw->datetime = stamp_to_version(buf);
+			if (rmd_fw->datetime == 0)
+				goto err_out;
+			memcpy(rmd_fw->stamp, buf, l);
+			rmd_fw->stamp[l] = 0;
+			break;
+		case fw_hdr_data:
+			if (l != len)
+				goto err_out;
+			rmd_fw->size = l;
+			memcpy(rmd_fw->data, buf, l);
+			break;
+		}
+		buf += l;
+		len -= l;
+	}
+
+	if (len == 0 && i == fw_hdr_max)
+		return rmd_fw;
+err_out:
+	dev_err(dev, "Invalid firmware in header %d\n", i);
+	kfree(rmd_fw);
+	return NULL;
+}
+
+static int store_fw(struct rmd *rmd, const void *data, size_t len)
+{
+	int ret, retlen = 0;
+	struct mtd_info *mtd;
+	struct erase_info erase = { 0, };
+
+	mtd = get_mtd_device_nm("firmware");
+	if (IS_ERR(mtd)) {
+		dev_err(rmd->dev, "Flash device 'firmware' not found.\n");
+		return -ENODEV;
+	}
+	erase.addr = 0,
+	erase.len = roundup(len, mtd->erasesize);
+
+	ret = rmd_disable(rmd, 1);
+	if (!ret)
+		ret = mtd_erase(mtd, &erase);
+	if (!ret)
+		ret = mtd_write(mtd, 0, len, &retlen, data);
+	if (!ret)
+		ret = rmd_disable(rmd, 0);
+
+	put_mtd_device(mtd);
+
+	return ret;
+}
+
+static int rmd_load_firmware(struct rmd *rmd)
+{
+	const struct firmware *fw;
+	struct rmd_firmware *rmd_fw;
+	u32 datetime;
+	int ret, i;
+
+	/* Load the image into the FPGA if necessary */
+	rmd_set_fpga_reset(rmd, 0);
+
+	/* Loading of the image requires a restart afterwards */
+	rmd_fpga_restart(rmd);
+
+	ret = request_firmware(&fw, rmd->fw_name, rmd->dev);
+
+	if (ret < 0)
+		return ret;
+
+	rmd_fw = parse_fw(rmd->dev, fw->data, fw->size);
+	release_firmware(fw);
+	if (!rmd_fw)
+		return -EINVAL;
+
+	datetime = ioread32(rmd->base + SARDTF_OFFS);
+
+	if (datetime == rmd_fw->datetime) {
+		dev_info(rmd->dev, "Firmware up to date: %s (%08x)",
+				rmd_fw->stamp, rmd_fw->datetime);
+	} else {
+		dev_info(rmd->dev, "Updating firmware (%zd bytes) to %s "
+				"%s (%08x) from %08x",
+				rmd_fw->size, rmd_fw->version,
+				rmd_fw->stamp, rmd_fw->datetime, datetime);
+		ret = store_fw(rmd, rmd_fw->data, rmd_fw->size);
+		for (i = 0; i < 100 && datetime != rmd_fw->datetime; i++) {
+			/* Wait for RMD to boot. It usually takes 100ms */
+			datetime = ioread32(rmd->base + SARDTF_OFFS);
+			mdelay(5);
+		}
+		dev_info(rmd->dev, "Firmware update %s (stamp: 0x%x)\n",
+			 ret == 0 && datetime == rmd_fw->datetime ?
+				"succeeded" : "failed", datetime);
+	}
+
+	kfree(rmd_fw);
+	return ret;
+}
+
+static bool rmd_softcopy_mode(struct rmd_config *config)
+{
+	return config->flags & RMD_FLAG_SOFTCOPY;
+}
+
+static struct rmd *rmd_channel_to_rmd(struct rmd_channel *chan)
+{
+	return (chan->direction == MEM_TO_DEV ?
+		container_of(chan, struct rmd_info, tx_channel) :
+		container_of(chan, struct rmd_info, rx_channel))->rmd;
+}
+
+static u32 *rmd_channel_idet_cs_virt(struct rmd_channel *chan)
+{
+	struct rmd *rmd = rmd_channel_to_rmd(chan);
+
+	return chan->direction == MEM_TO_DEV ?
+		rmd->base + rmd->info->config.idet_pdo_cs_offset :
+		rmd->base + rmd->info->config.idet_pdi_cs_offset;
+}
+
+static int rmd_channel_fifo_offset(struct rmd_channel *chan)
+{
+	struct rmd *rmd = rmd_channel_to_rmd(chan);
+
+	return chan->direction == MEM_TO_DEV ?
+		rmd->info->config.idet_fifo_pdo_offset :
+		rmd->info->config.idet_fifo_pdi_offset;
+}
+
+static size_t rmd_channel_fifo_size(struct rmd_channel *chan)
+{
+	struct rmd *rmd = rmd_channel_to_rmd(chan);
+
+	return chan->direction == MEM_TO_DEV ?
+		rmd->info->config.idet_fifo_pdo_size :
+		rmd->info->config.idet_fifo_pdi_size;
+}
+
+static __iomem u32 *rmd_channel_fifo_virt(struct rmd_channel *chan)
+{
+	return (u32 *)(rmd_channel_to_rmd(chan)->base +
+						rmd_channel_fifo_offset(chan));
+}
+
+static int rmd_channel_buf_offs(struct rmd_channel *chan, int n)
+{
+	return n * rmd_channel_to_rmd(chan)->fifo_max_size;
+}
+
+static u32 *rmd_channel_buf_virt(struct rmd_channel *chan)
+{
+	return (u32 *)(chan->buf +
+			rmd_channel_buf_offs(chan, chan->kernel_buf_idx));
+}
+
+static u32 *rmd_channel_tripple_buf_virt(struct rmd_channel *chan)
+{
+	return rmd_channel_to_rmd(chan)->info->tripple_buf_ctrl +
+						chan->tripple_buf_ctrl_offset;
+}
+
+static u32 rmd_channel_get_bit_pos(struct rmd_channel *chan,
+						enum rmd_idet_cs_field field)
+{
+	struct rmd *rmd = rmd_channel_to_rmd(chan);
+	struct rmd_config *config = &rmd->info->config;
+	u8 bit_offset;
+
+	switch (field) {
+	case COMMIT:
+		bit_offset = config->tx_commit_bit_offset;
+		break;
+	case DISCARD:
+		bit_offset = config->tx_discard_bit_offset;
+		break;
+	case READ_FINISH:
+		bit_offset = config->rx_read_finish_bit_offset;
+		break;
+	case DATA_VALID:
+		bit_offset = config->rx_data_valid_bit_offset;
+		break;
+	}
+
+	return BIT(bit_offset);
+}
+
+static inline void rmd_xchg_free_rx_buffer(struct rmd_channel *chan)
+{
+	u32 *buf_ctrl = rmd_channel_tripple_buf_virt(chan);
+	u32 curr_write_idx = chan->kernel_buf_idx | RMD_FLAG_NEW_DATA;
+	u32 next_write_buf;
+
+	next_write_buf = rmd_atomic_xchg(buf_ctrl, curr_write_idx);
+	chan->kernel_buf_idx = next_write_buf & ~RMD_FLAG_NEW_DATA;
+
+	if (chan->kernel_buf_idx >= RMD_BUFFER_INDEX_SIZE) {
+		chan->kernel_buf_idx = 0;
+		WARN_ONCE(1, "buffer index overflow. data could be corrupt:"
+							"%s\n", __func__);
+	}
+}
+
+static inline void rmd_xchg_free_tx_buffer(struct rmd_channel *chan)
+{
+	int i;
+	u32 *buf_ctrl = rmd_channel_tripple_buf_virt(chan);
+	u32 expected_free_idx, next_read_idx;
+	u32 curr_read_idx = chan->kernel_buf_idx;
+
+	for (i = 0; i < RMD_BUFFER_INDEX_SIZE; i++) {
+
+		/* skip our current index */
+		if (i == curr_read_idx)
+			continue;
+
+		expected_free_idx = i | RMD_FLAG_NEW_DATA;
+
+		next_read_idx = rmd_atomic_cmpxchg(buf_ctrl, expected_free_idx,
+								curr_read_idx);
+
+		if (next_read_idx == expected_free_idx) {
+			chan->kernel_buf_idx = next_read_idx &
+							~RMD_FLAG_NEW_DATA;
+
+			if (chan->kernel_buf_idx >= RMD_BUFFER_INDEX_SIZE) {
+				chan->kernel_buf_idx = 0;
+				WARN_ONCE(1, "buffer index overflow. data could"
+						"be corrupt: %s\n", __func__);
+			}
+
+			break;
+		}
+	}
+}
+
+static void rmd_rx_softcopy(struct rmd_channel *rx)
+{
+	int i;
+	u32 ack, *data;
+	u32 *idet_cs = rmd_channel_idet_cs_virt(rx);
+	u32 *fifo = rmd_channel_fifo_virt(rx);
+	size_t s = rmd_channel_fifo_size(rx);
+
+	data = rmd_channel_buf_virt(rx);
+	for (i = 0; i < s / sizeof(*fifo); i++)
+		data[i] = ioread32(fifo);
+
+	iowrite32(rmd_channel_get_bit_pos(rx, READ_FINISH), idet_cs);
+
+	ack = ioread32(idet_cs) & rmd_channel_get_bit_pos(rx, DATA_VALID);
+	if (ack)
+		rmd_xchg_free_rx_buffer(rx);
+}
+
+static void rmd_tx_softcopy(struct rmd_channel *tx)
+{
+	int i;
+	u32 *fifo = rmd_channel_fifo_virt(tx);
+	size_t s = rmd_channel_fifo_size(tx);
+	u32 *data;
+
+	rmd_xchg_free_tx_buffer(tx);
+
+	data = rmd_channel_buf_virt(tx);
+	for (i = 0; i < s / sizeof(*fifo); i++)
+		iowrite32(data[i], fifo);
+
+	iowrite32(rmd_channel_get_bit_pos(tx, COMMIT),
+						rmd_channel_idet_cs_virt(tx));
+}
+
+static irqreturn_t rmd_isr(int irq, void *data)
+{
+	struct rmd *rmd = data;
+
+	if (!rmd->info || !rmd->info->is_configured)
+		return IRQ_HANDLED;
+
+	if (rmd_softcopy_mode(&rmd->info->config)) {
+		disable_irq_nosync(irq);
+		rmd_rx_softcopy(&rmd->info->rx_channel);
+
+		if (rmd->info->is_tx_enabled)
+			rmd_tx_softcopy(&rmd->info->tx_channel);
+
+		enable_irq(irq);
+	}
+	complete(&rmd->completion);
+
+	return IRQ_HANDLED;
+}
+
+static int rmd_mem_setup_channel(struct rmd *rmd, struct rmd_channel *chan,
+					enum rmd_channel_direction direction)
+{
+	chan->buf_size = RMD_TRIPPLE_BUF_SIZE * rmd->fifo_max_size;
+	chan->direction = direction;
+
+	chan->buf = kzalloc(chan->buf_size, GFP_USER);
+	if (!chan->buf)
+		return -ENOMEM;
+
+	chan->user_buf_idx = 2;
+	chan->kernel_buf_idx = 0;
+	chan->tripple_buf_ctrl_offset = direction == DEV_TO_MEM ? 0 :
+								sizeof(u32);
+
+	*rmd_channel_tripple_buf_virt(chan) = 1;
+
+	return 0;
+}
+
+static int rmd_mem_setup(struct rmd_info *info)
+{
+
+	info->tripple_buf_ctrl = kzalloc(info->tripple_buf_ctrl_size, GFP_USER);
+	if (!info->tripple_buf_ctrl)
+		return -ENOMEM;
+
+	if (rmd_mem_setup_channel(info->rmd, &info->rx_channel, DEV_TO_MEM))
+		return -EINVAL;
+
+	if (rmd_mem_setup_channel(info->rmd, &info->tx_channel, MEM_TO_DEV)) {
+		kfree(info->rx_channel.buf);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int rmd_dma_setup_channel(struct rmd *rmd, struct rmd_channel *chan,
+					enum rmd_channel_direction direction)
+{
+	int ret = 0;
+	struct rmd_dma_info *dma_info;
+	char *chan_name;
+	enum dma_transfer_direction dma_dir;
+
+	dma_info = kzalloc(sizeof(*dma_info), GFP_KERNEL);
+	if (!dma_info)
+		return -ENOMEM;
+
+	chan->buf_size = RMD_TRIPPLE_BUF_SIZE * rmd->fifo_max_size;
+	chan->direction = direction;
+
+	chan->buf = dmam_alloc_coherent(rmd->dev, chan->buf_size,
+						&chan->buf_dma, GFP_USER);
+	if (!chan->buf) {
+		ret = -ENOMEM;
+		goto free_dma_info;
+	}
+	memset(chan->buf, 0, chan->buf_size);
+
+	if (chan->direction == DEV_TO_MEM) {
+		dma_dir = DMA_DEV_TO_MEM;
+		chan_name = "rx";
+	} else {
+		dma_dir = DMA_MEM_TO_DEV;
+		chan_name = "tx";
+	}
+
+	dma_info->chan = dma_request_chan(rmd->dev, chan_name);
+	if (IS_ERR(dma_info->chan)) {
+		ret = PTR_ERR(dma_info->chan);
+		goto free_chan_buf;
+	}
+
+	dma_info->buf = chan->buf_dma;
+
+	chan->tripple_buf_ctrl_offset = direction == DEV_TO_MEM ? 0 :
+								sizeof(u32);
+
+	dma_info->desc = dmaengine_prep_rmd(dma_info->chan, rmd->base_phys,
+					&rmd->info->config, dma_dir,
+					dma_info->buf,
+					rmd->info->tripple_buf_ctrl_dma +
+						chan->tripple_buf_ctrl_offset
+						* sizeof(u32),
+					rmd->fifo_max_size, 0);
+
+	if (!dma_info->desc) {
+		ret = -EINVAL;
+		goto dma_release_channel;
+	}
+
+	chan->user_buf_idx = 2;
+	*rmd_channel_tripple_buf_virt(chan) = 1;
+
+	chan->dma_info = dma_info;
+
+	dma_info->cookie = dmaengine_submit(dma_info->desc);
+	dma_async_issue_pending(dma_info->chan);
+
+	return 0;
+
+dma_release_channel:
+	dma_release_channel(dma_info->chan);
+free_chan_buf:
+	dmam_free_coherent(rmd->dev, chan->buf_size, chan->buf, chan->buf_dma);
+free_dma_info:
+	kfree(dma_info);
+
+	return ret;
+}
+
+static int rmd_dma_enable_disable_tx(struct rmd_info *info, int enable)
+{
+	struct rmd_dma_info *dma_info = info->tx_channel.dma_info;
+
+	info->is_tx_enabled = enable;
+	if (rmd_softcopy_mode(&info->config) || !dma_info)
+		return 0;
+
+	return enable ? dmaengine_resume(dma_info->chan) :
+			dmaengine_pause(dma_info->chan);
+}
+
+static void rmd_dma_release_channel(struct rmd_channel *chan)
+{
+	struct rmd *rmd = rmd_channel_to_rmd(chan);
+
+	dmaengine_terminate_sync(chan->dma_info->chan);
+	dma_release_channel(chan->dma_info->chan);
+	dmam_free_coherent(rmd->dev, chan->buf_size, chan->buf, chan->buf_dma);
+	kfree(chan->dma_info);
+}
+
+static int rmd_dma_setup(struct rmd_info *info)
+{
+	int ret;
+	struct rmd *rmd = info->rmd;
+
+	info->tripple_buf_ctrl_size = PAGE_SIZE;
+	info->tripple_buf_ctrl = dmam_alloc_coherent(rmd->dev,
+						info->tripple_buf_ctrl_size,
+						&info->tripple_buf_ctrl_dma,
+						GFP_USER);
+
+	if (!info->tripple_buf_ctrl)
+		return -ENOMEM;
+
+	ret = rmd_dma_setup_channel(rmd, &info->rx_channel, DEV_TO_MEM);
+	if (ret)
+		return ret;
+
+	ret = rmd_dma_setup_channel(rmd, &info->tx_channel, MEM_TO_DEV);
+	if (ret)
+		rmd_dma_release_channel(&info->rx_channel);
+	else
+		ret = rmd_dma_enable_disable_tx(info, 0);
+
+	return ret;
+}
+
+static int rmd_open(struct inode *inode, struct file *filep)
+{
+	struct rmd *rmd;
+	struct rmd_info *info;
+
+	mutex_lock(&rmd_base.idr_lock);
+	rmd = idr_find(&rmd_base.idr, iminor(inode));
+	mutex_unlock(&rmd_base.idr_lock);
+
+	if (!rmd)
+		return -ENODEV;
+
+	/* rmd device is allowed to be opened by
+	 * only one process.
+	 */
+	if (test_and_set_bit(0, &rmd->is_open))
+		return -EBUSY;
+
+	info = kzalloc(sizeof(*info), GFP_KERNEL);
+	if (!info) {
+		clear_bit(0, &rmd->is_open);
+		return -ENOMEM;
+	}
+
+	if (rmd->nrst && rmd->nce && rmd->nconfig) {
+		int ret = rmd_load_firmware(rmd);
+
+		if (ret < 0)
+			dev_warn(rmd->dev,
+				"Firmware loading failed with error %d. "
+				"Continuing anyway...\n", ret);
+	}
+
+	rmd->info = info;
+	info->rmd = rmd;
+	info->tripple_buf_ctrl_size = PAGE_SIZE;
+
+	filep->private_data = info;
+
+	return 0;
+}
+
+static void rmd_cleanup(struct rmd_info *info)
+{
+	struct rmd *rmd = info->rmd;
+	struct rmd_channel *rx = &info->rx_channel;
+	struct rmd_channel *tx = &info->tx_channel;
+
+	if (!info->is_configured)
+		return;
+
+	debugfs_remove_recursive(rmd->debug);
+
+	info->is_configured = false;
+	info->is_tx_enabled = false;
+
+	if (rmd_softcopy_mode(&info->config)) {
+		kfree(rx->buf);
+		kfree(tx->buf);
+	} else {
+		rmd_dma_release_channel(rx);
+		rmd_dma_release_channel(tx);
+	}
+}
+
+static int rmd_close(struct inode *inode, struct file *filep)
+{
+	struct rmd_info *info = filep->private_data;
+	struct rmd *rmd = info->rmd;
+
+	rmd_cleanup(info);
+
+	kfree(info);
+	filep->private_data = NULL;
+	clear_bit(0, &rmd->is_open);
+
+	return 0;
+}
+
+static int rmd_wait_for_next_cycle(struct rmd *rmd, void __user *argp)
+{
+	int ret;
+	struct rmd_cycle cycle;
+	ktime_t call_time;
+	s64 delta_time_us;
+
+	call_time = ktime_get();
+	delta_time_us = ktime_us_delta(call_time, rmd->last_call_time);
+	rmd->last_call_time = call_time;
+
+	if (delta_time_us < 0)
+		delta_time_us = 0;
+
+	ret = copy_from_user(&cycle, argp, sizeof(cycle));
+	if (ret)
+		return ret;
+
+	cycle.cycle_time_us = delta_time_us;
+
+	reinit_completion(&rmd->completion);
+	ret = wait_for_completion_interruptible_timeout(&rmd->completion,
+					usecs_to_jiffies(cycle.timeout_us));
+
+	if (!ret)
+		return -ETIMEDOUT;
+
+	return copy_to_user(argp, &cycle, sizeof(cycle));
+}
+
+static int rmd_setup(struct rmd_info *info, void __user *argp)
+{
+	int ret;
+	struct rmd *rmd = info->rmd;
+
+	rmd_cleanup(info);
+
+	if (copy_from_user(&info->config, argp, sizeof(info->config)))
+		return -EFAULT;
+
+	if (rmd_softcopy_mode(&info->config))
+		ret = rmd_mem_setup(info);
+	else
+		ret = rmd_dma_setup(info);
+
+	if (ret)
+		return ret;
+
+	info->is_configured = true;
+
+	rmd->debug = debugfs_create_dir(dev_name(rmd->dev), NULL);
+	debugfs_create_x32("tripple_buf_ctrl_rx", 0600, rmd->debug,
+				info->tripple_buf_ctrl);
+
+	debugfs_create_x32("tripple_buf_ctrl_tx", 0600, rmd->debug,
+				info->tripple_buf_ctrl + sizeof(u32));
+	return ret;
+}
+
+static long rmd_ioctl(struct file *filep, unsigned int cmd, unsigned long arg)
+{
+	struct rmd_info *info = filep->private_data;
+
+	switch (cmd) {
+	case RMD_SET_CONFIG:
+		return rmd_setup(info, (void __user *)arg);
+
+	case RMD_GET_CONFIG:
+		return copy_to_user((void __user *)arg, &info->config,
+							sizeof(info->config));
+
+	case RMD_GET_BUFFER_SIZE:
+		return info->rmd->fifo_max_size * RMD_TRIPPLE_BUF_SIZE;
+
+	case RMD_GET_DRV_VERSION:
+		return RMD_DRV_VERSION;
+
+	case RMD_GET_READ_BUFFER_IDX:
+		return info->rx_channel.user_buf_idx;
+
+	case RMD_GET_WRITE_BUFFER_IDX:
+		return info->tx_channel.user_buf_idx;
+
+	case RMD_WAIT_FOR_NEXT_CYCLE:
+		return rmd_wait_for_next_cycle(info->rmd, (void __user *)arg);
+
+	case RMD_ACTIVATE_TX_PATH:
+		return rmd_dma_enable_disable_tx(info, arg ? 1 : 0);
+
+	case RMD_RESTART_FPGA_IMAGE:
+		return rmd_fpga_restart(info->rmd);
+
+	case RMD_GET_READ_TRIPPLE_BUF_CTRL_OFFSET:
+		return info->rx_channel.tripple_buf_ctrl_offset;
+
+	case RMD_GET_WRITE_TRIPPLE_BUF_CTRL_OFFSET:
+		return info->tx_channel.tripple_buf_ctrl_offset;
+
+	case RMD_GET_BUFFER_CHUNK_SIZE:
+		return info->rmd->fifo_max_size;
+
+	case RMD_GET_TRIPPLE_BUF_CTRL_SIZE:
+		return info->tripple_buf_ctrl_size;
+	}
+
+	return -EINVAL;
+};
+
+static int rmd_mmap(struct file *filep, struct vm_area_struct *vma)
+{
+	struct rmd_info *info = filep->private_data;
+	struct rmd *rmd = info->rmd;
+	struct rmd_channel *chan;
+
+	if (!info->is_configured)
+		return -EINVAL;
+
+	if (!(vma->vm_flags & VM_SHARED))
+		return -EINVAL;
+
+	if ((vma->vm_flags & VM_READ) && (vma->vm_flags & VM_WRITE)) {
+		if (rmd_softcopy_mode(&info->config))
+			return remap_pfn_range(vma, vma->vm_start,
+				virt_to_phys(info->tripple_buf_ctrl) >>
+								PAGE_SHIFT,
+				info->tripple_buf_ctrl_size,
+				vma->vm_page_prot);
+		else
+			return dma_mmap_coherent(rmd->dev, vma,
+						info->tripple_buf_ctrl,
+						info->tripple_buf_ctrl_dma,
+						info->tripple_buf_ctrl_size);
+	}
+
+	if (vma->vm_flags & VM_READ)
+		chan = &info->rx_channel;
+	else if (vma->vm_flags & VM_WRITE)
+		chan = &info->tx_channel;
+	else
+		return -EINVAL;
+
+	if (rmd_softcopy_mode(&info->config))
+		return remap_pfn_range(vma, vma->vm_start,
+					virt_to_phys(chan->buf) >> PAGE_SHIFT,
+					chan->buf_size,
+					vma->vm_page_prot);
+	else
+		return dma_mmap_coherent(rmd->dev, vma, chan->buf,
+					chan->buf_dma, chan->buf_size);
+}
+
+static int of_rmd(struct device *dev, struct rmd *rmd)
+{
+	int ret;
+	struct resource res;
+	struct device_node *np = dev->of_node;
+
+	ret = of_address_to_resource(np, 0, &res);
+	if (ret)
+		return ret;
+
+	rmd->base = devm_ioremap(dev, res.start, resource_size(&res));
+	if (IS_ERR(rmd->base))
+		return PTR_ERR(rmd->base);
+
+	rmd->base_phys = res.start;
+
+	ret = of_property_read_u32(np, "rmd,fifo-size", &rmd->fifo_max_size);
+	if (ret)
+		return ret;
+
+	ret = of_irq_get(np, 0);
+	if (ret < 0)
+		return ret;
+
+	rmd->irq = ret;
+
+	ret = devm_request_irq(dev, rmd->irq, rmd_isr, IRQF_SHARED |
+						IRQF_THREAD_TBL_LOOKUP |
+						IRQ_TYPE_EDGE_RISING,
+						RMD_NAME, rmd);
+	if (ret)
+		return ret;
+
+	if (of_property_read_string(np, "firmware", &rmd->fw_name))
+		rmd->fw_name = DEFAULT_IMAGE;
+
+	rmd->nrst = devm_gpiod_get_optional(dev, "nrst", GPIOD_OUT_LOW);
+	if (IS_ERR(rmd->nrst))
+		return PTR_ERR(rmd->nrst);
+
+	rmd->nce = devm_gpiod_get_optional(dev, "nce", GPIOD_OUT_LOW);
+	if (IS_ERR(rmd->nce))
+		return PTR_ERR(rmd->nce);
+
+	rmd->nconfig = devm_gpiod_get_optional(dev, "nconfig", GPIOD_OUT_LOW);
+	if (IS_ERR(rmd->nconfig))
+		return PTR_ERR(rmd->nconfig);
+
+	return 0;
+}
+
+static int rmd_get_minor(struct rmd *rmd)
+{
+	int minor;
+
+	mutex_lock(&rmd_base.idr_lock);
+
+	minor = idr_alloc(&rmd_base.idr, rmd, 0, RMD_MAX_DEVICES, GFP_KERNEL);
+	if (minor < 0) {
+		mutex_unlock(&rmd_base.idr_lock);
+		return minor;
+	}
+
+	rmd->minor = minor;
+
+	mutex_unlock(&rmd_base.idr_lock);
+
+	return 0;
+}
+
+static void rmd_free_minor(struct rmd *rmd)
+{
+	mutex_lock(&rmd_base.idr_lock);
+	idr_remove(&rmd_base.idr, rmd->minor);
+	mutex_unlock(&rmd_base.idr_lock);
+}
+
+static int rmd_probe(struct platform_device *pdev)
+{
+	int ret;
+	struct rmd *rmd;
+	struct device *parent = &pdev->dev;
+
+	rmd = devm_kzalloc(parent, sizeof(*rmd), GFP_KERNEL);
+	if (!rmd)
+		return -ENOMEM;
+
+	ret = of_rmd(parent, rmd);
+	if (ret) {
+		dev_err(parent, "oftree handling failed\n");
+		return ret;
+	}
+
+	init_completion(&rmd->completion);
+
+	ret = rmd_get_minor(rmd);
+	if (ret) {
+		dev_err(parent, "could not get minor number\n");
+		return ret;
+	}
+
+	rmd->dev = device_create(rmd_base.class, parent,
+				MKDEV(rmd_base.major, rmd->minor), rmd,
+				"rmd%d", rmd->minor);
+	if (IS_ERR(rmd->dev)) {
+		dev_err(parent, "could not create rmd device\n");
+		rmd_free_minor(rmd);
+		return PTR_ERR(rmd->dev);
+	}
+	rmd->dev->coherent_dma_mask = DMA_BIT_MASK(32);
+	rmd->dev->of_node = parent->of_node;
+
+	platform_set_drvdata(pdev, rmd);
+	dev_set_drvdata(rmd->dev, rmd);
+
+	return 0;
+}
+
+static int rmd_remove(struct platform_device *pdev)
+{
+	struct rmd *rmd = platform_get_drvdata(pdev);
+
+	device_destroy(rmd_base.class, MKDEV(rmd_base.major, rmd->minor));
+	rmd_free_minor(rmd);
+
+	return 0;
+}
+
+static const struct file_operations rmd_fileops = {
+	.owner		= THIS_MODULE,
+	.open		= rmd_open,
+	.release	= rmd_close,
+	.unlocked_ioctl	= rmd_ioctl,
+	.mmap		= rmd_mmap,
+};
+
+static int rmd_major_init(void)
+{
+	int ret;
+	dev_t rmd_dev;
+
+	ret = alloc_chrdev_region(&rmd_dev, 0, RMD_MAX_DEVICES, RMD_NAME);
+	if (ret)
+		goto out;
+
+	rmd_base.cdev = cdev_alloc();
+	if (!rmd_base.cdev)
+		goto out_unregister;
+
+	cdev_init(rmd_base.cdev, &rmd_fileops);
+
+	ret = cdev_add(rmd_base.cdev, rmd_dev, RMD_MAX_DEVICES);
+	if (ret)
+		goto out_del;
+
+	rmd_base.major = MAJOR(rmd_dev);
+
+	return 0;
+
+out_del:
+	cdev_del(rmd_base.cdev);
+out_unregister:
+	unregister_chrdev_region(rmd_dev, RMD_MAX_DEVICES);
+out:
+	return ret;
+}
+
+static void rmd_major_exit(void)
+{
+	unregister_chrdev_region(MKDEV(rmd_base.major, 0), RMD_MAX_DEVICES);
+	cdev_del(rmd_base.cdev);
+}
+
+static const struct of_device_id rmd_of_match[] = {
+	{ .compatible = "wago,rmd", },
+	{ /* sentinel */ },
+};
+MODULE_DEVICE_TABLE(of, rmd_of_match);
+
+static struct platform_driver rmd_platform_driver = {
+	.probe = rmd_probe,
+	.remove = rmd_remove,
+	.driver = {
+		.name = RMD_NAME,
+		.of_match_table = rmd_of_match,
+	},
+};
+
+static int __init rmd_init(void)
+{
+	int ret;
+
+	mutex_init(&rmd_base.idr_lock);
+	idr_init(&rmd_base.idr);
+
+	ret = rmd_major_init();
+	if (ret)
+		goto out;
+
+	rmd_base.class = class_create(THIS_MODULE, RMD_NAME);
+	if (IS_ERR(rmd_base.class)) {
+		ret = PTR_ERR(rmd_base.class);
+		goto out_class;
+	}
+
+	ret = platform_driver_register(&rmd_platform_driver);
+	if (ret)
+		goto out_platform;
+
+	return 0;
+
+out_platform:
+	class_destroy(rmd_base.class);
+out_class:
+	rmd_major_exit();
+out:
+	idr_destroy(&rmd_base.idr);
+	mutex_destroy(&rmd_base.idr_lock);
+	return ret;
+}
+
+static void __exit rmd_exit(void)
+{
+	platform_driver_unregister(&rmd_platform_driver);
+	class_destroy(rmd_base.class);
+	rmd_major_exit();
+	idr_destroy(&rmd_base.idr);
+	mutex_destroy(&rmd_base.idr_lock);
+}
+
+module_init(rmd_init);
+module_exit(rmd_exit);
+
+MODULE_AUTHOR("Oleg Karfich <oleg.karfich@wago.com>");
+MODULE_DESCRIPTION("Driver for WAGO Rocket Master Device");
+MODULE_LICENSE("GPL");
diff --git a/drivers/char/tpm/tpm-dev-common.c b/drivers/char/tpm/tpm-dev-common.c
index 1784530b8387..c08cbb306636 100644
--- a/drivers/char/tpm/tpm-dev-common.c
+++ b/drivers/char/tpm/tpm-dev-common.c
@@ -20,7 +20,6 @@
 #include "tpm-dev.h"
 
 static struct workqueue_struct *tpm_dev_wq;
-static DEFINE_MUTEX(tpm_dev_wq_lock);
 
 static ssize_t tpm_dev_transmit(struct tpm_chip *chip, struct tpm_space *space,
 				u8 *buf, size_t bufsiz)
diff --git a/drivers/char/tpm/tpm_tis.c b/drivers/char/tpm/tpm_tis.c
index 4ed6e660273a..c2bd0d40b5fc 100644
--- a/drivers/char/tpm/tpm_tis.c
+++ b/drivers/char/tpm/tpm_tis.c
@@ -50,6 +50,31 @@ static inline struct tpm_tis_tcg_phy *to_tpm_tis_tcg_phy(struct tpm_tis_data *da
 	return container_of(data, struct tpm_tis_tcg_phy, priv);
 }
 
+#ifdef CONFIG_PREEMPT_RT
+/*
+ * Flushes previous write operations to chip so that a subsequent
+ * ioread*()s won't stall a cpu.
+ */
+static inline void tpm_tis_flush(void __iomem *iobase)
+{
+	ioread8(iobase + TPM_ACCESS(0));
+}
+#else
+#define tpm_tis_flush(iobase) do { } while (0)
+#endif
+
+static inline void tpm_tis_iowrite8(u8 b, void __iomem *iobase, u32 addr)
+{
+	iowrite8(b, iobase + addr);
+	tpm_tis_flush(iobase);
+}
+
+static inline void tpm_tis_iowrite32(u32 b, void __iomem *iobase, u32 addr)
+{
+	iowrite32(b, iobase + addr);
+	tpm_tis_flush(iobase);
+}
+
 static int interrupts = -1;
 module_param(interrupts, int, 0444);
 MODULE_PARM_DESC(interrupts, "Enable interrupts");
@@ -169,7 +194,7 @@ static int tpm_tcg_write_bytes(struct tpm_tis_data *data, u32 addr, u16 len,
 	struct tpm_tis_tcg_phy *phy = to_tpm_tis_tcg_phy(data);
 
 	while (len--)
-		iowrite8(*value++, phy->iobase + addr);
+		tpm_tis_iowrite8(*value++, phy->iobase, addr);
 
 	return 0;
 }
@@ -196,7 +221,7 @@ static int tpm_tcg_write32(struct tpm_tis_data *data, u32 addr, u32 value)
 {
 	struct tpm_tis_tcg_phy *phy = to_tpm_tis_tcg_phy(data);
 
-	iowrite32(value, phy->iobase + addr);
+	tpm_tis_iowrite32(value, phy->iobase, addr);
 
 	return 0;
 }
diff --git a/drivers/dma/dmaengine.c b/drivers/dma/dmaengine.c
index fe6a460c4373..3b9fd96b0c78 100644
--- a/drivers/dma/dmaengine.c
+++ b/drivers/dma/dmaengine.c
@@ -1215,6 +1215,12 @@ int dma_async_device_register(struct dma_device *device)
 		return -EIO;
 	}
 
+	if (dma_has_cap(DMA_RMD, device->cap_mask) && !device->device_prep_dma_rmd) {
+		dev_err(device->dev,
+			"Device claims capability %s, but op is not defined\n",
+			"DMA_RMD");
+		return -EIO;
+	}
 
 	if (!device->device_tx_status) {
 		dev_err(device->dev, "Device tx_status is not defined\n");
diff --git a/drivers/dma/ti/edma.c b/drivers/dma/ti/edma.c
index 35d81bd857f1..f229ead00cf9 100644
--- a/drivers/dma/ti/edma.c
+++ b/drivers/dma/ti/edma.c
@@ -20,6 +20,7 @@
 #include <linux/init.h>
 #include <linux/interrupt.h>
 #include <linux/list.h>
+#include <linux/rmd.h>
 #include <linux/module.h>
 #include <linux/platform_device.h>
 #include <linux/slab.h>
@@ -30,6 +31,7 @@
 #include <linux/of_address.h>
 #include <linux/of_device.h>
 #include <linux/pm_runtime.h>
+#include <linux/debugfs.h>
 
 #include <linux/platform_data/edma.h>
 
@@ -40,6 +42,7 @@
 #define PARM_OPT		0x00
 #define PARM_SRC		0x04
 #define PARM_A_B_CNT		0x08
+#define PARM_B_CNT		(PARM_A_B_CNT + 2)
 #define PARM_DST		0x0c
 #define PARM_SRC_DST_BIDX	0x10
 #define PARM_LINK_BCNTRLD	0x14
@@ -123,7 +126,7 @@
  * fail. Today davinci-pcm is the only user of this driver and
  * requires atleast 17 slots, so we setup the default to 20.
  */
-#define MAX_NR_SG		20
+#define MAX_NR_SG		32
 #define EDMA_MAX_SLOTS		MAX_NR_SG
 #define EDMA_DESCRIPTORS	16
 
@@ -133,6 +136,40 @@
 #define EDMA_CONT_PARAMS_FIXED_EXACT	 1002
 #define EDMA_CONT_PARAMS_FIXED_NOT_EXACT 1003
 
+/* Number of RMD transfer chains per channel */
+#define RMD_TRANSFER_CHAINS 3
+
+/* number of transfer working slots per channel */
+#define RMD_WORKING_SLOTS 1
+
+enum {
+	rmd_rx_fifo_trans = 0,
+	rmd_rx_ack_write = 1,
+	rmd_rx_lock_bufctrl = 2,
+	rmd_rx_data_valid = 3,
+	rmd_rx_data_switch = 4,
+	rmd_rx_unlock_bufctrl = 5,
+	rmd_rx_copy_bufctrl = 6,
+	rmd_rx_update_link = 7,
+	rmd_rx_update_bufctrl = 8,
+	rmd_rx_max
+};
+#define RMD_SLOTS_RX (rmd_rx_max * RMD_TRANSFER_CHAINS + RMD_WORKING_SLOTS)
+
+enum {
+	rmd_tx_new_data = 0,
+	rmd_tx_switch_buf = 1,
+	rmd_tx_lock_bufctrl = 2,
+	rmd_tx_get_bufnum = 3,
+	rmd_tx_update_link = 4,
+	rmd_tx_update_bufctrl = 5,
+	rmd_tx_unlock = 6,
+	rmd_tx_fifo_trans = 7,
+	rmd_tx_ack_write = 8,
+	rmd_tx_max
+};
+#define RMD_SLOTS_TX (rmd_tx_max * RMD_TRANSFER_CHAINS + RMD_WORKING_SLOTS)
+
 /*
  * 64bit array registers are split into two 32bit registers:
  * reg0: channel/event 0-31
@@ -172,6 +209,7 @@ struct edmacc_param {
 struct edma_pset {
 	u32				len;
 	dma_addr_t			addr;
+	int				link;
 	struct edmacc_param		param;
 };
 
@@ -179,9 +217,10 @@ struct edma_desc {
 	struct virt_dma_desc		vdesc;
 	struct list_head		node;
 	enum dma_transfer_direction	direction;
-	int				cyclic;
 	bool				polled;
-	int				absync;
+	int				cyclic:1;
+	int				rmd_transfer:1;
+	int				absync:1;
 	int				pset_nr;
 	struct edma_chan		*echan;
 	int				processed;
@@ -210,6 +249,12 @@ struct edma_desc {
 	u32				sg_len;
 	u32				residue;
 	u32				residue_stat;
+	dma_addr_t			rmd_mem;
+	size_t				rmd_chunk_size;
+	struct dentry			*debug;
+	const char * const		*rmd_task_names;
+	struct debugfs_regset32		rmd_work_parm;
+	int				rmd_task_num;
 
 	struct edma_pset		pset[];
 };
@@ -272,6 +317,7 @@ struct edma_cc {
 	struct edma_chan		*slave_chans;
 	struct edma_tc			*tc_list;
 	int				dummy_slot;
+	phys_addr_t			phys;
 };
 
 /* dummy param set used to (re)initialize parameter RAM slots */
@@ -751,6 +797,11 @@ static void edma_free_channel(struct edma_chan *echan)
 	edma_setup_interrupt(echan, false);
 }
 
+static dma_addr_t param_addr(struct edma_chan *echan, int slot_num)
+{
+	return echan->ecc->phys + PARM_OFFSET(echan->slot[slot_num]);
+}
+
 static inline struct edma_cc *to_edma_cc(struct dma_device *d)
 {
 	return container_of(d, struct edma_cc, dma_slave);
@@ -768,9 +819,341 @@ static inline struct edma_desc *to_edma_desc(struct dma_async_tx_descriptor *tx)
 
 static void edma_desc_free(struct virt_dma_desc *vdesc)
 {
-	kfree(container_of(vdesc, struct edma_desc, vdesc));
+	struct edma_desc *edesc = container_of(vdesc, struct edma_desc, vdesc);
+
+	debugfs_remove_recursive(edesc->debug);
+	kfree(edesc);
+}
+
+#ifdef CONFIG_DEBUG_FS
+static struct debugfs_reg32 param_reg[] = {
+	{ "OPT  ",  0 },
+	{ "SRC  ",  4 },
+	{ "ABCNT",  8 },
+	{ "DST  ", 12 },
+	{ "BIDX ", 16 },
+	{ "BCLNK", 20 },
+	{ "CIDX ", 24 },
+	{ "CCNT ", 28 },
+};
+
+static char *dfs_param_text(u32 addr, char *buf)
+{
+	int idx, offs;
+
+	buf[0] = 0;
+
+	if (addr > 0x2000)
+		return buf;
+
+	idx = addr / PARM_SIZE;
+	offs = addr % PARM_SIZE;
+
+	sprintf(buf, " %i:%s+%d", idx, param_reg[offs>>2].name, offs & 0x3);
+	return buf;
+}
+
+static int dfs_print_param(struct edma_desc *edesc, int i,
+				char *buf, size_t len)
+{
+	struct edma_chan *echan = edesc->echan;
+	char srctxt[128] = "", dsttxt[128] = "";
+	struct edmacc_param *p = &edesc->pset[i].param;
+	u32 param_addr = echan->ecc->phys + EDMA_PARM;
+
+	if (i >= edesc->pset_nr)
+		return 0;
+
+	return snprintf(buf, len,
+		 "%d\n"
+		 "%d\n"
+		 "Offset:%04x %d\n"
+		 "%d\n"
+		 "%08x%s%s%s\n"
+		 "%08x%s\n"
+		 "A:%04x B:%04x\n"
+		 "%08x%s\n"
+		 "D:%04x S:%04x\n"
+		 "Link:%04x\n"
+		 "D:%04x S:%04x\n"
+		 "%08x\n",
+		 i, echan->ch_num,
+		 PARM_OFFSET(echan->slot[i]), echan->slot[i],
+		 edesc->pset[i].link, p->opt,
+		 (p->opt & TCCHEN) ? " CHAIN" : "",
+		 (p->opt & TCINTEN) ? " IRQ" : "",
+		 ((p->opt >> 12) & 0x3F) == echan->ch_num &&
+			 (p->opt & (TCCMODE)) >> 11 == 0 &&
+			 (p->opt & STATIC) == 0 ? " OK" : " BAD",
+		 p->src, dfs_param_text(p->src - param_addr, srctxt),
+		 p->a_b_cnt & 0xffff, p->a_b_cnt >> 16,
+		 p->dst, dfs_param_text(p->dst - param_addr, dsttxt),
+		 p->src_dst_bidx >> 16, p->src_dst_bidx & 0xffff,
+		 p->link_bcntrld & 0xffff,
+		 p->src_dst_cidx >> 16, p->src_dst_cidx & 0xffff,
+		 p->ccnt);
+}
+
+static void dfs_print_table(struct seq_file *m, int cnt, const char **bufs,
+			const char *capt)
+{
+	int lengths[cnt], i, len, capt_offs;
+	int capt_len = capt ? strlen(capt) : 0;
+
+	for (i = 0, len = 0; i < cnt; i++) {
+		const char *p, *q;
+
+		lengths[i] = 0;
+		for (p = bufs[i]; p && *p; p = q+1) {
+			q = strchr(p, '\n');
+			if (!q)
+				break;
+			if (q-p > lengths[i])
+				lengths[i] = q-p;
+		}
+		len += lengths[i] + 3;
+	}
+	capt_offs = (len - capt_len) / 2 + capt_len;
+	if (capt)
+		seq_printf(m, "%*s\n%*.*s\n", capt_offs, capt,
+				capt_offs, capt_len,
+				"-------------------------------------");
+	while (1) {
+		for (i = 0; i < cnt; i++) {
+			const char *p = bufs[i];
+			const char *end = strchr(p, '\n');
+
+			if (!end)
+				return;
+			seq_printf(m, "%s%*.*s", i?"| " : "", lengths[i] + 1,
+					(int)(end-p), p);
+			bufs[i] = *end ? end+1 : end;
+		}
+		seq_putc(m, '\n');
+	}
 }
 
+#define PARAM_NAMES "pset\nchnum\nslot\nlink\nopt\n"\
+		    "src\nabcnt\ndst\nbidx\nlnkreld\ncidx\ncnt\n"
+
+static int dfs_show_param_task(struct seq_file *m, void *_p)
+{
+	const char *nam = m->file->f_path.dentry->d_name.name;
+	struct edma_desc *edesc = m->private;
+	char *buf, *p;
+	const char *cols[4] = { PARAM_NAMES, };
+	int i, type;
+
+	if (strlen(nam) < 4)
+		return -EINVAL;
+
+	for (type = 0; type < edesc->rmd_task_num; type++) {
+		if (!strcmp(nam + 3, edesc->rmd_task_names[type]))
+			break;
+	}
+	if (type++ >= edesc->rmd_task_num)
+		return -EINVAL;
+
+	buf = p = kmalloc(4096, GFP_KERNEL);
+	if (!buf)
+		return -ENOMEM;
+
+	for (i = 0; i < 3 && (p-buf) < 4096; i++) {
+		cols[i+1] = p;
+		p += dfs_print_param(edesc, type + i*edesc->rmd_task_num,
+					p, 4096 - (p-buf)) + 1;
+	}
+	if (i == 3)
+		dfs_print_table(m, 4, cols, nam);
+	kfree(buf);
+	return 0;
+}
+
+static int dfs_show_phys_params(struct seq_file *m, void *_p)
+{
+	struct edma_desc *edesc = m->private;
+	int i;
+
+	for (i = 1; i < edesc->pset_nr; i++) {
+		int offs = PARM_OFFSET(edesc->echan->slot[i]);
+
+		if (edesc->rmd_task_names && edesc->rmd_task_num) {
+			int task = (i-1) % edesc->rmd_task_num;
+
+			seq_printf(m, "Slot[%d]: %d %04x: %s[%d]\n",
+				i, edesc->echan->slot[i], offs,
+				edesc->rmd_task_names[task],
+				(i-1) / edesc->rmd_task_num);
+		} else {
+			seq_printf(m, "Slot[%d]: %d %04x\n",
+				i, edesc->echan->slot[i], offs);
+		}
+		debugfs_print_regs32(m, param_reg, ARRAY_SIZE(param_reg),
+			edesc->echan->ecc->base + offs, "  ");
+	}
+	return 0;
+}
+
+static int dfs_open_param_task(struct inode *inode, struct file *filep)
+{
+	return single_open(filep, dfs_show_param_task, inode->i_private);
+}
+
+static const struct file_operations dfs_print_param_array_task = {
+
+	.owner = THIS_MODULE,
+	.open = dfs_open_param_task,
+	.release = single_release,
+	.read = seq_read,
+	.llseek = seq_lseek
+};
+
+static int dfs_open_phys_params(struct inode *inode, struct file *file)
+{
+	return single_open(file, dfs_show_phys_params, inode->i_private);
+}
+
+static const struct file_operations dfs_print_phys_params = {
+
+	.owner = THIS_MODULE,
+	.open = dfs_open_phys_params,
+	.release = single_release,
+	.read = seq_read,
+	.llseek = seq_lseek
+};
+
+ssize_t dfs_trigger_write(struct file *file, const char __user *user,
+			  size_t len, loff_t *ppos)
+{
+	struct edma_desc *edesc = file->private_data;
+	struct edma_chan *echan = edesc->echan;
+	struct edma_cc *ecc = echan->ecc;
+	int channel = EDMA_CHAN_SLOT(echan->ch_num);
+	char buf[8];
+	ssize_t ret;
+
+	if (*ppos) {
+		*ppos += len;
+		return len;
+	}
+	simple_write_to_buffer(buf, sizeof(buf), ppos, user, len);
+	switch (buf[0]) {
+	case 's':
+	case 'S':
+		edma_param_modify(ecc, PARM_OPT, echan->slot[0],
+				  ~TCCHEN, 0);
+		/* fall */
+	case 'c':
+	case 'C':
+		edma_shadow0_write_array(ecc, SH_ESR, channel >> 5,
+					 BIT(channel & 0x1f));
+	}
+	return ret;
+}
+
+static const struct file_operations dfs_trigger = {
+
+	.owner = THIS_MODULE,
+	.open = simple_open,
+	.write = dfs_trigger_write
+};
+
+static void dfs_setup(struct edma_desc *edesc)
+{
+	struct edma_chan *echan = edesc->echan;
+	struct edma_cc *ecc = echan->ecc;
+	int i;
+
+	edesc->debug = debugfs_create_dir(dma_chan_name(&echan->vchan.chan),
+					 NULL);
+	if (!edesc->debug)
+		return;
+
+	edesc->rmd_work_parm.regs = param_reg;
+	edesc->rmd_work_parm.nregs = ARRAY_SIZE(param_reg);
+	edesc->rmd_work_parm.base = ecc->base + PARM_OFFSET(echan->slot[0]);
+	debugfs_create_regset32("Work-PaRAM", 0444, edesc->debug,
+				&edesc->rmd_work_parm);
+
+	for (i = 0; i < edesc->rmd_task_num; i++) {
+		char buf[256];
+
+		snprintf(buf, sizeof(buf), "%02d-%s", i,
+				edesc->rmd_task_names[i]);
+		debugfs_create_file(buf, 0444, edesc->debug, edesc,
+					&dfs_print_param_array_task);
+	}
+	debugfs_create_file("Hardware-PaRAMs", 0444, edesc->debug, edesc,
+				&dfs_print_phys_params);
+	debugfs_create_file("Trigger", 0600, edesc->debug, edesc,
+				&dfs_trigger);
+}
+
+static void dump_param(struct edma_desc *edesc, int i)
+{
+	char buf[256];
+	const char *cols[2] = { PARAM_NAMES, buf};
+	struct seq_file s;
+
+	memset(&s, 0, sizeof(s));
+	s.size = PAGE_SIZE;
+	s.buf = kzalloc(s.size, GFP_KERNEL);
+
+	if (!s.buf)
+		return;
+
+	dfs_print_param(edesc, i, buf, sizeof(buf));
+	dfs_print_table(&s, 2, cols, NULL);
+	dev_dbg(edesc->echan->vchan.chan.device->dev, "%s", s.buf);
+	kfree(s.buf);
+}
+
+static void setup_task_names(struct edma_desc *edesc)
+{
+	static const char * const rmd_rx_task_names[] = {
+
+		[rmd_rx_fifo_trans]	= "FIFO-Trans",
+		[rmd_rx_ack_write]	= "ACK-Write",
+		[rmd_rx_lock_bufctrl]	= "Buffer-Lock",
+		[rmd_rx_data_valid]	= "Data-Valid",
+		[rmd_rx_data_switch]	= "Data-Switch",
+		[rmd_rx_unlock_bufctrl]	= "Buffer-Unlock",
+		[rmd_rx_copy_bufctrl]	= "Copy-BufCtrl",
+		[rmd_rx_update_link]	= "Update-Link",
+		[rmd_rx_update_bufctrl]	= "Update-BufCtrl",
+		[rmd_rx_max] = NULL
+	};
+
+	static const char * const rmd_tx_task_names[] = {
+
+		[rmd_tx_new_data]	= "New-Data",
+		[rmd_tx_switch_buf]	= "Switch-Buf",
+		[rmd_tx_lock_bufctrl]	= "Lock-BufCtrl",
+		[rmd_tx_get_bufnum]	= "Get-BufNum",
+		[rmd_tx_update_link]	= "Update-Link",
+		[rmd_tx_update_bufctrl]	= "Update-BufCtrl",
+		[rmd_tx_unlock]		= "Unlock-BufCtrl",
+		[rmd_tx_fifo_trans]	= "FIFO-Trans",
+		[rmd_tx_ack_write]	= "ACK-Write",
+		[rmd_tx_max] = NULL
+	};
+
+	edesc->rmd_task_names = edesc->direction == DMA_DEV_TO_MEM ?
+		rmd_rx_task_names : rmd_tx_task_names;
+}
+
+#else
+static void dfs_setup(struct edma_desc *edesc)
+{
+}
+static void dump_param(struct edma_desc *edesc, int i)
+{
+}
+static void setup_task_names(struct edma_desc *edesc)
+{
+}
+#endif
+
 /* Dispatch a queued descriptor to the controller (caller holds lock) */
 static void edma_execute(struct edma_chan *echan)
 {
@@ -801,30 +1184,14 @@ static void edma_execute(struct edma_chan *echan)
 		j = i + edesc->processed;
 		edma_write_slot(ecc, echan->slot[i], &edesc->pset[j].param);
 		edesc->sg_len += edesc->pset[j].len;
-		dev_vdbg(dev,
-			 "\n pset[%d]:\n"
-			 "  chnum\t%d\n"
-			 "  slot\t%d\n"
-			 "  opt\t%08x\n"
-			 "  src\t%08x\n"
-			 "  dst\t%08x\n"
-			 "  abcnt\t%08x\n"
-			 "  ccnt\t%08x\n"
-			 "  bidx\t%08x\n"
-			 "  cidx\t%08x\n"
-			 "  lkrld\t%08x\n",
-			 j, echan->ch_num, echan->slot[i],
-			 edesc->pset[j].param.opt,
-			 edesc->pset[j].param.src,
-			 edesc->pset[j].param.dst,
-			 edesc->pset[j].param.a_b_cnt,
-			 edesc->pset[j].param.ccnt,
-			 edesc->pset[j].param.src_dst_bidx,
-			 edesc->pset[j].param.src_dst_cidx,
-			 edesc->pset[j].param.link_bcntrld);
 		/* Link to the previous slot if not the last set */
-		if (i != (nslots - 1))
-			edma_link(ecc, echan->slot[i], echan->slot[i + 1]);
+		if (i != nslots - 1) {
+			int target = i + 1;
+
+			if (edesc->rmd_transfer && edesc->pset[i].link != -1)
+				target = edesc->pset[i].link;
+			edma_link(ecc, echan->slot[i], echan->slot[target]);
+		}
 	}
 
 	edesc->processed += nslots;
@@ -836,10 +1203,14 @@ static void edma_execute(struct edma_chan *echan)
 	 */
 	if (edesc->processed == edesc->pset_nr) {
 		if (edesc->cyclic)
-			edma_link(ecc, echan->slot[nslots - 1], echan->slot[1]);
-		else
-			edma_link(ecc, echan->slot[nslots - 1],
-				  echan->ecc->dummy_slot);
+			edma_link(ecc, echan->slot[nslots-1], echan->slot[1]);
+		else if (edesc->rmd_transfer &&
+				edesc->pset[nslots-1].link != -1) {
+			edma_link(ecc, echan->slot[nslots-1],
+				echan->slot[edesc->pset[nslots-1].link]);
+		} else
+			edma_link(ecc, echan->slot[nslots-1],
+				echan->ecc->dummy_slot);
 	}
 
 	if (echan->missed) {
@@ -1016,6 +1387,7 @@ static int edma_config_pset(struct dma_chan *chan, struct edma_pset *epset,
 	}
 
 	epset->len = dma_length;
+	epset->link = -1;
 
 	if (direction == DMA_MEM_TO_DEV) {
 		src_bidx = acnt;
@@ -1464,27 +1836,7 @@ static struct dma_async_tx_descriptor *edma_prep_dma_cyclic(
 			src_addr += period_len;
 
 		dev_vdbg(dev, "%s: Configure period %d of buf:\n", __func__, i);
-		dev_vdbg(dev,
-			"\n pset[%d]:\n"
-			"  chnum\t%d\n"
-			"  slot\t%d\n"
-			"  opt\t%08x\n"
-			"  src\t%08x\n"
-			"  dst\t%08x\n"
-			"  abcnt\t%08x\n"
-			"  ccnt\t%08x\n"
-			"  bidx\t%08x\n"
-			"  cidx\t%08x\n"
-			"  lkrld\t%08x\n",
-			i, echan->ch_num, echan->slot[i],
-			edesc->pset[i].param.opt,
-			edesc->pset[i].param.src,
-			edesc->pset[i].param.dst,
-			edesc->pset[i].param.a_b_cnt,
-			edesc->pset[i].param.ccnt,
-			edesc->pset[i].param.src_dst_bidx,
-			edesc->pset[i].param.src_dst_cidx,
-			edesc->pset[i].param.link_bcntrld);
+		dump_param(edesc, i);
 
 		edesc->absync = ret;
 
@@ -1507,6 +1859,465 @@ static struct dma_async_tx_descriptor *edma_prep_dma_cyclic(
 	return vchan_tx_prep(&echan->vchan, &edesc->vdesc, tx_flags);
 }
 
+static void update_link(struct edma_desc *edesc, int tx_start, int dest)
+{
+	int n, cidx, chain_num;
+	struct edma_chan *echan = edesc->echan;
+
+	/* The CIDX of "Update-Link" (dest) contains the relative addresses of
+	 * "tx_start[1]" in DSTCIDX and "tx_start[2]" in SRCCIDX
+	 */
+	n = tx_start + edesc->rmd_task_num + 1;
+	cidx =  PARM_OFFSET(echan->slot[n]) |
+		PARM_OFFSET(echan->slot[n + edesc->rmd_task_num]) << 16;
+
+	/* Write the CIDX to all 3 "Update-Link":SRC_DST_CIDX */
+	for (chain_num = 0; chain_num < RMD_TRANSFER_CHAINS; chain_num++) {
+		int slot_idx = dest + chain_num * edesc->rmd_task_num + 1;
+		edesc->pset[slot_idx].param.src_dst_cidx = cidx;
+	}
+}
+
+static struct edma_desc *device_prep_dma_rmd_from_device(
+	struct edma_chan *echan, dma_addr_t fpga_base, struct rmd_config *rmd,
+	dma_addr_t mem, size_t chunk_size, dma_addr_t buf_ctrl)
+{
+	struct edma_desc *edesc;
+	struct dma_chan *chan = &echan->vchan.chan;
+	struct device *dev = chan->device->dev;
+	struct edma_cc *ecc = echan->ecc;
+	int i, n, ret, len = rmd->idet_fifo_pdi_size;
+
+	edesc = kzalloc(sizeof(*edesc) + RMD_SLOTS_RX*sizeof(edesc->pset[0]),
+			GFP_ATOMIC);
+	if (!edesc)
+		return NULL;
+
+	edesc->pset_nr = RMD_SLOTS_RX;
+	edesc->residue = edesc->residue_stat = len;
+	edesc->direction = DMA_DEV_TO_MEM;
+	edesc->echan = echan;
+	edesc->rmd_mem = mem;
+	edesc->rmd_chunk_size = chunk_size;
+	edesc->rmd_transfer = 1;
+	edesc->rmd_task_num = rmd_rx_max;
+	setup_task_names(edesc);
+
+	BUILD_BUG_ON(RMD_SLOTS_RX > EDMA_MAX_SLOTS);
+
+	for (i = 0; i < RMD_SLOTS_RX; i++) {
+		/* Chain 0, 1 or 2 == tripple buffer number */
+		int chain_num = (i-1) / edesc->rmd_task_num;
+
+		if (echan->slot[i] < 0)
+			echan->slot[i] = edma_alloc_slot(ecc, EDMA_SLOT_ANY);
+		if (echan->slot[i] < 0) {
+			dev_err(dev, "%s: Failed to allocate slot\n", __func__);
+			return NULL;
+		}
+
+		/* Setup 3 chains: 1-9 (Buf0), 10-19 (Buf1), 19-27 (Buf2) */
+		switch ((i-1) % edesc->rmd_task_num) {
+		case -1:
+			/* Working buffer. Initially a dummy set */
+			ret = edma_config_pset(chan, edesc->pset + i,
+						0, 0, 0, 1, 0, DMA_MEM_TO_MEM);
+			/* Links and triggers "Fifo-Trans" */
+			edesc->pset[i].param.opt |= TCCHEN;
+			break;
+		case rmd_rx_fifo_trans:
+			/* "FIFO-Trans": Transfers up to 2K FIFO data
+			 * from the RMD to the DRAM Buffer
+			 */
+			ret = edma_config_pset(chan, edesc->pset + i,
+				fpga_base + rmd->idet_fifo_pdi_offset,
+				mem + chain_num * chunk_size,
+				len / DMA_SLAVE_BUSWIDTH_4_BYTES,
+				DMA_SLAVE_BUSWIDTH_4_BYTES, len,
+				DMA_DEV_TO_MEM);
+			/* "FIFO-Trans" links and triggers "ACK-Write" */
+			edesc->pset[i].param.opt |= TCCHEN;
+			break;
+		case rmd_rx_ack_write:
+			/* "ACK-Write": Acknowledge data transmition.
+			 * Set the ACK bit in the FPGA:IDET_CS after transfer.
+			 * The unused PARM_SRC_DST_CIDX is used as source.
+			 */
+			ret = edma_config_pset(chan, edesc->pset + i,
+				param_addr(echan, i) + PARM_SRC_DST_CIDX,
+				fpga_base + rmd->idet_pdi_cs_offset, 1,
+				sizeof(u32), sizeof(u32), DMA_MEM_TO_MEM);
+			edesc->pset[i].param.src_dst_cidx =
+					BIT(rmd->rx_read_finish_bit_offset);
+			/* "ACK-write" links and triggers "Data-Valid" */
+			edesc->pset[i].param.opt |= TCCHEN;
+			break;
+		case rmd_rx_lock_bufctrl:
+			/* "Buffer-Lock": Sets "buf_ctrl:flag" word to 0x101 to
+			 * temporarily lock out the consumer.
+			 */
+			ret = edma_config_pset(chan, edesc->pset + i,
+				param_addr(echan, i) + PARM_SRC_DST_CIDX,
+				buf_ctrl + RMD_FLAG_NEW_DATA_BYTE, 1,
+				sizeof(u16), sizeof(u16), DMA_MEM_TO_MEM);
+			edesc->pset[i].param.src_dst_cidx = RMD_FLAG_LOCK >> 16;
+
+			/* "Buffer-Lock" links and triggers "Data-Valid" */
+			edesc->pset[i].param.opt |= TCCHEN;
+			break;
+		case rmd_rx_data_valid:
+			/* "Data-Valid": Re-programs the linked "Data-Switch".
+			 * Transfers 8bit DataValidBit(3) from FPGA:IDET_CS
+			 * to "Data-Switch":ACNT
+			 * Because loading the linked PaRAM already happened
+			 * when this transfer takes place,
+			 * we need to overwrite ACNT in the working slot (i=0)
+			 * and not slot[case rmd_rx_data_switch]
+			 */
+			ret = edma_config_pset(chan, edesc->pset + i,
+					fpga_base + rmd->idet_pdi_cs_offset,
+					param_addr(echan, 0) + PARM_A_B_CNT,
+					1, 1, 1, DMA_MEM_TO_MEM);
+			if (rmd->rx_data_valid_bit_offset != 2)
+				ret = -EINVAL;
+			/* "Data-Valid" links and triggers "Data-Switch" */
+			edesc->pset[i].param.opt |= TCCHEN;
+			break;
+		case rmd_rx_data_switch:
+			/* "Data-Switch": Transfers (or not) "Copy-BufCtrl"
+			 * to slot[0]. Depending on the ACNT,
+			 * set by "Data-Valid"
+			 */
+			ret = edma_config_pset(chan, edesc->pset + i,
+				0, // SRC will be set by "Copy-BufCtrl" setup
+				param_addr(echan, 0), PARM_SIZE / sizeof(u32),
+				sizeof(u32), PARM_SIZE, DMA_MEM_TO_MEM);
+			/* "Data-Switch" links "Buffer-Unlock" to prepare
+			 * "FIFO-Trans" with the same buffer
+			 * if "Data-Valid" was not set
+			 */
+			edesc->pset[i].param.opt |= TCCHEN;
+			break;
+		case rmd_rx_unlock_bufctrl:
+			/* "Buffer-Unlock": Sets "buf_ctrl:flag" word to 0 to
+			 * release the buffer management word and indicate:
+			 * "No new data"
+			 */
+			ret = edma_config_pset(chan, edesc->pset + i,
+				param_addr(echan, i) + PARM_SRC_DST_CIDX,
+				buf_ctrl + RMD_FLAG_NEW_DATA_BYTE, 1,
+				sizeof(u16), sizeof(u16), DMA_MEM_TO_MEM);
+			edesc->pset[i].param.src_dst_cidx = 0;
+
+			/* "Buffer-Unlock" links "FIFO-Trans" and repeats
+			 * with the same buffer on the next TR event
+			 */
+			n = i - (rmd_rx_unlock_bufctrl - rmd_rx_fifo_trans);
+			edesc->pset[i].link = n;
+			break;
+		case rmd_rx_copy_bufctrl:
+			/* "Copy-BufCtrl": Copy number of third, unused buffer
+			 * 0, 1 or 2 to "Update-Link":BCNT in slot 0
+			 */
+			ret = edma_config_pset(chan, edesc->pset + i,
+				buf_ctrl,
+				param_addr(echan, 0) + PARM_B_CNT,
+				1, 1, 1, DMA_MEM_TO_DEV);
+
+			/* Set SRC of "Data-Switch" transfer to us */
+			n = i - (rmd_rx_copy_bufctrl - rmd_rx_data_switch);
+			edesc->pset[n].param.src = param_addr(echan, i);
+
+			/* "Copy-BufCtrl" links and triggers "Update-Link" */
+			edesc->pset[i].param.opt |= TCCHEN;
+			break;
+		case rmd_rx_update_link:
+			/* "Update-Link": Update the LINK register of
+			 * "Update-BufCtrl" in slot[0]
+			 * Copies either nothing, DST_CIDX or SRC_CIDX
+			 * to "Update-BufCtrl":LINK in slot 0
+			 * DST_CIDX and SRC_CIDX cannot be calculated, yet.
+			 * Will happen in another loop below
+			 */
+			ret = edma_config_pset(chan, edesc->pset + i,
+				param_addr(echan, i) + PARM_SRC_DST_CIDX,
+				param_addr(echan, 0) + PARM_LINK_BCNTRLD,
+				sizeof(u16), sizeof(u16),
+				(RMD_TRANSFER_CHAINS -1) * sizeof(u16),
+				DMA_MEM_TO_DEV);
+
+			/* "Update-Link" links and triggers "Update-BufCtrl" */
+			edesc->pset[i].param.opt |= TCCHEN;
+			break;
+		case rmd_rx_update_bufctrl:
+			/* "Update-BufCtrl": write our index and
+			 * RMD_FLAG_NEW_DATA to BufCtrl and also reset the
+			 * RMD_FLAG_LOCK bit
+			 */
+			ret = edma_config_pset(chan, edesc->pset + i,
+				param_addr(echan, i) + PARM_SRC_DST_CIDX,
+				buf_ctrl, 1, sizeof(u32), sizeof(u32),
+				DMA_MEM_TO_DEV);
+
+			edesc->pset[i].param.src_dst_cidx =
+						chain_num + RMD_FLAG_NEW_DATA;
+			/* Chaintrigger ends and awaits the next HW trigger
+			 * "Update-BufCtrl" links FIRST "FIFO-Trans"
+			 */
+			edesc->pset[i].link = rmd_rx_fifo_trans + 1;
+			/* If we want to trigger an interrupt after the
+			 * write - read cycle:
+				edesc->pset[i].param.opt |= TCINTEN;
+			 */
+			break;
+		}
+
+		if (ret < 0) {
+			kfree(edesc);
+			return NULL;
+		}
+	}
+
+	update_link(edesc, rmd_rx_fifo_trans, rmd_rx_update_link);
+	return edesc;
+}
+
+static struct edma_desc *device_prep_dma_rmd_to_device(
+	struct edma_chan *echan, dma_addr_t fpga_base, struct rmd_config *rmd,
+	dma_addr_t mem, size_t chunk_size, dma_addr_t buf_ctrl)
+{
+	struct edma_desc *edesc;
+	struct dma_chan *chan = &echan->vchan.chan;
+	struct device *dev = chan->device->dev;
+	struct edma_cc *ecc = echan->ecc;
+	int i, ret, len = rmd->idet_fifo_pdo_size, n;
+
+	edesc = kzalloc(sizeof(*edesc) + RMD_SLOTS_TX*sizeof(edesc->pset[0]),
+			GFP_ATOMIC);
+	if (!edesc)
+		return NULL;
+
+	edesc->pset_nr = RMD_SLOTS_TX;
+	edesc->residue = edesc->residue_stat = len;
+	edesc->direction = DMA_MEM_TO_DEV;
+	edesc->echan = echan;
+	edesc->rmd_mem = mem;
+	edesc->rmd_chunk_size = chunk_size;
+	edesc->rmd_transfer = 1;
+	edesc->rmd_task_num = rmd_tx_max;
+	setup_task_names(edesc);
+
+	BUILD_BUG_ON(RMD_SLOTS_TX > EDMA_MAX_SLOTS);
+
+	for (i = 0; i < RMD_SLOTS_TX; i++) {
+		/* Chain 0, 1 or 2 == tripple buffer number */
+		int chain_num = (i-1) / edesc->rmd_task_num;
+
+		if (echan->slot[i] < 0)
+			echan->slot[i] = edma_alloc_slot(ecc, EDMA_SLOT_ANY);
+		if (echan->slot[i] < 0) {
+			dev_err(dev, "%s: Failed to allocate slot\n", __func__);
+			return NULL;
+		}
+
+		/* Setup 3 chains: 1-5 (Buf0), 5-8 (Buf1), 9-12 (Buf2) */
+		switch ((i-1) % edesc->rmd_task_num) {
+		case -1:
+			/* Working buffer. Initially a dummy set */
+			ret = edma_config_pset(chan, edesc->pset + i,
+						0, 0, 0, 1, 0, DMA_MEM_TO_MEM);
+			/* Links and triggers "New-Data" */
+			edesc->pset[i].param.opt |= TCCHEN;
+			break;
+		case rmd_tx_new_data:
+			/* "New-Data": Copy new-data flag to
+			 * "Switch-Buf":BCNT in slot 0
+			 */
+			ret = edma_config_pset(chan, edesc->pset + i,
+				buf_ctrl + 2,
+				param_addr(echan, 0) + PARM_A_B_CNT + 2,
+				1, 1, 1, DMA_MEM_TO_DEV);
+			/* Links and triggers "Switch-Buf" */
+			edesc->pset[i].param.opt |= TCCHEN;
+			break;
+		case rmd_tx_switch_buf:
+			/* "Switch-Buf:" Copy (or not) "Lock-BufCtrl" to
+			 * slot 0. If not, we link to "FIFO-Trans"
+			 */
+			ret = edma_config_pset(chan, edesc->pset + i,
+				0, /* SRC will be set by "Lock-BufCtrl" */
+				param_addr(echan, 0), 1, PARM_SIZE,
+				PARM_SIZE, DMA_MEM_TO_MEM);
+			/* "Switch-Buf" links "FIFO-Trans" to read from the old
+			 * buffer without switching
+			 */
+			edesc->pset[i].param.opt |= TCCHEN;
+			edesc->pset[i].link = i - rmd_tx_switch_buf
+						+ rmd_tx_fifo_trans;
+			break;
+		case rmd_tx_lock_bufctrl:
+			/* "Lock-BufCtrl": Sets "buf_ctrl:flag" byte to 0x1 to
+			 * temporarily lock out the producer.
+			 */
+			ret = edma_config_pset(chan, edesc->pset + i,
+				param_addr(echan, i) + PARM_SRC_DST_CIDX,
+				buf_ctrl + RMD_FLAG_LOCK_BYTE,
+				1, 1, 1, DMA_MEM_TO_MEM);
+			edesc->pset[i].param.src_dst_cidx = RMD_FLAG_LOCK >> 24;
+
+			/* Set SRC of "Switch-Buf" transfer to us */
+			n = i - (rmd_tx_lock_bufctrl - rmd_tx_switch_buf);
+			edesc->pset[n].param.src = param_addr(echan, i);
+
+			/* "Lock-BufCtrl" links and triggers "Get-BufNum" */
+			edesc->pset[i].param.opt |= TCCHEN;
+			break;
+		case rmd_tx_get_bufnum:
+			/* "Get-BufNum": Copy BufNum -> "Update-Link":BCNT"
+			 * in slot 0
+			 */
+			ret = edma_config_pset(chan, edesc->pset + i,
+				buf_ctrl,
+				param_addr(echan, 0) + PARM_B_CNT,
+				1, 1, 1, DMA_MEM_TO_DEV);
+			/* Links and triggers "Switch-Buf" */
+			edesc->pset[i].param.opt |= TCCHEN;
+			break;
+		case rmd_tx_update_link:
+			/* "Update-Link": Set "Update-BufCtrl":LINK to
+			 * one of the other 2 chains.
+			 * Copies either nothing, DST_CIDX or SRC_CIDX
+			 * to "Update-BufCtrl":LINK in slot 0
+			 * DST_CIDX and SRC_CIDX cannot be calculated, yet.
+			 * Will happen in another loop below
+			 */
+			ret = edma_config_pset(chan, edesc->pset + i,
+				param_addr(echan, i) + PARM_SRC_DST_CIDX,
+				param_addr(echan, 0) + PARM_LINK_BCNTRLD,
+				sizeof(u16), sizeof(u16),
+				(RMD_TRANSFER_CHAINS -1) * sizeof(u16),
+				DMA_MEM_TO_DEV);
+
+			/* "Update-Link" links and triggers "Update-BufCtrl" */
+			edesc->pset[i].param.opt |= TCCHEN;
+			break;
+		case rmd_tx_update_bufctrl:
+			/* "Update-BufCtrl": write our index and reset
+			 * RMD_FLAG_NEW_DATA and keep RMD_FLAG_LOCK bit
+			 * in buf_ctrl
+			 */
+			ret = edma_config_pset(chan, edesc->pset + i,
+				param_addr(echan, i) + PARM_SRC_DST_CIDX,
+				buf_ctrl, 1,
+				sizeof(u32), sizeof(u32), DMA_MEM_TO_DEV);
+
+			edesc->pset[i].param.src_dst_cidx =
+						chain_num | RMD_FLAG_LOCK;
+			/* "Update-BufCtrl" triggers "Unlock-BufCtrl" */
+			edesc->pset[i].param.opt |= TCCHEN;
+			edesc->pset[i].link = rmd_tx_unlock + 1;
+			break;
+		case rmd_tx_unlock:
+			/* "Unlock-BufCtrl":reset RMD_FLAG_LOCK bit
+			 * in buf_ctrl
+			 */
+			ret = edma_config_pset(chan, edesc->pset + i,
+				param_addr(echan, i) + PARM_SRC_DST_CIDX,
+				buf_ctrl + RMD_FLAG_LOCK_BYTE,
+				1, 1, 1, DMA_MEM_TO_MEM);
+			edesc->pset[i].param.src_dst_cidx = 0;
+
+			/* "Update-BufCtrl" triggers "FIFO-Trans"
+			 * in the first chain, because "Update-Link"
+			 * for Buffer 0 does not copy anything.
+			 */
+			edesc->pset[i].param.opt |= TCCHEN;
+			break;
+		case rmd_tx_fifo_trans:
+			/* "FIFO-Trans": Transfers up to 2K FIFO data
+			 * from the DRAM Buffer to the RMD TX FIFO
+			 */
+			ret = edma_config_pset(chan, edesc->pset + i,
+				mem + chain_num * chunk_size,
+				fpga_base + rmd->idet_fifo_pdo_offset,
+				len / DMA_SLAVE_BUSWIDTH_4_BYTES,
+				DMA_SLAVE_BUSWIDTH_4_BYTES, len,
+				DMA_MEM_TO_DEV);
+			/* "FIFO-Trans" links and triggers "ACK-Write" */
+			edesc->pset[i].param.opt |= TCCHEN;
+			break;
+		case rmd_tx_ack_write:
+			/* "ACK-Write": Acknowledge data transmition.
+			 * Set the ACK bit in the FPGA:IDET_CS after transfer.
+			 * The unused PARM_SRC_DST_BIDX is used as source.
+			 */
+			ret = edma_config_pset(chan, edesc->pset + i,
+				param_addr(echan, i) + PARM_SRC_DST_CIDX,
+				fpga_base + rmd->idet_pdo_cs_offset, 1,
+				sizeof(u32), sizeof(u32), DMA_MEM_TO_MEM);
+
+			edesc->pset[i].param.src_dst_cidx =
+					BIT(rmd->tx_commit_bit_offset);
+
+			/* Chaining ends here and awaits the next HW trigger */
+			edesc->pset[i].link = i - rmd_tx_ack_write
+						+ rmd_tx_new_data;
+			break;
+		}
+
+		if (ret < 0) {
+			kfree(edesc);
+			return NULL;
+		}
+	}
+
+	update_link(edesc, rmd_tx_unlock, rmd_tx_update_link);
+	return edesc;
+}
+
+static struct dma_async_tx_descriptor *edma_prep_dma_rmd(
+	struct dma_chan *chan, dma_addr_t fpga_base, struct rmd_config *rmd,
+	enum dma_transfer_direction direction, dma_addr_t mem,
+	dma_addr_t buf_ctrl,
+	size_t chunk_size, unsigned long tx_flags)
+{
+	struct edma_chan *echan = to_edma_chan(chan);
+	struct device *dev = chan->device->dev;
+	struct edma_desc *edesc;
+
+	if (unlikely(!echan || !rmd || !rmd->idet_fifo_pdi_size ||
+					!rmd->idet_fifo_pdo_size)) {
+		if (dev && rmd)
+			dev_dbg(dev, "Invalid RMD FIFO configuration %d %d\n",
+				rmd->idet_fifo_pdi_size,
+				rmd->idet_fifo_pdo_size);
+		return NULL;
+	}
+	switch (direction) {
+	case DMA_DEV_TO_MEM:
+		edesc = device_prep_dma_rmd_from_device(echan, fpga_base, rmd,
+						mem, chunk_size, buf_ctrl);
+		break;
+	case DMA_MEM_TO_DEV:
+		edesc = device_prep_dma_rmd_to_device(echan, fpga_base, rmd,
+						mem, chunk_size, buf_ctrl);
+		break;
+	default:
+		dev_err(dev, "%s: bad direction: %d\n", __func__, direction);
+		return NULL;
+	}
+	if (!edesc)
+		return NULL;
+
+	dfs_setup(edesc);
+
+	/* Place channel to highest priority queue */
+	if (!echan->tc)
+		edma_assign_channel_eventq(echan, EVENTQ_0);
+
+	return vchan_tx_prep(&echan->vchan, &edesc->vdesc, tx_flags);
+}
+
 static void edma_completion_handler(struct edma_chan *echan)
 {
 	struct device *dev = echan->vchan.chan.device->dev;
@@ -1523,6 +2334,11 @@ static void edma_completion_handler(struct edma_chan *echan)
 			edesc->residue = 0;
 			edma_stop(echan);
 			vchan_cookie_complete(&edesc->vdesc);
+			if (edesc->pset_nr > 2) {
+				edma_read_slot(echan->ecc, echan->slot[2],
+						&edesc->pset[2].param);
+				dump_param(edesc, 2);
+			}
 			echan->edesc = NULL;
 
 			dev_dbg(dev, "Transfer completed on channel %d\n",
@@ -1987,6 +2803,7 @@ static void edma_dma_init(struct edma_cc *ecc, bool legacy_mode)
 	dma_cap_zero(s_ddev->cap_mask);
 	dma_cap_set(DMA_SLAVE, s_ddev->cap_mask);
 	dma_cap_set(DMA_CYCLIC, s_ddev->cap_mask);
+	dma_cap_set(DMA_RMD, s_ddev->cap_mask);
 	if (ecc->legacy_mode && !memcpy_channels) {
 		dev_warn(ecc->dev,
 			 "Legacy memcpy is enabled, things might not work\n");
@@ -2000,6 +2817,7 @@ static void edma_dma_init(struct edma_cc *ecc, bool legacy_mode)
 
 	s_ddev->device_prep_slave_sg = edma_prep_slave_sg;
 	s_ddev->device_prep_dma_cyclic = edma_prep_dma_cyclic;
+	s_ddev->device_prep_dma_rmd = edma_prep_dma_rmd;
 	s_ddev->device_alloc_chan_resources = edma_alloc_chan_resources;
 	s_ddev->device_free_chan_resources = edma_free_chan_resources;
 	s_ddev->device_issue_pending = edma_issue_pending;
@@ -2396,6 +3214,7 @@ static int edma_probe(struct platform_device *pdev)
 	if (IS_ERR(ecc->base))
 		return PTR_ERR(ecc->base);
 
+	ecc->phys = mem->start;
 	platform_set_drvdata(pdev, ecc);
 
 	pm_runtime_enable(dev);
diff --git a/drivers/firewire/ohci.c b/drivers/firewire/ohci.c
index 9811c40956e5..17c9d825188b 100644
--- a/drivers/firewire/ohci.c
+++ b/drivers/firewire/ohci.c
@@ -2545,7 +2545,7 @@ static int ohci_cancel_packet(struct fw_card *card, struct fw_packet *packet)
 	struct driver_data *driver_data = packet->driver_data;
 	int ret = -ENOENT;
 
-	tasklet_disable(&ctx->tasklet);
+	tasklet_disable_in_atomic(&ctx->tasklet);
 
 	if (packet->ack != 0)
 		goto out;
@@ -3465,7 +3465,7 @@ static int ohci_flush_iso_completions(struct fw_iso_context *base)
 	struct iso_context *ctx = container_of(base, struct iso_context, base);
 	int ret = 0;
 
-	tasklet_disable(&ctx->context.tasklet);
+	tasklet_disable_in_atomic(&ctx->context.tasklet);
 
 	if (!test_and_set_bit_lock(0, &ctx->flushing_completions)) {
 		context_tasklet((unsigned long)&ctx->context);
diff --git a/drivers/firmware/efi/efi.c b/drivers/firmware/efi/efi.c
index df3f9bcab581..709c65c0a816 100644
--- a/drivers/firmware/efi/efi.c
+++ b/drivers/firmware/efi/efi.c
@@ -66,7 +66,7 @@ struct mm_struct efi_mm = {
 
 struct workqueue_struct *efi_rts_wq;
 
-static bool disable_runtime;
+static bool disable_runtime = IS_ENABLED(CONFIG_PREEMPT_RT);
 static int __init setup_noefi(char *arg)
 {
 	disable_runtime = true;
@@ -97,6 +97,9 @@ static int __init parse_efi_cmdline(char *str)
 	if (parse_option_str(str, "noruntime"))
 		disable_runtime = true;
 
+	if (parse_option_str(str, "runtime"))
+		disable_runtime = false;
+
 	if (parse_option_str(str, "nosoftreserve"))
 		set_bit(EFI_MEM_NO_SOFT_RESERVE, &efi.flags);
 
diff --git a/drivers/gpio/gpio-omap.c b/drivers/gpio/gpio-omap.c
index f7ceb2b11afc..c15fce0c8195 100644
--- a/drivers/gpio/gpio-omap.c
+++ b/drivers/gpio/gpio-omap.c
@@ -1057,7 +1057,7 @@ static int omap_gpio_chip_init(struct gpio_bank *bank, struct irq_chip *irqc)
 
 	ret = devm_request_irq(bank->chip.parent, bank->irq,
 			       omap_gpio_irq_handler,
-			       0, dev_name(bank->chip.parent), bank);
+			       IRQF_THREAD_TBL_LOOKUP, dev_name(bank->chip.parent), bank);
 	if (ret)
 		gpiochip_remove(&bank->chip);
 
diff --git a/drivers/gpu/drm/i915/display/intel_sprite.c b/drivers/gpu/drm/i915/display/intel_sprite.c
index 12f7128b777f..a65061e3e1d3 100644
--- a/drivers/gpu/drm/i915/display/intel_sprite.c
+++ b/drivers/gpu/drm/i915/display/intel_sprite.c
@@ -118,7 +118,8 @@ void intel_pipe_update_start(const struct intel_crtc_state *new_crtc_state)
 			"PSR idle timed out 0x%x, atomic update may fail\n",
 			psr_status);
 
-	local_irq_disable();
+	if (!IS_ENABLED(CONFIG_PREEMPT_RT))
+		local_irq_disable();
 
 	crtc->debug.min_vbl = min;
 	crtc->debug.max_vbl = max;
@@ -143,11 +144,13 @@ void intel_pipe_update_start(const struct intel_crtc_state *new_crtc_state)
 			break;
 		}
 
-		local_irq_enable();
+		if (!IS_ENABLED(CONFIG_PREEMPT_RT))
+			local_irq_enable();
 
 		timeout = schedule_timeout(timeout);
 
-		local_irq_disable();
+		if (!IS_ENABLED(CONFIG_PREEMPT_RT))
+			local_irq_disable();
 	}
 
 	finish_wait(wq, &wait);
@@ -180,7 +183,8 @@ void intel_pipe_update_start(const struct intel_crtc_state *new_crtc_state)
 	return;
 
 irq_disable:
-	local_irq_disable();
+	if (!IS_ENABLED(CONFIG_PREEMPT_RT))
+		local_irq_disable();
 }
 
 /**
@@ -218,7 +222,8 @@ void intel_pipe_update_end(struct intel_crtc_state *new_crtc_state)
 		new_crtc_state->uapi.event = NULL;
 	}
 
-	local_irq_enable();
+	if (!IS_ENABLED(CONFIG_PREEMPT_RT))
+		local_irq_enable();
 
 	if (intel_vgpu_active(dev_priv))
 		return;
diff --git a/drivers/gpu/drm/i915/gem/i915_gem_execbuffer.c b/drivers/gpu/drm/i915/gem/i915_gem_execbuffer.c
index bd3046e5a934..1850f13e9d19 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_execbuffer.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_execbuffer.c
@@ -1081,7 +1081,7 @@ static void reloc_cache_reset(struct reloc_cache *cache, struct i915_execbuffer
 		struct i915_ggtt *ggtt = cache_to_ggtt(cache);
 
 		intel_gt_flush_ggtt_writes(ggtt->vm.gt);
-		io_mapping_unmap_atomic((void __iomem *)vaddr);
+		io_mapping_unmap_local((void __iomem *)vaddr);
 
 		if (drm_mm_node_allocated(&cache->node)) {
 			ggtt->vm.clear_range(&ggtt->vm,
@@ -1147,7 +1147,7 @@ static void *reloc_iomap(struct drm_i915_gem_object *obj,
 
 	if (cache->vaddr) {
 		intel_gt_flush_ggtt_writes(ggtt->vm.gt);
-		io_mapping_unmap_atomic((void __force __iomem *) unmask_page(cache->vaddr));
+		io_mapping_unmap_local((void __force __iomem *) unmask_page(cache->vaddr));
 	} else {
 		struct i915_vma *vma;
 		int err;
@@ -1195,8 +1195,7 @@ static void *reloc_iomap(struct drm_i915_gem_object *obj,
 		offset += page << PAGE_SHIFT;
 	}
 
-	vaddr = (void __force *)io_mapping_map_atomic_wc(&ggtt->iomap,
-							 offset);
+	vaddr = (void __force *)io_mapping_map_local_wc(&ggtt->iomap, offset);
 	cache->page = page;
 	cache->vaddr = (unsigned long)vaddr;
 
diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
index f7b2e07e2229..313d8a28e776 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
@@ -60,9 +60,10 @@ static int __engine_unpark(struct intel_wakeref *wf)
 
 static inline unsigned long __timeline_mark_lock(struct intel_context *ce)
 {
-	unsigned long flags;
+	unsigned long flags = 0;
 
-	local_irq_save(flags);
+	if (!force_irqthreads)
+		local_irq_save(flags);
 	mutex_acquire(&ce->timeline->mutex.dep_map, 2, 0, _THIS_IP_);
 
 	return flags;
@@ -72,7 +73,8 @@ static inline void __timeline_mark_unlock(struct intel_context *ce,
 					  unsigned long flags)
 {
 	mutex_release(&ce->timeline->mutex.dep_map, _THIS_IP_);
-	local_irq_restore(flags);
+	if (!force_irqthreads)
+		local_irq_restore(flags);
 }
 
 #else
diff --git a/drivers/gpu/drm/i915/i915_gem.c b/drivers/gpu/drm/i915/i915_gem.c
index 58276694c848..88944c3b1bc8 100644
--- a/drivers/gpu/drm/i915/i915_gem.c
+++ b/drivers/gpu/drm/i915/i915_gem.c
@@ -355,22 +355,15 @@ gtt_user_read(struct io_mapping *mapping,
 	      char __user *user_data, int length)
 {
 	void __iomem *vaddr;
-	unsigned long unwritten;
+	bool fail = false;
 
 	/* We can use the cpu mem copy function because this is X86. */
-	vaddr = io_mapping_map_atomic_wc(mapping, base);
-	unwritten = __copy_to_user_inatomic(user_data,
-					    (void __force *)vaddr + offset,
-					    length);
-	io_mapping_unmap_atomic(vaddr);
-	if (unwritten) {
-		vaddr = io_mapping_map_wc(mapping, base, PAGE_SIZE);
-		unwritten = copy_to_user(user_data,
-					 (void __force *)vaddr + offset,
-					 length);
-		io_mapping_unmap(vaddr);
-	}
-	return unwritten;
+	vaddr = io_mapping_map_local_wc(mapping, base);
+	if (copy_to_user(user_data, (void __force *)vaddr + offset, length))
+		fail = true;
+	io_mapping_unmap_local(vaddr);
+
+	return fail;
 }
 
 static int
@@ -539,21 +532,14 @@ ggtt_write(struct io_mapping *mapping,
 	   char __user *user_data, int length)
 {
 	void __iomem *vaddr;
-	unsigned long unwritten;
+	bool fail = false;
 
 	/* We can use the cpu mem copy function because this is X86. */
-	vaddr = io_mapping_map_atomic_wc(mapping, base);
-	unwritten = __copy_from_user_inatomic_nocache((void __force *)vaddr + offset,
-						      user_data, length);
-	io_mapping_unmap_atomic(vaddr);
-	if (unwritten) {
-		vaddr = io_mapping_map_wc(mapping, base, PAGE_SIZE);
-		unwritten = copy_from_user((void __force *)vaddr + offset,
-					   user_data, length);
-		io_mapping_unmap(vaddr);
-	}
-
-	return unwritten;
+	vaddr = io_mapping_map_local_wc(mapping, base);
+	if (copy_from_user((void __force *)vaddr + offset, user_data, length))
+		fail = true;
+	io_mapping_unmap_local(vaddr);
+	return fail;
 }
 
 /**
diff --git a/drivers/gpu/drm/i915/i915_irq.c b/drivers/gpu/drm/i915/i915_irq.c
index 759f523c6a6b..7339a42ab2b8 100644
--- a/drivers/gpu/drm/i915/i915_irq.c
+++ b/drivers/gpu/drm/i915/i915_irq.c
@@ -847,6 +847,7 @@ static bool i915_get_crtc_scanoutpos(struct drm_crtc *_crtc,
 	spin_lock_irqsave(&dev_priv->uncore.lock, irqflags);
 
 	/* preempt_disable_rt() should go right here in PREEMPT_RT patchset. */
+	preempt_disable_rt();
 
 	/* Get optional system timestamp before query. */
 	if (stime)
@@ -898,6 +899,7 @@ static bool i915_get_crtc_scanoutpos(struct drm_crtc *_crtc,
 		*etime = ktime_get();
 
 	/* preempt_enable_rt() should go right here in PREEMPT_RT patchset. */
+	preempt_enable_rt();
 
 	spin_unlock_irqrestore(&dev_priv->uncore.lock, irqflags);
 
diff --git a/drivers/gpu/drm/i915/i915_trace.h b/drivers/gpu/drm/i915/i915_trace.h
index a4addcc64978..396b6598694d 100644
--- a/drivers/gpu/drm/i915/i915_trace.h
+++ b/drivers/gpu/drm/i915/i915_trace.h
@@ -2,6 +2,10 @@
 #if !defined(_I915_TRACE_H_) || defined(TRACE_HEADER_MULTI_READ)
 #define _I915_TRACE_H_
 
+#ifdef CONFIG_PREEMPT_RT
+#define NOTRACE
+#endif
+
 #include <linux/stringify.h>
 #include <linux/types.h>
 #include <linux/tracepoint.h>
@@ -778,7 +782,7 @@ DEFINE_EVENT(i915_request, i915_request_add,
 	    TP_ARGS(rq)
 );
 
-#if defined(CONFIG_DRM_I915_LOW_LEVEL_TRACEPOINTS)
+#if defined(CONFIG_DRM_I915_LOW_LEVEL_TRACEPOINTS) && !defined(NOTRACE)
 DEFINE_EVENT(i915_request, i915_request_submit,
 	     TP_PROTO(struct i915_request *rq),
 	     TP_ARGS(rq)
diff --git a/drivers/gpu/drm/i915/selftests/i915_gem.c b/drivers/gpu/drm/i915/selftests/i915_gem.c
index 412e21604a05..432493183d20 100644
--- a/drivers/gpu/drm/i915/selftests/i915_gem.c
+++ b/drivers/gpu/drm/i915/selftests/i915_gem.c
@@ -57,12 +57,12 @@ static void trash_stolen(struct drm_i915_private *i915)
 
 		ggtt->vm.insert_page(&ggtt->vm, dma, slot, I915_CACHE_NONE, 0);
 
-		s = io_mapping_map_atomic_wc(&ggtt->iomap, slot);
+		s = io_mapping_map_local_wc(&ggtt->iomap, slot);
 		for (x = 0; x < PAGE_SIZE / sizeof(u32); x++) {
 			prng = next_pseudo_random32(prng);
 			iowrite32(prng, &s[x]);
 		}
-		io_mapping_unmap_atomic(s);
+		io_mapping_unmap_local(s);
 	}
 
 	ggtt->vm.clear_range(&ggtt->vm, slot, PAGE_SIZE);
diff --git a/drivers/gpu/drm/i915/selftests/i915_gem_gtt.c b/drivers/gpu/drm/i915/selftests/i915_gem_gtt.c
index 713770fb2b92..a68bed4e6b64 100644
--- a/drivers/gpu/drm/i915/selftests/i915_gem_gtt.c
+++ b/drivers/gpu/drm/i915/selftests/i915_gem_gtt.c
@@ -1200,9 +1200,9 @@ static int igt_ggtt_page(void *arg)
 		u64 offset = tmp.start + order[n] * PAGE_SIZE;
 		u32 __iomem *vaddr;
 
-		vaddr = io_mapping_map_atomic_wc(&ggtt->iomap, offset);
+		vaddr = io_mapping_map_local_wc(&ggtt->iomap, offset);
 		iowrite32(n, vaddr + n);
-		io_mapping_unmap_atomic(vaddr);
+		io_mapping_unmap_local(vaddr);
 	}
 	intel_gt_flush_ggtt_writes(ggtt->vm.gt);
 
@@ -1212,9 +1212,9 @@ static int igt_ggtt_page(void *arg)
 		u32 __iomem *vaddr;
 		u32 val;
 
-		vaddr = io_mapping_map_atomic_wc(&ggtt->iomap, offset);
+		vaddr = io_mapping_map_local_wc(&ggtt->iomap, offset);
 		val = ioread32(vaddr + n);
-		io_mapping_unmap_atomic(vaddr);
+		io_mapping_unmap_local(vaddr);
 
 		if (val != n) {
 			pr_err("insert page failed: found %d, expected %d\n",
diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/devinit/fbmem.h b/drivers/gpu/drm/nouveau/nvkm/subdev/devinit/fbmem.h
index 6c5bbff12eb4..411f91ee20fa 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/devinit/fbmem.h
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/devinit/fbmem.h
@@ -60,19 +60,19 @@ fbmem_fini(struct io_mapping *fb)
 static inline u32
 fbmem_peek(struct io_mapping *fb, u32 off)
 {
-	u8 __iomem *p = io_mapping_map_atomic_wc(fb, off & PAGE_MASK);
+	u8 __iomem *p = io_mapping_map_local_wc(fb, off & PAGE_MASK);
 	u32 val = ioread32(p + (off & ~PAGE_MASK));
-	io_mapping_unmap_atomic(p);
+	io_mapping_unmap_local(p);
 	return val;
 }
 
 static inline void
 fbmem_poke(struct io_mapping *fb, u32 off, u32 val)
 {
-	u8 __iomem *p = io_mapping_map_atomic_wc(fb, off & PAGE_MASK);
+	u8 __iomem *p = io_mapping_map_local_wc(fb, off & PAGE_MASK);
 	iowrite32(val, p + (off & ~PAGE_MASK));
 	wmb();
-	io_mapping_unmap_atomic(p);
+	io_mapping_unmap_local(p);
 }
 
 static inline bool
diff --git a/drivers/gpu/drm/panel/panel-simple.c b/drivers/gpu/drm/panel/panel-simple.c
index 204674fccd64..bbbb9cb34a87 100644
--- a/drivers/gpu/drm/panel/panel-simple.c
+++ b/drivers/gpu/drm/panel/panel-simple.c
@@ -1625,6 +1625,31 @@ static const struct panel_desc dataimage_scf0700c48ggu18 = {
 	.bus_flags = DRM_BUS_FLAG_DE_HIGH | DRM_BUS_FLAG_PIXDATA_DRIVE_POSEDGE,
 };
 
+static const struct drm_display_mode dataimage_fg040321duswmg01_mode = {
+	.clock = 9000,
+	.hdisplay = 480,
+	.hsync_start = 480 + 2,
+	.hsync_end = 480 + 2 + 2,
+	.htotal = 480 + 2 + 2 + 41,
+	.vdisplay = 272,
+	.vsync_start = 272 + 2,
+	.vsync_end = 272 + 2 + 2,
+	.vtotal = 272 + 2 + 2 + 10,
+	.flags = DRM_MODE_FLAG_NVSYNC | DRM_MODE_FLAG_NHSYNC |
+			 DRM_BUS_FLAG_DE_HIGH | DRM_BUS_FLAG_PIXDATA_DRIVE_POSEDGE,
+};
+
+static const struct panel_desc dataimage_fg040321duswmg01 = {
+	.modes = &dataimage_fg040321duswmg01_mode,
+	.num_modes = 1,
+	.bpc = 8,
+	.size = {
+		.width = 95,
+		.height = 54,
+	},
+	.bus_flags = DRM_BUS_FLAG_PIXDATA_DRIVE_POSEDGE,
+};
+
 static const struct display_timing dlc_dlc0700yzg_1_timing = {
 	.pixelclock = { 45000000, 51200000, 57000000 },
 	.hactive = { 1024, 1024, 1024 },
@@ -4015,6 +4040,9 @@ static const struct of_device_id platform_of_match[] = {
 	}, {
 		.compatible = "dataimage,scf0700c48ggu18",
 		.data = &dataimage_scf0700c48ggu18,
+	}, {
+		.compatible = "dataimage,fg040321duswmg01",
+		.data = &dataimage_fg040321duswmg01,
 	}, {
 		.compatible = "dlc,dlc0700yzg-1",
 		.data = &dlc_dlc0700yzg_1,
diff --git a/drivers/gpu/drm/qxl/qxl_image.c b/drivers/gpu/drm/qxl/qxl_image.c
index 60ab7151b84d..93f92ccd42e5 100644
--- a/drivers/gpu/drm/qxl/qxl_image.c
+++ b/drivers/gpu/drm/qxl/qxl_image.c
@@ -124,12 +124,12 @@ qxl_image_init_helper(struct qxl_device *qdev,
 				  wrong (check the bitmaps are sent correctly
 				  first) */
 
-	ptr = qxl_bo_kmap_atomic_page(qdev, chunk_bo, 0);
+	ptr = qxl_bo_kmap_local_page(qdev, chunk_bo, 0);
 	chunk = ptr;
 	chunk->data_size = height * chunk_stride;
 	chunk->prev_chunk = 0;
 	chunk->next_chunk = 0;
-	qxl_bo_kunmap_atomic_page(qdev, chunk_bo, ptr);
+	qxl_bo_kunmap_local_page(qdev, chunk_bo, ptr);
 
 	{
 		void *k_data, *i_data;
@@ -143,7 +143,7 @@ qxl_image_init_helper(struct qxl_device *qdev,
 			i_data = (void *)data;
 
 			while (remain > 0) {
-				ptr = qxl_bo_kmap_atomic_page(qdev, chunk_bo, page << PAGE_SHIFT);
+				ptr = qxl_bo_kmap_local_page(qdev, chunk_bo, page << PAGE_SHIFT);
 
 				if (page == 0) {
 					chunk = ptr;
@@ -157,7 +157,7 @@ qxl_image_init_helper(struct qxl_device *qdev,
 
 				memcpy(k_data, i_data, size);
 
-				qxl_bo_kunmap_atomic_page(qdev, chunk_bo, ptr);
+				qxl_bo_kunmap_local_page(qdev, chunk_bo, ptr);
 				i_data += size;
 				remain -= size;
 				page++;
@@ -175,10 +175,10 @@ qxl_image_init_helper(struct qxl_device *qdev,
 					page_offset = offset_in_page(out_offset);
 					size = min((int)(PAGE_SIZE - page_offset), remain);
 
-					ptr = qxl_bo_kmap_atomic_page(qdev, chunk_bo, page_base);
+					ptr = qxl_bo_kmap_local_page(qdev, chunk_bo, page_base);
 					k_data = ptr + page_offset;
 					memcpy(k_data, i_data, size);
-					qxl_bo_kunmap_atomic_page(qdev, chunk_bo, ptr);
+					qxl_bo_kunmap_local_page(qdev, chunk_bo, ptr);
 					remain -= size;
 					i_data += size;
 					out_offset += size;
@@ -189,7 +189,7 @@ qxl_image_init_helper(struct qxl_device *qdev,
 	qxl_bo_kunmap(chunk_bo);
 
 	image_bo = dimage->bo;
-	ptr = qxl_bo_kmap_atomic_page(qdev, image_bo, 0);
+	ptr = qxl_bo_kmap_local_page(qdev, image_bo, 0);
 	image = ptr;
 
 	image->descriptor.id = 0;
@@ -212,7 +212,7 @@ qxl_image_init_helper(struct qxl_device *qdev,
 		break;
 	default:
 		DRM_ERROR("unsupported image bit depth\n");
-		qxl_bo_kunmap_atomic_page(qdev, image_bo, ptr);
+		qxl_bo_kunmap_local_page(qdev, image_bo, ptr);
 		return -EINVAL;
 	}
 	image->u.bitmap.flags = QXL_BITMAP_TOP_DOWN;
@@ -222,7 +222,7 @@ qxl_image_init_helper(struct qxl_device *qdev,
 	image->u.bitmap.palette = 0;
 	image->u.bitmap.data = qxl_bo_physical_address(qdev, chunk_bo, 0);
 
-	qxl_bo_kunmap_atomic_page(qdev, image_bo, ptr);
+	qxl_bo_kunmap_local_page(qdev, image_bo, ptr);
 
 	return 0;
 }
diff --git a/drivers/gpu/drm/qxl/qxl_ioctl.c b/drivers/gpu/drm/qxl/qxl_ioctl.c
index 5cea6eea72ab..785023081b79 100644
--- a/drivers/gpu/drm/qxl/qxl_ioctl.c
+++ b/drivers/gpu/drm/qxl/qxl_ioctl.c
@@ -89,11 +89,11 @@ apply_reloc(struct qxl_device *qdev, struct qxl_reloc_info *info)
 {
 	void *reloc_page;
 
-	reloc_page = qxl_bo_kmap_atomic_page(qdev, info->dst_bo, info->dst_offset & PAGE_MASK);
+	reloc_page = qxl_bo_kmap_local_page(qdev, info->dst_bo, info->dst_offset & PAGE_MASK);
 	*(uint64_t *)(reloc_page + (info->dst_offset & ~PAGE_MASK)) = qxl_bo_physical_address(qdev,
 											      info->src_bo,
 											      info->src_offset);
-	qxl_bo_kunmap_atomic_page(qdev, info->dst_bo, reloc_page);
+	qxl_bo_kunmap_local_page(qdev, info->dst_bo, reloc_page);
 }
 
 static void
@@ -105,9 +105,9 @@ apply_surf_reloc(struct qxl_device *qdev, struct qxl_reloc_info *info)
 	if (info->src_bo && !info->src_bo->is_primary)
 		id = info->src_bo->surface_id;
 
-	reloc_page = qxl_bo_kmap_atomic_page(qdev, info->dst_bo, info->dst_offset & PAGE_MASK);
+	reloc_page = qxl_bo_kmap_local_page(qdev, info->dst_bo, info->dst_offset & PAGE_MASK);
 	*(uint32_t *)(reloc_page + (info->dst_offset & ~PAGE_MASK)) = id;
-	qxl_bo_kunmap_atomic_page(qdev, info->dst_bo, reloc_page);
+	qxl_bo_kunmap_local_page(qdev, info->dst_bo, reloc_page);
 }
 
 /* return holding the reference to this object */
@@ -149,7 +149,6 @@ static int qxl_process_single_command(struct qxl_device *qdev,
 	struct qxl_bo *cmd_bo;
 	void *fb_cmd;
 	int i, ret, num_relocs;
-	int unwritten;
 
 	switch (cmd->type) {
 	case QXL_CMD_DRAW:
@@ -185,21 +184,21 @@ static int qxl_process_single_command(struct qxl_device *qdev,
 		goto out_free_reloc;
 
 	/* TODO copy slow path code from i915 */
-	fb_cmd = qxl_bo_kmap_atomic_page(qdev, cmd_bo, (release->release_offset & PAGE_MASK));
-	unwritten = __copy_from_user_inatomic_nocache
-		(fb_cmd + sizeof(union qxl_release_info) + (release->release_offset & ~PAGE_MASK),
-		 u64_to_user_ptr(cmd->command), cmd->command_size);
+	fb_cmd = qxl_bo_kmap_local_page(qdev, cmd_bo, (release->release_offset & PAGE_MASK));
 
-	{
+	if (copy_from_user(fb_cmd + sizeof(union qxl_release_info) +
+			   (release->release_offset & ~PAGE_MASK),
+			   u64_to_user_ptr(cmd->command), cmd->command_size)) {
+		ret = -EFAULT;
+	} else {
 		struct qxl_drawable *draw = fb_cmd;
 
 		draw->mm_time = qdev->rom->mm_clock;
 	}
 
-	qxl_bo_kunmap_atomic_page(qdev, cmd_bo, fb_cmd);
-	if (unwritten) {
-		DRM_ERROR("got unwritten %d\n", unwritten);
-		ret = -EFAULT;
+	qxl_bo_kunmap_local_page(qdev, cmd_bo, fb_cmd);
+	if (ret) {
+		DRM_ERROR("copy from user failed %d\n", ret);
 		goto out_free_release;
 	}
 
diff --git a/drivers/gpu/drm/qxl/qxl_object.c b/drivers/gpu/drm/qxl/qxl_object.c
index 2bc364412e8b..9350d238ba54 100644
--- a/drivers/gpu/drm/qxl/qxl_object.c
+++ b/drivers/gpu/drm/qxl/qxl_object.c
@@ -172,8 +172,8 @@ int qxl_bo_kmap(struct qxl_bo *bo, void **ptr)
 	return 0;
 }
 
-void *qxl_bo_kmap_atomic_page(struct qxl_device *qdev,
-			      struct qxl_bo *bo, int page_offset)
+void *qxl_bo_kmap_local_page(struct qxl_device *qdev,
+			     struct qxl_bo *bo, int page_offset)
 {
 	unsigned long offset;
 	void *rptr;
@@ -188,7 +188,7 @@ void *qxl_bo_kmap_atomic_page(struct qxl_device *qdev,
 		goto fallback;
 
 	offset = bo->tbo.mem.start << PAGE_SHIFT;
-	return io_mapping_map_atomic_wc(map, offset + page_offset);
+	return io_mapping_map_local_wc(map, offset + page_offset);
 fallback:
 	if (bo->kptr) {
 		rptr = bo->kptr + (page_offset * PAGE_SIZE);
@@ -214,14 +214,14 @@ void qxl_bo_kunmap(struct qxl_bo *bo)
 	ttm_bo_kunmap(&bo->kmap);
 }
 
-void qxl_bo_kunmap_atomic_page(struct qxl_device *qdev,
-			       struct qxl_bo *bo, void *pmap)
+void qxl_bo_kunmap_local_page(struct qxl_device *qdev,
+			      struct qxl_bo *bo, void *pmap)
 {
 	if ((bo->tbo.mem.mem_type != TTM_PL_VRAM) &&
 	    (bo->tbo.mem.mem_type != TTM_PL_PRIV))
 		goto fallback;
 
-	io_mapping_unmap_atomic(pmap);
+	io_mapping_unmap_local(pmap);
 	return;
  fallback:
 	qxl_bo_kunmap(bo);
diff --git a/drivers/gpu/drm/qxl/qxl_object.h b/drivers/gpu/drm/qxl/qxl_object.h
index 6b434e5ef795..02f1e0374228 100644
--- a/drivers/gpu/drm/qxl/qxl_object.h
+++ b/drivers/gpu/drm/qxl/qxl_object.h
@@ -88,8 +88,8 @@ extern int qxl_bo_create(struct qxl_device *qdev,
 			 struct qxl_bo **bo_ptr);
 extern int qxl_bo_kmap(struct qxl_bo *bo, void **ptr);
 extern void qxl_bo_kunmap(struct qxl_bo *bo);
-void *qxl_bo_kmap_atomic_page(struct qxl_device *qdev, struct qxl_bo *bo, int page_offset);
-void qxl_bo_kunmap_atomic_page(struct qxl_device *qdev, struct qxl_bo *bo, void *map);
+void *qxl_bo_kmap_local_page(struct qxl_device *qdev, struct qxl_bo *bo, int page_offset);
+void qxl_bo_kunmap_local_page(struct qxl_device *qdev, struct qxl_bo *bo, void *map);
 extern struct qxl_bo *qxl_bo_ref(struct qxl_bo *bo);
 extern void qxl_bo_unref(struct qxl_bo **bo);
 extern int qxl_bo_pin(struct qxl_bo *bo);
diff --git a/drivers/gpu/drm/qxl/qxl_release.c b/drivers/gpu/drm/qxl/qxl_release.c
index 4fae3e393da1..9f37b51e61c6 100644
--- a/drivers/gpu/drm/qxl/qxl_release.c
+++ b/drivers/gpu/drm/qxl/qxl_release.c
@@ -408,7 +408,7 @@ union qxl_release_info *qxl_release_map(struct qxl_device *qdev,
 	union qxl_release_info *info;
 	struct qxl_bo *bo = release->release_bo;
 
-	ptr = qxl_bo_kmap_atomic_page(qdev, bo, release->release_offset & PAGE_MASK);
+	ptr = qxl_bo_kmap_local_page(qdev, bo, release->release_offset & PAGE_MASK);
 	if (!ptr)
 		return NULL;
 	info = ptr + (release->release_offset & ~PAGE_MASK);
@@ -423,7 +423,7 @@ void qxl_release_unmap(struct qxl_device *qdev,
 	void *ptr;
 
 	ptr = ((void *)info) - (release->release_offset & ~PAGE_MASK);
-	qxl_bo_kunmap_atomic_page(qdev, bo, ptr);
+	qxl_bo_kunmap_local_page(qdev, bo, ptr);
 }
 
 void qxl_release_fence_buffer_objects(struct qxl_release *release)
diff --git a/drivers/gpu/drm/radeon/radeon_display.c b/drivers/gpu/drm/radeon/radeon_display.c
index e0ae911ef427..781edf550436 100644
--- a/drivers/gpu/drm/radeon/radeon_display.c
+++ b/drivers/gpu/drm/radeon/radeon_display.c
@@ -1822,6 +1822,7 @@ int radeon_get_crtc_scanoutpos(struct drm_device *dev, unsigned int pipe,
 	struct radeon_device *rdev = dev->dev_private;
 
 	/* preempt_disable_rt() should go right here in PREEMPT_RT patchset. */
+	preempt_disable_rt();
 
 	/* Get optional system timestamp before query. */
 	if (stime)
@@ -1914,6 +1915,7 @@ int radeon_get_crtc_scanoutpos(struct drm_device *dev, unsigned int pipe,
 		*etime = ktime_get();
 
 	/* preempt_enable_rt() should go right here in PREEMPT_RT patchset. */
+	preempt_enable_rt();
 
 	/* Decode into vertical and horizontal scanout position. */
 	*vpos = position & 0x1fff;
diff --git a/drivers/gpu/drm/ttm/ttm_bo_util.c b/drivers/gpu/drm/ttm/ttm_bo_util.c
index fb2a25f8408f..164b9a015d32 100644
--- a/drivers/gpu/drm/ttm/ttm_bo_util.c
+++ b/drivers/gpu/drm/ttm/ttm_bo_util.c
@@ -181,13 +181,15 @@ static int ttm_copy_io_ttm_page(struct ttm_tt *ttm, void *src,
 		return -ENOMEM;
 
 	src = (void *)((unsigned long)src + (page << PAGE_SHIFT));
-	dst = kmap_atomic_prot(d, prot);
-	if (!dst)
-		return -ENOMEM;
+	/*
+	 * Ensure that a highmem page is mapped with the correct
+	 * pgprot. For non highmem the mapping is already there.
+	 */
+	dst = kmap_local_page_prot(d, prot);
 
 	memcpy_fromio(dst, src, PAGE_SIZE);
 
-	kunmap_atomic(dst);
+	kunmap_local(dst);
 
 	return 0;
 }
@@ -203,13 +205,15 @@ static int ttm_copy_ttm_io_page(struct ttm_tt *ttm, void *dst,
 		return -ENOMEM;
 
 	dst = (void *)((unsigned long)dst + (page << PAGE_SHIFT));
-	src = kmap_atomic_prot(s, prot);
-	if (!src)
-		return -ENOMEM;
+	/*
+	 * Ensure that a highmem page is mapped with the correct
+	 * pgprot. For non highmem the mapping is already there.
+	 */
+	src = kmap_local_page_prot(s, prot);
 
 	memcpy_toio(dst, src, PAGE_SIZE);
 
-	kunmap_atomic(src);
+	kunmap_local(src);
 
 	return 0;
 }
diff --git a/drivers/gpu/drm/vmwgfx/vmwgfx_blit.c b/drivers/gpu/drm/vmwgfx/vmwgfx_blit.c
index e8d66182cd7b..71dba228f68e 100644
--- a/drivers/gpu/drm/vmwgfx/vmwgfx_blit.c
+++ b/drivers/gpu/drm/vmwgfx/vmwgfx_blit.c
@@ -375,12 +375,12 @@ static int vmw_bo_cpu_blit_line(struct vmw_bo_blit_line_data *d,
 		copy_size = min_t(u32, copy_size, PAGE_SIZE - src_page_offset);
 
 		if (unmap_src) {
-			kunmap_atomic(d->src_addr);
+			kunmap_local(d->src_addr);
 			d->src_addr = NULL;
 		}
 
 		if (unmap_dst) {
-			kunmap_atomic(d->dst_addr);
+			kunmap_local(d->dst_addr);
 			d->dst_addr = NULL;
 		}
 
@@ -388,12 +388,8 @@ static int vmw_bo_cpu_blit_line(struct vmw_bo_blit_line_data *d,
 			if (WARN_ON_ONCE(dst_page >= d->dst_num_pages))
 				return -EINVAL;
 
-			d->dst_addr =
-				kmap_atomic_prot(d->dst_pages[dst_page],
-						 d->dst_prot);
-			if (!d->dst_addr)
-				return -ENOMEM;
-
+			d->dst_addr = kmap_local_page_prot(d->dst_pages[dst_page],
+							   d->dst_prot);
 			d->mapped_dst = dst_page;
 		}
 
@@ -401,12 +397,8 @@ static int vmw_bo_cpu_blit_line(struct vmw_bo_blit_line_data *d,
 			if (WARN_ON_ONCE(src_page >= d->src_num_pages))
 				return -EINVAL;
 
-			d->src_addr =
-				kmap_atomic_prot(d->src_pages[src_page],
-						 d->src_prot);
-			if (!d->src_addr)
-				return -ENOMEM;
-
+			d->src_addr = kmap_local_page_prot(d->src_pages[src_page],
+							   d->src_prot);
 			d->mapped_src = src_page;
 		}
 		diff->do_cpy(diff, d->dst_addr + dst_page_offset,
@@ -436,8 +428,10 @@ static int vmw_bo_cpu_blit_line(struct vmw_bo_blit_line_data *d,
  *
  * Performs a CPU blit from one buffer object to another avoiding a full
  * bo vmap which may exhaust- or fragment vmalloc space.
- * On supported architectures (x86), we're using kmap_atomic which avoids
- * cross-processor TLB- and cache flushes and may, on non-HIGHMEM systems
+ *
+ * On supported architectures (x86), we're using kmap_local_prot() which
+ * avoids cross-processor TLB- and cache flushes. kmap_local_prot() will
+ * either map a highmem page with the proper pgprot on HIGHMEM=y systems or
  * reference already set-up mappings.
  *
  * Neither of the buffer objects may be placed in PCI memory
@@ -500,9 +494,9 @@ int vmw_bo_cpu_blit(struct ttm_buffer_object *dst,
 	}
 out:
 	if (d.src_addr)
-		kunmap_atomic(d.src_addr);
+		kunmap_local(d.src_addr);
 	if (d.dst_addr)
-		kunmap_atomic(d.dst_addr);
+		kunmap_local(d.dst_addr);
 
 	return ret;
 }
diff --git a/drivers/hid/Kconfig b/drivers/hid/Kconfig
index 9b56226ce0d1..88d397b0b33f 100644
--- a/drivers/hid/Kconfig
+++ b/drivers/hid/Kconfig
@@ -719,6 +719,16 @@ config HID_MULTITOUCH
 	  To compile this driver as a module, choose M here: the
 	  module will be called hid-multitouch.
 
+if HID_MULTITOUCH
+	config HID_MULTITOUCH_DISABLE_SINGLETOUCH_EVENTS
+	bool "Disable singletouch events"
+	depends on USB_HID
+	help
+	This option disables single touch events generated by the hid-multitouch driver.
+	If this option is not activated the driver is emitting multitouch- and singletouch-
+	events simultaneously.
+endif # HID_MULTITOUCH
+
 config HID_NTI
 	tristate "NTI keyboard adapters"
 	help
diff --git a/drivers/hv/hyperv_vmbus.h b/drivers/hv/hyperv_vmbus.h
index 40e2b9f91163..d9de4813ffac 100644
--- a/drivers/hv/hyperv_vmbus.h
+++ b/drivers/hv/hyperv_vmbus.h
@@ -18,6 +18,7 @@
 #include <linux/atomic.h>
 #include <linux/hyperv.h>
 #include <linux/interrupt.h>
+#include <linux/irq.h>
 
 #include "hv_trace.h"
 
diff --git a/drivers/hv/vmbus_drv.c b/drivers/hv/vmbus_drv.c
index a5a402e776c7..d0763ea99ded 100644
--- a/drivers/hv/vmbus_drv.c
+++ b/drivers/hv/vmbus_drv.c
@@ -22,6 +22,7 @@
 #include <linux/clockchips.h>
 #include <linux/cpu.h>
 #include <linux/sched/task_stack.h>
+#include <linux/irq.h>
 
 #include <linux/delay.h>
 #include <linux/notifier.h>
@@ -1307,6 +1308,8 @@ static void vmbus_isr(void)
 	void *page_addr = hv_cpu->synic_event_page;
 	struct hv_message *msg;
 	union hv_synic_event_flags *event;
+	struct pt_regs *regs = get_irq_regs();
+	u64 ip = regs ? instruction_pointer(regs) : 0;
 	bool handled = false;
 
 	if (unlikely(page_addr == NULL))
@@ -1351,7 +1354,7 @@ static void vmbus_isr(void)
 			tasklet_schedule(&hv_cpu->msg_dpc);
 	}
 
-	add_interrupt_randomness(hv_get_vector(), 0);
+	add_interrupt_randomness(hv_get_vector(), 0, ip);
 }
 
 /*
@@ -1359,7 +1362,8 @@ static void vmbus_isr(void)
  * buffer and call into Hyper-V to transfer the data.
  */
 static void hv_kmsg_dump(struct kmsg_dumper *dumper,
-			 enum kmsg_dump_reason reason)
+			 enum kmsg_dump_reason reason,
+			 struct kmsg_dumper_iter *iter)
 {
 	size_t bytes_written;
 	phys_addr_t panic_pa;
@@ -1374,7 +1378,7 @@ static void hv_kmsg_dump(struct kmsg_dumper *dumper,
 	 * Write dump contents to the page. No need to synchronize; panic should
 	 * be single-threaded.
 	 */
-	kmsg_dump_get_buffer(dumper, false, hv_panic_page, HV_HYP_PAGE_SIZE,
+	kmsg_dump_get_buffer(iter, false, hv_panic_page, HV_HYP_PAGE_SIZE,
 			     &bytes_written);
 	if (bytes_written)
 		hyperv_report_panic_msg(panic_pa, bytes_written);
diff --git a/drivers/i2c/busses/i2c-stm32f7.c b/drivers/i2c/busses/i2c-stm32f7.c
index 674735334547..038f9eebff38 100644
--- a/drivers/i2c/busses/i2c-stm32f7.c
+++ b/drivers/i2c/busses/i2c-stm32f7.c
@@ -430,8 +430,7 @@ static int stm32f7_i2c_compute_timing(struct stm32f7_i2c_dev *i2c_dev,
 {
 	struct stm32f7_i2c_spec *specs;
 	u32 p_prev = STM32F7_PRESC_MAX;
-	u32 i2cclk = DIV_ROUND_CLOSEST(NSEC_PER_SEC,
-				       setup->clock_src);
+	u32 i2cclk = ((uint32_t) NSEC_PER_SEC) / setup->clock_src;
 	u32 i2cbus = DIV_ROUND_CLOSEST(NSEC_PER_SEC,
 				       setup->speed_freq);
 	u32 clk_error_prev = i2cbus;
diff --git a/drivers/iio/adc/stm32-adc-core.c b/drivers/iio/adc/stm32-adc-core.c
index a83199b212a4..63b1b011e915 100644
--- a/drivers/iio/adc/stm32-adc-core.c
+++ b/drivers/iio/adc/stm32-adc-core.c
@@ -93,6 +93,7 @@ struct stm32_adc_priv_cfg {
  */
 struct stm32_adc_priv {
 	int				irq[STM32_ADC_MAX_ADCS];
+	u32				ier_bkp[STM32_ADC_MAX_ADCS];
 	struct irq_domain		*domain;
 	struct clk			*aclk;
 	struct clk			*bclk;
@@ -343,11 +344,49 @@ static void stm32_adc_irq_handler(struct irq_desc *desc)
 	chained_irq_exit(chip, desc);
 };
 
+static void irq_mask(struct irq_data *data)
+{
+	struct stm32_adc_priv *priv = irq_data_get_irq_chip_data(data);
+	int adc = data->hwirq;
+
+	if (likely(adc >= 0 && adc < priv->cfg->num_irqs))
+		/* use nosync since the interrupt is already in progress */
+		disable_irq_nosync(priv->irq[adc]);
+	else
+		pr_err_once("Unsupported ADC number %d\n", adc);
+}
+
+static void irq_unmask(struct irq_data *data)
+{
+	struct stm32_adc_priv *priv = irq_data_get_irq_chip_data(data);
+	int adc = data->hwirq;
+
+	if (likely(adc >= 0 && adc < priv->cfg->num_irqs))
+		enable_irq(priv->irq[adc]);
+	else
+		pr_err_once("Unsupported ADC number %d\n", adc);
+}
+
+static unsigned int noop_ret(struct irq_data *data)
+{
+	return 0;
+}
+
+static void noop(struct irq_data *data) { }
+
+static struct irq_chip stm32adc_core_irq_chip = {
+	.name		= "stm32adc-core",
+	.irq_mask	= irq_mask,
+	.irq_unmask	= irq_unmask,
+	.irq_startup    = noop_ret,
+	.irq_shutdown   = noop,
+};
+
 static int stm32_adc_domain_map(struct irq_domain *d, unsigned int irq,
 				irq_hw_number_t hwirq)
 {
 	irq_set_chip_data(irq, d->host_data);
-	irq_set_chip_and_handler(irq, &dummy_irq_chip, handle_level_irq);
+	irq_set_chip_and_handler(irq, &stm32adc_core_irq_chip, handle_level_irq);
 
 	return 0;
 }
@@ -379,6 +418,15 @@ static int stm32_adc_irq_probe(struct platform_device *pdev,
 		priv->irq[i] = platform_get_irq(pdev, i);
 		if (priv->irq[i] < 0)
 			return priv->irq[i];
+		/* The enable_irq and disable_irq are used to mask ADC
+		 * interrupts while the chained interrupt handling is in
+		 * progress to ensure the main irq is not activated and
+		 * stm32_adc_irq_handler() is not called before the chained
+		 * interrupt handler (stm32_adc_isr()) is finished.
+		 * Therefore, the lazy interrupt masking is disabled to
+		 * ensure the interrupt is really masked on disable_irq().
+		 */
+		irq_set_status_flags(priv->irq[i], IRQ_DISABLE_UNLAZY);
 	}
 
 	priv->domain = irq_domain_add_simple(np, STM32_ADC_MAX_ADCS, 0,
diff --git a/drivers/iio/adc/stm32-adc.c b/drivers/iio/adc/stm32-adc.c
index 16c02c30dec7..65627326a00e 100644
--- a/drivers/iio/adc/stm32-adc.c
+++ b/drivers/iio/adc/stm32-adc.c
@@ -201,6 +201,7 @@ struct stm32_adc {
 	const struct stm32_adc_cfg	*cfg;
 	struct completion	completion;
 	u16			buffer[STM32_ADC_MAX_SQ];
+	u32			status;
 	struct clk		*clk;
 	int			irq;
 	spinlock_t		lock;		/* interrupt lock */
@@ -1263,18 +1264,13 @@ static irqreturn_t stm32_adc_threaded_isr(int irq, void *data)
 	struct iio_dev *indio_dev = data;
 	struct stm32_adc *adc = iio_priv(indio_dev);
 	const struct stm32_adc_regspec *regs = adc->cfg->regs;
-	u32 status = stm32_adc_readl(adc, regs->isr_eoc.reg);
 	u32 mask = stm32_adc_readl(adc, regs->ier_eoc.reg);
+	u32 status = adc->status;
 
-	/* Check ovr status right now, as ovr mask should be already disabled */
+	/* Check ovr status */
 	if (status & regs->isr_ovr.mask) {
-		/*
-		 * Clear ovr bit to avoid subsequent calls to IRQ handler.
-		 * This requires to stop ADC first. OVR bit state in ISR,
-		 * is propaged to CSR register by hardware.
-		 */
-		adc->cfg->stop_conv(indio_dev);
-		stm32_adc_irq_clear(indio_dev, regs->isr_ovr.mask);
+		stm32_adc_ovr_irq_disable(adc);
+		stm32_adc_conv_irq_disable(adc);
 		dev_err(&indio_dev->dev, "Overrun, stopping: restart needed\n");
 		return IRQ_HANDLED;
 	}
@@ -1293,10 +1289,8 @@ static irqreturn_t stm32_adc_isr(int irq, void *data)
 	struct stm32_adc *adc = iio_priv(indio_dev);
 	const struct stm32_adc_regspec *regs = adc->cfg->regs;
 	u32 status = stm32_adc_readl(adc, regs->isr_eoc.reg);
-	u32 mask = stm32_adc_readl(adc, regs->ier_eoc.reg);
 
-	if (!(status & mask))
-		return IRQ_WAKE_THREAD;
+	adc->status = status;
 
 	if (status & regs->isr_ovr.mask) {
 		/*
@@ -1306,8 +1300,14 @@ static irqreturn_t stm32_adc_isr(int irq, void *data)
 		 * Restarting the capture can be done by disabling, then
 		 * re-enabling it (e.g. write 0, then 1 to buffer/enable).
 		 */
-		stm32_adc_ovr_irq_disable(adc);
-		stm32_adc_conv_irq_disable(adc);
+		/* The interrupt must be disabled in the threaded interrupt
+		 * handler, otherwise it would get re-enabled by interrupt
+		 * controller (stm32-adc-core). On the other hand, the IRQ
+		 * must be cleared here - otherwise it would be immediately
+		 * triggered again when the interrupt controller unmask it.
+		 */
+		stm32_adc_irq_clear(indio_dev, regs->isr_ovr.mask);
+		adc->cfg->stop_conv(indio_dev);
 		return IRQ_WAKE_THREAD;
 	}
 
@@ -1326,7 +1326,7 @@ static irqreturn_t stm32_adc_isr(int irq, void *data)
 		return IRQ_HANDLED;
 	}
 
-	return IRQ_NONE;
+	return IRQ_WAKE_THREAD;
 }
 
 /**
diff --git a/drivers/iio/dac/stm32-dac-core.h b/drivers/iio/dac/stm32-dac-core.h
index d3b415fb9575..84b4c54cd8f4 100644
--- a/drivers/iio/dac/stm32-dac-core.h
+++ b/drivers/iio/dac/stm32-dac-core.h
@@ -17,6 +17,7 @@
 #define STM32_DAC_DHR12R2	0x14
 #define STM32_DAC_DOR1		0x2C
 #define STM32_DAC_DOR2		0x30
+#define STM32_DAC_MCR		0x3C
 
 /* STM32_DAC_CR bit fields */
 #define STM32_DAC_CR_EN1		BIT(0)
diff --git a/drivers/iio/dac/stm32-dac.c b/drivers/iio/dac/stm32-dac.c
index 12dec68c16f7..6ad29b12c662 100644
--- a/drivers/iio/dac/stm32-dac.c
+++ b/drivers/iio/dac/stm32-dac.c
@@ -69,6 +69,11 @@ static int stm32_dac_set_enable_state(struct iio_dev *indio_dev, int ch,
 	}
 
 	if (enable) {
+		/* disable buffering DAC1 and DAC2 */
+		ret = regmap_write(dac->common->regmap, STM32_DAC_MCR, 0x00020002);
+		if (ret < 0) {
+			dev_err(&indio_dev->dev, "failed disable buffering\n");
+		}
 		ret = pm_runtime_get_sync(dev);
 		if (ret < 0) {
 			pm_runtime_put_noidle(dev);
diff --git a/drivers/input/input-mt.c b/drivers/input/input-mt.c
index 44fe6f2f063c..cd6be43c3fbc 100644
--- a/drivers/input/input-mt.c
+++ b/drivers/input/input-mt.c
@@ -306,7 +306,9 @@ void input_mt_sync_frame(struct input_dev *dev)
 	if ((mt->flags & INPUT_MT_POINTER) && !(mt->flags & INPUT_MT_SEMI_MT))
 		use_count = true;
 
+#ifndef CONFIG_HID_MULTITOUCH_DISABLE_SINGLETOUCH_EVENTS
 	input_mt_report_pointer_emulation(dev, use_count);
+#endif
 
 	mt->frame++;
 }
diff --git a/drivers/input/keyboard/gpio_keys.c b/drivers/input/keyboard/gpio_keys.c
index f2d4e4daa818..41da8d5a7d46 100644
--- a/drivers/input/keyboard/gpio_keys.c
+++ b/drivers/input/keyboard/gpio_keys.c
@@ -21,7 +21,8 @@
 #include <linux/platform_device.h>
 #include <linux/input.h>
 #include <linux/gpio_keys.h>
-#include <linux/workqueue.h>
+#include <uapi/linux/sched/types.h>
+#include <linux/kthread.h>
 #include <linux/gpio.h>
 #include <linux/gpio/consumer.h>
 #include <linux/of.h>
@@ -29,6 +30,8 @@
 #include <linux/spinlock.h>
 #include <dt-bindings/input/gpio-keys.h>
 
+#define GPIO_KEYS_KWORKER_PRIO        90
+
 struct gpio_button_data {
 	const struct gpio_keys_button *button;
 	struct input_dev *input;
@@ -39,7 +42,7 @@ struct gpio_button_data {
 	struct timer_list release_timer;
 	unsigned int release_delay;	/* in msecs, for IRQ-only buttons */
 
-	struct delayed_work work;
+	struct kthread_work kwork;
 	unsigned int software_debounce;	/* in msecs, for GPIO-driven buttons */
 
 	unsigned int irq;
@@ -55,6 +58,8 @@ struct gpio_keys_drvdata {
 	struct input_dev *input;
 	struct mutex disable_lock;
 	unsigned short *keymap;
+	struct kthread_worker irq_kworker;      /* kthread worker high prio post-actions */
+	struct task_struct *irq_kworker_task;   /* task for irq_kworker */
 	struct gpio_button_data data[];
 };
 
@@ -143,9 +148,7 @@ static void gpio_keys_disable_button(struct gpio_button_data *bdata)
 		 */
 		disable_irq(bdata->irq);
 
-		if (bdata->gpiod)
-			cancel_delayed_work_sync(&bdata->work);
-		else
+		if (bdata->software_debounce)
 			del_timer_sync(&bdata->release_timer);
 
 		bdata->disabled = true;
@@ -376,10 +379,10 @@ static void gpio_keys_gpio_report_event(struct gpio_button_data *bdata)
 	input_sync(input);
 }
 
-static void gpio_keys_gpio_work_func(struct work_struct *work)
+static void gpio_keys_gpio_work_func(struct kthread_work *work)
 {
 	struct gpio_button_data *bdata =
-		container_of(work, struct gpio_button_data, work.work);
+		container_of(work, struct gpio_button_data, kwork);
 
 	gpio_keys_gpio_report_event(bdata);
 
@@ -390,6 +393,8 @@ static void gpio_keys_gpio_work_func(struct work_struct *work)
 static irqreturn_t gpio_keys_gpio_isr(int irq, void *dev_id)
 {
 	struct gpio_button_data *bdata = dev_id;
+	struct input_dev *input = bdata->input;
+	struct gpio_keys_drvdata *ddata = input_get_drvdata(input);
 
 	BUG_ON(irq != bdata->irq);
 
@@ -408,13 +413,24 @@ static irqreturn_t gpio_keys_gpio_isr(int irq, void *dev_id)
 		}
 	}
 
-	mod_delayed_work(system_wq,
-			 &bdata->work,
-			 msecs_to_jiffies(bdata->software_debounce));
+	if (bdata->software_debounce)
+		mod_timer(&bdata->release_timer,
+			jiffies + msecs_to_jiffies(bdata->software_debounce));
+	else
+		kthread_queue_work(&ddata->irq_kworker, &bdata->kwork);
 
 	return IRQ_HANDLED;
 }
 
+static void gpio_keys_gpio_timer(struct timer_list *t)
+{
+	struct gpio_button_data *bdata = from_timer(bdata, t, release_timer);
+	struct input_dev *input = bdata->input;
+	struct gpio_keys_drvdata *ddata = input_get_drvdata(input);
+
+	kthread_queue_work(&ddata->irq_kworker, &bdata->kwork);
+}
+
 static void gpio_keys_irq_timer(struct timer_list *t)
 {
 	struct gpio_button_data *bdata = from_timer(bdata, t, release_timer);
@@ -469,7 +485,7 @@ static void gpio_keys_quiesce_key(void *data)
 	struct gpio_button_data *bdata = data;
 
 	if (bdata->gpiod)
-		cancel_delayed_work_sync(&bdata->work);
+		kthread_flush_work(&bdata->kwork);
 	else
 		del_timer_sync(&bdata->release_timer);
 }
@@ -559,7 +575,9 @@ static int gpio_keys_setup_key(struct platform_device *pdev,
 			bdata->irq = irq;
 		}
 
-		INIT_DELAYED_WORK(&bdata->work, gpio_keys_gpio_work_func);
+		kthread_init_work(&bdata->kwork, gpio_keys_gpio_work_func);
+		timer_setup(&bdata->release_timer,
+			    gpio_keys_gpio_timer, 0);
 
 		isr = gpio_keys_gpio_isr;
 		irqflags = IRQF_TRIGGER_RISING | IRQF_TRIGGER_FALLING;
@@ -628,8 +646,9 @@ static int gpio_keys_setup_key(struct platform_device *pdev,
 	if (!button->can_disable)
 		irqflags |= IRQF_SHARED;
 
-	error = devm_request_any_context_irq(dev, bdata->irq, isr, irqflags,
-					     desc, bdata);
+	error = devm_request_any_context_irq(dev, bdata->irq, isr, irqflags |
+					     IRQF_THREAD_TBL_LOOKUP, desc,
+					     bdata);
 	if (error < 0) {
 		dev_err(dev, "Unable to claim irq %d; error %d\n",
 			bdata->irq, error);
@@ -768,6 +787,8 @@ static int gpio_keys_probe(struct platform_device *pdev)
 	int i, error;
 	int wakeup = 0;
 
+	struct sched_param kwork_param = { .sched_priority = GPIO_KEYS_KWORKER_PRIO };
+
 	if (!pdata) {
 		pdata = gpio_keys_get_devtree_pdata(dev);
 		if (IS_ERR(pdata))
@@ -819,6 +840,20 @@ static int gpio_keys_probe(struct platform_device *pdev)
 	if (pdata->rep)
 		__set_bit(EV_REP, input->evbit);
 
+	/* initialize kworker thread */
+	kthread_init_worker(&ddata->irq_kworker);
+	ddata->irq_kworker_task = kthread_run(kthread_worker_fn,
+			&ddata->irq_kworker, "kworker/gpiokey");
+	if (IS_ERR(ddata->irq_kworker_task)) {
+		dev_err(dev, "Could not create gpio-keys kthread, error: %ld\n",
+				PTR_ERR(ddata->irq_kworker_task));
+		platform_set_drvdata(pdev, NULL);
+		kthread_flush_worker(&ddata->irq_kworker);
+		kthread_stop(ddata->irq_kworker_task);
+		return error;
+	}
+	sched_setscheduler(ddata->irq_kworker_task, SCHED_FIFO, &kwork_param);
+
 	for (i = 0; i < pdata->nbuttons; i++) {
 		const struct gpio_keys_button *button = &pdata->buttons[i];
 
@@ -1000,6 +1035,11 @@ static SIMPLE_DEV_PM_OPS(gpio_keys_pm_ops, gpio_keys_suspend, gpio_keys_resume);
 static void gpio_keys_shutdown(struct platform_device *pdev)
 {
 	int ret;
+	struct gpio_keys_drvdata *ddata = dev_get_drvdata(&pdev->dev);
+
+	/* kill worker */
+	kthread_flush_worker(&ddata->irq_kworker);
+	kthread_stop(ddata->irq_kworker_task);
 
 	ret = gpio_keys_suspend(&pdev->dev);
 	if (ret)
diff --git a/drivers/input/keyboard/qt1070.c b/drivers/input/keyboard/qt1070.c
index 7174e1df1ee3..2610eb437772 100644
--- a/drivers/input/keyboard/qt1070.c
+++ b/drivers/input/keyboard/qt1070.c
@@ -30,6 +30,9 @@
 #define DET_STATUS         0x02
 
 #define KEY_STATUS         0x03
+#define NTHR_KEY0          0x20
+#define AVE_AKS0           0x27
+#define DI0                0x2E
 
 /* Calibrate */
 #define CALIBRATE_CMD      0x38
@@ -51,6 +54,8 @@ struct qt1070_data {
 	unsigned int irq;
 	unsigned short keycodes[ARRAY_SIZE(qt1070_key2code)];
 	u8 last_keys;
+	u8 active_keys;
+	u8 ave_aks;
 };
 
 static int qt1070_read(struct i2c_client *client, u8 reg)
@@ -77,6 +82,28 @@ static int qt1070_write(struct i2c_client *client, u8 reg, u8 data)
 	return ret;
 }
 
+static void qt1070_write_register_value(struct i2c_client *client,
+				u8 value, u8 reg)
+{
+	int i;
+
+	for (i = 0; i < 7; i++)
+		qt1070_write(client, reg + i, value);
+}
+
+static void qt1070_activate_keys(struct i2c_client *client,
+				u8 ave_aks, u8 active_keys)
+{
+	int i;
+
+	for (i = 0; i < 7; i++) {
+		if ((1 << i) & active_keys)
+			qt1070_write(client, AVE_AKS0 + i, ave_aks);
+		else
+			qt1070_write(client, AVE_AKS0 + i, 0);
+	}
+}
+
 static bool qt1070_identify(struct i2c_client *client)
 {
 	int id, ver;
@@ -126,11 +153,142 @@ static irqreturn_t qt1070_interrupt(int irq, void *dev_id)
 	return IRQ_HANDLED;
 }
 
+static ssize_t nthr_value_show(struct device *dev,
+				struct device_attribute *attr,
+				char *buf)
+{
+	struct i2c_client *client = to_i2c_client(dev);
+
+	return sprintf(buf, "%d\n", qt1070_read(client, NTHR_KEY0));
+}
+
+static ssize_t nthr_value_store(struct device *dev,
+				struct device_attribute *attr,
+				const char *buf,
+				size_t count)
+{
+	struct i2c_client *client = to_i2c_client(dev);
+	int ret;
+	u8 val;
+
+	ret = kstrtou8(buf, 0, &val);
+	if (ret)
+		return ret;
+
+	qt1070_write_register_value(client, val, NTHR_KEY0);
+	return count;
+}
+
+static DEVICE_ATTR_RW(nthr_value);
+
+/* ------- averaging Factor/adjacent Key Suppression AVE_AKS -------- */
+static ssize_t ave_aks_value_show(struct device *dev,
+				struct device_attribute *attr,
+				char *buf)
+{
+	struct i2c_client *client = to_i2c_client(dev);
+	struct qt1070_data *data = i2c_get_clientdata(client);
+
+	return sprintf(buf, "%d\n", data->ave_aks);
+}
+
+static ssize_t ave_aks_value_store(struct device *dev,
+				struct device_attribute *attr,
+				const char *buf,
+				size_t count)
+{
+	struct i2c_client *client = to_i2c_client(dev);
+	struct qt1070_data *data = i2c_get_clientdata(client);
+	int ret;
+
+	ret = kstrtou8(buf, 0, &data->ave_aks);
+	if (ret)
+		return ret;
+
+	qt1070_write_register_value(client, data->ave_aks, AVE_AKS0);
+	return count;
+}
+
+static DEVICE_ATTR_RW(ave_aks_value);
+
+/* -------------------- detection integrator DI --------------------- */
+static ssize_t di_value_show(struct device *dev,
+				struct device_attribute *attr,
+				char *buf)
+{
+	struct i2c_client *client = to_i2c_client(dev);
+
+	return sprintf(buf, "%d\n", qt1070_read(client, DI0));
+}
+
+static ssize_t di_value_store(struct device *dev,
+				struct device_attribute *attr,
+				const char *buf,
+				size_t count)
+{
+	struct i2c_client *client = to_i2c_client(dev);
+	int ret;
+	u8 val;
+
+	ret = kstrtou8(buf, 0, &val);
+	if (ret)
+		return ret;
+
+	qt1070_write_register_value(client, val, DI0);
+	return count;
+}
+
+static DEVICE_ATTR_RW(di_value);
+
+/* -------------------- active keys --------------------------------- */
+static ssize_t active_keys_show(struct device *dev,
+				struct device_attribute *attr,
+				char *buf)
+{
+	struct i2c_client *client = to_i2c_client(dev);
+	struct qt1070_data *data = i2c_get_clientdata(client);
+
+	return sprintf(buf, "%d\n", data->active_keys);
+}
+
+static ssize_t active_keys_store(struct device *dev,
+				struct device_attribute *attr,
+				const char *buf,
+				size_t count)
+{
+	struct i2c_client *client = to_i2c_client(dev);
+	struct qt1070_data *data = i2c_get_clientdata(client);
+	int ret;
+
+	ret = kstrtou8(buf, 0, &data->active_keys);
+	if (ret)
+		return ret;
+
+	qt1070_activate_keys(client, data->ave_aks, data->active_keys);
+
+	return count;
+}
+
+static DEVICE_ATTR_RW(active_keys);
+
+static struct attribute *qt1070_attrs[] = {
+	&dev_attr_nthr_value.attr,
+	&dev_attr_ave_aks_value.attr,
+	&dev_attr_di_value.attr,
+	&dev_attr_active_keys.attr,
+	NULL,
+};
+
+static const struct attribute_group qt1070_attr_group = {
+	.attrs = qt1070_attrs,
+};
+
 static int qt1070_probe(struct i2c_client *client,
 				const struct i2c_device_id *id)
 {
 	struct qt1070_data *data;
 	struct input_dev *input;
+	u8 nthr, di;
 	int i;
 	int err;
 
@@ -186,6 +344,20 @@ static int qt1070_probe(struct i2c_client *client,
 	qt1070_write(client, RESET, 1);
 	msleep(QT1070_RESET_TIME);
 
+	if (!of_property_read_u8(client->dev.of_node, "at,threshold", &nthr))
+		qt1070_write_register_value(client, nthr, NTHR_KEY0);
+
+	if (!of_property_read_u8(client->dev.of_node, "at,ave_aks",
+		&data->ave_aks))
+		qt1070_write_register_value(client, data->ave_aks, AVE_AKS0);
+
+	if (!of_property_read_u8(client->dev.of_node, "at,di", &di))
+		qt1070_write_register_value(client, di, DI0);
+
+	if (!of_property_read_u8(client->dev.of_node, "at,active_keys",
+		&data->active_keys))
+		qt1070_activate_keys(client, data->ave_aks, data->active_keys);
+
 	err = request_threaded_irq(client->irq, NULL, qt1070_interrupt,
 				   IRQF_TRIGGER_NONE | IRQF_ONESHOT,
 				   client->dev.driver->name, data);
@@ -206,6 +378,12 @@ static int qt1070_probe(struct i2c_client *client,
 	/* Read to clear the chang line */
 	qt1070_read(client, DET_STATUS);
 
+	err = sysfs_create_group(&client->dev.kobj, &qt1070_attr_group);
+	if (err) {
+		dev_err(&client->dev, "sysfs creation failed\n");
+		goto err_free_irq;
+	}
+
 	return 0;
 
 err_free_irq:
@@ -220,6 +398,8 @@ static int qt1070_remove(struct i2c_client *client)
 {
 	struct qt1070_data *data = i2c_get_clientdata(client);
 
+	sysfs_remove_group(&client->dev.kobj, &qt1070_attr_group);
+
 	/* Release IRQ */
 	free_irq(client->irq, data);
 
diff --git a/drivers/input/touchscreen/ads7846.c b/drivers/input/touchscreen/ads7846.c
index ff97897feaf2..7cb2ab6d28e6 100644
--- a/drivers/input/touchscreen/ads7846.c
+++ b/drivers/input/touchscreen/ads7846.c
@@ -1298,7 +1298,6 @@ static int ads7846_probe(struct spi_device *spi)
 	 * may not.  So we stick to very-portable 8 bit words, both RX and TX.
 	 */
 	spi->bits_per_word = 8;
-	spi->mode = SPI_MODE_0;
 	err = spi_setup(spi);
 	if (err < 0)
 		return err;
diff --git a/drivers/leds/trigger/Kconfig b/drivers/leds/trigger/Kconfig
index ce9429ca6dde..29ccbd6acf43 100644
--- a/drivers/leds/trigger/Kconfig
+++ b/drivers/leds/trigger/Kconfig
@@ -64,6 +64,7 @@ config LEDS_TRIGGER_BACKLIGHT
 
 config LEDS_TRIGGER_CPU
 	bool "LED CPU Trigger"
+	depends on !PREEMPT_RT
 	help
 	  This allows LEDs to be controlled by active CPUs. This shows
 	  the active CPUs across an array of LEDs so you can see which
diff --git a/drivers/md/raid5.c b/drivers/md/raid5.c
index 39343479ac2a..6b53816e71c9 100644
--- a/drivers/md/raid5.c
+++ b/drivers/md/raid5.c
@@ -2216,8 +2216,9 @@ static void raid_run_ops(struct stripe_head *sh, unsigned long ops_request)
 	struct raid5_percpu *percpu;
 	unsigned long cpu;
 
-	cpu = get_cpu();
+	cpu = get_cpu_light();
 	percpu = per_cpu_ptr(conf->percpu, cpu);
+	spin_lock(&percpu->lock);
 	if (test_bit(STRIPE_OP_BIOFILL, &ops_request)) {
 		ops_run_biofill(sh);
 		overlap_clear++;
@@ -2276,7 +2277,8 @@ static void raid_run_ops(struct stripe_head *sh, unsigned long ops_request)
 			if (test_and_clear_bit(R5_Overlap, &dev->flags))
 				wake_up(&sh->raid_conf->wait_for_overlap);
 		}
-	put_cpu();
+	spin_unlock(&percpu->lock);
+	put_cpu_light();
 }
 
 static void free_stripe(struct kmem_cache *sc, struct stripe_head *sh)
@@ -7098,6 +7100,7 @@ static int raid456_cpu_up_prepare(unsigned int cpu, struct hlist_node *node)
 			__func__, cpu);
 		return -ENOMEM;
 	}
+	spin_lock_init(&per_cpu_ptr(conf->percpu, cpu)->lock);
 	return 0;
 }
 
diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 5c05acf20e1f..665fe138ab4f 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -635,6 +635,7 @@ struct r5conf {
 	int			recovery_disabled;
 	/* per cpu variables */
 	struct raid5_percpu {
+		spinlock_t	lock;		/* Protection for -RT */
 		struct page	*spare_page; /* Used when checking P/Q in raid6 */
 		void		*scribble;  /* space for constructing buffer
 					     * lists and performing address
diff --git a/drivers/memory/omap-gpmc.c b/drivers/memory/omap-gpmc.c
index cfa730cfd145..e59e251e28de 100644
--- a/drivers/memory/omap-gpmc.c
+++ b/drivers/memory/omap-gpmc.c
@@ -76,6 +76,8 @@
 #define GPMC_ECC_CTRL_ECCREG9		0x009
 
 #define GPMC_CONFIG_LIMITEDADDRESS		BIT(1)
+#define GPMC_CONFIG_WAIT0PINPOLARITY		BIT(8)
+#define GPMC_CONFIG_WAIT1PINPOLARITY		BIT(9)
 
 #define GPMC_STATUS_EMPTYWRITEBUFFERSTATUS	BIT(0)
 
@@ -247,6 +249,8 @@ static unsigned int gpmc_capability;
 static void __iomem *gpmc_base;
 
 static struct clk *gpmc_l3_clk;
+static bool gpmc_wait0_active_high;
+static bool gpmc_wait1_active_high;
 
 static irqreturn_t gpmc_handle_irq(int irq, void *dev);
 
@@ -378,6 +382,18 @@ static inline void gpmc_cs_modify_reg(int cs, int reg, u32 mask, bool value)
 	gpmc_cs_write_reg(cs, reg, l);
 }
 
+static inline void gpmc_config_modify(u32 mask, int value)
+{
+	u32 l;
+
+	l = gpmc_read_reg(GPMC_CONFIG);
+	if (value)
+		l |= mask;
+	else
+		l &= ~mask;
+	gpmc_write_reg(GPMC_CONFIG, l);
+}
+
 static void gpmc_cs_bool_timings(int cs, const struct gpmc_bool_timings *p)
 {
 	gpmc_cs_modify_reg(cs, GPMC_CS_CONFIG1,
@@ -1034,16 +1050,9 @@ EXPORT_SYMBOL(gpmc_cs_free);
  */
 int gpmc_configure(int cmd, int wval)
 {
-	u32 regval;
-
 	switch (cmd) {
 	case GPMC_CONFIG_WP:
-		regval = gpmc_read_reg(GPMC_CONFIG);
-		if (wval)
-			regval &= ~GPMC_CONFIG_WRITEPROTECT; /* WP is ON */
-		else
-			regval |= GPMC_CONFIG_WRITEPROTECT;  /* WP is OFF */
-		gpmc_write_reg(GPMC_CONFIG, regval);
+		gpmc_config_modify(GPMC_CONFIG_WRITEPROTECT, !wval);
 		break;
 
 	default:
@@ -2229,9 +2238,7 @@ static int gpmc_probe_generic_child(struct platform_device *pdev,
 	}
 
 	/* Clear limited address i.e. enable A26-A11 */
-	val = gpmc_read_reg(GPMC_CONFIG);
-	val &= ~GPMC_CONFIG_LIMITEDADDRESS;
-	gpmc_write_reg(GPMC_CONFIG, val);
+	gpmc_config_modify(GPMC_CONFIG_LIMITEDADDRESS, 0);
 
 	/* Enable CS region */
 	gpmc_cs_enable_mem(cs);
@@ -2293,6 +2300,12 @@ static int gpmc_probe_dt(struct platform_device *pdev)
 		return ret;
 	}
 
+	gpmc_wait0_active_high = of_property_read_bool(pdev->dev.of_node,
+						"gpmc,wait0-active-high");
+
+	gpmc_wait1_active_high = of_property_read_bool(pdev->dev.of_node,
+						"gpmc,wait1-active-high");
+
 	return 0;
 }
 
@@ -2469,6 +2482,9 @@ static int gpmc_probe(struct platform_device *pdev)
 
 	gpmc_probe_dt_children(pdev);
 
+	gpmc_config_modify(GPMC_CONFIG_WAIT0PINPOLARITY, gpmc_wait0_active_high);
+	gpmc_config_modify(GPMC_CONFIG_WAIT1PINPOLARITY, gpmc_wait1_active_high);
+
 	return 0;
 
 gpio_init_failed:
diff --git a/drivers/mfd/stpmic1.c b/drivers/mfd/stpmic1.c
index eb3da558c3fb..403fb2dd50ac 100644
--- a/drivers/mfd/stpmic1.c
+++ b/drivers/mfd/stpmic1.c
@@ -149,6 +149,16 @@ static int stpmic1_probe(struct i2c_client *i2c,
 	}
 	dev_info(dev, "PMIC Chip Version: 0x%x\n", reg);
 
+	/* enable power cycling on turn-OFF condition - restart the processor
+	 * and do not wait on external wake-up by default
+	 */
+	ret = regmap_update_bits(ddata->regmap,	SWOFF_PWRCTRL_CR,
+			RESTART_REQUEST_ENABLED, RESTART_REQUEST_ENABLED);
+	if (ret) {
+		dev_err(dev, "Unable to update PMIC main control register: %d\n", ret);
+		return ret;
+	}
+
 	/* Initialize PMIC IRQ Chip & associated IRQ domains */
 	ret = devm_regmap_add_irq_chip(dev, ddata->regmap, ddata->irq,
 				       IRQF_ONESHOT | IRQF_SHARED,
diff --git a/drivers/misc/Kconfig b/drivers/misc/Kconfig
index fafa8b0d8099..d7f3890e36ae 100644
--- a/drivers/misc/Kconfig
+++ b/drivers/misc/Kconfig
@@ -466,6 +466,25 @@ config HISI_HIKEY_USB
 	  switching between the dual-role USB-C port and the USB-A host ports
 	  using only one USB controller.
 
+config ENCSW
+	tristate "GPIO based encoder switch driver"
+	default m
+	help
+		This driver allows GPIO connected encoder switches e.g. dip switches
+		or hexadicmal encoders to group its inputs logically and make
+		their compounded values accessible over the sysfs.
+
+config TI_SN74LV165A
+	tristate "Texas Instruments SN74LV165A"
+	depends on SYSFS && GPIO_OMAP && GPIOLIB && OF_GPIO
+	help
+	  If you say yes here you get support for the Texas Instruments
+	  SN74LV165A 8-bit shift register.
+
+	  This driver can also be built as a module. If so, the module
+	  will be called ti_sn74lv165a.
+
+
 source "drivers/misc/c2port/Kconfig"
 source "drivers/misc/eeprom/Kconfig"
 source "drivers/misc/cb710/Kconfig"
diff --git a/drivers/misc/Makefile b/drivers/misc/Makefile
index d23231e73330..27f43997bc26 100644
--- a/drivers/misc/Makefile
+++ b/drivers/misc/Makefile
@@ -57,3 +57,5 @@ obj-$(CONFIG_HABANA_AI)		+= habanalabs/
 obj-$(CONFIG_UACCE)		+= uacce/
 obj-$(CONFIG_XILINX_SDFEC)	+= xilinx_sdfec.o
 obj-$(CONFIG_HISI_HIKEY_USB)	+= hisi_hikey_usb.o
+obj-$(CONFIG_ENCSW)		+= encsw.o
+obj-$(CONFIG_TI_SN74LV165A)	+= ti_sn74lv165a.o
diff --git a/drivers/misc/encsw.c b/drivers/misc/encsw.c
new file mode 100644
index 000000000000..bc2b9a86c7de
--- /dev/null
+++ b/drivers/misc/encsw.c
@@ -0,0 +1,151 @@
+// SPDX-License-Identifier: GPL-2.0-only
+
+#include <linux/module.h>
+#include <linux/platform_device.h>
+#include <linux/of_gpio.h>
+#include <linux/slab.h>
+#include <linux/gpio.h>
+#include <linux/gpio/consumer.h>
+#include <linux/fs.h>
+
+#define DRIVER_NAME "encsw"
+
+
+static const struct of_device_id encsw_of_ids[] = {
+	{.compatible = DRIVER_NAME},
+	{}
+};
+
+MODULE_DEVICE_TABLE(of, encsw_of_ids);
+
+struct encsw_dev_data {
+	int gpios_num;
+	struct device *sysfs_dev;
+	struct gpio_desc *gpios[];
+};
+
+static ssize_t encsw_sysfs_value_show(struct device *dev,
+				      struct device_attribute *attr,
+				      char *buf)
+{
+	int i;
+	struct encsw_dev_data const *data = dev_get_drvdata(dev);
+	int value = 0;
+
+	for (i = 0; i < data->gpios_num; i++)
+		if (gpiod_get_value_cansleep(data->gpios[i]) > 0)
+			value |= 1 << i;
+
+	return sprintf(buf, "%x", value);
+}
+
+DEVICE_ATTR(value, 0444, encsw_sysfs_value_show, NULL);
+
+static struct attribute *encsw_attrs[] = {
+	&dev_attr_value.attr,
+	NULL,
+};
+
+static struct attribute_group encsw_attr_group = {
+	.attrs = encsw_attrs,
+};
+
+static int encsw_probe(struct platform_device *pdev)
+{
+	struct encsw_dev_data *data;
+	int err, i;
+	struct device *dev = &pdev->dev;
+	int const gpios_num = of_gpio_named_count(dev->of_node, "encsw-gpios");
+
+	if (gpios_num <= 0)
+		return -EINVAL;
+
+	data = devm_kzalloc(dev,
+			    sizeof(*data) + gpios_num * sizeof(*(data->gpios)),
+			    GFP_KERNEL);
+
+	if (!data)
+		return -ENOMEM;
+
+	for (i = 0; i < gpios_num; i++) {
+		data->gpios[i] = gpiod_get_index(
+					dev, DRIVER_NAME, i, GPIOD_IN);
+		if (IS_ERR(data->gpios[i])) {
+			err = PTR_ERR(data->gpios[i]);
+			goto out_with_err_message;
+		}
+		if (gpiod_direction_input(data->gpios[i])) {
+			err = -EINVAL;
+			goto out_with_err_message;
+		}
+	}
+	data->gpios_num = gpios_num;
+
+	err = sysfs_create_group(&dev->kobj, &encsw_attr_group);
+	if (err) {
+		dev_err(dev, "cant export dip-switch sysfs group attribute\n");
+		goto out_with_err_message;
+	}
+	dev_set_drvdata(dev, data);
+
+	dev_info(&pdev->dev, "device installed successfully\n");
+
+	return 0;
+
+out_with_err_message:
+
+	dev_info(&pdev->dev, "failed to install device ret=%d\n", err);
+	return err;
+}
+
+
+static int encsw_remove(struct platform_device *pdev)
+{
+	struct encsw_dev_data *data = platform_get_drvdata(pdev);
+	struct device *dev = &pdev->dev;
+	int i;
+
+	for (i = 0; i < data->gpios_num; i++)
+		gpiod_put(data->gpios[i]);
+
+	dev_set_drvdata(&pdev->dev, NULL);
+	sysfs_remove_group(&dev->kobj, &encsw_attr_group);
+
+	dev_info(&pdev->dev, "device removed\n");
+
+	return 0;
+}
+
+static struct platform_driver encsw_driver = {
+	.driver	= {
+		.name  = DRIVER_NAME,
+		.owner = THIS_MODULE,
+		.of_match_table	= of_match_ptr(encsw_of_ids),
+	},
+	.probe	= encsw_probe,
+	.remove	= encsw_remove,
+};
+
+static int __init encsw_init(void)
+{
+	int ret = platform_driver_register(&encsw_driver);
+
+	if (ret)
+		pr_err("register encsw driver failed %d\n", ret);
+
+	return ret;
+}
+
+
+static void __exit encsw_exit(void)
+{
+	platform_driver_unregister(&encsw_driver);
+}
+
+module_exit(encsw_exit);
+module_init(encsw_init);
+
+
+MODULE_DESCRIPTION("Encoder Switch");
+MODULE_AUTHOR("Peter Galka <peter.galka@wago.com>");
+MODULE_LICENSE("GPL");
diff --git a/drivers/misc/ti_sn74lv165a.c b/drivers/misc/ti_sn74lv165a.c
new file mode 100644
index 000000000000..7b42493fddfe
--- /dev/null
+++ b/drivers/misc/ti_sn74lv165a.c
@@ -0,0 +1,324 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+
+/* Linux kernel driver for Texas Instruments SN74LV165A
+ * 8-bit-shift-register on gpio lines
+ *
+ * Copyright (C) 2015 WAGO Automation
+ *
+ * Author: Oleg Karfich
+ */
+
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/platform_device.h>
+#include <linux/of_platform.h>
+#include <linux/of_gpio.h>
+#include <linux/module.h>
+#include <linux/err.h>
+#include <linux/delay.h>
+
+#define DEFAULT_PERIOD 4 /* milliseconds */
+#define NUM_BITS 8
+
+struct sn74_gpio {
+	unsigned int num;
+	bool active_low;
+};
+
+struct sn74_gpio_info {
+	struct sn74_gpio clk;
+	struct sn74_gpio load;
+	struct sn74_gpio input;
+};
+
+struct sn74_platform_data {
+	struct sn74_gpio_info *ctrl_gpios;
+	unsigned long period;
+	unsigned int bits;
+};
+
+static int sn74_get_input(struct sn74_gpio *gpio)
+{
+	return (gpio_get_value(gpio->num) ^ gpio->active_low);
+}
+
+static void sn74_set_ctrl_gpio(struct sn74_gpio *gpio, int what)
+{
+	gpio_set_value(gpio->num, what ^ gpio->active_low);
+}
+
+static ssize_t sn74_input_value_show(struct device *dev,
+				     struct device_attribute *attr, char *buf)
+{
+	struct sn74_platform_data *pdata;
+	struct sn74_gpio_info *ctrl_gpios;
+	unsigned long half_period;
+	int i, value = 0;
+
+	pr_debug("## %s:%d\n", __func__, __LINE__);
+
+	pdata = dev_get_drvdata(dev);
+	if (!pdata) {
+		dev_warn(dev, "%s:%d > could not get platform-data\n", __func__,
+			 __LINE__);
+		return -1;
+	}
+
+	ctrl_gpios = pdata->ctrl_gpios;
+	half_period = (pdata->period / 2);
+
+	/* load register */
+	sn74_set_ctrl_gpio(&ctrl_gpios->load, 1);
+	ndelay(half_period);
+	sn74_set_ctrl_gpio(&ctrl_gpios->load, 0);
+
+	for (i = 0; i < pdata->bits; i++) {
+		value <<= 1;
+		value |= sn74_get_input(&ctrl_gpios->input);
+
+		/* do one clk cycle */
+		sn74_set_ctrl_gpio(&ctrl_gpios->clk, 1);
+		ndelay(half_period);
+		sn74_set_ctrl_gpio(&ctrl_gpios->clk, 0);
+		ndelay(half_period);
+	}
+
+	return sprintf(buf, "0x%02x\n", value);
+}
+
+/*
+ * ATTRIBUTES:
+ *
+ *	/sys/devices/<dts-node-name>/value
+ *
+ */
+static DEVICE_ATTR(value, 0444, sn74_input_value_show, NULL);
+
+static struct attribute *sn74_attrs[] = {
+	&dev_attr_value.attr,
+	NULL,
+};
+
+static struct attribute_group sn74_attr_group = {
+	.attrs = sn74_attrs,
+};
+
+#ifdef CONFIG_OF
+
+static struct sn74_platform_data *sn74_get_devtree_pdata(struct device *dev)
+{
+	struct device_node *node;
+	struct sn74_platform_data *pdata = NULL;
+	struct sn74_gpio_info *ctrl_gpios;
+	enum of_gpio_flags flags;
+	int gpio, period, ret;
+
+	dev_dbg(dev, "## %s:%d\n", __func__, __LINE__);
+
+	node = dev->of_node;
+	if (!node) {
+		ret = -ENODEV;
+		goto err_out;
+	}
+
+	/* automatically freed on driver detach */
+	pdata = devm_kzalloc(dev, sizeof(*pdata), GFP_KERNEL);
+	if (!pdata) {
+		ret = -ENOMEM;
+		goto err_free;
+	}
+
+	ctrl_gpios = devm_kzalloc(dev, sizeof(*ctrl_gpios), GFP_KERNEL);
+	if (!ctrl_gpios) {
+		ret = -ENOMEM;
+		goto err_free;
+	}
+
+	pdata->ctrl_gpios = ctrl_gpios;
+
+	if (!of_find_property(node, "gpios", NULL)) {
+		dev_err(dev, "failed to get control gpios\n");
+		ret = -ENODEV;
+		goto err_free;
+	}
+
+	gpio = of_get_gpio_flags(node, 0, &flags);
+	if (gpio < 0) {
+		ret = gpio;
+		goto err_free;
+	}
+
+	pdata->ctrl_gpios->clk.num = gpio;
+	pdata->ctrl_gpios->clk.active_low = flags & OF_GPIO_ACTIVE_LOW;
+
+	dev_dbg(dev, "clk gpio-num : %d, active low : %d\n",
+		pdata->ctrl_gpios->clk.num, pdata->ctrl_gpios->clk.active_low);
+
+	gpio = of_get_gpio_flags(node, 1, &flags);
+	if (gpio < 0) {
+		ret = gpio;
+		goto err_free;
+	}
+
+	pdata->ctrl_gpios->input.num = gpio;
+	pdata->ctrl_gpios->input.active_low = flags & OF_GPIO_ACTIVE_LOW;
+
+	dev_dbg(dev, "input gpio-num : %d, active low : %d\n",
+		pdata->ctrl_gpios->input.num,
+		pdata->ctrl_gpios->input.active_low);
+
+	gpio = of_get_gpio_flags(node, 2, &flags);
+	if (gpio < 0) {
+		ret = gpio;
+		goto err_free;
+	}
+
+	pdata->ctrl_gpios->load.num = gpio;
+	pdata->ctrl_gpios->load.active_low = flags & OF_GPIO_ACTIVE_LOW;
+
+	dev_dbg(dev, "load gpio-num : %d, active low : %d\n",
+		pdata->ctrl_gpios->load.num,
+		pdata->ctrl_gpios->load.active_low);
+
+	if (of_property_read_u32(node, "clk,period", &period)) {
+		period = DEFAULT_PERIOD;
+		dev_warn(dev, "not freq value found, using default value: %d\n",
+			 period);
+	}
+	pdata->period = (((unsigned long)period) *
+			 1000000); /* convert to nano-seconds for udelay */
+
+	dev_dbg(dev, "clk period : %ld ns\n", pdata->period);
+
+	return pdata;
+
+err_free:
+	devm_kfree(dev, pdata);
+	devm_kfree(dev, ctrl_gpios);
+
+err_out:
+	return ERR_PTR(ret);
+
+}
+
+static const struct of_device_id sn74_of_match[] = {
+	{
+		.compatible = "ti,sn74lv165a",
+	},
+	{},
+};
+MODULE_DEVICE_TABLE(of, sn74_of_match);
+
+#else
+
+static struct sn74_platform_data *sn74_get_devtree_pdata(struct device *dev)
+{
+	return ERR_PTR(-ENODEV);
+}
+
+#endif
+
+static int sn74_request_gpios(struct device *dev)
+{
+	struct sn74_platform_data *pdata;
+	int ret = -1;
+
+	dev_dbg(dev, "## %s:%d\n", __func__, __LINE__);
+
+	pdata = dev_get_drvdata(dev);
+	if (pdata) {
+		ret = devm_gpio_request(dev, pdata->ctrl_gpios->clk.num,
+					"sn74_clk");
+		if (ret)
+			return ret;
+
+		gpio_direction_output(pdata->ctrl_gpios->clk.num, 0);
+
+		ret = devm_gpio_request(dev, pdata->ctrl_gpios->load.num,
+					"sn74_load");
+		if (ret)
+			return ret;
+
+		gpio_direction_output(pdata->ctrl_gpios->load.num, 1);
+
+		ret = devm_gpio_request(dev, pdata->ctrl_gpios->input.num,
+					"sn74_input");
+		if (ret)
+			return ret;
+
+		gpio_direction_input(pdata->ctrl_gpios->input.num);
+	}
+	return ret;
+}
+
+static int sn74_probe(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct sn74_platform_data *pdata;
+	int ret;
+
+	dev_dbg(dev, "## %s:%d\n", __func__, __LINE__);
+
+	pdata = dev_get_platdata(dev);
+	if (IS_ERR_OR_NULL(pdata)) {
+		pdata = sn74_get_devtree_pdata(dev);
+		if (IS_ERR_OR_NULL(pdata))
+			return PTR_ERR(pdata);
+	}
+
+	pdata->bits = NUM_BITS;
+
+	dev_set_drvdata(dev, pdata);
+
+	ret = sn74_request_gpios(dev);
+	if (ret) {
+		dev_err(dev, "could not request gpios\n");
+		return ret;
+	}
+
+	ret = sysfs_create_group(&dev->kobj, &sn74_attr_group);
+	if (ret)
+		dev_err(dev, "cant export sn74 sysfs group attribute\n");
+
+	dev_info(dev, "probe success\n");
+
+	return ret;
+}
+
+static int sn74_remove(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+
+	dev_dbg(dev, "## %s:%d\n", __func__, __LINE__);
+
+	sysfs_remove_group(&dev->kobj, &sn74_attr_group);
+
+	return 0;
+}
+
+static struct platform_driver
+	sn74_device_driver = { .probe = sn74_probe,
+			       .remove = sn74_remove,
+			       .driver = {
+				       .name = "sn74lv165a",
+				       .owner = THIS_MODULE,
+				       .of_match_table =
+					       of_match_ptr(sn74_of_match),
+			       } };
+
+static int __init sn74_init(void)
+{
+	return platform_driver_register(&sn74_device_driver);
+}
+
+static void __exit sn74_exit(void)
+{
+	platform_driver_unregister(&sn74_device_driver);
+}
+
+module_init(sn74_init);
+module_exit(sn74_exit);
+
+MODULE_AUTHOR("Oleg Karfich <oleg.karfich@wago.com>");
+MODULE_DESCRIPTION("SN74LV165A 8-bit shift register");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/mmc/core/core.c b/drivers/mmc/core/core.c
index d42037f0f10d..94238b6d0c9d 100644
--- a/drivers/mmc/core/core.c
+++ b/drivers/mmc/core/core.c
@@ -173,7 +173,8 @@ void mmc_request_done(struct mmc_host *host, struct mmc_request *mrq)
 	if (!err || !cmd->retries || mmc_card_removed(host->card)) {
 		mmc_should_fail_request(host, mrq);
 
-		if (!host->ongoing_mrq)
+		// trigger the led only if the SD card is there
+		if ((!host->ongoing_mrq) && (host->card))
 			led_trigger_event(host->led, LED_OFF);
 
 		if (mrq->sbc) {
@@ -351,7 +352,10 @@ int mmc_start_request(struct mmc_host *host, struct mmc_request *mrq)
 	if (err)
 		return err;
 
-	led_trigger_event(host->led, LED_FULL);
+	// trigger the led only if the SD card is there
+	if (host->card)
+		led_trigger_event(host->led, LED_FULL);
+
 	__mmc_start_request(host, mrq);
 
 	return 0;
diff --git a/drivers/mtd/devices/Kconfig b/drivers/mtd/devices/Kconfig
index 0f4c2d823de8..1443fb0f00d4 100644
--- a/drivers/mtd/devices/Kconfig
+++ b/drivers/mtd/devices/Kconfig
@@ -89,6 +89,17 @@ config MTD_MCHP23K256
 	  platform data, or a device tree description if you want to
 	  specify device partitioning
 
+config MTD_ANV32AA1W
+	tristate "Anvo-Systems Dresden ANV32AA1W"
+	depends on SPI_MASTER
+	help
+	  This enables access to Anvo-Systems Dresden ANV32AA1W 1Mb SRAM chips,
+	  using SPI.
+
+	  Set up your spi devices with the right board-specific
+	  platform data, or a device tree description if you want to
+	  specify device partitioning
+
 config MTD_SPEAR_SMI
 	tristate "SPEAR MTD NOR Support through SMI controller"
 	depends on PLAT_SPEAR || COMPILE_TEST
diff --git a/drivers/mtd/devices/Makefile b/drivers/mtd/devices/Makefile
index 991c8d12c016..2311a038528a 100644
--- a/drivers/mtd/devices/Makefile
+++ b/drivers/mtd/devices/Makefile
@@ -18,6 +18,7 @@ obj-$(CONFIG_MTD_SST25L)	+= sst25l.o
 obj-$(CONFIG_MTD_BCM47XXSFLASH)	+= bcm47xxsflash.o
 obj-$(CONFIG_MTD_ST_SPI_FSM)    += st_spi_fsm.o
 obj-$(CONFIG_MTD_POWERNV_FLASH)	+= powernv_flash.o
+obj-$(CONFIG_MTD_ANV32AA1W)	+= anv32aa1w.o
 
 
 CFLAGS_docg3.o			+= -I$(src)
diff --git a/drivers/mtd/devices/anv32aa1w.c b/drivers/mtd/devices/anv32aa1w.c
new file mode 100644
index 000000000000..6f7e1aa53730
--- /dev/null
+++ b/drivers/mtd/devices/anv32aa1w.c
@@ -0,0 +1,302 @@
+/*
+ * anv32aa1w.c
+ *
+ * This driver is based on the Microchip 23k256 SPI RAM driver
+ *
+ * Copyright © 2019 Heinrich Toews <heinrich.toews@wago.com>
+ *
+ * This code is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ */
+#include <linux/device.h>
+#include <linux/module.h>
+#include <linux/mtd/mtd.h>
+#include <linux/mtd/partitions.h>
+#include <linux/mutex.h>
+#include <linux/sched.h>
+#include <linux/sizes.h>
+#include <linux/spi/flash.h>
+#include <linux/spi/spi.h>
+#include <linux/of_device.h>
+#include <linux/platform_data/spi-omap2-mcspi.h>
+
+#define CONFIG_ANV32AA1W__AM335X_OPT
+
+struct anv32aa1w_flash {
+	struct spi_device	*spi;
+	struct mutex		lock;
+	struct mtd_info		mtd;
+};
+
+#define anv32aa1w_CMD_WRITE_STATUS	0x01
+#define anv32aa1w_CMD_WREN	        0x06
+#define anv32aa1w_CMD_WRITE		0x02
+#define anv32aa1w_CMD_READ		0x03
+
+#define to_anv32aa1w_flash(x) container_of(x, struct anv32aa1w_flash, mtd)
+
+static struct omap2_mcspi_device_config anv32aa1w_cd = {
+	.turbo_mode = 1,
+};
+
+static bool anv32aa1w_32bit_mode = 0;
+
+/*
+ * Put the device into 'write enabled' mode.
+ * This is needed on every write cycle.
+ *
+ */
+static int anv32aa1w_write_enable(struct spi_device *spi)
+{
+	struct spi_transfer transfer = {};
+	struct spi_message message;
+	unsigned char command = anv32aa1w_CMD_WREN;
+
+	spi_message_init(&message);
+
+	transfer.tx_buf = &command;
+	transfer.len = sizeof(command);
+	spi_message_add_tail(&transfer, &message);
+
+	return spi_sync(spi, &message);
+}
+
+#if defined(CONFIG_ANV32AA1W__AM335X_OPT)
+#define ANV32AA1W_MAX_TURBO_LEN	65534
+static void
+init_data_transfers_am335x(struct spi_message *msg, struct spi_transfer *t,
+			   const unsigned char *buf, size_t len, int is_tx)
+{
+	unsigned char *p = (unsigned char *) buf;
+	int total = len, transfer_bytes = 0, i;
+
+	/*
+	 * AM335x McSPI optimization:
+	 * In order to make use of the Turbo mode
+	 * to minimize the inter-word GAP,
+	 * split the transfer into chunks of max 65535 words.
+	 * The data length must be an even number to have
+	 * at least a fifo depth of '2' bytes.
+	 * If this criteria is not met TURBO MODE will be ignored.
+	 */
+	for (i = 0; total > 0 && i < 3; i++) {
+		p += transfer_bytes;
+
+		if (total > ANV32AA1W_MAX_TURBO_LEN)
+			transfer_bytes = ANV32AA1W_MAX_TURBO_LEN;
+		else if (total > 1 && total % 2)
+			transfer_bytes = total - 1;
+		else
+			transfer_bytes = total;
+
+		if (is_tx)
+			t[i].tx_buf = p;
+		else
+			t[i].rx_buf = p;
+
+		t[i].len = transfer_bytes;
+		spi_message_add_tail(&t[i], msg);
+
+		total -= transfer_bytes;
+#ifdef DEBUG
+		trace_printk("%s: p %p, total %d, tbytes %d\n",
+			     is_tx ? "TX" : "RX",
+			     p, total, transfer_bytes);
+#endif
+	}
+}
+#else
+static void
+init_data_transfers_am335x(struct spi_message *msg, struct spi_transfer *t,
+			   const unsigned char *buf, size_t len, int is_tx)
+{
+	t->tx_buf = buf;
+	t->len = len;
+	spi_message_add_tail(t, &msg);
+}
+#endif
+
+static int
+init_data_transfers_32b(struct spi_message *msg,
+				  struct spi_transfer *t, unsigned char *buf,
+				  loff_t offset, size_t len, int is_tx)
+{
+	if (is_tx)
+		t->tx_buf = buf;
+	else
+		t->rx_buf = buf;
+
+	t->len = len;
+	t->bits_per_word = 32;
+	spi_message_add_tail(t, msg);
+
+	return 0;
+}
+
+static int anv32aa1w_rw(struct mtd_info *mtd, loff_t offs, size_t len,
+			size_t *retlen, unsigned char *buf, bool write)
+{
+	struct anv32aa1w_flash *flash = to_anv32aa1w_flash(mtd);
+	/* 1x cmd + max. 3x data transfers */
+	struct spi_transfer transfer[4] = {};
+	struct spi_message message;
+	unsigned char command[] = {
+		write ? anv32aa1w_CMD_WRITE : anv32aa1w_CMD_READ,
+		offs >> 16, offs >> 8, offs
+	};
+	int ret;
+
+	if (write)
+		anv32aa1w_write_enable(flash->spi);
+
+	spi_message_init(&message);
+
+	transfer[0].tx_buf = command;
+	transfer[0].len = sizeof(command);
+
+	spi_message_add_tail(transfer, &message);
+
+	if (anv32aa1w_32bit_mode) {
+		if (offs % 4 || len % 4)
+			return -1;
+
+		init_data_transfers_32b(&message, transfer + 1,
+						buf, offs, len, write);
+	} else {
+		init_data_transfers_am335x(&message, transfer + 1,
+						buf, len, write);
+	}
+
+	mutex_lock(&flash->lock);
+
+	ret = spi_sync(flash->spi, &message);
+
+	if (retlen && message.actual_length > sizeof(command))
+		*retlen += message.actual_length - sizeof(command);
+
+	mutex_unlock(&flash->lock);
+
+	return ret;
+}
+
+static int anv32aa1w_write(struct mtd_info *mtd, loff_t to, size_t len,
+			    size_t *retlen, const unsigned char *buf)
+{
+	return anv32aa1w_rw(mtd, to, len, retlen, (unsigned char *)buf, true);
+}
+
+static int anv32aa1w_read(struct mtd_info *mtd, loff_t from, size_t len,
+			   size_t *retlen, unsigned char *buf)
+{
+	return anv32aa1w_rw(mtd, from, len, retlen, buf, false);
+}
+
+
+static ssize_t set_turbo_mode_store(struct device_driver *driver,
+				    const char *buf, size_t count)
+{
+	bool b;
+
+	if (kstrtobool(buf, &b) < 0)
+		return -EINVAL;
+
+	anv32aa1w_cd.turbo_mode = b ? 1 : 0;
+	return count;
+}
+
+static ssize_t set_turbo_mode_show(struct device_driver *driver, char *buf)
+{
+	return sprintf(buf, "%d\n", anv32aa1w_cd.turbo_mode);
+}
+
+DRIVER_ATTR_RW(set_turbo_mode);
+
+static ssize_t set_32bit_mode_store(struct device_driver *driver,
+				    const char *buf, size_t count)
+{
+	return kstrtobool(buf, &anv32aa1w_32bit_mode) < 0 ? -EINVAL : count;
+}
+
+static ssize_t set_32bit_mode_show(struct device_driver *driver, char *buf)
+{
+	return sprintf(buf, "%d\n", anv32aa1w_32bit_mode);
+}
+
+DRIVER_ATTR_RW(set_32bit_mode);
+
+static struct attribute *driver_attrs[] = {
+        &driver_attr_set_turbo_mode.attr,
+        &driver_attr_set_32bit_mode.attr,
+        NULL,
+};
+
+static struct attribute_group driver_attr_group = {
+        .attrs = driver_attrs,
+};
+
+static const struct attribute_group *driver_attr_groups[] = {
+        &driver_attr_group,
+        NULL,
+};
+
+static int anv32aa1w_probe(struct spi_device *spi)
+{
+	struct anv32aa1w_flash *flash;
+	struct flash_platform_data *data;
+
+	flash = devm_kzalloc(&spi->dev, sizeof(*flash), GFP_KERNEL);
+	if (!flash)
+		return -ENOMEM;
+
+	spi->controller_data = &anv32aa1w_cd;
+
+	flash->spi = spi;
+	mutex_init(&flash->lock);
+	spi_set_drvdata(spi, flash);
+
+	data = dev_get_platdata(&spi->dev);
+
+	flash->mtd.dev.parent	= &spi->dev;
+	flash->mtd.type		= MTD_RAM;
+	flash->mtd.flags	= MTD_CAP_RAM;
+	flash->mtd.writesize	= 1;
+	flash->mtd.size		= SZ_128K;
+	flash->mtd._read	= anv32aa1w_read;
+	flash->mtd._write	= anv32aa1w_write;
+
+	return mtd_device_parse_register(&flash->mtd, NULL, NULL,
+					 data ? data->parts : NULL,
+					 data ? data->nr_parts : 0);
+}
+
+static int anv32aa1w_remove(struct spi_device *spi)
+{
+	struct anv32aa1w_flash *flash = spi_get_drvdata(spi);
+
+	return mtd_device_unregister(&flash->mtd);
+}
+
+static const struct of_device_id anv32aa1w_of_table[] = {
+	{ .compatible = "anvo-systems,anv32aa1w" },
+	{}
+};
+MODULE_DEVICE_TABLE(of, anv32aa1w_of_table);
+
+static struct spi_driver anv32aa1w_driver = {
+	.driver = {
+		.name	= "anv32aa1w",
+		.of_match_table = of_match_ptr(anv32aa1w_of_table),
+		.groups = driver_attr_groups,
+	},
+	.probe		= anv32aa1w_probe,
+	.remove		= anv32aa1w_remove,
+};
+
+module_spi_driver(anv32aa1w_driver);
+
+MODULE_DESCRIPTION("MTD SPI driver for anv32aa1w RAM chips");
+MODULE_AUTHOR("Heinrich Toews <heinrich.toews@wago.com>");
+MODULE_LICENSE("GPL v2");
+MODULE_ALIAS("spi:anv32aa1w");
diff --git a/drivers/mtd/mtdoops.c b/drivers/mtd/mtdoops.c
index 774970bfcf85..6bc2c728adb7 100644
--- a/drivers/mtd/mtdoops.c
+++ b/drivers/mtd/mtdoops.c
@@ -267,7 +267,8 @@ static void find_next_position(struct mtdoops_context *cxt)
 }
 
 static void mtdoops_do_dump(struct kmsg_dumper *dumper,
-			    enum kmsg_dump_reason reason)
+			    enum kmsg_dump_reason reason,
+			    struct kmsg_dumper_iter *iter)
 {
 	struct mtdoops_context *cxt = container_of(dumper,
 			struct mtdoops_context, dump);
@@ -276,7 +277,7 @@ static void mtdoops_do_dump(struct kmsg_dumper *dumper,
 	if (reason == KMSG_DUMP_OOPS && !dump_oops)
 		return;
 
-	kmsg_dump_get_buffer(dumper, true, cxt->oops_buf + MTDOOPS_HEADER_SIZE,
+	kmsg_dump_get_buffer(iter, true, cxt->oops_buf + MTDOOPS_HEADER_SIZE,
 			     record_size - MTDOOPS_HEADER_SIZE, NULL);
 
 	if (reason != KMSG_DUMP_OOPS) {
diff --git a/drivers/mtd/spi-nor/core.c b/drivers/mtd/spi-nor/core.c
index 06e1bf01fd92..62ea75427ee7 100644
--- a/drivers/mtd/spi-nor/core.c
+++ b/drivers/mtd/spi-nor/core.c
@@ -3063,7 +3063,9 @@ static const struct flash_info *spi_nor_get_flash_info(struct spi_nor *nor,
 	 * If caller has specified name of flash model that can normally be
 	 * detected using JEDEC, let's verify it.
 	 */
-	if (name && info->id_len) {
+	if (name && info->id_len &&
+			!of_property_read_bool(nor->mtd.dev.of_node,
+						"spi-nor,offline-probe")) {
 		const struct flash_info *jinfo;
 
 		jinfo = spi_nor_read_id(nor);
diff --git a/drivers/net/arcnet/arc-rimi.c b/drivers/net/arcnet/arc-rimi.c
index 98df38fe553c..12d085405bd0 100644
--- a/drivers/net/arcnet/arc-rimi.c
+++ b/drivers/net/arcnet/arc-rimi.c
@@ -332,7 +332,7 @@ static int __init arc_rimi_init(void)
 		dev->irq = 9;
 
 	if (arcrimi_probe(dev)) {
-		free_netdev(dev);
+		free_arcdev(dev);
 		return -EIO;
 	}
 
@@ -349,7 +349,7 @@ static void __exit arc_rimi_exit(void)
 	iounmap(lp->mem_start);
 	release_mem_region(dev->mem_start, dev->mem_end - dev->mem_start + 1);
 	free_irq(dev->irq, dev);
-	free_netdev(dev);
+	free_arcdev(dev);
 }
 
 #ifndef MODULE
diff --git a/drivers/net/arcnet/arcdevice.h b/drivers/net/arcnet/arcdevice.h
index 22a49c6d7ae6..5d4a4c7efbbf 100644
--- a/drivers/net/arcnet/arcdevice.h
+++ b/drivers/net/arcnet/arcdevice.h
@@ -298,6 +298,10 @@ struct arcnet_local {
 
 	int excnak_pending;    /* We just got an excesive nak interrupt */
 
+	/* RESET flag handling */
+	int reset_in_progress;
+	struct work_struct reset_work;
+
 	struct {
 		uint16_t sequence;	/* sequence number (incs with each packet) */
 		__be16 aborted_seq;
@@ -350,7 +354,9 @@ void arcnet_dump_skb(struct net_device *dev, struct sk_buff *skb, char *desc)
 
 void arcnet_unregister_proto(struct ArcProto *proto);
 irqreturn_t arcnet_interrupt(int irq, void *dev_id);
+
 struct net_device *alloc_arcdev(const char *name);
+void free_arcdev(struct net_device *dev);
 
 int arcnet_open(struct net_device *dev);
 int arcnet_close(struct net_device *dev);
diff --git a/drivers/net/arcnet/arcnet.c b/drivers/net/arcnet/arcnet.c
index e04efc0a5c97..d76dd7d14299 100644
--- a/drivers/net/arcnet/arcnet.c
+++ b/drivers/net/arcnet/arcnet.c
@@ -387,10 +387,44 @@ static void arcnet_timer(struct timer_list *t)
 	struct arcnet_local *lp = from_timer(lp, t, timer);
 	struct net_device *dev = lp->dev;
 
-	if (!netif_carrier_ok(dev)) {
+	spin_lock_irq(&lp->lock);
+
+	if (!lp->reset_in_progress && !netif_carrier_ok(dev)) {
 		netif_carrier_on(dev);
 		netdev_info(dev, "link up\n");
 	}
+
+	spin_unlock_irq(&lp->lock);
+}
+
+static void reset_device_work(struct work_struct *work)
+{
+	struct arcnet_local *lp;
+	struct net_device *dev;
+
+	lp = container_of(work, struct arcnet_local, reset_work);
+	dev = lp->dev;
+
+	/* Do not bring the network interface back up if an ifdown
+	 * was already done.
+	 */
+	if (!netif_running(dev) || !lp->reset_in_progress)
+		return;
+
+	rtnl_lock();
+
+	/* Do another check, in case of an ifdown that was triggered in
+	 * the small race window between the exit condition above and
+	 * acquiring RTNL.
+	 */
+	if (!netif_running(dev) || !lp->reset_in_progress)
+		goto out;
+
+	dev_close(dev);
+	dev_open(dev, NULL);
+
+out:
+	rtnl_unlock();
 }
 
 static void arcnet_reply_tasklet(unsigned long data)
@@ -452,12 +486,25 @@ struct net_device *alloc_arcdev(const char *name)
 		lp->dev = dev;
 		spin_lock_init(&lp->lock);
 		timer_setup(&lp->timer, arcnet_timer, 0);
+		INIT_WORK(&lp->reset_work, reset_device_work);
 	}
 
 	return dev;
 }
 EXPORT_SYMBOL(alloc_arcdev);
 
+void free_arcdev(struct net_device *dev)
+{
+	struct arcnet_local *lp = netdev_priv(dev);
+
+	/* Do not cancel this at ->ndo_close(), as the workqueue itself
+	 * indirectly calls the ifdown path through dev_close().
+	 */
+	cancel_work_sync(&lp->reset_work);
+	free_netdev(dev);
+}
+EXPORT_SYMBOL(free_arcdev);
+
 /* Open/initialize the board.  This is called sometime after booting when
  * the 'ifconfig' program is run.
  *
@@ -587,6 +634,10 @@ int arcnet_close(struct net_device *dev)
 
 	/* shut down the card */
 	lp->hw.close(dev);
+
+	/* reset counters */
+	lp->reset_in_progress = 0;
+
 	module_put(lp->hw.owner);
 	return 0;
 }
@@ -820,6 +871,9 @@ irqreturn_t arcnet_interrupt(int irq, void *dev_id)
 
 	spin_lock_irqsave(&lp->lock, flags);
 
+	if (lp->reset_in_progress)
+		goto out;
+
 	/* RESET flag was enabled - if device is not running, we must
 	 * clear it right away (but nothing else).
 	 */
@@ -852,11 +906,14 @@ irqreturn_t arcnet_interrupt(int irq, void *dev_id)
 		if (status & RESETflag) {
 			arc_printk(D_NORMAL, dev, "spurious reset (status=%Xh)\n",
 				   status);
-			arcnet_close(dev);
-			arcnet_open(dev);
+
+			lp->reset_in_progress = 1;
+			netif_stop_queue(dev);
+			netif_carrier_off(dev);
+			schedule_work(&lp->reset_work);
 
 			/* get out of the interrupt handler! */
-			break;
+			goto out;
 		}
 		/* RX is inhibited - we must have received something.
 		 * Prepare to receive into the next buffer.
@@ -1052,6 +1109,7 @@ irqreturn_t arcnet_interrupt(int irq, void *dev_id)
 	udelay(1);
 	lp->hw.intmask(dev, lp->intmask);
 
+out:
 	spin_unlock_irqrestore(&lp->lock, flags);
 	return retval;
 }
diff --git a/drivers/net/arcnet/com20020-isa.c b/drivers/net/arcnet/com20020-isa.c
index f983c4ce6b07..be618e4b9ed5 100644
--- a/drivers/net/arcnet/com20020-isa.c
+++ b/drivers/net/arcnet/com20020-isa.c
@@ -169,7 +169,7 @@ static int __init com20020_init(void)
 		dev->irq = 9;
 
 	if (com20020isa_probe(dev)) {
-		free_netdev(dev);
+		free_arcdev(dev);
 		return -EIO;
 	}
 
@@ -182,7 +182,7 @@ static void __exit com20020_exit(void)
 	unregister_netdev(my_dev);
 	free_irq(my_dev->irq, my_dev);
 	release_region(my_dev->base_addr, ARCNET_TOTAL_SIZE);
-	free_netdev(my_dev);
+	free_arcdev(my_dev);
 }
 
 #ifndef MODULE
diff --git a/drivers/net/arcnet/com20020-pci.c b/drivers/net/arcnet/com20020-pci.c
index eb7f76753c9c..8bdc44b7e09a 100644
--- a/drivers/net/arcnet/com20020-pci.c
+++ b/drivers/net/arcnet/com20020-pci.c
@@ -291,7 +291,7 @@ static void com20020pci_remove(struct pci_dev *pdev)
 
 		unregister_netdev(dev);
 		free_irq(dev->irq, dev);
-		free_netdev(dev);
+		free_arcdev(dev);
 	}
 }
 
diff --git a/drivers/net/arcnet/com20020_cs.c b/drivers/net/arcnet/com20020_cs.c
index cf607ffcf358..9cc5eb6a8e90 100644
--- a/drivers/net/arcnet/com20020_cs.c
+++ b/drivers/net/arcnet/com20020_cs.c
@@ -177,7 +177,7 @@ static void com20020_detach(struct pcmcia_device *link)
 		dev = info->dev;
 		if (dev) {
 			dev_dbg(&link->dev, "kfree...\n");
-			free_netdev(dev);
+			free_arcdev(dev);
 		}
 		dev_dbg(&link->dev, "kfree2...\n");
 		kfree(info);
diff --git a/drivers/net/arcnet/com90io.c b/drivers/net/arcnet/com90io.c
index cf214b730671..3856b447d38e 100644
--- a/drivers/net/arcnet/com90io.c
+++ b/drivers/net/arcnet/com90io.c
@@ -396,7 +396,7 @@ static int __init com90io_init(void)
 	err = com90io_probe(dev);
 
 	if (err) {
-		free_netdev(dev);
+		free_arcdev(dev);
 		return err;
 	}
 
@@ -419,7 +419,7 @@ static void __exit com90io_exit(void)
 
 	free_irq(dev->irq, dev);
 	release_region(dev->base_addr, ARCNET_TOTAL_SIZE);
-	free_netdev(dev);
+	free_arcdev(dev);
 }
 
 module_init(com90io_init)
diff --git a/drivers/net/arcnet/com90xx.c b/drivers/net/arcnet/com90xx.c
index 3dc3d533cb19..d8dfb9ea0de8 100644
--- a/drivers/net/arcnet/com90xx.c
+++ b/drivers/net/arcnet/com90xx.c
@@ -554,7 +554,7 @@ static int __init com90xx_found(int ioaddr, int airq, u_long shmem,
 err_release_mem:
 	release_mem_region(dev->mem_start, dev->mem_end - dev->mem_start + 1);
 err_free_dev:
-	free_netdev(dev);
+	free_arcdev(dev);
 	return -EIO;
 }
 
@@ -672,7 +672,7 @@ static void __exit com90xx_exit(void)
 		release_region(dev->base_addr, ARCNET_TOTAL_SIZE);
 		release_mem_region(dev->mem_start,
 				   dev->mem_end - dev->mem_start + 1);
-		free_netdev(dev);
+		free_arcdev(dev);
 	}
 }
 
diff --git a/drivers/net/can/c_can/c_can.c b/drivers/net/can/c_can/c_can.c
index 1a9e9b9a4bf6..0c582b792e55 100644
--- a/drivers/net/can/c_can/c_can.c
+++ b/drivers/net/can/c_can/c_can.c
@@ -522,7 +522,7 @@ static int c_can_set_bittiming(struct net_device *dev)
 			(tseg2 << BTR_TSEG2_SHIFT);
 	reg_brpe = brpe & BRP_EXT_BRPE_MASK;
 
-	netdev_info(dev,
+	netdev_dbg(dev,
 		"setting BTR=%04x BRPE=%04x\n", reg_btr, reg_brpe);
 
 	ctrl_save = priv->read_reg(priv, C_CAN_CTRL_REG);
@@ -908,6 +908,8 @@ static int c_can_handle_state_change(struct net_device *dev,
 	switch (error_type) {
 	case C_CAN_NO_ERROR:
 		priv->can.state = CAN_STATE_ERROR_ACTIVE;
+		priv->write_reg(priv, C_CAN_CTRL_REG, CONTROL_ENABLE_AR);
+		netdev_dbg(dev, "enable AR\n");
 		break;
 	case C_CAN_ERROR_WARNING:
 		/* error warning state */
@@ -918,11 +920,15 @@ static int c_can_handle_state_change(struct net_device *dev,
 		/* error passive state */
 		priv->can.can_stats.error_passive++;
 		priv->can.state = CAN_STATE_ERROR_PASSIVE;
+		priv->write_reg(priv, C_CAN_CTRL_REG, CONTROL_DISABLE_AR);
+		netdev_dbg(dev, "disable AR\n");
 		break;
 	case C_CAN_BUS_OFF:
 		/* bus-off state */
 		priv->can.state = CAN_STATE_BUS_OFF;
 		priv->can.can_stats.bus_off++;
+		priv->write_reg(priv, C_CAN_CTRL_REG, CONTROL_DISABLE_AR);
+		netdev_dbg(dev, "disable AR\n");
 		break;
 	default:
 		break;
@@ -930,8 +936,13 @@ static int c_can_handle_state_change(struct net_device *dev,
 
 	/* propagate the error condition to the CAN stack */
 	skb = alloc_can_err_skb(dev, &cf);
-	if (unlikely(!skb))
-		return 0;
+	if (unlikely(!skb)) {
+		if (printk_ratelimit())
+			netdev_err(dev,
+				"c_can_error: alloc_can_err_skb() failed\n");
+		return -ENOMEM;
+	}
+
 
 	__c_can_get_berr_counter(dev, &bec);
 	reg_err_counter = priv->read_reg(priv, C_CAN_ERR_CNT_REG);
@@ -1071,20 +1082,22 @@ static int c_can_poll(struct napi_struct *napi, int quota)
 	}
 
 	/* handle state changes */
+	if ((curr & STATUS_BOFF) && (!(last & STATUS_BOFF))) {
+		netdev_dbg(dev, "entered bus off state\n");
+		work_done += c_can_handle_state_change(dev, C_CAN_BUS_OFF);
+		goto end;
+	}
+
 	if ((curr & STATUS_EWARN) && (!(last & STATUS_EWARN))) {
 		netdev_dbg(dev, "entered error warning state\n");
-		work_done += c_can_handle_state_change(dev, C_CAN_ERROR_WARNING);
+		if (work_done < quota)
+			work_done += c_can_handle_state_change(dev, C_CAN_ERROR_WARNING);
 	}
 
 	if ((curr & STATUS_EPASS) && (!(last & STATUS_EPASS))) {
 		netdev_dbg(dev, "entered error passive state\n");
-		work_done += c_can_handle_state_change(dev, C_CAN_ERROR_PASSIVE);
-	}
-
-	if ((curr & STATUS_BOFF) && (!(last & STATUS_BOFF))) {
-		netdev_dbg(dev, "entered bus off state\n");
-		work_done += c_can_handle_state_change(dev, C_CAN_BUS_OFF);
-		goto end;
+		if (work_done < quota)
+			work_done += c_can_handle_state_change(dev, C_CAN_ERROR_PASSIVE);
 	}
 
 	/* handle bus recovery events */
@@ -1104,19 +1117,22 @@ static int c_can_poll(struct napi_struct *napi, int quota)
 	}
 
 	/* handle lec errors on the bus */
-	work_done += c_can_handle_bus_err(dev, curr & LEC_MASK);
+	if (work_done < quota)
+		work_done += c_can_handle_bus_err(dev, curr & LEC_MASK);
 
+end:
 	/* Handle Tx/Rx events. We do this unconditionally */
-	work_done += c_can_do_rx_poll(dev, (quota - work_done));
-	c_can_do_tx(dev);
+	if (work_done < quota)
+		work_done += c_can_do_rx_poll(dev, (quota - work_done));
 
-end:
-	if (work_done < quota) {
+	if (priv->can.state != CAN_STATE_BUS_OFF)
+		c_can_do_tx(dev);
+
+	if (work_done < quota)
 		napi_complete_done(napi, work_done);
-		/* enable all IRQs if we are not in bus off state */
-		if (priv->can.state != CAN_STATE_BUS_OFF)
-			c_can_irq_control(priv, true);
-	}
+
+	c_can_irq_control(priv, true);
+
 
 	return work_done;
 }
diff --git a/drivers/net/can/ti_hecc.c b/drivers/net/can/ti_hecc.c
index 2c22f40e12bd..bd1451dd824f 100644
--- a/drivers/net/can/ti_hecc.c
+++ b/drivers/net/can/ti_hecc.c
@@ -686,8 +686,10 @@ static irqreturn_t ti_hecc_interrupt(int irq, void *dev_id)
 			       priv->use_hecc1int ?
 			       HECC_CANGIF1 : HECC_CANGIF0);
 
+/* this results blocking in kernel 4.9
 	if (!int_status)
 		return IRQ_NONE;
+*/
 
 	err_status = hecc_read(priv, HECC_CANES);
 	if (unlikely(err_status & HECC_CANES_FLAGS))
@@ -794,8 +796,8 @@ static int ti_hecc_open(struct net_device *ndev)
 	struct ti_hecc_priv *priv = netdev_priv(ndev);
 	int err;
 
-	err = request_irq(ndev->irq, ti_hecc_interrupt, IRQF_SHARED,
-			  ndev->name, ndev);
+	err = request_irq(ndev->irq, ti_hecc_interrupt, IRQF_SHARED |
+			  IRQF_THREAD_TBL_LOOKUP, ndev->name, ndev);
 	if (err) {
 		netdev_err(ndev, "error requesting interrupt\n");
 		return err;
diff --git a/drivers/net/dsa/Kconfig b/drivers/net/dsa/Kconfig
index 2451f61a38e4..99ca05df9915 100644
--- a/drivers/net/dsa/Kconfig
+++ b/drivers/net/dsa/Kconfig
@@ -2,6 +2,14 @@
 menu "Distributed Switch Architecture drivers"
 	depends on HAVE_NET_DSA
 
+config NET_DSA_KSZ8863
+	tristate "Micrel KSZ8863 ethernet switch chip support"
+	select NET_DSA
+	select NET_DSA_TAG_TAIL
+	help
+	  This enables support for the Micrel KSZ8863 ethernet switch
+	  chip.
+
 source "drivers/net/dsa/b53/Kconfig"
 
 config NET_DSA_BCM_SF2
diff --git a/drivers/net/dsa/Makefile b/drivers/net/dsa/Makefile
index 4a943ccc2ca4..42642e678380 100644
--- a/drivers/net/dsa/Makefile
+++ b/drivers/net/dsa/Makefile
@@ -23,3 +23,4 @@ obj-y				+= mv88e6xxx/
 obj-y				+= ocelot/
 obj-y				+= qca/
 obj-y				+= sja1105/
+obj-y				+= ksz886x/
diff --git a/drivers/net/dsa/ksz886x/Makefile b/drivers/net/dsa/ksz886x/Makefile
new file mode 100644
index 000000000000..9e878f4626d3
--- /dev/null
+++ b/drivers/net/dsa/ksz886x/Makefile
@@ -0,0 +1 @@
+obj-$(CONFIG_NET_DSA_KSZ8863)	+= ksz8863.o debugfs.o
diff --git a/drivers/net/dsa/ksz886x/debugfs.c b/drivers/net/dsa/ksz886x/debugfs.c
new file mode 100644
index 000000000000..26246b57f369
--- /dev/null
+++ b/drivers/net/dsa/ksz886x/debugfs.c
@@ -0,0 +1,125 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/* Micrel KSZ8863 Ethernet switch debugfs support
+ *
+ * Copyright (c) 2018 WAGO Kontakttechnick GmbH
+ */
+
+#include <linux/kernel.h>
+#include <linux/debugfs.h>
+
+#include <linux/ksz8863.h>
+#include "debugfs.h"
+
+#define DUMP_SWITCH_REGS_ID 0
+#define DUMP_PORT_REGS_ID 1
+
+/* list of files published via debugfs*/
+static char const *const files[] = { [DUMP_SWITCH_REGS_ID] = "dump_switch_regs",
+				     [DUMP_PORT_REGS_ID] = "dump_phy_regs" };
+
+static inline struct ksz8863_chip *get_chip(struct file *filp)
+{
+	return filp->f_inode->i_private;
+}
+
+static int ksz8863_read(struct ksz8863_chip *chip, int reg, u8 *val)
+{
+	return chip->mii_ops->read(chip, reg, val);
+}
+
+static bool ksz8863_check_filename(struct file *filp, int file_index)
+{
+	return strcmp(files[file_index], filp->f_path.dentry->d_iname) == 0;
+}
+
+static ssize_t ksz8863_switch_reg_dump_read(struct file *filp,
+					    char __user *user_buf, size_t count,
+					    loff_t *off)
+{
+	struct ksz8863_chip *chip = get_chip(filp);
+	const int buf_size = 10000;
+	char *reg_dump_buf = kmalloc(buf_size, GFP_KERNEL);
+	int ret, port;
+	char *pos = reg_dump_buf;
+	u8 sw_gl_reg, port_reg;
+
+	pos += sprintf(pos, "=== SWITCH GCTL REG 0 - 15 dump ===\n");
+
+	mutex_lock(&chip->reg_lock);
+
+	for (sw_gl_reg = 0; sw_gl_reg < KSZ8863_REG_PORTS_BASE; sw_gl_reg++) {
+		u8 reg_val;
+
+		ret = ksz8863_read(chip, sw_gl_reg, &reg_val);
+		pos += sprintf(pos, "  REG %d (0x%02X) value: 0x%02X\n",
+			       sw_gl_reg, sw_gl_reg, reg_val);
+	}
+
+	for (port = 0; port < KSZ8863_NUM_PORTS; port++) {
+		pos += sprintf(pos, "=== SWITCH PORT %d dump ===\n", port);
+		for (port_reg = KSZ8863_REG_PORT1_CTRL0;
+		     port_reg <= KSZ8863_REG_PORT1_CTRL13; port_reg++) {
+			u8 reg = port * KSZ8863_REG_PORTS_BASE + port_reg;
+			u8 reg_val;
+
+			ret = ksz8863_read(chip, reg, &reg_val);
+			pos += sprintf(pos, "  REG %d (0x%02X) value: 0x%02X\n",
+				       reg, reg, reg_val);
+		}
+	}
+
+	mutex_unlock(&chip->reg_lock);
+	ret = simple_read_from_buffer(user_buf, count, off, reg_dump_buf,
+				      strlen(reg_dump_buf));
+
+	kfree(reg_dump_buf);
+	return ret;
+}
+
+static ssize_t ksz8863_debugfs_read(struct file *filp, char __user *buffer,
+				    size_t count, loff_t *off)
+{
+	if (ksz8863_check_filename(filp, DUMP_SWITCH_REGS_ID))
+		return ksz8863_switch_reg_dump_read(filp, buffer, count, off);
+
+	return 0;
+}
+
+static ssize_t ksz8863_debugfs_write(struct file *filp, const char __user *s,
+				     size_t count, loff_t *off)
+{
+	return 0;
+}
+
+static const struct file_operations debugfs_ops = {
+	.read = ksz8863_debugfs_read,
+	.write = ksz8863_debugfs_write
+};
+
+void ksz8863_debugfs_destroy(struct ksz8863_chip *chip)
+{
+	if (!IS_ERR_OR_NULL(chip->debugfs_root_entry))
+		debugfs_remove_recursive(chip->debugfs_root_entry);
+}
+
+int ksz8863_debugfs_setup(struct ksz8863_chip *chip)
+{
+	struct dentry *entry;
+
+	entry = debugfs_create_dir("ksz8863", NULL);
+	if (IS_ERR_OR_NULL(entry))
+		goto out_err;
+
+	chip->debugfs_root_entry = entry;
+
+	entry = debugfs_create_file(files[DUMP_SWITCH_REGS_ID], 0600,
+				    chip->debugfs_root_entry, chip,
+				    &debugfs_ops);
+	if (IS_ERR_OR_NULL(entry))
+		goto out_err;
+
+	return 0;
+
+out_err:
+	return (entry) ? PTR_ERR(entry) : -EINVAL;
+}
diff --git a/drivers/net/dsa/ksz886x/debugfs.h b/drivers/net/dsa/ksz886x/debugfs.h
new file mode 100644
index 000000000000..47e05e8db987
--- /dev/null
+++ b/drivers/net/dsa/ksz886x/debugfs.h
@@ -0,0 +1,11 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+
+#ifndef _KSZ8863_DEBUGFS_H
+#define _KSZ8863_DEBUGFS_H
+
+#include <linux/ksz8863.h>
+
+void ksz8863_debugfs_destroy(struct ksz8863_chip *chip);
+int ksz8863_debugfs_setup(struct ksz8863_chip *chip);
+
+#endif /* _KSZ8863_DEBUGFS_H */
diff --git a/drivers/net/dsa/ksz886x/ksz8863.c b/drivers/net/dsa/ksz886x/ksz8863.c
new file mode 100644
index 000000000000..640fe8826692
--- /dev/null
+++ b/drivers/net/dsa/ksz886x/ksz8863.c
@@ -0,0 +1,930 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/* net/dsa/ksz886x/ksz8863.c - Micrel KSZ8863 switch chip support
+ *
+ * Copyright (c) 2010 SAGEMCOM
+ * Copyright (C) 2017 Wago Kontakttechnik GmbH
+ *
+ * Author: Karl Beldan <karl.beldan@sagemcom.com>
+ * Author: Heinrich Toews <heinrich.toews@wago.com>
+ * Author: Andreas Schmidt <andreas.schmidt@wago.com>
+ *
+ * This driver is oriented on Marvell dsa driver for mv88e6xxx switches.
+ * Some parts and approaches are copied from ksz8863 driver implementation
+ * of Karl Beldan.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ */
+
+#define pr_fmt(fmt) "dsa: ksz8863: " fmt
+
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/of_net.h>
+#include <linux/mdio.h>
+#include <linux/phy.h>
+#include <linux/gpio/consumer.h>
+#include <net/dsa.h>
+#include <linux/delay.h>
+#include <linux/etherdevice.h>
+#include <linux/ksz8863.h>
+
+#include "debugfs.h"
+
+static inline int port_to_index(int port)
+{
+	return port - KSZ8863_EXT_PORT1_ID;
+}
+
+static inline int index_to_port(int index)
+{
+	return index + KSZ8863_EXT_PORT1_ID;
+}
+
+static enum dsa_tag_protocol
+ksz8863_get_tag_protocol(struct dsa_switch *ds, int port,
+			 enum dsa_tag_protocol mprot)
+{
+	return DSA_TAG_PROTO_KSZ8863;
+}
+
+static struct net_device *ksz8863_get_bridge_of_port(struct ksz8863_chip *chip,
+						     int port)
+{
+	return chip->ports[port_to_index(port)].bridge;
+}
+
+static void ksz8863_set_bridge_of_port(struct ksz8863_chip *chip, int port,
+				       struct net_device *bridge)
+{
+	chip->ports[port_to_index(port)].bridge = bridge;
+}
+
+static inline void ksz8863_reg_lock(struct ksz8863_chip *chip)
+{
+	mutex_lock(&chip->reg_lock);
+}
+
+static inline void ksz8863_reg_unlock(struct ksz8863_chip *chip)
+{
+	mutex_unlock(&chip->reg_lock);
+}
+
+static inline void ksz8863_assert_reg_lock(struct ksz8863_chip *chip)
+{
+	if (unlikely(!mutex_is_locked(&chip->reg_lock))) {
+		dev_err(chip->dev, "switch register lock not held!");
+		dump_stack();
+	}
+}
+
+static inline int ksz8863_mii_read(struct ksz8863_chip *chip, int reg, u8 *rv)
+{
+	ksz8863_assert_reg_lock(chip);
+	return chip->mii_ops->read(chip, reg, rv);
+}
+
+static inline int ksz8863_mii_write(struct ksz8863_chip *chip, int reg, u8 val)
+{
+	ksz8863_assert_reg_lock(chip);
+	return chip->mii_ops->write(chip, reg, val);
+}
+
+static inline int ksz8863_set_bits_unlocked(struct ksz8863_chip *chip, int reg,
+					    u8 reset, u8 set)
+{
+	u8 rv, orig;
+	int err = ksz8863_mii_read(chip, reg, &rv);
+
+	orig = rv;
+	rv &= ~reset;
+	rv |= set;
+
+	if (!err && rv != orig)
+		err = ksz8863_mii_write(chip, reg, rv);
+
+	return err;
+}
+
+static inline int ksz8863_set_bits(struct ksz8863_chip *chip, int reg, u8 reset,
+				   u8 set)
+{
+	int err;
+
+	ksz8863_reg_lock(chip);
+
+	err = ksz8863_set_bits_unlocked(chip, reg, reset, set);
+
+	ksz8863_reg_unlock(chip);
+
+	return err;
+}
+
+static int ksz8863_set_eth_addr(struct ksz8863_chip *chip)
+{
+	int err;
+	int i;
+	struct ksz8863_static_mac_tbl_entry entry;
+
+	if (!is_valid_ether_addr(chip->eth_addr))
+		return 0;
+
+	memset(&entry, 0, sizeof(entry));
+
+	for (i = 0; i < ETH_ALEN; ++i)
+		entry.mac[i] = chip->eth_addr[ETH_ALEN - 1 - i];
+
+	entry.forward_ports = KSZ8863_SMAC_ENTRY_FWD_PORTS_PORT3;
+	entry.valid = 1;
+	entry.override = 1;
+	entry.use_fid = 0;
+	entry.fid = 0;
+
+	err = chip->mii_ops->write_table(chip, KSZ8863_TBL_STATIC_MAC, 0,
+					 (u8 *)&entry, sizeof(entry));
+	return err;
+}
+
+static int ksz8863_setup_global(struct ksz8863_chip *chip)
+{
+	int err;
+
+	ksz8863_reg_lock(chip);
+
+	/* Enable tail tagging */
+	err = ksz8863_set_bits_unlocked(chip, KSZ8863_REG_GL_CTRL1, 0,
+					KSZ8863_REG_GL_CTRL1_TAIL_TAG_ENABLE);
+	if (err)
+		goto out_unlock;
+
+	/* Turn on IGMP Snooping */
+	err = ksz8863_set_bits_unlocked(chip, KSZ8863_REG_GL_CTRL3, 0,
+					KSZ8863_REG_GL_CTRL3_IGMP_SNOOP);
+	if (err)
+		goto out_unlock;
+
+	/* Set broadcast and unknown MAC address forwarding */
+	err = ksz8863_set_bits_unlocked(
+		chip, KSZ8863_REG_GL_CTRL12, 0,
+		KSZ8863_REG_GL_CTRL12_PORT_MASK |
+			KSZ8863_REG_GL_CTRL12_UNKNOWN_DA_ENABLE);
+	if (err)
+		goto out_unlock;
+
+out_unlock:
+	ksz8863_reg_unlock(chip);
+	return err;
+}
+
+static int ksz8863_setup_port(struct ksz8863_chip *chip, int port)
+{
+	int err;
+	int pb = KSZ8863_REG_PORTS_BASE * port_to_index(port);
+
+	if (!dsa_is_cpu_port(chip->ds, port)) {
+		struct phy_device *phydev;
+
+		/* Scan the mdiobus for the internal phys.
+		 * This is needed because libphy may scan the
+		 * bus when the switch is still in reset.
+		 */
+		phydev = mdiobus_scan(chip->sw_bus, port);
+		if (!phydev || (IS_ERR(phydev))) {
+			pr_err("failed to probe phydev via mdio at port %i\n",
+			       port);
+			return PTR_ERR(phydev);
+		}
+
+		/* Enable broadcast storm protection for all ports
+		 * except cpu port.
+		 */
+		err = ksz8863_set_bits(chip, pb + KSZ8863_REG_PORT1_CTRL0, 0,
+				       KSZ8863_REG_PORT_CTRL0_BROADCAST_STORM);
+		if (err)
+			return err;
+	} else {
+		/* set MAC mode for cpu port */
+		err = ksz8863_set_bits(chip, pb + KSZ8863_REG_PORT1_CTRL5, 0,
+				       KSZ8863_REG_PORT_CTRL5_3_MII_MAC_MODE);
+		if (err)
+			return err;
+	}
+
+	return 0;
+}
+
+static int ksz8863_setup_adv_ctrl(struct ksz8863_chip *chip)
+{
+	/* LEDs - yellow led -> link, green led -> act */
+	int err = ksz8863_set_bits(
+		chip, KSZ8863_REG_PWR_LED,
+		KSZ8863_REG_PWR_LED_LED_MODE_SEL_MASK,
+		KSZ8863_REG_PWR_LED_LED_MODE_1
+			<< KSZ8863_REG_PWR_LED_LED_MODE_SEL_SHIFT);
+
+	return err ? err : ksz8863_set_eth_addr(chip);
+}
+
+static int ksz8863_setup(struct dsa_switch *ds)
+{
+	struct ksz8863_chip *chip = ds->priv;
+	int port;
+	int err;
+
+	ds->slave_mii_bus = chip->sw_bus;
+
+	err = ksz8863_setup_global(chip);
+	if (err)
+		return err;
+
+	for (port = KSZ8863_EXT_PORT1_ID;
+	     port < KSZ8863_NUM_PORTS + KSZ8863_EXT_PORT1_ID; port++) {
+		err = ksz8863_setup_port(chip, port);
+		if (err)
+			return err;
+	}
+
+	err = ksz8863_setup_adv_ctrl(chip);
+	return err;
+}
+
+static int ksz8863_phy_read(struct dsa_switch *ds, int port, int reg)
+{
+	struct ksz8863_chip *chip = ds->priv;
+
+	return mdiobus_read_nested(chip->sw_bus, port, reg);
+}
+
+static int ksz8863_phy_write(struct dsa_switch *ds, int port, int reg, u16 val)
+{
+	struct ksz8863_chip *chip = ds->priv;
+
+	return mdiobus_write_nested(chip->sw_bus, port, reg, val);
+}
+
+static int ksz8863_flush_dyn_mac_tables(struct ksz8863_chip *chip)
+{
+	return ksz8863_set_bits_unlocked(
+		chip, KSZ8863_REG_GL_CTRL0, 0,
+		KSZ8863_REG_GL_CTRL0_FLUSH_DYN_MAC_TABLE);
+}
+
+static int ksz8863_flush_mac_tables(struct ksz8863_chip *chip)
+{
+	int err;
+	int port_index;
+	u8 states[KSZ8863_NUM_PORTS];
+
+	ksz8863_reg_lock(chip);
+
+	/* backup port states */
+	for (port_index = 0; port_index < KSZ8863_NUM_PORTS; port_index++) {
+		int pb = KSZ8863_REG_PORTS_BASE * port_index;
+
+		err = ksz8863_mii_read(chip, pb + KSZ8863_REG_PORT1_CTRL2,
+				       &states[port_index]);
+		if (err)
+			goto out_unlock;
+	}
+
+	/* disable learning and disable receiving while flushing dynamic mac
+	 * table
+	 */
+	for (port_index = 0; port_index < KSZ8863_NUM_PORTS; port_index++) {
+		int pb = KSZ8863_REG_PORTS_BASE * port_index;
+
+		err = ksz8863_mii_write(chip, pb + KSZ8863_REG_PORT1_CTRL2,
+					KSZ8863_REG_PORT_CTRL2_LEARN_DISABLE);
+		if (err)
+			goto out_unlock;
+	}
+
+	/* flush dynamic mac table */
+	err = ksz8863_flush_dyn_mac_tables(chip);
+	if (err)
+		goto out_unlock;
+
+	/* restore port states */
+	for (port_index = 0; port_index < KSZ8863_NUM_PORTS; port_index++) {
+		int pb = KSZ8863_REG_PORTS_BASE * port_index;
+
+		err = ksz8863_mii_write(chip, pb + KSZ8863_REG_PORT1_CTRL2,
+					states[port_index]);
+		if (err)
+			goto out_unlock;
+	}
+
+out_unlock:
+	ksz8863_reg_unlock(chip);
+
+	return err;
+}
+
+static void ksz8863_adjust_link(struct dsa_switch *ds, int port,
+				struct phy_device *phydev)
+{
+	struct ksz8863_chip *chip = ds->priv;
+	int pb = KSZ8863_REG_PORTS_BASE * port_to_index(port);
+
+	phy_print_status(phydev);
+
+	if (ksz8863_flush_mac_tables(chip))
+		return;
+
+	if (phydev->state == PHY_HALTED || phydev->state == PHY_DOWN)
+		ksz8863_set_bits(chip, pb + KSZ8863_REG_PORT1_CTRL13, 0,
+				 KSZ8863_REG_PORT_CTRL13_POWER_DOWN);
+	else
+		ksz8863_set_bits(chip, pb + KSZ8863_REG_PORT1_CTRL13,
+				 KSZ8863_REG_PORT_CTRL13_POWER_DOWN, 0);
+}
+
+static int ksz8863_set_port_vlan_membership(struct ksz8863_chip *chip,
+					    int port1, int port2)
+{
+	int err;
+
+	err = ksz8863_set_bits_unlocked(chip, KSZ8863_REG_PORT1_CTRL1,
+					KSZ8863_REG_PORT_CTRL1_VLAN_MEMBERSHIP,
+					port1);
+
+	if (err)
+		goto out_err;
+
+	err = ksz8863_set_bits_unlocked(chip, KSZ8863_REG_PORT2_CTRL1,
+					KSZ8863_REG_PORT_CTRL1_VLAN_MEMBERSHIP,
+					port2);
+
+out_err:
+	return err;
+}
+
+static int ksz8863_set_switched_mode(struct ksz8863_chip *chip)
+{
+	const int all_ports = KSZ8863_PORT_CTL1_VLAN_MEMBERSHIP_PORT1 |
+			      KSZ8863_PORT_CTL1_VLAN_MEMBERSHIP_PORT2 |
+			      KSZ8863_PORT_CTL1_VLAN_MEMBERSHIP_PORT3;
+
+	int err = ksz8863_set_port_vlan_membership(chip, all_ports, all_ports);
+
+	if (!err) {
+		dev_info(chip->dev, "ksz8863 in switched mode\n");
+		chip->switched = true;
+	}
+
+	return err;
+}
+
+static int ksz8863_set_separated_mode(struct ksz8863_chip *chip)
+{
+	const int port1_members = KSZ8863_PORT_CTL1_VLAN_MEMBERSHIP_PORT1 |
+				  KSZ8863_PORT_CTL1_VLAN_MEMBERSHIP_PORT3;
+	const int port2_members = KSZ8863_PORT_CTL1_VLAN_MEMBERSHIP_PORT2 |
+				  KSZ8863_PORT_CTL1_VLAN_MEMBERSHIP_PORT3;
+
+	int err = ksz8863_set_port_vlan_membership(chip, port1_members,
+						   port2_members);
+
+	if (!err) {
+		dev_info(chip->dev, "ksz8863 in separated mode\n");
+		chip->switched = false;
+	}
+
+	return err;
+}
+
+static int ksz8863_set_bridged_mode(struct ksz8863_chip *chip)
+{
+	int err;
+	int ret = 0;
+
+	ksz8863_reg_lock(chip);
+
+	if (ksz8863_get_bridge_of_port(chip, KSZ8863_EXT_PORT1_ID) ==
+	    ksz8863_get_bridge_of_port(chip, KSZ8863_EXT_PORT2_ID)) {
+		if (!chip->switched)
+			ret = ksz8863_set_switched_mode(chip);
+	} else {
+		if (chip->switched)
+			ret = ksz8863_set_separated_mode(chip);
+	}
+
+	if (ret) {
+		dev_err(chip->dev, "failed to set bridge mode\n");
+			goto out_unlock;
+	}
+
+	err = ksz8863_flush_dyn_mac_tables(chip);
+	if (err)
+		dev_err(chip->dev,
+			"failed to flush fdb after changing bridge mode\n");
+
+out_unlock:
+	ksz8863_reg_unlock(chip);
+
+	return err;
+}
+
+static int ksz8863_port_bridge_join(struct dsa_switch *ds, int port,
+				    struct net_device *bridge)
+{
+	struct ksz8863_chip *chip = ds->priv;
+
+	if (port >= (KSZ8863_NUM_PORTS + KSZ8863_EXT_PORT1_ID) ||
+	    dsa_is_cpu_port(ds, port))
+		return -EINVAL;
+
+	if (ksz8863_get_bridge_of_port(chip, port) &&
+	    ksz8863_get_bridge_of_port(chip, port) != bridge)
+		return -EBUSY;
+
+	ksz8863_set_bridge_of_port(chip, port, bridge);
+
+	return ksz8863_set_bridged_mode(chip);
+}
+
+static void ksz8863_port_bridge_leave(struct dsa_switch *ds, int port,
+				      struct net_device *bridge)
+{
+	struct ksz8863_chip *chip = ds->priv;
+
+	if (port >= (KSZ8863_NUM_PORTS + KSZ8863_EXT_PORT1_ID) ||
+	    dsa_is_cpu_port(ds, port))
+		return;
+
+	ksz8863_set_bridge_of_port(chip, port, NULL);
+
+	ksz8863_set_bridged_mode(chip);
+}
+
+static struct dsa_switch_ops ksz8863_switch_ops = {
+	.get_tag_protocol = ksz8863_get_tag_protocol,
+	.setup = ksz8863_setup,
+	.phy_read = ksz8863_phy_read,
+	.phy_write = ksz8863_phy_write,
+	.adjust_link = ksz8863_adjust_link,
+	.port_bridge_join = ksz8863_port_bridge_join,
+	.port_bridge_leave = ksz8863_port_bridge_leave,
+};
+
+static int ksz8863_smi_read(struct ksz8863_chip *chip, int reg, u8 *val)
+{
+	int ret;
+
+	if (!chip->sw_bus)
+		return -ENODEV;
+
+	ksz8863_assert_reg_lock(chip);
+
+	ret = mdiobus_read_nested(chip->sw_bus, chip->sw_addr,
+				  reg | MII_ADDR_KSZ);
+	if (ret < 0 || ret > 0xFF) {
+		/* ksz8863 never return values over 0xFF in smi mode */
+		if (ret > 0xFF)
+			ret = -ENODATA;
+
+		return ret;
+	}
+
+	*val = ret & 0xff;
+
+	return 0;
+}
+
+static int ksz8863_smi_write(struct ksz8863_chip *chip, int reg, u8 val)
+{
+	int ret;
+
+	if (!chip->sw_bus)
+		return -ENODEV;
+
+	ksz8863_assert_reg_lock(chip);
+
+	ret = mdiobus_write_nested(chip->sw_bus, chip->sw_addr,
+				   reg | MII_ADDR_KSZ, val);
+	return ret;
+}
+
+static int ksz8863_tbl_write(struct ksz8863_chip *chip,
+			     enum ksz8863_tables table, u16 addr, u8 *buffer,
+			     size_t size)
+{
+	int err, i;
+	u8 iac0v, iac1v;
+
+	/* check if address has only 9 bits width */
+	if (addr & ~(BIT(10) - 1))
+		return -EINVAL;
+
+	if (!size || size > KSZ8863_REG_INDIRECT_DATA_MAX_SIZE)
+		return -EINVAL;
+
+	ksz8863_reg_lock(chip);
+
+	for (i = 0; i < size; i++) {
+		int reg = KSZ8863_REG_INDIRECT_DATA_REG0 - i;
+		u8 mask = (i != KSZ8863_REG_INDIRECT_DATA_REG8) ?
+				  0xFF :
+				  KSZ8863_REG_INDIRECT_DATA_REG8_DATA_MASK;
+
+		err = ksz8863_smi_write(chip, reg, buffer[i] & mask);
+		if (err)
+			goto out_unlock;
+	}
+
+	iac0v = KSZ8863_REG_INDIRECT_ACCESS_CTRL0_WRITE;
+	iac0v |= table << KSZ8863_REG_INDIRECT_ACCESS_CTRL0_TBL_SHIFT;
+	iac0v |= (addr >> 8) & KSZ8863_REG_INDIRECT_ACCESS_CTRL0_ADDR_MASK;
+	iac1v = addr & 0xFF;
+
+	err = ksz8863_smi_write(chip, KSZ8863_REG_INDIRECT_ACCESS_CTRL0, iac0v);
+	if (err)
+		goto out_unlock;
+
+	err = ksz8863_smi_write(chip, KSZ8863_REG_INDIRECT_ACCESS_CTRL1, iac1v);
+
+out_unlock:
+	ksz8863_reg_unlock(chip);
+
+	return err;
+}
+
+static int ksz8863_tbl_wait_read_ready(struct ksz8863_chip *chip)
+{
+	int err;
+	unsigned long time = jiffies + HZ;
+	u8 val;
+
+	do {
+		err = ksz8863_smi_read(chip, KSZ8863_REG_INDIRECT_DATA_REG8,
+				       &val);
+		if (err)
+			return err;
+
+		if (!(val & KSZ8863_REG_INDIRECT_DATA_REG8_CPU_READ_WIP))
+			return 0;
+
+		usleep_range(10000, 20000);
+	} while (time_after(time, jiffies));
+
+	return -ETIMEDOUT;
+}
+
+static int ksz8863_tbl_read(struct ksz8863_chip *chip,
+			    enum ksz8863_tables table, u16 addr, u8 *buffer,
+			    size_t size)
+{
+	int err, i;
+	u8 iac0v, iac1v;
+
+	/* check if address has only 9 bits width */
+	if (addr & ~(BIT(10) - 1))
+		return -EINVAL;
+
+	if (!size || size > KSZ8863_REG_INDIRECT_DATA_MAX_SIZE)
+		return -EINVAL;
+
+	ksz8863_reg_lock(chip);
+
+	iac0v = KSZ8863_REG_INDIRECT_ACCESS_CTRL0_READ;
+	iac0v |= table << KSZ8863_REG_INDIRECT_ACCESS_CTRL0_TBL_SHIFT;
+	iac0v |= (addr >> 8) & KSZ8863_REG_INDIRECT_ACCESS_CTRL0_ADDR_MASK;
+	iac1v = addr & 0xFF;
+
+	err = ksz8863_smi_write(chip, KSZ8863_REG_INDIRECT_ACCESS_CTRL0, iac0v);
+	if (err)
+		goto out_unlock;
+
+	err = ksz8863_smi_write(chip, KSZ8863_REG_INDIRECT_ACCESS_CTRL1, iac1v);
+	if (err)
+		goto out_unlock;
+
+	err = ksz8863_tbl_wait_read_ready(chip);
+	if (err)
+		goto out_unlock;
+
+	for (i = 0; i < size; i++) {
+		int reg = KSZ8863_REG_INDIRECT_DATA_REG0 - i;
+
+		err = ksz8863_smi_read(chip, reg, &buffer[i]);
+		if (err)
+			goto out_unlock;
+
+		if (reg == KSZ8863_REG_INDIRECT_DATA_REG8)
+			buffer[i] &= KSZ8863_REG_INDIRECT_DATA_REG8_DATA_MASK;
+	}
+
+out_unlock:
+	ksz8863_reg_unlock(chip);
+
+	return err;
+}
+
+static int of_get_ksz8863_data(struct ksz8863_chip *chip)
+{
+	struct device *dev = chip->dev;
+	struct device_node *np = dev->of_node;
+	struct device_node *of_aliases, *of_ethernet;
+	const char *eth0_prop_name, *mac_addr;
+
+	if (!np)
+		return 0;
+
+	if (of_find_property(np, "ksz,disable-internal-ldo", NULL))
+		chip->disable_internal_ldo = true;
+
+	chip->reset_gpio =
+		devm_gpiod_get_optional(dev, "ksz,reset", GPIOD_OUT_LOW);
+	if (IS_ERR(chip->reset_gpio)) {
+		if (PTR_ERR(chip->reset_gpio) == -EPROBE_DEFER)
+			return -EPROBE_DEFER;
+
+		dev_warn(dev, "No gpio reset pin given\n");
+	} else if (of_find_property(np, "ksz,reset-switch", NULL)) {
+		chip->reset = true;
+	}
+
+	/* get device mac address from device-tree */
+	of_aliases = of_find_node_by_path("/aliases");
+	if (!of_aliases) {
+		dev_err(dev, "No aliases property found");
+		return -EINVAL;
+	}
+
+	if (of_property_read_string(of_aliases, "ethernet0", &eth0_prop_name)) {
+		dev_err(dev, "No ethernet0 alias found");
+		return -EINVAL;
+	}
+
+	of_ethernet = of_find_node_by_path(eth0_prop_name);
+	if (!of_ethernet) {
+		dev_err(dev, "No ethernet0 = \"%s\" property found",
+			eth0_prop_name);
+		return -EINVAL;
+	}
+
+	mac_addr = of_get_mac_address(of_ethernet);
+	if (IS_ERR_OR_NULL(mac_addr)) {
+		dev_warn(dev, "No valid ethernet address found: %ld",
+			 PTR_ERR(mac_addr));
+		return 0;
+	}
+
+	ether_addr_copy(chip->eth_addr, mac_addr);
+	return 0;
+}
+
+static struct ksz8863_chip *ksz8863_alloc_chip(struct device *dev)
+{
+	struct ksz8863_chip *chip;
+
+	chip = devm_kzalloc(dev, sizeof(*chip), GFP_KERNEL);
+	if (!chip)
+		return NULL;
+
+	chip->dev = dev;
+
+	/* switch starts in switched mode */
+	chip->switched = true;
+
+	mutex_init(&chip->reg_lock);
+
+	return chip;
+}
+
+static void ksz8863_free_chip(struct ksz8863_chip *chip)
+{
+	struct device *dev = chip->dev;
+
+	devm_kfree(dev, chip);
+}
+
+static const struct ksz8863_bus_ops ksz8863_smi_ops = {
+	.read = ksz8863_smi_read,
+	.write = ksz8863_smi_write,
+	.read_table = ksz8863_tbl_read,
+	.write_table = ksz8863_tbl_write,
+};
+
+static void ksz8863_smi_init(struct ksz8863_chip *chip, struct mii_bus *bus,
+			     int sw_addr)
+{
+	chip->sw_bus = bus;
+	chip->sw_addr = sw_addr;
+
+	chip->mii_ops = &ksz8863_smi_ops;
+}
+
+static int ksz8863_hw_reset(struct ksz8863_chip *chip)
+{
+	if (!chip->reset)
+		return 0;
+
+	gpiod_set_value_cansleep(chip->reset_gpio, 1);
+	usleep_range(10000, 20000);
+	gpiod_set_value_cansleep(chip->reset_gpio, 0);
+
+	/* it recommended to wait after reset minimum 100µs,
+	 * but we wait ten times more to be sure.
+	 */
+	usleep_range(1000, 2000);
+
+	dev_info(chip->dev, "ksz8863 switch reset\n");
+
+	chip->reset = false;
+
+	return 0;
+}
+
+static int ksz8863_detect(struct ksz8863_chip *chip)
+{
+	u8 val;
+	u8 chip_family;
+	u8 chip_id;
+	u8 chip_rev;
+	int err;
+
+	ksz8863_reg_lock(chip);
+
+	err = ksz8863_smi_read(chip, KSZ8863_REG_CHIP_ID0, &val);
+	if (err)
+		goto out_unlock;
+
+	chip_family = val;
+	if (chip_family != KSZ8863_REG_CHIP_ID0_FAMILY_ID) {
+		dev_err(chip->dev, "could not detect ksz8863 switch\n");
+		err = -ENODEV;
+		goto out_unlock;
+	}
+
+	err = ksz8863_smi_read(chip, KSZ8863_REG_CHIP_ID1, &val);
+	if (err)
+		goto out_unlock;
+
+	chip_id = val & KSZ8863_REG_CHIP_ID1_ID_MASK;
+	chip_id >>= KSZ8863_REG_CHIP_ID1_ID_SHIFT;
+
+	chip_rev = val & KSZ8863_REG_CHIP_ID1_REV_ID_MASK;
+	chip_rev >>= KSZ8863_REG_CHIP_ID1_REV_ID_SHIFT;
+
+	if (chip_id != KSZ8863_REG_CHIP_ID1_ID) {
+		dev_err(chip->dev, "could not detect ksz8863 switch\n");
+		err = -ENODEV;
+		goto out_unlock;
+	}
+
+	dev_info(chip->dev,
+		 "detected family id 0x%x, chip id 0x%x, revision id 0x%x\n",
+		 chip_family, chip_id, chip_rev);
+
+out_unlock:
+	ksz8863_reg_unlock(chip);
+
+	return err;
+}
+
+static int ksz8863_set_internal_ldo(struct ksz8863_chip *chip)
+{
+	int err;
+
+	if (!chip->disable_internal_ldo)
+		return 0;
+
+	err = ksz8863_set_bits(chip, KSZ8863_REG_INTERNAL_1V8_LDO_CTRL, 0,
+			       KSZ8863_REG_INTERNAL_1V8_LDO_CTRL_DISABLE);
+	if (err)
+		return err;
+
+	dev_info(chip->dev, "1.8V LDO disabled\n");
+
+	return 0;
+}
+
+static int ksz8863_register_switch(struct ksz8863_chip *chip)
+{
+	int err;
+	struct device *dev = chip->dev;
+	struct dsa_switch *ds;
+
+	ds = devm_kzalloc(dev, sizeof(*ds), GFP_KERNEL);
+	if (!ds)
+		return -ENOMEM;
+
+	// the ksz8863 switch address the ports starting with 1
+	// whereas the dsa framework expects the a start at 0.
+	ds->num_ports = KSZ8863_NUM_PORTS + KSZ8863_PORT_OFFSET;
+
+	ds->dev = dev;
+	ds->priv = chip;
+	ds->ops = &ksz8863_switch_ops;
+
+	dev_set_drvdata(dev, ds);
+
+	chip->ds = ds;
+
+	err = dsa_register_switch(ds);
+	if (err) {
+		dev_err(chip->dev, "register ksz8863 on dsa failed\n");
+		devm_kfree(chip->dev, ds);
+		chip->ds = NULL;
+		dev_set_drvdata(dev, NULL);
+		return err;
+	}
+
+	return 0;
+}
+
+static void ksz8863_unregister_switch(struct ksz8863_chip *chip)
+{
+	dsa_unregister_switch(chip->ds);
+
+	devm_kfree(chip->dev, chip->ds);
+	chip->ds = NULL;
+}
+
+static int ksz8863_probe(struct mdio_device *mdiodev)
+{
+	struct device *dev = &mdiodev->dev;
+	struct ksz8863_chip *chip;
+	int err;
+
+	chip = ksz8863_alloc_chip(dev);
+	if (!chip)
+		return -ENOMEM;
+
+	chip->ops.flush_dyn_mac_table = &ksz8863_flush_dyn_mac_tables;
+
+	err = of_get_ksz8863_data(chip);
+	if (err)
+		goto err_free;
+
+	ksz8863_smi_init(chip, mdiodev->bus, mdiodev->addr);
+
+	err = ksz8863_hw_reset(chip);
+	if (err)
+		goto err_free;
+
+	err = ksz8863_detect(chip);
+	if (err)
+		goto err_free;
+
+	err = ksz8863_set_internal_ldo(chip);
+	if (err)
+		goto err_free;
+
+	err = ksz8863_register_switch(chip);
+	if (err)
+		goto err_free;
+
+	if (ksz8863_debugfs_setup(chip))
+		dev_warn(chip->dev, "failed to setup debugfs\n");
+
+	dev_info(chip->dev, "ksz8863 switch probed successful\n");
+
+	return 0;
+
+err_free:
+	dev_err(chip->dev, "ksz8863 probe failed (%d)\n", err);
+	ksz8863_free_chip(chip);
+
+	return err;
+}
+
+static void ksz8863_remove(struct mdio_device *mdiodev)
+{
+	struct dsa_switch *ds = dev_get_drvdata(&mdiodev->dev);
+	struct ksz8863_chip *chip = ds->priv;
+
+	ksz8863_unregister_switch(chip);
+	ksz8863_debugfs_destroy(chip);
+	ksz8863_free_chip(chip);
+}
+
+// clang-format off
+static const struct of_device_id ksz8863_of_match[] = {
+	{
+		.compatible = "micrel,ksz8863",
+	},
+	{ /* sentinel */ },
+};
+
+MODULE_DEVICE_TABLE(of, ksz8863_of_match);
+
+static struct mdio_driver ksz8863_driver = {
+	.probe	= ksz8863_probe,
+	.remove = ksz8863_remove,
+	.mdiodrv.driver = {
+		.name = "ksz8863",
+		.of_match_table = ksz8863_of_match,
+	},
+};
+
+// clang-format on
+
+mdio_module_driver(ksz8863_driver);
+
+MODULE_AUTHOR("Andreas Schmidt <andreas.schmidt@wago.com>");
+MODULE_DESCRIPTION("Driver for Micrel KSZ886X ethernet switch chips");
+MODULE_LICENSE("GPL");
diff --git a/drivers/net/dsa/mv88e6xxx/Makefile b/drivers/net/dsa/mv88e6xxx/Makefile
index 4b080b448ce7..b067d59b7e09 100644
--- a/drivers/net/dsa/mv88e6xxx/Makefile
+++ b/drivers/net/dsa/mv88e6xxx/Makefile
@@ -1,10 +1,10 @@
-# SPDX-License-Identifier: GPL-2.0
-obj-$(CONFIG_NET_DSA_MV88E6XXX) += mv88e6xxx.o
+obj-$(CONFIG_NET_DSA_MV88E6XXX) += mv88e6xxx.o mv88e6321_tcam.o
 mv88e6xxx-objs := chip.o
 mv88e6xxx-objs += devlink.o
 mv88e6xxx-objs += global1.o
 mv88e6xxx-objs += global1_atu.o
 mv88e6xxx-objs += global1_vtu.o
+mv88e6xxx-objs += debugfs.o
 mv88e6xxx-$(CONFIG_NET_DSA_MV88E6XXX_GLOBAL2) += global2.o
 mv88e6xxx-$(CONFIG_NET_DSA_MV88E6XXX_GLOBAL2) += global2_avb.o
 mv88e6xxx-$(CONFIG_NET_DSA_MV88E6XXX_GLOBAL2) += global2_scratch.o
diff --git a/drivers/net/dsa/mv88e6xxx/chip.c b/drivers/net/dsa/mv88e6xxx/chip.c
index 87160e723dfc..b48a2ad903c4 100644
--- a/drivers/net/dsa/mv88e6xxx/chip.c
+++ b/drivers/net/dsa/mv88e6xxx/chip.c
@@ -41,6 +41,8 @@
 #include "ptp.h"
 #include "serdes.h"
 #include "smi.h"
+#include "debugfs.h"
+#include "mv88e6321_tcam.h"
 
 static void assert_reg_lock(struct mv88e6xxx_chip *chip)
 {
@@ -2301,6 +2303,8 @@ static void mv88e6xxx_hardware_reset(struct mv88e6xxx_chip *chip)
 		usleep_range(10000, 20000);
 		gpiod_set_value_cansleep(gpiod, 0);
 		usleep_range(10000, 20000);
+		usleep_range(300000, 400000);
+
 
 		mv88e6xxx_g1_wait_eeprom_done(chip);
 	}
@@ -2410,14 +2414,9 @@ static int mv88e6xxx_setup_message_port(struct mv88e6xxx_chip *chip, int port)
 
 static int mv88e6xxx_setup_egress_floods(struct mv88e6xxx_chip *chip, int port)
 {
-	struct dsa_switch *ds = chip->ds;
-	bool flood;
-
-	/* Upstream ports flood frames with unknown unicast or multicast DA */
-	flood = dsa_is_cpu_port(ds, port) || dsa_is_dsa_port(ds, port);
 	if (chip->info->ops->port_set_egress_floods)
 		return chip->info->ops->port_set_egress_floods(chip, port,
-							       flood, flood);
+							       true, true);
 
 	return 0;
 }
@@ -2558,6 +2557,231 @@ static int mv88e6xxx_setup_upstream_port(struct mv88e6xxx_chip *chip, int port)
 	return 0;
 }
 
+static bool mv88e6xxx_is_external_phy(struct dsa_switch *ds, int port)
+{
+	struct dsa_port *p = dsa_to_port(ds, port);
+	struct device_node *np = p->dn;
+
+	if (!np)
+		return false;
+
+	return of_property_read_bool(np, "phy-external");
+}
+
+static int mv88e6xxx_vtu_setup_pnio_vlan(struct mv88e6xxx_chip *chip)
+{
+	int pn_ports[] = {3, 4, 6};
+	int err;
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(pn_ports); i++) {
+		err = mv88e6xxx_port_vlan_join(chip, pn_ports[i], 1338,
+				MV88E6XXX_G1_VTU_DATA_MEMBER_TAG_UNMODIFIED,
+				false);
+		if (err)
+			return err;
+	}
+
+	return 0;
+}
+
+static void mv88e6xxx_tcam_cleanup(struct mv88e6xxx_chip *chip)
+{
+	struct mv88e6xxx_tcam_info *tcam_info = NULL;
+	struct mv88e6xxx_tcam_info *tcam_info_tmp = NULL;
+
+	mv88e6321_flush_tcam(chip->bus, -1);
+
+	list_for_each_entry_safe(tcam_info, tcam_info_tmp,
+				 &chip->tcam.info_head.list, list) {
+		device_remove_file(chip->dev, &tcam_info->dev_attr);
+		list_del(&tcam_info->list);
+		devm_kfree(chip->dev, tcam_info);
+	}
+}
+
+static struct mv88e6xxx_tcam_info*
+mv88e6xxx_get_tcam_info_by_title(struct mv88e6xxx_chip *chip, const char *title)
+{
+	struct mv88e6xxx_tcam_info *tcam_info;
+
+	list_for_each_entry(tcam_info, &chip->tcam.info_head.list, list) {
+		if (!strcmp(tcam_info->title, title))
+			return tcam_info;
+	}
+
+	return NULL;
+}
+
+static ssize_t mv88e6xxx_store_tcam_entry_enable(struct device *dev,
+						 struct device_attribute *attr,
+						 const char *buf, size_t count)
+{
+	ssize_t ret;
+	struct dsa_switch *ds = dev_get_drvdata(dev);
+	struct mv88e6xxx_chip *chip = ds->priv;
+	struct mii_bus *bus = chip->bus;
+	struct mv88e6xxx_tcam_info *tcam_info;
+
+	if (!count)
+		return -EINVAL;
+
+	tcam_info = mv88e6xxx_get_tcam_info_by_title(chip, attr->attr.name);
+	if (!tcam_info)
+		return -EEXIST;
+
+	if (*buf == '0' || *buf == 0)
+		ret = mv88e6321_disable_tcam(bus, tcam_info->id);
+	else
+		ret = mv88e6321_enable_tcam(bus, tcam_info->id,
+					    tcam_info->reg_frame_type);
+
+	return ret ? : count;
+}
+
+static ssize_t mv88e6xxx_show_tcam_entry_enable(struct device *dev,
+						struct device_attribute *attr,
+						char *buf)
+{
+	struct dsa_switch *ds = dev_get_drvdata(dev);
+	struct mv88e6xxx_chip *chip = ds->priv;
+	struct mii_bus *bus = chip->bus;
+	struct mv88e6xxx_tcam_info *tcam_info;
+
+	tcam_info = mv88e6xxx_get_tcam_info_by_title(chip, attr->attr.name);
+	if (!tcam_info)
+		return -EEXIST;
+
+	if (mv88e6321_is_tcam_enabled(bus, tcam_info->id))
+		snprintf(buf, PAGE_SIZE, "%d", 1);
+	else
+		snprintf(buf, PAGE_SIZE, "%d", 0);
+
+	return 1;
+}
+
+static int mv88e6xxx_add_device_file(struct device *dev,
+				     struct mv88e6xxx_tcam_info *tcam_info)
+{
+	int err = 0;
+
+	tcam_info->dev_attr.attr.name = tcam_info->title;
+	tcam_info->dev_attr.attr.mode = 0644;
+
+	tcam_info->dev_attr.show = mv88e6xxx_show_tcam_entry_enable;
+	tcam_info->dev_attr.store = mv88e6xxx_store_tcam_entry_enable;
+
+	err = device_create_file(dev, &tcam_info->dev_attr);
+
+	return err;
+}
+
+static int mv88e6xxx_set_tcam_entry(struct mv88e6xxx_chip *chip,
+				    struct tcam_entry *tcam_entry)
+{
+	int err = 0;
+	u16 reg_frame_type = tcam_entry->reg_frame_type;
+	struct mv88e6xxx_tcam_info *tcam_info;
+
+	/* deactivate TCAM entry by default */
+	tcam_entry->reg_frame_type = 0x00FF;
+
+	err = mv88e6321_load_tcam(chip->bus, tcam_entry);
+	if (err)
+		goto out;
+
+	tcam_info = devm_kzalloc(chip->dev, sizeof(*tcam_info), GFP_KERNEL);
+	if (!tcam_info) {
+		err = -ENOMEM;
+		goto out;
+	}
+
+	tcam_info->title = tcam_entry->title;
+	tcam_info->id = tcam_entry->orig_id;
+	if (tcam_entry->is96frame)
+		tcam_info->next_id = tcam_entry->next_id;
+	else
+		tcam_info->next_id = -1;
+	tcam_info->reg_frame_type = reg_frame_type;
+
+	list_add(&tcam_info->list, &chip->tcam.info_head.list);
+
+	err = mv88e6xxx_add_device_file(chip->dev, tcam_info);
+	if (err)
+		goto out_free_tcam_info;
+
+	return 0;
+
+out_free_tcam_info:
+	list_del(&tcam_info->list);
+	devm_kfree(chip->dev, tcam_info);
+out:
+	return err;
+}
+
+static int mv88e6xxx_tcam_setup(struct mv88e6xxx_chip *chip)
+{
+	int err;
+	int ret;
+	struct tcam_entries tcam_entries = {0};
+	struct tcam_entry *tcam_entry;
+	struct tcam_entry *tcam_entry_tmp;
+	struct device_node *np = of_root;
+
+	if (!np)
+		return -ENODATA;
+
+	INIT_LIST_HEAD(&tcam_entries.head.list);
+
+	ret = of_get_tcam_entry(&tcam_entries, np);
+	if (!ret || ret < 0) {
+		err = ret;
+		goto out;
+	}
+
+	mv88e6xxx_tcam_cleanup(chip);
+
+	err = mv88e6321_set_tcam_mode(chip->bus, &tcam_entries);
+	if (err)
+		goto out_free_tcam_infos;
+
+	list_for_each_entry(tcam_entry, &tcam_entries.head.list, list) {
+		err = mv88e6xxx_set_tcam_entry(chip, tcam_entry);
+		if (err)
+			goto out_free_tcam_infos;
+	}
+
+	goto out_free_tcam_entries;
+
+out_free_tcam_infos:
+	mv88e6xxx_tcam_cleanup(chip);
+out_free_tcam_entries:
+	list_for_each_entry_safe(tcam_entry, tcam_entry_tmp,
+				 &tcam_entries.head.list, list) {
+		list_del(&tcam_entry->list);
+		kfree(tcam_entry);
+	}
+out:
+
+	return err;
+}
+
+static u16 mv88e6xxx_of_get_led_ctrl(struct mv88e6xxx_chip *chip, int port,
+				     u16 def_value)
+{
+	u16 led_ctrl[DSA_MAX_PORTS];
+	struct device_node *np = chip->dev->of_node;
+
+	if (WARN_ON(mv88e6xxx_num_ports(chip) > DSA_MAX_PORTS))
+		return def_value;
+
+	if (!of_property_read_u16_array(np, "led-ctrl", led_ctrl,
+					mv88e6xxx_num_ports(chip)))
+		return led_ctrl[port];
+
+	return def_value;
+}
+
 static int mv88e6xxx_setup_port(struct mv88e6xxx_chip *chip, int port)
 {
 	struct dsa_switch *ds = chip->ds;
@@ -2567,6 +2791,13 @@ static int mv88e6xxx_setup_port(struct mv88e6xxx_chip *chip, int port)
 	chip->ports[port].chip = chip;
 	chip->ports[port].port = port;
 
+	reg = MV88E6XXX_PORT_LED_CTL_UPDATE
+		| mv88e6xxx_of_get_led_ctrl(chip, port,
+		    MV88E6XXX_PORT_LED_CTL_LED0_SEL_10XX_CONT |
+		    MV88E6XXX_PORT_LED_CTL_LED1_SEL_ACT);
+
+	mv88e6xxx_port_write(chip, port, MV88E6XXX_PORT_LED_CTL, reg);
+
 	/* MAC Forcing register: don't force link, speed, duplex or flow control
 	 * state to any particular values on physical ports, but force the CPU
 	 * port and all DSA ports to their maximum bandwidth and full duplex.
@@ -2627,8 +2858,13 @@ static int mv88e6xxx_setup_port(struct mv88e6xxx_chip *chip, int port)
 	if (err)
 		return err;
 
-	err = mv88e6xxx_port_set_8021q_mode(chip, port,
-				MV88E6XXX_PORT_CTL2_8021Q_MODE_DISABLED);
+	if (dsa_is_cpu_port(ds, port))
+		err = mv88e6xxx_port_set_8021q_mode(chip, port,
+					MV88E6XXX_PORT_CTL2_8021Q_MODE_FALLBACK);
+	else
+		err = mv88e6xxx_port_set_8021q_mode(chip, port,
+					MV88E6XXX_PORT_CTL2_8021Q_MODE_DISABLED);
+
 	if (err)
 		return err;
 
@@ -2710,7 +2946,24 @@ static int mv88e6xxx_setup_port(struct mv88e6xxx_chip *chip, int port)
 	/* Default VLAN ID and priority: don't set a default VLAN
 	 * ID, and set the default packet priority to zero.
 	 */
-	return mv88e6xxx_port_write(chip, port, MV88E6XXX_PORT_DEFAULT_VLAN, 0);
+	err = mv88e6xxx_port_write(chip, port, MV88E6XXX_PORT_DEFAULT_VLAN, 0);
+	if (err)
+		return err;
+
+	/* Enable PHY detection for external PHYs */
+	if (mv88e6xxx_is_external_phy(ds, port)) {
+		err = mv88e6xxx_port_read(chip, port, MV88E6XXX_PORT_STS, &reg);
+		if (err)
+			return err;
+
+		reg |= MV88E6XXX_PORT_STS_PHY_DETECT;
+
+		err = mv88e6xxx_port_write(chip, port, MV88E6XXX_PORT_STS, reg);
+		if (err)
+			return err;
+	}
+
+	return 0;
 }
 
 static int mv88e6xxx_get_max_mtu(struct dsa_switch *ds, int port)
@@ -2961,6 +3214,16 @@ static int mv88e6xxx_setup(struct dsa_switch *ds)
 	if (err)
 		goto unlock;
 
+	if (chip->info->tcam_support) {
+		err = mv88e6xxx_tcam_setup(chip);
+		if (err)
+			goto unlock;
+	}
+
+	err = mv88e6xxx_vtu_setup_pnio_vlan(chip);
+	if (err)
+		goto unlock;
+
 unlock:
 	mv88e6xxx_reg_unlock(chip);
 
@@ -4141,6 +4404,8 @@ static const struct mv88e6xxx_ops mv88e6321_ops = {
 	.set_cpu_port = mv88e6095_g1_set_cpu_port,
 	.set_egress_port = mv88e6095_g1_set_egress_port,
 	.watchdog_ops = &mv88e6390_watchdog_ops,
+	.mgmt_rsvd2cpu = mv88e6352_g2_mgmt_rsvd2cpu,
+	.pot_clear = mv88e6xxx_g2_pot_clear,
 	.reset = mv88e6352_g1_reset,
 	.vtu_getnext = mv88e6185_g1_vtu_getnext,
 	.vtu_loadpurge = mv88e6185_g1_vtu_loadpurge,
@@ -4988,6 +5253,7 @@ static const struct mv88e6xxx_info mv88e6xxx_table[] = {
 		.multi_chip = true,
 		.tag_protocol = DSA_TAG_PROTO_EDSA,
 		.ptp_support = true,
+		.tcam_support = true,
 		.ops = &mv88e6321_ops,
 	},
 
@@ -5086,6 +5352,7 @@ static const struct mv88e6xxx_info mv88e6xxx_table[] = {
 		.ptp_support = true,
 		.ops = &mv88e6352_ops,
 	},
+
 	[MV88E6390] = {
 		.prod_num = MV88E6XXX_PORT_SWITCH_ID_PROD_6390,
 		.family = MV88E6XXX_FAMILY_6390,
@@ -5110,6 +5377,7 @@ static const struct mv88e6xxx_info mv88e6xxx_table[] = {
 		.ptp_support = true,
 		.ops = &mv88e6390_ops,
 	},
+
 	[MV88E6390X] = {
 		.prod_num = MV88E6XXX_PORT_SWITCH_ID_PROD_6390X,
 		.family = MV88E6XXX_FAMILY_6390,
@@ -5388,6 +5656,25 @@ static const struct dsa_switch_ops mv88e6xxx_switch_ops = {
 	.devlink_info_get	= mv88e6xxx_devlink_info_get,
 };
 
+static ssize_t mv88e6xxx_g1_atu_flush_store(struct device *dev,
+					    struct device_attribute *attr,
+					    const char *buf, size_t count)
+{
+	int ret;
+	struct dsa_switch *ds = dev_get_drvdata(dev);
+	struct mv88e6xxx_chip *chip = ds->priv;
+
+	if (count > 0 && *buf && *buf != '0') {
+		mutex_lock(&chip->reg_lock);
+		ret = mv88e6xxx_g1_atu_flush(chip, 0, false);
+		mutex_unlock(&chip->reg_lock);
+	}
+
+	return (ret) ? : count;
+}
+
+static DEVICE_ATTR(atu_flush, 0200, NULL, mv88e6xxx_g1_atu_flush_store);
+
 static int mv88e6xxx_register_switch(struct mv88e6xxx_chip *chip)
 {
 	struct device *dev = chip->dev;
@@ -5397,7 +5684,6 @@ static int mv88e6xxx_register_switch(struct mv88e6xxx_chip *chip)
 	if (!ds)
 		return -ENOMEM;
 
-	ds->dev = dev;
 	ds->num_ports = mv88e6xxx_num_ports(chip);
 	ds->priv = chip;
 	ds->dev = dev;
@@ -5496,7 +5782,7 @@ static int mv88e6xxx_probe(struct mdio_device *mdiodev)
 		goto out;
 	}
 	if (chip->reset)
-		usleep_range(1000, 2000);
+		usleep_range(300000, 400000);
 
 	err = mv88e6xxx_detect(chip);
 	if (err)
@@ -5549,10 +5835,32 @@ static int mv88e6xxx_probe(struct mdio_device *mdiodev)
 			goto out_g1_irq;
 	}
 
-	err = mv88e6xxx_g1_atu_prob_irq_setup(chip);
+	err = device_create_file(chip->dev, &dev_attr_atu_flush);
 	if (err)
 		goto out_g2_irq;
 
+	chip->class_dev = NULL;
+
+	if (wsysinit_sysfs_class) {
+		struct device *class_dev;
+
+		class_dev = device_create(wsysinit_sysfs_class, chip->dev,
+					  MKDEV(0, 0), chip, "mv88e6xxx_%d",
+					  MINOR(chip->dev->devt));
+		if (IS_ERR(dev)) {
+			err = PTR_ERR(dev);
+			goto out_g2_irq;
+		}
+
+		chip->class_dev = class_dev;
+	}
+
+	INIT_LIST_HEAD(&chip->tcam.info_head.list);
+
+	err = mv88e6xxx_g1_atu_prob_irq_setup(chip);
+	if (err)
+		goto out_remove_class_device;
+
 	err = mv88e6xxx_g1_vtu_prob_irq_setup(chip);
 	if (err)
 		goto out_g1_atu_prob_irq;
@@ -5565,6 +5873,8 @@ static int mv88e6xxx_probe(struct mdio_device *mdiodev)
 	if (err)
 		goto out_mdio;
 
+	mv88e6xxx_debugfs_setup(chip);
+
 	return 0;
 
 out_mdio:
@@ -5573,6 +5883,9 @@ static int mv88e6xxx_probe(struct mdio_device *mdiodev)
 	mv88e6xxx_g1_vtu_prob_irq_free(chip);
 out_g1_atu_prob_irq:
 	mv88e6xxx_g1_atu_prob_irq_free(chip);
+out_remove_class_device:
+	if (chip->class_dev)
+		device_destroy(wsysinit_sysfs_class, chip->class_dev->devt);
 out_g2_irq:
 	if (chip->info->g2_irqs > 0)
 		mv88e6xxx_g2_irq_free(chip);
@@ -5597,7 +5910,12 @@ static void mv88e6xxx_remove(struct mdio_device *mdiodev)
 		mv88e6xxx_hwtstamp_free(chip);
 		mv88e6xxx_ptp_free(chip);
 	}
+	if (chip->class_dev)
+		device_destroy(wsysinit_sysfs_class, chip->class_dev->devt);
 
+	device_remove_file(chip->dev, &dev_attr_atu_flush);
+	mv88e6xxx_tcam_cleanup(chip);
+	mv88e6xxx_debugfs_destroy(chip);
 	mv88e6xxx_phy_destroy(chip);
 	mv88e6xxx_unregister_switch(chip);
 	mv88e6xxx_mdios_unregister(chip);
diff --git a/drivers/net/dsa/mv88e6xxx/chip.h b/drivers/net/dsa/mv88e6xxx/chip.h
index 81c244fc0419..99fee623f91b 100644
--- a/drivers/net/dsa/mv88e6xxx/chip.h
+++ b/drivers/net/dsa/mv88e6xxx/chip.h
@@ -18,6 +18,8 @@
 #include <linux/timecounter.h>
 #include <net/dsa.h>
 
+extern struct class *wsysinit_sysfs_class;
+
 #define MV88E6XXX_N_FID		4096
 
 /* PVT limits for 4-bit port and 5-bit switch */
@@ -139,6 +141,9 @@ struct mv88e6xxx_info {
 
 	/* Supports PTP */
 	bool ptp_support;
+
+	/* TCAM info */
+	bool tcam_support;
 };
 
 struct mv88e6xxx_atu_entry {
@@ -253,6 +258,19 @@ struct mv88e6xxx_region_priv {
 	enum mv88e6xxx_region_id id;
 };
 
+struct mv88e6xxx_tcam_info {
+	struct list_head list;
+	struct device_attribute dev_attr;
+	const char *title;
+	u8 id;
+	s16 next_id;
+	u16 reg_frame_type;
+};
+
+struct mv88e6xxx_tcam {
+	struct mv88e6xxx_tcam_info info_head;
+};
+
 struct mv88e6xxx_chip {
 	const struct mv88e6xxx_info *info;
 
@@ -350,6 +368,15 @@ struct mv88e6xxx_chip {
 
 	/* devlink regions */
 	struct devlink_region *regions[_MV88E6XXX_REGION_MAX];
+
+	/* tcam infos */
+	struct mv88e6xxx_tcam tcam;
+
+	/* directory within debugfs */
+	struct dentry *debugfs_root_entry;
+
+	/* wago device class */
+	struct device *class_dev;
 };
 
 struct mv88e6xxx_bus_ops {
diff --git a/drivers/net/dsa/mv88e6xxx/debugfs.c b/drivers/net/dsa/mv88e6xxx/debugfs.c
new file mode 100644
index 000000000000..132d10cc0d87
--- /dev/null
+++ b/drivers/net/dsa/mv88e6xxx/debugfs.c
@@ -0,0 +1,334 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/* Marvell 88e6xxx Ethernet switch debugfs support
+ *
+ * Copyright (c) 2018 WAGO Kontakttechnick GmbH
+ */
+
+#include <linux/kernel.h>
+#include <linux/debugfs.h>
+#include <linux/phy.h>
+
+#include "chip.h"
+
+enum {
+	GBE_TEST_MODE_ID,
+	PHY_REG_DUMP_ID,
+	SWITCH_REG_DUMP_ID,
+};
+
+/* GBE ctrl register number */
+#define STD_GCCTRL_REG 0x09
+
+static char const *const text[] = {
+	[GBE_TEST_MODE_ID] = "GBE Compliance Test Mode\n"
+			     "Write 1 to 4 to enter test mode\n"
+			     "Write 0 to disable\n",
+};
+
+/* list of files published via debugfs*/
+static char const *const files[] = {
+	[GBE_TEST_MODE_ID] = "gbe_compliance",
+	[PHY_REG_DUMP_ID] = "phy_reg_dump",
+	[SWITCH_REG_DUMP_ID] = "switch_reg_dump",
+};
+
+#define BUF_SIZE(s, pstart, pend)                                              \
+	({                                                                     \
+		int sleft = (pend) - (pstart);                                 \
+		sleft < (s) ? sleft : 0;                                       \
+	})
+
+static int mv88e6xxx_phy_read(struct mv88e6xxx_chip *chip, int phy, int reg,
+			      u16 *val)
+{
+	int addr = phy; /* PHY devices addresses start at 0x0 */
+
+	if (!chip->info->ops->phy_read)
+		return -EOPNOTSUPP;
+
+	return chip->info->ops->phy_read(chip, chip->bus, addr, reg, val);
+}
+
+static int mv88e6xxx_phy_write(struct mv88e6xxx_chip *chip, int phy, int reg,
+			       u16 val)
+{
+	int addr = phy; /* PHY devices addresses start at 0x0 */
+
+	if (!chip->info->ops->phy_write)
+		return -EOPNOTSUPP;
+
+	return chip->info->ops->phy_write(chip, chip->bus, addr, reg, val);
+}
+
+static bool mv88e6xxx_check_filename(struct file *filp, int file_index)
+{
+	return strcmp(files[file_index], filp->f_path.dentry->d_iname) == 0;
+}
+
+static ssize_t mv88e6xxx_debugfs_generic_read(struct file *filp,
+					      char __user *buffer, size_t count,
+					      loff_t *off, int file_index)
+{
+	return simple_read_from_buffer(buffer, count, off, text[file_index],
+				       strlen(text[file_index]));
+}
+
+static ssize_t mv88e6xxx_phy_reg_dump_read(struct file *filp,
+					   char __user *user_buf, size_t count,
+					   loff_t *off)
+{
+	int ret;
+	const int buf_size = 2048;
+	char *buf = kmalloc(buf_size, GFP_KERNEL);
+	const int phys[] = { 0, 1, 3, 4 };
+	char *pos = buf;
+	u8 phy_id;
+	struct mv88e6xxx_chip *chip = filp->f_inode->i_private;
+
+	mutex_lock(&chip->reg_lock);
+	for (phy_id = 0; phy_id < ARRAY_SIZE(phys); ++phy_id) {
+		u8 addr = phys[phy_id];
+		u8 phy_reg;
+		int reg_val, i;
+		u8 dev3_regs[] = { 0, 1, 20, 22 };
+		u8 dev7_regs[] = { 60, 61 };
+		struct phy_device *phy_dev = mdiobus_get_phy(chip->bus, addr);
+
+		pos += snprintf(pos, BUF_SIZE(buf_size, buf, pos),
+				"=== PHY %d (addr: %d) dump ===\n", phy_id,
+				addr);
+		for (phy_reg = 0; phy_reg <= 26; ++phy_reg) {
+			u16 reg_val;
+
+			if (phy_reg == 11 || phy_reg == 12 || phy_reg == 23 ||
+			    phy_reg == 24 || phy_reg == 25)
+				continue;
+
+			ret = mv88e6xxx_phy_read(chip, addr, phy_reg, &reg_val);
+			pos += snprintf(pos, BUF_SIZE(buf_size, buf, pos),
+					"REG %d value: %04X\n", phy_reg,
+					reg_val);
+		}
+
+		pos += snprintf(pos, BUF_SIZE(buf_size, buf, pos),
+				"-- PHY %d (addr: %d) XMDIO regs\n", phy_id,
+				addr);
+
+		if (!phy_dev)
+			continue;
+
+		/* unlock chip->reg_lock for reading of MMD to avoid
+		 * deadlock with mdio_bus->mdio_lock that will locked in
+		 * phy_read_mmd_indirect
+		 */
+		mutex_unlock(&chip->reg_lock);
+
+		for (i = 0; i < ARRAY_SIZE(dev3_regs); ++i) {
+			reg_val = phy_read_mmd(phy_dev, dev3_regs[i], 3);
+			if (reg_val < 0)
+				continue;
+			pos += snprintf(pos, BUF_SIZE(buf_size, buf, pos),
+					"Dev: 3 Reg: %d value: %04X\n",
+					dev3_regs[i], (uint16_t)reg_val);
+		}
+
+		for (i = 0; i < ARRAY_SIZE(dev7_regs); ++i) {
+			reg_val = phy_read_mmd(phy_dev, dev7_regs[i], 7);
+			if (reg_val < 0)
+				continue;
+			pos += snprintf(pos, BUF_SIZE(buf_size, buf, pos),
+					"Dev: 7 Reg: %d value: %04X\n",
+					dev7_regs[i], (uint16_t)reg_val);
+		}
+
+		mutex_lock(&chip->reg_lock);
+	}
+
+	mutex_unlock(&chip->reg_lock);
+
+	if (WARN_ON(pos - buf >= buf_size))
+		ret = -ENOSPC;
+	else
+		ret = simple_read_from_buffer(user_buf, count, off, buf,
+					      pos - buf);
+
+	kfree(buf);
+	return ret;
+}
+
+static ssize_t mv88e6xxx_switch_reg_dump_read(struct file *filp,
+					      char __user *user_buf,
+					      size_t count, loff_t *off)
+{
+	int ret;
+	const int buf_size = 10240;
+	char *buf = kmalloc(buf_size, GFP_KERNEL);
+	const int sw_port_regs[] = { 0x10, 0x11, 0x12, 0x13, 0x14,
+				     0x15, 0x16, 0x1B, 0x1C };
+	const int sw_gl_regs[] = { 0x1B, 0x1C };
+	char *pos = buf;
+	u8 sw_port_id, sw_gl_id;
+	int sw_port_regs_size = ARRAY_SIZE(sw_port_regs);
+	int sw_gl_regs_size = ARRAY_SIZE(sw_gl_regs);
+	struct mv88e6xxx_chip *chip = filp->f_inode->i_private;
+
+	mutex_lock(&chip->reg_lock);
+	for (sw_port_id = 0; sw_port_id < sw_port_regs_size; sw_port_id++) {
+		u8 addr = sw_port_regs[sw_port_id];
+		u8 sw_port_reg;
+
+		pos += snprintf(pos, BUF_SIZE(buf_size, buf, pos),
+				"=== SWITCH PORT REG %d (addr: %d) dump ===\n",
+				sw_port_id, addr);
+
+		for (sw_port_reg = 0; sw_port_reg < 0x20; ++sw_port_reg) {
+			u16 reg_val;
+
+			if ((sw_port_reg >= 0x10 && sw_port_reg <= 0x15) ||
+			    sw_port_reg == 0x17 || sw_port_reg == 0x1A ||
+			    sw_port_reg == 0x1C || sw_port_reg == 0x1D)
+				continue;
+
+			ret = mv88e6xxx_read(chip, addr, sw_port_reg, &reg_val);
+			pos += snprintf(pos, BUF_SIZE(buf_size, buf, pos),
+					"  REG %d (0x%02X) value: 0x%04X\n",
+					sw_port_reg, sw_port_reg, reg_val);
+		}
+	}
+
+	for (sw_gl_id = 0; sw_gl_id < sw_gl_regs_size; sw_gl_id++) {
+		u8 addr = sw_gl_regs[sw_gl_id];
+		u8 sw_gl_reg;
+
+		pos += snprintf(pos, BUF_SIZE(buf_size, buf, pos),
+				"=== SWITCH GLOBAL %d REG (addr: %d) dump ===\n",
+				sw_gl_id + 1, addr);
+
+		for (sw_gl_reg = 0; sw_gl_reg < 0x20; ++sw_gl_reg) {
+			u16 reg_val;
+
+			if (!sw_gl_reg) {
+				if (sw_gl_reg >= 0x10 && sw_gl_reg <= 0x17)
+					continue;
+			} else {
+				if ((sw_gl_reg >= 0x10 && sw_gl_reg <= 0x13) ||
+				    sw_gl_reg == 0x1E || sw_gl_reg == 0x1F)
+					continue;
+			}
+
+			ret = mv88e6xxx_read(chip, addr, sw_gl_reg, &reg_val);
+			pos += snprintf(pos, BUF_SIZE(buf_size, buf, pos),
+					"  REG %d (0x%02X) value: 0x%04X\n",
+					sw_gl_reg, sw_gl_reg, reg_val);
+		}
+	}
+
+	mutex_unlock(&chip->reg_lock);
+
+	if (WARN_ON(pos - buf >= buf_size))
+		ret = -ENOSPC;
+	else
+		ret = simple_read_from_buffer(user_buf, count, off, buf,
+					      pos - buf);
+
+	kfree(buf);
+	return ret;
+}
+
+static ssize_t mv88e6xxx_debugfs_read(struct file *filp, char __user *buffer,
+				      size_t count, loff_t *off)
+{
+	if (mv88e6xxx_check_filename(filp, GBE_TEST_MODE_ID))
+		return mv88e6xxx_debugfs_generic_read(filp, buffer, count, off,
+						      GBE_TEST_MODE_ID);
+
+	if (mv88e6xxx_check_filename(filp, PHY_REG_DUMP_ID))
+		return mv88e6xxx_phy_reg_dump_read(filp, buffer, count, off);
+
+	if (mv88e6xxx_check_filename(filp, SWITCH_REG_DUMP_ID))
+		return mv88e6xxx_switch_reg_dump_read(filp, buffer, count, off);
+
+	return 0;
+}
+
+static void mv88e6xxx_debugfs_set_gbe_testmode(struct mv88e6xxx_chip *chip,
+					       const char __user *s,
+					       size_t count)
+{
+	const u16 transmitter_test_mode_mask = (u16)((BIT(13) - 1));
+	const int phys[] = { 0, 1, 3, 4 };
+	u16 mode, reg;
+	int i;
+
+	if (kstrtou16_from_user(s, count, 10, &mode) == 0) {
+		mutex_lock(&chip->reg_lock);
+		for (i = 0; i < 4; ++i) {
+			mv88e6xxx_phy_read(chip, phys[i], STD_GCCTRL_REG, &reg);
+			reg = (mode << 13) | (reg & transmitter_test_mode_mask);
+			mv88e6xxx_phy_write(chip, phys[i], STD_GCCTRL_REG, reg);
+		}
+		mutex_unlock(&chip->reg_lock);
+
+		if (mode)
+			pr_info("gbe entering test mode %d\n", mode);
+		else
+			pr_info("gbe leaving test mode\n");
+	}
+}
+
+static ssize_t mv88e6xxx_debugfs_write(struct file *filp, const char __user *s,
+				       size_t count, loff_t *off)
+{
+	struct mv88e6xxx_chip *chip = filp->f_inode->i_private;
+
+	if (mv88e6xxx_check_filename(filp, GBE_TEST_MODE_ID))
+		mv88e6xxx_debugfs_set_gbe_testmode(chip, s, count);
+
+	return count;
+}
+
+static const struct file_operations debugfs_ops = {
+	.read = mv88e6xxx_debugfs_read,
+	.write = mv88e6xxx_debugfs_write
+};
+
+void mv88e6xxx_debugfs_destroy(struct mv88e6xxx_chip *chip)
+{
+	if (!IS_ERR_OR_NULL(chip->debugfs_root_entry))
+		debugfs_remove_recursive(chip->debugfs_root_entry);
+}
+
+int mv88e6xxx_debugfs_setup(struct mv88e6xxx_chip *chip)
+{
+	struct dentry *entry;
+
+	entry = debugfs_create_dir("mv88e6xxx", NULL);
+	if (IS_ERR_OR_NULL(entry))
+		goto out_err;
+
+	chip->debugfs_root_entry = entry;
+
+	entry = debugfs_create_file(files[GBE_TEST_MODE_ID], 0600,
+				    chip->debugfs_root_entry, chip,
+				    &debugfs_ops);
+	if (IS_ERR_OR_NULL(entry))
+		goto out_err;
+
+	entry = debugfs_create_file(files[PHY_REG_DUMP_ID], 0400,
+				    chip->debugfs_root_entry, chip,
+				    &debugfs_ops);
+	if (IS_ERR_OR_NULL(entry))
+		goto out_err;
+
+	entry = debugfs_create_file(files[SWITCH_REG_DUMP_ID], 0400,
+				    chip->debugfs_root_entry, chip,
+				    &debugfs_ops);
+	if (IS_ERR_OR_NULL(entry))
+		goto out_err;
+
+	return 0;
+
+out_err:
+	mv88e6xxx_debugfs_destroy(chip);
+	return (entry) ? PTR_ERR(entry) : -EINVAL;
+}
diff --git a/drivers/net/dsa/mv88e6xxx/debugfs.h b/drivers/net/dsa/mv88e6xxx/debugfs.h
new file mode 100644
index 000000000000..97562ab3153c
--- /dev/null
+++ b/drivers/net/dsa/mv88e6xxx/debugfs.h
@@ -0,0 +1,11 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+
+#ifndef _MV88E6XXX_DEBUGFS_H
+#define _MV88E6XXX_DEBUGFS_H
+
+#include "chip.h"
+
+void mv88e6xxx_debugfs_destroy(struct mv88e6xxx_chip *chip);
+int mv88e6xxx_debugfs_setup(struct mv88e6xxx_chip *chip);
+
+#endif /* _MV88E6XXX_DEBUGFS_H */
diff --git a/drivers/net/dsa/mv88e6xxx/global2.c b/drivers/net/dsa/mv88e6xxx/global2.c
index 75b227d0f73b..8fb2d047c6db 100644
--- a/drivers/net/dsa/mv88e6xxx/global2.c
+++ b/drivers/net/dsa/mv88e6xxx/global2.c
@@ -87,9 +87,10 @@ int mv88e6185_g2_mgmt_rsvd2cpu(struct mv88e6xxx_chip *chip)
 	int err;
 
 	/* Consider the frames with reserved multicast destination
-	 * addresses matching 01:80:c2:00:00:0x as MGMT.
+	 * addresses matching 01:80:c2:00:00:0x as MGMT, except
+	 * frames with mac 01:80:c2:00:00:00 (BPDU)
 	 */
-	err = mv88e6xxx_g2_mgmt_enable_0x(chip, 0xffff);
+	err = mv88e6xxx_g2_mgmt_enable_0x(chip, 0xfffe);
 	if (err)
 		return err;
 
@@ -346,6 +347,8 @@ static int mv88e6xxx_g2_eeprom_cmd(struct mv88e6xxx_chip *chip, u16 cmd)
 	if (err)
 		return err;
 
+	usleep_range(10000, 15000);
+
 	return mv88e6xxx_g2_eeprom_wait(chip);
 }
 
diff --git a/drivers/net/dsa/mv88e6xxx/mv88e6321_tcam.c b/drivers/net/dsa/mv88e6xxx/mv88e6321_tcam.c
new file mode 100644
index 000000000000..5742521cbf9b
--- /dev/null
+++ b/drivers/net/dsa/mv88e6xxx/mv88e6321_tcam.c
@@ -0,0 +1,975 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+
+#define pr_fmt(fmt)     "mv88e6321: tcam: " fmt
+
+#include <linux/delay.h>
+#include "mv88e6321_tcam.h"
+#include "chip.h"
+#include "port.h"
+
+#define OF_TCAM_ENTRIES_PROP_READ_U8(__np, __name, __out, __ret, __goto) do { \
+u32 val__; \
+(__ret) = of_property_read_u32(__np, __name, &val__); \
+if ((__ret) < 0) { \
+	pr_err("no %s property found for tcam entry %s ret: (%d)\n", \
+	       __name, __np->name, __ret); \
+	goto __goto; \
+} \
+tcam_entries->__out = (u8)val__; \
+pr_debug("tcam set %s = 0x%04X\n", __name, val__); \
+} while (0)
+
+#define OF_TCAM_ENTRIES_PROP_READ_OPT_U8(__np, __name, __out, __ret, __goto) \
+do { \
+u32 val__; \
+if (of_property_read_u32(__np, __name, &val__) >= 0) { \
+	tcam_entries->__out = (u8)val__; \
+	pr_debug("tcam set %s = 0x%04X\n", __name, val__); \
+} \
+} while (0)
+
+#define OF_TCAM_PROP_READ_U8(__np, __name, __out, __ret, __goto) do { \
+u32 val__; \
+(__ret) = of_property_read_u32(__np, __name, &val__); \
+if ((__ret) < 0) { \
+	pr_err("no %s property found for tcam entry %s ret: (%d)\n", \
+	       __name, __np->name, __ret); \
+	kfree(tcam_entry); \
+	goto __goto; \
+} \
+tcam_entry->__out = (u8)val__; \
+pr_debug("tcam set %s = 0x%04X\n", __name, val__); \
+} while (0)
+
+#define OF_TCAM_PROP_READ_OPT_U8(__np, __name, __out) do { \
+u32 val__; \
+if (of_property_read_u32(__np, __name, &val__) >= 0) { \
+	tcam_entry->__out = (u8)val__; \
+	pr_debug("tcam set %s = 0x%04X\n", __name, val__); \
+} \
+} while (0)
+
+#define OF_TCAM_PROP_READ_U16(__np, __name, __out, __ret, __goto) do { \
+u32 val__; \
+__ret = of_property_read_u32(__np, __name, &val__); \
+if ((__ret) < 0) { \
+	pr_err("no %s property found for tcam entry %s\n", \
+	       __name, __np->name); \
+	kfree(tcam_entry); \
+	goto __goto; \
+} \
+tcam_entry->__out = (u16)val__; \
+pr_debug("tcam set %s = 0x%04X\n", __name, val__); \
+} while (0)
+
+#define OF_TCAM_PROP_READ_COND_U8(__np, __name, __mask, __out, __ret, __goto) \
+do { \
+	if (tcam_entry->__mask) \
+		OF_TCAM_PROP_READ_U8(__np, __name, __out, __ret, __goto); \
+} while (0)
+
+#define OF_TCAM_PROP_READ_COND_U16(__np, __name, __mask, __out, __ret, __goto) \
+do { \
+	if (tcam_entry->__mask) \
+		OF_TCAM_PROP_READ_U16(__np, __name, __out, __ret, __goto); \
+} while (0)
+
+#define OF_TCAM_PROP_READ_BOOL(__np, __name, __out) do {\
+if (of_property_read_bool((__np), (__name))) {\
+	tcam_entry->__out = 1; \
+	pr_debug("tcam set %s = 1\n", (__name)); \
+} \
+} while (0)
+
+static int of_set_frame_data(struct tcam_entry *tcam_entry,
+			     struct device_node *np)
+{
+	int ret;
+	u8 buf[TCAM_FRAME_DATA_MAX_SIZE];
+	u8 is_mask = 0;
+	u8 is_96_mask = 0;
+
+	/* get frame data mask */
+	ret = of_property_read_variable_u8_array(np, "frame-data-mask", buf, 0,
+						 TCAM_FRAME_DATA_MAX_SIZE);
+	if (ret < 0)
+		return ret;
+
+	if (!ret)
+		return -ENODATA;
+
+	while (ret--) {
+		pr_debug("frame_data[%d].mask = 0x%02X\n", ret, buf[ret]);
+		tcam_entry->frame_data[ret].mask = buf[ret];
+		is_mask |= buf[ret];
+
+		if (ret >= 48)
+			is_96_mask |= buf[ret];
+	}
+
+	tcam_entry->is96frame = (is_96_mask) ? 1 : 0;
+
+	if (!is_mask)
+		return 0;
+
+	/* get frame data */
+	ret = of_property_read_variable_u8_array(np, "frame-data", buf, 0,
+						 TCAM_FRAME_DATA_MAX_SIZE);
+	if (ret < 0)
+		return ret;
+
+	if (!ret)
+		return -ENODATA;
+
+	while (ret--) {
+		pr_debug("frame_data[%d].data = 0x%02X\n", ret, buf[ret]);
+		tcam_entry->frame_data[ret].data = buf[ret];
+	}
+
+	return 0;
+}
+
+int of_get_tcam_entry(struct tcam_entries *tcam_entries, struct device_node *np)
+{
+	int ctr = 0;
+	int err;
+	struct device_node *tcam_np;
+	struct device_node *child = NULL;
+	struct tcam_entry *tcam_entry = NULL;
+	struct tcam_entry *tcam_entry_tmp = NULL;
+
+	tcam_np = of_find_compatible_node(np, NULL, "mv88e6321,tcam");
+
+	if (!tcam_np)
+		return ctr;
+
+	OF_TCAM_ENTRIES_PROP_READ_U8(tcam_np, "tcam-mode-port-mask", port_mask,
+				     err, out);
+
+	OF_TCAM_ENTRIES_PROP_READ_OPT_U8(tcam_np, "debug-port", debug_port,
+					 err, out);
+
+	for_each_child_of_node(tcam_np, child) {
+		pr_info("process tcam entry node %s\n",	child->name);
+
+		tcam_entry = kzalloc(sizeof(*tcam_entry), GFP_KERNEL);
+		if (!tcam_entry) {
+			err = -ENOMEM;
+			goto out_free;
+		}
+
+		tcam_entry->title = child->name;
+
+		OF_TCAM_PROP_READ_U8(child, "id", orig_id, err, out_free);
+		OF_TCAM_PROP_READ_OPT_U8(child, "frame-type-mask",
+					 mask_frame_type);
+		OF_TCAM_PROP_READ_COND_U8(child, "frame-type", mask_frame_type,
+					  frame_type, err, out_free);
+		OF_TCAM_PROP_READ_U8(child, "ppri-mask", mask_ppri, err,
+				     out_free);
+		OF_TCAM_PROP_READ_COND_U8(child, "ppri", mask_ppri, orig_ppri,
+					  err, out_free);
+		OF_TCAM_PROP_READ_U8(child, "pvid-mask", mask_pvid, err,
+				     out_free);
+		OF_TCAM_PROP_READ_COND_U16(child, "pvid", mask_pvid, orig_pvid,
+					   err, out_free);
+		OF_TCAM_PROP_READ_OPT_U8(child, "spv-mask", mask_spv);
+		OF_TCAM_PROP_READ_COND_U8(child, "spv", mask_spv, spv, err,
+					  out_free);
+		OF_TCAM_PROP_READ_BOOL(child, "vid-override", vid_override);
+		OF_TCAM_PROP_READ_COND_U16(child, "vid-data", vid_override, vid,
+					   err, out_free);
+		OF_TCAM_PROP_READ_BOOL(child, "interrupt", interrupt);
+		OF_TCAM_PROP_READ_BOOL(child, "inc-tcam-ctr", inc_tcam_ctr);
+		OF_TCAM_PROP_READ_BOOL(child, "fpri-override", fpri_override);
+		OF_TCAM_PROP_READ_COND_U8(child, "fpri-data", fpri_override,
+					  fpri, err, out_free);
+		OF_TCAM_PROP_READ_BOOL(child, "qpri-override", qpri_override);
+		OF_TCAM_PROP_READ_COND_U8(child, "qpri-data", qpri_override,
+					  qpri, err, out_free);
+		OF_TCAM_PROP_READ_U8(child, "next-id", next_id, err, out_free);
+		OF_TCAM_PROP_READ_BOOL(child, "dpv-override", dpv_override);
+		OF_TCAM_PROP_READ_COND_U8(child, "dpv-data", dpv_override, dpv,
+					  err, out_free);
+		OF_TCAM_PROP_READ_BOOL(child, "ld-balance-override",
+				       ld_balance_override);
+		OF_TCAM_PROP_READ_COND_U8(child, "ld-balance-data",
+					  ld_balance_override, ld_balance, err,
+					  out_free);
+		OF_TCAM_PROP_READ_BOOL(child, "action-override",
+				       action_override);
+		OF_TCAM_PROP_READ_COND_U16(child, "action-data",
+					   action_override, action, err,
+					   out_free);
+
+		err = of_set_frame_data(tcam_entry, child);
+		if (err) {
+			kfree(tcam_entry);
+			goto out_free;
+		}
+
+		/* let every tcam_entry have same debug port */
+		tcam_entry->debug_port = tcam_entries->debug_port;
+
+		list_add(&tcam_entry->list, &tcam_entries->head.list);
+
+		ctr++;
+	}
+
+	return ctr;
+
+out_free:
+	list_for_each_entry_safe(tcam_entry, tcam_entry_tmp,
+				 &tcam_entries->head.list, list) {
+		list_del(&tcam_entry->list);
+		kfree(tcam_entry);
+	}
+out:
+	return err;
+}
+
+static inline int mv88e6321_write_tcam_pg_hdr(struct mii_bus *bus,
+					      struct tcam_entry *tcam_entry)
+{
+	tcam_entry->busy = 1;
+
+	return mdiobus_write(bus, TCAM_ADDR, TCAM_REG_PG_HDR,
+			     tcam_entry->reg_pg_hdr);
+}
+
+static inline int mv88e6321_wait_tcam_ready(struct mii_bus *bus)
+{
+	int ret;
+	int attempt;
+
+	for (attempt = 0; attempt < 16; ++attempt) {
+		ret = mdiobus_read(bus, TCAM_ADDR, TCAM_REG_PG_HDR);
+		if (ret < 0)
+			return ret;
+
+		if (!(ret & TCAM_PG_HDR_BUSY))
+			return ret;
+
+		usleep_range(1000, 2000);
+	}
+
+	pr_err("timeout while waiting for tcam ready\n");
+
+	return -ETIMEDOUT;
+}
+
+static int mv88e6321_load_tcam_page2(struct mii_bus *bus,
+				     struct tcam_entry *tcam_entry,
+				     int entry_no)
+{
+	int ret;
+
+	tcam_entry->pg = 2;
+
+	pr_debug("load page 2\n");
+
+	ret = mv88e6321_wait_tcam_ready(bus);
+	if (ret < 0)
+		return ret;
+
+	if (entry_no == 0 && tcam_entry->is96frame)
+		tcam_entry->cnt = 1;
+	else
+		tcam_entry->cnt = 0;
+
+	ret = mdiobus_write(bus, TCAM_ADDR, TCAM_REG_PG2_ACTION_4,
+			    tcam_entry->reg_action_4);
+	if (ret < 0)
+		return ret;
+
+	ret = mdiobus_write(bus, TCAM_ADDR, TCAM_REG_PG2_ACTION_3,
+			    tcam_entry->reg_action_3);
+	if (ret < 0)
+		return ret;
+
+	ret = mdiobus_write(bus, TCAM_ADDR, TCAM_REG_PG2_ACTION_2,
+			    tcam_entry->reg_action_2);
+	if (ret < 0)
+		return ret;
+
+	ret = mdiobus_write(bus, TCAM_ADDR, TCAM_REG_PG2_ACTION_1,
+			    tcam_entry->reg_action_1);
+	if (ret < 0)
+		return ret;
+
+	ret = mdiobus_write(bus, TCAM_ADDR, TCAM_REG_PG2_DEBUG_PORT,
+			    tcam_entry->reg_debug);
+	if (ret < 0)
+		return ret;
+
+	return mv88e6321_write_tcam_pg_hdr(bus, tcam_entry);
+}
+
+static int mv88e6321_load_tcam_page1(struct mii_bus *bus,
+				     struct tcam_entry *tcam_entry,
+				     int entry_no)
+{
+	int ret;
+	int i = 22;
+	int max = 0;
+	int reg = 0;
+
+	tcam_entry->pg = 1;
+
+	pr_debug("load page 1\n");
+
+	ret = mv88e6321_wait_tcam_ready(bus);
+	if (ret < 0)
+		return ret;
+
+	if (entry_no > 0)
+		i = 70;
+
+	max = i + 26;
+
+	for (reg = 2; i < max; ++i, ++reg) {
+		ret = mdiobus_write(bus, TCAM_ADDR, reg,
+				    tcam_entry->frame_data[i].reg_data);
+		if (ret < 0)
+			return ret;
+	}
+
+	return 0;
+}
+
+static int mv88e6321_load_tcam_page0(struct mii_bus *bus,
+				     struct tcam_entry *tcam_entry, int entry_no)
+{
+	int ret;
+	int i = 0;
+	int max = 0;
+	int reg = 0;
+	u16 reg_frame_type = 0;
+
+	tcam_entry->pg = 0;
+
+	pr_debug("load page 0\n");
+
+	if (tcam_entry->frame_type == TCAM_FT_PROV_TAG && entry_no == 0) {
+		tcam_entry->id_pvid = (tcam_entry->orig_pvid & 0xFF);
+		tcam_entry->pvid = ((tcam_entry->orig_pvid >> 8) & 0xF);
+		tcam_entry->mask_id_pvid = (tcam_entry->mask_pvid & 0xFF);
+		tcam_entry->mask_ppri_pvid = (((u16)tcam_entry->mask_ppri) << 8);
+		tcam_entry->mask_ppri_pvid += ((tcam_entry->mask_pvid >> 8) & 0xF);
+	}
+
+	if (entry_no > 0) {
+		reg_frame_type = 0xC000;
+		reg_frame_type += TCAM_FT_CONTINUE;
+	} else {
+		reg_frame_type = tcam_entry->reg_frame_type;
+	}
+
+	ret = mv88e6321_wait_tcam_ready(bus);
+	if (ret < 0)
+		return ret;
+
+	ret = mdiobus_write(bus, TCAM_ADDR, TCAM_REG_PG0_KEY_1, reg_frame_type);
+	if (ret < 0)
+		return ret;
+
+	ret = mdiobus_write(bus, TCAM_ADDR, TCAM_REG_PG0_KEY_2,
+			    tcam_entry->reg_spv);
+	if (ret < 0)
+		return ret;
+
+	ret = mdiobus_write(bus, TCAM_ADDR, TCAM_REG_PG0_KEY_3,
+			    tcam_entry->reg_ppri_pvid);
+	if (ret < 0)
+		return ret;
+
+	ret = mdiobus_write(bus, TCAM_ADDR, TCAM_REG_PG0_KEY_4,
+			    tcam_entry->reg_id_ppri);
+	if (ret < 0)
+		return ret;
+
+	if (entry_no > 0)
+		i = 48;
+
+	max = i + 22;
+
+	for (reg = 6; i < max; ++i, ++reg) {
+		ret = mdiobus_write(bus, TCAM_ADDR, reg,
+				    tcam_entry->frame_data[i].reg_data);
+		if (ret < 0)
+			return ret;
+	}
+
+	return mv88e6321_write_tcam_pg_hdr(bus, tcam_entry);
+}
+
+int mv88e6321_load_tcam(struct mii_bus *bus,
+			struct tcam_entry *tcam_entry)
+{
+	int ret;
+	int entry_no = 1;
+
+	if (!bus)
+		return -ENODEV;
+
+	tcam_entry->op = TCAM_OP_LOAD;
+
+	if (tcam_entry->is96frame)
+		entry_no = 2;
+
+	while (entry_no--) {
+		if (entry_no > 0) {
+			tcam_entry->id_pvid = tcam_entry->next_id;
+			tcam_entry->mask_id_pvid = 0xFF;
+			tcam_entry->id = tcam_entry->next_id;
+			tcam_entry->next_id = 0;
+		} else {
+			tcam_entry->id_pvid = 0;
+			tcam_entry->mask_id_pvid = 0;
+			tcam_entry->next_id = tcam_entry->id;
+			tcam_entry->id = tcam_entry->orig_id;
+		}
+
+		ret = mv88e6321_load_tcam_page2(bus, tcam_entry, entry_no);
+		if (ret < 0)
+			return ret;
+
+		ret = mv88e6321_load_tcam_page1(bus, tcam_entry, entry_no);
+		if (ret < 0)
+			return ret;
+
+		ret = mv88e6321_load_tcam_page0(bus, tcam_entry, entry_no);
+		if (ret < 0)
+			return ret;
+	}
+
+	return 0;
+}
+
+static int mv88e6321_get_tcam_pg0(struct mii_bus *bus, struct tcam_entry
+				  *tcam_entry, int entry_no)
+{
+	int ret;
+	int i = 0;
+	int reg = 0;
+	int max = 0;
+
+	pr_debug("read tcam page 0\n");
+
+	tcam_entry->pg = 0;
+
+	ret = mv88e6321_write_tcam_pg_hdr(bus, tcam_entry);
+	if (ret < 0)
+		return ret;
+
+	if (entry_no == 0) {
+		ret = mdiobus_read(bus, TCAM_ADDR, TCAM_REG_PG0_KEY_1);
+		if (ret < 0)
+			return ret;
+
+		tcam_entry->reg_frame_type = ((u16)ret);
+
+		if (tcam_entry->reg_frame_type != 0x00FF)
+			tcam_entry->is_valid = 1;
+		else
+			return ret;
+
+		ret = mdiobus_read(bus, TCAM_ADDR, TCAM_REG_PG0_KEY_2);
+		if (ret < 0)
+			return ret;
+
+		tcam_entry->reg_spv = ((u16)ret);
+
+		ret = mdiobus_read(bus, TCAM_ADDR, TCAM_REG_PG0_KEY_3);
+		if (ret < 0)
+			return ret;
+
+		tcam_entry->reg_ppri_pvid = ((u16)ret);
+		tcam_entry->mask_ppri = (((u8)tcam_entry->mask_ppri_pvid) >> 4);
+		tcam_entry->orig_pvid = tcam_entry->pvid;
+		tcam_entry->orig_pvid <<= 8;
+		tcam_entry->mask_pvid = (tcam_entry->mask_ppri_pvid & 0xF);
+		tcam_entry->mask_pvid <<= 8;
+
+		ret = mdiobus_read(bus, TCAM_ADDR, TCAM_REG_PG0_KEY_4);
+		if (ret < 0)
+			return ret;
+
+		tcam_entry->reg_id_ppri = ((u16)ret);
+		tcam_entry->orig_pvid += tcam_entry->id_pvid;
+		tcam_entry->mask_pvid += tcam_entry->mask_id_pvid;
+	}
+
+	if (entry_no > 0)
+		i = 48;
+
+	max = i + 22;
+
+	for (reg = 6; i < max; ++i, ++reg) {
+		ret = mdiobus_read(bus, TCAM_ADDR, reg);
+		if (ret < 0)
+			return ret;
+
+		tcam_entry->frame_data[i].reg_data = ((u16)ret);
+	}
+
+	return 0;
+}
+
+static int mv88e6321_get_tcam_pg1(struct mii_bus *bus, struct tcam_entry
+				  *tcam_entry, int entry_no)
+{
+	int ret;
+	int i = 22;
+	int reg = 0;
+	int max = 0;
+
+	pr_debug("read tcam page 1\n");
+
+	tcam_entry->pg = 1;
+
+	ret = mv88e6321_write_tcam_pg_hdr(bus, tcam_entry);
+	if (ret < 0)
+		return ret;
+
+	if (entry_no > 0)
+		i = 70;
+
+	max = i + 26;
+
+	for (reg = 2; i < max; ++i, ++reg) {
+		ret = mdiobus_read(bus, TCAM_ADDR, reg);
+		if (ret < 0)
+			return ret;
+
+		tcam_entry->frame_data[i].reg_data = ((u16)ret);
+	}
+
+	return 0;
+}
+
+static int mv88e6321_get_tcam_pg2(struct mii_bus *bus, struct tcam_entry
+				  *tcam_entry, int entry_no)
+{
+	int ret;
+
+	pr_debug("read tcam page 2\n");
+
+	tcam_entry->pg = 2;
+
+	if (entry_no)
+		return 0;
+
+	ret = mv88e6321_write_tcam_pg_hdr(bus, tcam_entry);
+	if (ret < 0)
+		return ret;
+
+	ret = mdiobus_read(bus, TCAM_ADDR, TCAM_REG_PG2_ACTION_1);
+	if (ret < 0)
+		return ret;
+
+	tcam_entry->reg_action_1 = ((u16)ret);
+
+	ret = mdiobus_read(bus, TCAM_ADDR, TCAM_REG_PG2_ACTION_2);
+	if (ret < 0)
+		return ret;
+
+	tcam_entry->reg_action_2 = ((u16)ret);
+
+	ret = mdiobus_read(bus, TCAM_ADDR, TCAM_REG_PG2_ACTION_3);
+	if (ret < 0)
+		return ret;
+
+	tcam_entry->reg_action_3 = ((u16)ret);
+
+	ret = mdiobus_read(bus, TCAM_ADDR, TCAM_REG_PG2_ACTION_4);
+	if (ret < 0)
+		return ret;
+
+	tcam_entry->reg_action_4 = ((u16)ret);
+
+	ret = mdiobus_read(bus, TCAM_ADDR, TCAM_REG_PG2_DEBUG_PORT);
+	if (ret < 0)
+		return ret;
+
+	tcam_entry->reg_debug = ((u16)ret);
+
+	ret = mdiobus_read(bus, TCAM_ADDR, TCAM_REG_PG2_DEBUG);
+	if (ret < 0)
+		return ret;
+
+	tcam_entry->reg_hit = ((u16)ret);
+
+	return 0;
+}
+
+int mv88e6321_get_tcam(struct mii_bus *bus, u8 id,
+		       struct tcam_entry *tcam_entry)
+{
+	int ret;
+	int i = 0;
+
+	if (!bus)
+		return -ENODEV;
+
+	tcam_entry->orig_id = id;
+	tcam_entry->id = id;
+	tcam_entry->op = TCAM_OP_READ;
+
+	for (i = 0; i < 2; ++i) {
+		if (i > 0)
+			tcam_entry->id = tcam_entry->next_id;
+
+		ret = mv88e6321_get_tcam_pg0(bus, tcam_entry, i);
+		if (ret < 0 || !tcam_entry->is_valid)
+			return ret;
+
+		ret = mv88e6321_get_tcam_pg1(bus, tcam_entry, i);
+		if (ret < 0)
+			return ret;
+
+		ret = mv88e6321_get_tcam_pg2(bus, tcam_entry, i);
+		if (ret < 0)
+			return ret;
+
+		if (!tcam_entry->cnt)
+			break;
+	}
+
+	return 0;
+}
+
+int mv88e6321_tcam_to_string(struct tcam_entry *tcam_entry,
+			     char *buffer, size_t size)
+{
+	int i = 0;
+	char *p = buffer;
+
+	if (!tcam_entry->is_valid) {
+		p += sprintf(p, "TCAM with ID %d is disabled", tcam_entry->id);
+		return 0;
+	}
+
+	p += sprintf(p, "\n\t\tTCAM            ID: %d\n", tcam_entry->orig_id);
+	p += sprintf(p, "\t\tTCAM    frame type: 0x%02X mask: 0x%02X\n",
+		     tcam_entry->frame_type, tcam_entry->mask_frame_type);
+	p += sprintf(p, "\t\tTCAM           spv: 0x%02X mask: 0x%02X\n",
+		     tcam_entry->spv, tcam_entry->mask_spv);
+	p += sprintf(p, "\t\tTCAM          ppri: 0x%02X mask: 0x%02X\n",
+		     tcam_entry->ppri, tcam_entry->mask_ppri);
+
+	if (tcam_entry->mask_frame_type &&
+	    tcam_entry->frame_type == TCAM_FT_PROV_TAG) {
+		p += sprintf(p, "\t\tTCAM          pvid: 0x%04X mask: 0x%04X\n",
+			     tcam_entry->orig_pvid, tcam_entry->mask_pvid);
+	}
+
+	p += sprintf(p, "\t\tTCAM        cnt id: 0x%02X mask: 0x%02X\n",
+		     tcam_entry->id_pvid, tcam_entry->mask_id_pvid);
+	p += sprintf(p, "\t\tTCAM    frame data: [MASK:DATA]");
+
+	for (i = 0; i < 48; ++i) {
+		if (!(i % 16))
+			p += sprintf(p, "\n\t\t\t");
+
+		p += sprintf(p, "%02X:%02X ", tcam_entry->frame_data[i].mask,
+			     tcam_entry->frame_data[i].data);
+	}
+
+	if (tcam_entry->cnt) {
+		for (i = 48; i < TCAM_FRAME_DATA_MAX_SIZE; ++i) {
+			if (!(i % 16))
+				p += sprintf(p, "\n\t\t\t");
+
+			p += sprintf(p, "%02X:%02X ",
+				     tcam_entry->frame_data[i].mask,
+				     tcam_entry->frame_data[i].data);
+		}
+	}
+
+	p += sprintf(p, "\n\t\tTCAM      continue: %d\n", tcam_entry->cnt);
+	p += sprintf(p, "\t\tTCAM     interrupt: %d\n", tcam_entry->interrupt);
+	p += sprintf(p, "\t\tTCAM      inc ctrl: %d\n",
+		     tcam_entry->inc_tcam_ctr);
+	p += sprintf(p, "\t\tTCAM  override vid: %d vid: 0x%04X\n",
+		     tcam_entry->vid_override, tcam_entry->vid);
+	p += sprintf(p, "\t\tTCAM    next index: %d\n", tcam_entry->next_id);
+	p += sprintf(p, "\t\tTCAM override qpri: %d qpri: 0x%02X\n",
+		     tcam_entry->qpri_override, tcam_entry->qpri);
+	p += sprintf(p, "\t\tTCAM override fpri: %d fpri: 0x%02X\n",
+		     tcam_entry->fpri_override, tcam_entry->fpri);
+	p += sprintf(p, "\t\tTACM  override dpv: %d dpv: 0x%04X\n",
+		     tcam_entry->dpv_override, tcam_entry->dpv);
+	p += sprintf(p, "\t\tTCAM  override act: %d action: 0x%04X\n",
+		     tcam_entry->action_override, tcam_entry->action);
+	p += sprintf(p, "\t\tTCAM   override lb: %d load balance: 0x%02X\n",
+		     tcam_entry->ld_balance_override, tcam_entry->ld_balance);
+	p += sprintf(p, "\t\tTCAM    debug port: %d\n", tcam_entry->debug_port);
+	p += sprintf(p, "\t\tTCAM           hit: 0x%04X\n",
+		     tcam_entry->reg_hit);
+
+	return 0;
+}
+
+#define MV88E6321_PORTS		7
+
+#define PORT_REG(__reg)		(0x10 + (__reg))
+
+static inline int mv88e6321_set_port_state(struct mii_bus *bus, u16 addr,
+					   u16 state, u16 *old_reg_val)
+{
+	int ret;
+	u16 reg = MV88E6XXX_PORT_CTL0;
+
+	ret = mdiobus_read(bus, addr, reg);
+	if (ret < 0)
+		return ret;
+
+	if (old_reg_val)
+		*old_reg_val = (u16)ret;
+
+	ret &= ~MV88E6XXX_PORT_CTL0_STATE_MASK;
+	ret |= state;
+
+	ret = mdiobus_write(bus, addr, reg, ret);
+	if (ret < 0)
+		return ret;
+
+	return 0;
+}
+
+static int mv88e6321_enable_tcam_mode(struct mii_bus *bus, u16 addr,
+				      u16 port_prio_val)
+{
+	int ret;
+	u16 reg = MV88E6XXX_PORT_PRI_OVERRIDE;
+	u16 port_ctrl_val = 0;
+
+	/*TODO: currently this function support only 96byte tcam mode.
+	 * This mode includes 48byte tcam mode too. In future only
+	 * 48byte mode could be implemented.
+	 */
+
+	/* disable port before chage tcam mode */
+	ret = mv88e6321_set_port_state(bus, addr,
+				       MV88E6XXX_PORT_CTL0_STATE_DISABLED,
+				       &port_ctrl_val);
+	if (ret < 0)
+		return ret;
+
+	port_prio_val &= ~MV88E6XXX_PORT_PRI_OVERRIDE_TCAM_MODE_MASK;
+	port_prio_val |= MV88E6XXX_PORT_PRI_OVERRIDE_TCAM_MODE_96;
+
+	ret = mdiobus_write(bus, addr, reg, port_prio_val);
+	if (ret < 0)
+		return ret;
+
+	/* set old port state after tcam mode changing */
+	return mv88e6321_set_port_state(bus, addr, port_ctrl_val &
+					MV88E6XXX_PORT_CTL0_STATE_MASK, NULL);
+}
+
+int mv88e6321_set_tcam_mode(struct mii_bus *bus,
+			    struct tcam_entries *tcam_entries)
+{
+	int ret;
+	int addr = PORT_REG(0);
+	u8 mask = 1;
+
+	if (!bus)
+		return -ENODEV;
+
+	for (; addr < PORT_REG(MV88E6321_PORTS); ++addr) {
+		u16 reg = MV88E6XXX_PORT_PRI_OVERRIDE;
+
+		if (mask & tcam_entries->port_mask) {
+			ret = mdiobus_read(bus, addr, reg);
+			if (ret < 0)
+				return ret;
+
+			if ((ret & MV88E6XXX_PORT_PRI_OVERRIDE_TCAM_MODE_MASK) !=
+			    MV88E6XXX_PORT_PRI_OVERRIDE_TCAM_MODE_96) {
+				ret = mv88e6321_enable_tcam_mode(bus, addr,
+								 ret);
+			}
+		}
+		mask <<= 1;
+	}
+
+	return 0;
+}
+
+int mv88e6321_flush_tcam(struct mii_bus *bus, int id)
+{
+	int ret;
+	struct tcam_entry tcam_entry;
+
+	if (!bus)
+		return -ENODEV;
+
+	memset(&tcam_entry, 0, sizeof(struct tcam_entry));
+
+	if (id < 0)
+		tcam_entry.op = TCAM_OP_FLUSH_ALL;
+	else if (id >= 0 && id <= 255)
+		tcam_entry.op = TCAM_OP_FLUSH;
+	else
+		return 0;
+
+	tcam_entry.id = id;
+
+	if (id < 0)
+		pr_debug("flush all TCAM\n");
+	else
+		pr_debug("flush TCAM ID:%d\n", id);
+
+	ret = mv88e6321_wait_tcam_ready(bus);
+	if (ret < 0)
+		return ret;
+
+	ret = mv88e6321_write_tcam_pg_hdr(bus, &tcam_entry);
+	if (ret < 0)
+		return ret;
+
+	return mv88e6321_wait_tcam_ready(bus);
+}
+
+int mv88e6321_enable_tcam(struct mii_bus *bus, u8 id, u16 reg_frame_type)
+{
+	int ret;
+	struct tcam_entry tcam_entry;
+
+	if (!bus)
+		return -ENODEV;
+
+	memset(&tcam_entry, 0, sizeof(struct tcam_entry));
+
+	tcam_entry.op = TCAM_OP_READ;
+	tcam_entry.pg = 0;
+	tcam_entry.id = id;
+
+	ret = mv88e6321_wait_tcam_ready(bus);
+	if (ret < 0)
+		return ret;
+
+	ret = mv88e6321_write_tcam_pg_hdr(bus, &tcam_entry);
+	if (ret < 0)
+		return ret;
+
+	ret = mv88e6321_wait_tcam_ready(bus);
+	if (ret < 0)
+		return ret;
+
+	ret = mdiobus_write(bus, TCAM_ADDR, TCAM_REG_PG0_KEY_1, reg_frame_type);
+	if (ret < 0)
+		return ret;
+
+	tcam_entry.op = TCAM_OP_LOAD;
+
+	ret = mv88e6321_write_tcam_pg_hdr(bus, &tcam_entry);
+	if (ret < 0)
+		return ret;
+
+	return mv88e6321_wait_tcam_ready(bus);
+}
+
+int mv88e6321_disable_tcam(struct mii_bus *bus, u8 id)
+{
+	int ret = 0;
+	struct tcam_entry tcam_entry;
+
+	if (!bus)
+		return -ENODEV;
+
+	memset(&tcam_entry, 0, sizeof(struct tcam_entry));
+
+	tcam_entry.op = TCAM_OP_READ;
+	tcam_entry.pg = 0;
+	tcam_entry.id = id;
+
+	ret = mv88e6321_wait_tcam_ready(bus);
+	if (ret < 0)
+		return ret;
+
+	ret = mv88e6321_write_tcam_pg_hdr(bus, &tcam_entry);
+	if (ret < 0)
+		return ret;
+
+	ret = mv88e6321_wait_tcam_ready(bus);
+	if (ret < 0)
+		return ret;
+
+	ret = mdiobus_write(bus, TCAM_ADDR, TCAM_REG_PG0_KEY_1, 0xFF);
+	if (ret < 0)
+		return ret;
+
+	tcam_entry.op = TCAM_OP_LOAD;
+
+	ret = mv88e6321_write_tcam_pg_hdr(bus, &tcam_entry);
+	if (ret < 0)
+		return ret;
+
+	return mv88e6321_wait_tcam_ready(bus);
+}
+
+int mv88e6321_is_tcam_enabled(struct mii_bus *bus, u8 id)
+{
+	int ret = 0;
+	struct tcam_entry tcam_entry;
+
+	if (!bus)
+		return -ENODEV;
+
+	memset(&tcam_entry, 0, sizeof(struct tcam_entry));
+
+	tcam_entry.op = TCAM_OP_READ;
+	tcam_entry.pg = 0;
+	tcam_entry.id = id;
+
+	ret = mv88e6321_wait_tcam_ready(bus);
+	if (ret < 0)
+		return ret;
+
+	ret = mv88e6321_write_tcam_pg_hdr(bus, &tcam_entry);
+	if (ret < 0)
+		return ret;
+
+	ret = mv88e6321_wait_tcam_ready(bus);
+	if (ret < 0)
+		return ret;
+
+	ret = mdiobus_read(bus, TCAM_ADDR, TCAM_REG_PG0_KEY_1);
+	if (ret < 0)
+		return ret;
+
+	return (ret == 0xFF) ? 0 : 1;
+}
+
+int mv88e6321_get_tcam_debug(struct mii_bus *bus, u8 id)
+{
+	int ret = 0;
+	struct tcam_entry tcam_entry;
+
+	if (!bus)
+		return -ENODEV;
+
+	memset(&tcam_entry, 0, sizeof(struct tcam_entry));
+
+	tcam_entry.op = TCAM_OP_READ;
+	tcam_entry.pg = 2;
+	tcam_entry.id = id;
+
+	ret = mv88e6321_wait_tcam_ready(bus);
+	if (ret < 0)
+		return ret;
+
+	ret = mv88e6321_write_tcam_pg_hdr(bus, &tcam_entry);
+	if (ret < 0)
+		return ret;
+
+	ret = mv88e6321_wait_tcam_ready(bus);
+	if (ret < 0)
+		return ret;
+
+	return mdiobus_read(bus, TCAM_ADDR, TCAM_REG_PG2_DEBUG);
+}
diff --git a/drivers/net/dsa/mv88e6xxx/mv88e6321_tcam.h b/drivers/net/dsa/mv88e6xxx/mv88e6321_tcam.h
new file mode 100644
index 000000000000..71c4d41fd8fa
--- /dev/null
+++ b/drivers/net/dsa/mv88e6xxx/mv88e6321_tcam.h
@@ -0,0 +1,204 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+
+#ifndef __MV88E6321_TCAM__
+#define __MV88E6321_TCAM__
+
+#include <linux/mdio.h>
+#include <linux/list.h>
+#include <linux/of.h>
+
+#define TCAM_FRAME_DATA_MAX_SIZE 96
+
+struct tcam_frame_data {
+	union {
+		struct {
+			u16 data:8;
+			u16 mask:8;
+		};
+
+		u16 reg_data;
+	};
+};
+
+struct tcam_entry {
+	const char *title;
+	struct list_head list;
+	u8 orig_id;
+	u8 is_valid;
+	union {
+		struct {
+			u16 id:8;
+			u16 res0:2;
+			u16 pg:2;
+			u16 op:3;
+			u16 busy:1;
+		};
+
+		u16 reg_pg_hdr;
+	};
+
+	union {
+		struct {
+			u16 res1:6;
+			u16 frame_type:2;
+			u16 res2:6;
+			u16 mask_frame_type:2;
+		};
+
+		u16 reg_frame_type;
+	};
+
+	union {
+		struct {
+			u16 spv:7;
+			u16 res3:1;
+			u16 mask_spv:7;
+		};
+
+		u16 reg_spv;
+	};
+
+	u16 orig_pvid;
+	u8 orig_ppri;
+	u16 mask_pvid;
+	u8 mask_ppri;
+	union {
+		struct {
+			u16 pvid:4;
+			u16 ppri:4;
+			u16 mask_ppri_pvid:8;
+		};
+
+		u16 reg_ppri_pvid;
+	};
+
+	union {
+		struct {
+			u16 id_pvid:8;
+			u16 mask_id_pvid:8;
+		};
+
+		u16 reg_id_ppri;
+	};
+
+	u8 is96frame;
+	struct tcam_frame_data frame_data[TCAM_FRAME_DATA_MAX_SIZE];
+
+	union {
+		struct {
+			u16 vid:12;
+			u16 vid_override:1;
+			u16 inc_tcam_ctr:1;
+			u16 interrupt:1;
+			u16 cnt:1;
+		};
+
+		u16 reg_action_1;
+	};
+
+	union {
+		struct {
+			u16 fpri:3;
+			u16 fpri_override:1;
+			u16 qpri:2;
+			u16 res4:1;
+			u16 qpri_override:1;
+			u16 next_id:8;
+		};
+
+		u16 reg_action_2;
+	};
+
+	union {
+		struct {
+			u16 dpv:7;
+			u16 res5:4;
+			u16 dpv_override:1;
+		};
+
+		u16 reg_action_3;
+	};
+
+	union {
+		struct {
+			u16 ld_balance:3;
+			u16 ld_balance_override:1;
+			u16 action:11;
+			u16 action_override:1;
+		};
+
+		u16 reg_action_4;
+	};
+
+	union {
+		struct {
+			u16 debug_port:4;
+		};
+
+		u16 reg_debug;
+	};
+
+	union {
+		struct {
+			u16 hit_low:8;
+			u16 hit_high:8;
+		};
+
+		u16 reg_hit;
+	};
+};
+
+struct tcam_entries {
+	u8 port_mask;
+	u8 debug_port;
+
+	struct tcam_entry head;
+};
+
+#define TCAM_OP_NOP		0
+#define TCAM_OP_FLUSH_ALL	1
+#define TCAM_OP_FLUSH		2
+#define TCAM_OP_LOAD		3
+#define TCAM_OP_GET_NEXT	4
+#define TCAM_OP_READ		5
+
+#define TCAM_FT_NORM_NET	0
+#define TCAM_FT_CONTINUE	0
+#define TCAM_FT_DSA_TAG		1
+#define TCAM_FT_PROV_TAG	2
+
+#define TCAM_ADDR		0x1D
+
+#define TCAM_REG_PG_HDR		0
+
+#define TCAM_REG_PG0_KEY_1	0x2
+#define TCAM_REG_PG0_KEY_2	0x3
+#define TCAM_REG_PG0_KEY_3	0x4
+#define TCAM_REG_PG0_KEY_4	0x5
+
+#define TCAM_REG_PG2_ACTION_1	0x2
+#define TCAM_REG_PG2_ACTION_2	0x3
+#define TCAM_REG_PG2_ACTION_3	0x4
+#define TCAM_REG_PG2_ACTION_4	0x5
+#define TCAM_REG_PG2_DEBUG_PORT	0x1C
+#define TCAM_REG_PG2_DEBUG	0x1F
+
+#define TCAM_PG_HDR_BUSY	((u16)1 << 15)
+
+int of_get_tcam_entry(struct tcam_entries *tcam_entries,
+		      struct device_node *np);
+int mv88e6321_set_tcam_mode(struct mii_bus *bus,
+			    struct tcam_entries *tcam_entries);
+int mv88e6321_load_tcam(struct mii_bus *bus,
+			struct tcam_entry *tcam_entry);
+int mv88e6321_get_tcam(struct mii_bus *bus, u8 id,
+		       struct tcam_entry *tcam_entry);
+int mv88e6321_flush_tcam(struct mii_bus *bus, int id);
+int mv88e6321_enable_tcam(struct mii_bus *bus, u8 id, u16 reg_frame_type);
+int mv88e6321_disable_tcam(struct mii_bus *bus, u8 id);
+int mv88e6321_is_tcam_enabled(struct mii_bus *bus, u8 id);
+int mv88e6321_get_tcam_debug(struct mii_bus *bus, u8 id);
+int mv88e6321_tcam_to_string(struct tcam_entry *tcam_entry,
+			     char *buffer, size_t size);
+
+#endif
diff --git a/drivers/net/dsa/mv88e6xxx/port.h b/drivers/net/dsa/mv88e6xxx/port.h
index 44d76ac973f6..433cba61f8bc 100644
--- a/drivers/net/dsa/mv88e6xxx/port.h
+++ b/drivers/net/dsa/mv88e6xxx/port.h
@@ -219,7 +219,20 @@
 #define MV88E6XXX_PORT_ATU_CTL		0x0c
 
 /* Offset 0x0D: Priority Override Register */
-#define MV88E6XXX_PORT_PRI_OVERRIDE	0x0d
+#define MV88E6XXX_PORT_PRI_OVERRIDE			0x0d
+#define MV88E6XXX_PORT_PRI_OVERRIDE_DA_MASK		(0x3 << 14)
+#define MV88E6XXX_PORT_PRI_OVERRIDE_SA_MASK		(0x3 << 12)
+#define MV88E6XXX_PORT_PRI_OVERRIDE_VTU_MASK		(0x3 << 10)
+#define MV88E6XXX_PORT_PRI_OVERRIDE_MIRROR_SA_MISS	BIT(9)
+#define MV88E6XXX_PORT_PRI_OVERRIDE_MIRROR_VTU_MISS	BIT(8)
+#define MV88E6XXX_PORT_PRI_OVERRIDE_TRAP_DA_MISS	BIT(7)
+#define MV88E6XXX_PORT_PRI_OVERRIDE_TRAP_SA_MISS	BIT(6)
+#define MV88E6XXX_PORT_PRI_OVERRIDE_TRAP_VTU_MISS	BIT(5)
+#define MV88E6XXX_PORT_PRI_OVERRIDE_TRAP_TCAM_MISS	BIT(4)
+#define MV88E6XXX_PORT_PRI_OVERRIDE_TCAM_MODE_MASK	0x3
+#define MV88E6XXX_PORT_PRI_OVERRIDE_TCAM_MODE_DISABLED	0x0
+#define MV88E6XXX_PORT_PRI_OVERRIDE_TCAM_MODE_48	0x1
+#define MV88E6XXX_PORT_PRI_OVERRIDE_TCAM_MODE_96	0x2
 
 /* Offset 0x0E: Policy Control Register */
 #define MV88E6XXX_PORT_POLICY_CTL		0x0e
@@ -252,6 +265,12 @@
 /* Offset 0x13: OutFiltered Counter */
 #define MV88E6XXX_PORT_OUT_FILTERED	0x13
 
+/* Offset 0x16: LED Control */
+#define MV88E6XXX_PORT_LED_CTL				0x16
+#define MV88E6XXX_PORT_LED_CTL_UPDATE			0x8000
+#define MV88E6XXX_PORT_LED_CTL_LED0_SEL_10XX_CONT	0x0008
+#define MV88E6XXX_PORT_LED_CTL_LED1_SEL_ACT		0x0080
+
 /* Offset 0x18: IEEE Priority Mapping Table */
 #define MV88E6390_PORT_IEEE_PRIO_MAP_TABLE			0x18
 #define MV88E6390_PORT_IEEE_PRIO_MAP_TABLE_UPDATE		0x8000
diff --git a/drivers/net/ethernet/chelsio/cxgb/common.h b/drivers/net/ethernet/chelsio/cxgb/common.h
index 6475060649e9..0321be77366c 100644
--- a/drivers/net/ethernet/chelsio/cxgb/common.h
+++ b/drivers/net/ethernet/chelsio/cxgb/common.h
@@ -238,7 +238,6 @@ struct adapter {
 	int msg_enable;
 	u32 mmio_len;
 
-	struct work_struct ext_intr_handler_task;
 	struct adapter_params params;
 
 	/* Terminator modules. */
@@ -257,6 +256,7 @@ struct adapter {
 
 	/* guards async operations */
 	spinlock_t async_lock ____cacheline_aligned;
+	u32 pending_thread_intr;
 	u32 slow_intr_mask;
 	int t1powersave;
 };
@@ -334,8 +334,7 @@ void t1_interrupts_enable(adapter_t *adapter);
 void t1_interrupts_disable(adapter_t *adapter);
 void t1_interrupts_clear(adapter_t *adapter);
 int t1_elmer0_ext_intr_handler(adapter_t *adapter);
-void t1_elmer0_ext_intr(adapter_t *adapter);
-int t1_slow_intr_handler(adapter_t *adapter);
+irqreturn_t t1_slow_intr_handler(adapter_t *adapter);
 
 int t1_link_start(struct cphy *phy, struct cmac *mac, struct link_config *lc);
 const struct board_info *t1_get_board_info(unsigned int board_id);
@@ -347,7 +346,6 @@ int t1_get_board_rev(adapter_t *adapter, const struct board_info *bi,
 int t1_init_hw_modules(adapter_t *adapter);
 int t1_init_sw_modules(adapter_t *adapter, const struct board_info *bi);
 void t1_free_sw_modules(adapter_t *adapter);
-void t1_fatal_err(adapter_t *adapter);
 void t1_link_changed(adapter_t *adapter, int port_id);
 void t1_link_negotiated(adapter_t *adapter, int port_id, int link_stat,
 			    int speed, int duplex, int pause);
diff --git a/drivers/net/ethernet/chelsio/cxgb/cxgb2.c b/drivers/net/ethernet/chelsio/cxgb/cxgb2.c
index 0e4a0f413960..512da98019c6 100644
--- a/drivers/net/ethernet/chelsio/cxgb/cxgb2.c
+++ b/drivers/net/ethernet/chelsio/cxgb/cxgb2.c
@@ -211,9 +211,10 @@ static int cxgb_up(struct adapter *adapter)
 	t1_interrupts_clear(adapter);
 
 	adapter->params.has_msi = !disable_msi && !pci_enable_msi(adapter->pdev);
-	err = request_irq(adapter->pdev->irq, t1_interrupt,
-			  adapter->params.has_msi ? 0 : IRQF_SHARED,
-			  adapter->name, adapter);
+	err = request_threaded_irq(adapter->pdev->irq, t1_interrupt,
+				   t1_interrupt_thread,
+				   adapter->params.has_msi ? 0 : IRQF_SHARED,
+				   adapter->name, adapter);
 	if (err) {
 		if (adapter->params.has_msi)
 			pci_disable_msi(adapter->pdev);
@@ -916,51 +917,6 @@ static void mac_stats_task(struct work_struct *work)
 	spin_unlock(&adapter->work_lock);
 }
 
-/*
- * Processes elmer0 external interrupts in process context.
- */
-static void ext_intr_task(struct work_struct *work)
-{
-	struct adapter *adapter =
-		container_of(work, struct adapter, ext_intr_handler_task);
-
-	t1_elmer0_ext_intr_handler(adapter);
-
-	/* Now reenable external interrupts */
-	spin_lock_irq(&adapter->async_lock);
-	adapter->slow_intr_mask |= F_PL_INTR_EXT;
-	writel(F_PL_INTR_EXT, adapter->regs + A_PL_CAUSE);
-	writel(adapter->slow_intr_mask | F_PL_INTR_SGE_DATA,
-		   adapter->regs + A_PL_ENABLE);
-	spin_unlock_irq(&adapter->async_lock);
-}
-
-/*
- * Interrupt-context handler for elmer0 external interrupts.
- */
-void t1_elmer0_ext_intr(struct adapter *adapter)
-{
-	/*
-	 * Schedule a task to handle external interrupts as we require
-	 * a process context.  We disable EXT interrupts in the interim
-	 * and let the task reenable them when it's done.
-	 */
-	adapter->slow_intr_mask &= ~F_PL_INTR_EXT;
-	writel(adapter->slow_intr_mask | F_PL_INTR_SGE_DATA,
-		   adapter->regs + A_PL_ENABLE);
-	schedule_work(&adapter->ext_intr_handler_task);
-}
-
-void t1_fatal_err(struct adapter *adapter)
-{
-	if (adapter->flags & FULL_INIT_DONE) {
-		t1_sge_stop(adapter->sge);
-		t1_interrupts_disable(adapter);
-	}
-	pr_alert("%s: encountered fatal error, operation suspended\n",
-		 adapter->name);
-}
-
 static const struct net_device_ops cxgb_netdev_ops = {
 	.ndo_open		= cxgb_open,
 	.ndo_stop		= cxgb_close,
@@ -1062,8 +1018,6 @@ static int init_one(struct pci_dev *pdev, const struct pci_device_id *ent)
 			spin_lock_init(&adapter->async_lock);
 			spin_lock_init(&adapter->mac_lock);
 
-			INIT_WORK(&adapter->ext_intr_handler_task,
-				  ext_intr_task);
 			INIT_DELAYED_WORK(&adapter->stats_update_task,
 					  mac_stats_task);
 
diff --git a/drivers/net/ethernet/chelsio/cxgb/sge.c b/drivers/net/ethernet/chelsio/cxgb/sge.c
index 2d9c2b5a690a..cda01f22c71c 100644
--- a/drivers/net/ethernet/chelsio/cxgb/sge.c
+++ b/drivers/net/ethernet/chelsio/cxgb/sge.c
@@ -940,10 +940,11 @@ void t1_sge_intr_clear(struct sge *sge)
 /*
  * SGE 'Error' interrupt handler
  */
-int t1_sge_intr_error_handler(struct sge *sge)
+bool t1_sge_intr_error_handler(struct sge *sge)
 {
 	struct adapter *adapter = sge->adapter;
 	u32 cause = readl(adapter->regs + A_SG_INT_CAUSE);
+	bool wake = false;
 
 	if (adapter->port[0].dev->hw_features & NETIF_F_TSO)
 		cause &= ~F_PACKET_TOO_BIG;
@@ -967,11 +968,14 @@ int t1_sge_intr_error_handler(struct sge *sge)
 		sge->stats.pkt_mismatch++;
 		pr_alert("%s: SGE packet mismatch\n", adapter->name);
 	}
-	if (cause & SGE_INT_FATAL)
-		t1_fatal_err(adapter);
+	if (cause & SGE_INT_FATAL) {
+		t1_interrupts_disable(adapter);
+		adapter->pending_thread_intr |= F_PL_INTR_SGE_ERR;
+		wake = true;
+	}
 
 	writel(cause, adapter->regs + A_SG_INT_CAUSE);
-	return 0;
+	return wake;
 }
 
 const struct sge_intr_counts *t1_sge_get_intr_counts(const struct sge *sge)
@@ -1619,11 +1623,46 @@ int t1_poll(struct napi_struct *napi, int budget)
 	return work_done;
 }
 
+irqreturn_t t1_interrupt_thread(int irq, void *data)
+{
+	struct adapter *adapter = data;
+	u32 pending_thread_intr;
+
+	spin_lock_irq(&adapter->async_lock);
+	pending_thread_intr = adapter->pending_thread_intr;
+	adapter->pending_thread_intr = 0;
+	spin_unlock_irq(&adapter->async_lock);
+
+	if (!pending_thread_intr)
+		return IRQ_NONE;
+
+	if (pending_thread_intr & F_PL_INTR_EXT)
+		t1_elmer0_ext_intr_handler(adapter);
+
+	/* This error is fatal, interrupts remain off */
+	if (pending_thread_intr & F_PL_INTR_SGE_ERR) {
+		pr_alert("%s: encountered fatal error, operation suspended\n",
+			 adapter->name);
+		t1_sge_stop(adapter->sge);
+		return IRQ_HANDLED;
+	}
+
+	spin_lock_irq(&adapter->async_lock);
+	adapter->slow_intr_mask |= F_PL_INTR_EXT;
+
+	writel(F_PL_INTR_EXT, adapter->regs + A_PL_CAUSE);
+	writel(adapter->slow_intr_mask | F_PL_INTR_SGE_DATA,
+	       adapter->regs + A_PL_ENABLE);
+	spin_unlock_irq(&adapter->async_lock);
+
+	return IRQ_HANDLED;
+}
+
 irqreturn_t t1_interrupt(int irq, void *data)
 {
 	struct adapter *adapter = data;
 	struct sge *sge = adapter->sge;
-	int handled;
+	irqreturn_t handled;
 
 	if (likely(responses_pending(adapter))) {
 		writel(F_PL_INTR_SGE_DATA, adapter->regs + A_PL_CAUSE);
@@ -1645,10 +1684,10 @@ irqreturn_t t1_interrupt(int irq, void *data)
 	handled = t1_slow_intr_handler(adapter);
 	spin_unlock(&adapter->async_lock);
 
-	if (!handled)
+	if (handled == IRQ_NONE)
 		sge->stats.unhandled_irqs++;
 
-	return IRQ_RETVAL(handled != 0);
+	return handled;
 }
 
 /*
diff --git a/drivers/net/ethernet/chelsio/cxgb/sge.h b/drivers/net/ethernet/chelsio/cxgb/sge.h
index a1ba591b3431..716705b96f26 100644
--- a/drivers/net/ethernet/chelsio/cxgb/sge.h
+++ b/drivers/net/ethernet/chelsio/cxgb/sge.h
@@ -74,6 +74,7 @@ struct sge *t1_sge_create(struct adapter *, struct sge_params *);
 int t1_sge_configure(struct sge *, struct sge_params *);
 int t1_sge_set_coalesce_params(struct sge *, struct sge_params *);
 void t1_sge_destroy(struct sge *);
+irqreturn_t t1_interrupt_thread(int irq, void *data);
 irqreturn_t t1_interrupt(int irq, void *cookie);
 int t1_poll(struct napi_struct *, int);
 
@@ -81,7 +82,7 @@ netdev_tx_t t1_start_xmit(struct sk_buff *skb, struct net_device *dev);
 void t1_vlan_mode(struct adapter *adapter, netdev_features_t features);
 void t1_sge_start(struct sge *);
 void t1_sge_stop(struct sge *);
-int t1_sge_intr_error_handler(struct sge *);
+bool t1_sge_intr_error_handler(struct sge *sge);
 void t1_sge_intr_enable(struct sge *);
 void t1_sge_intr_disable(struct sge *);
 void t1_sge_intr_clear(struct sge *);
diff --git a/drivers/net/ethernet/chelsio/cxgb/subr.c b/drivers/net/ethernet/chelsio/cxgb/subr.c
index ea0f8741d7cf..310add28fcf5 100644
--- a/drivers/net/ethernet/chelsio/cxgb/subr.c
+++ b/drivers/net/ethernet/chelsio/cxgb/subr.c
@@ -170,7 +170,7 @@ void t1_link_changed(adapter_t *adapter, int port_id)
 	t1_link_negotiated(adapter, port_id, link_ok, speed, duplex, fc);
 }
 
-static int t1_pci_intr_handler(adapter_t *adapter)
+static bool t1_pci_intr_handler(adapter_t *adapter)
 {
 	u32 pcix_cause;
 
@@ -179,9 +179,13 @@ static int t1_pci_intr_handler(adapter_t *adapter)
 	if (pcix_cause) {
 		pci_write_config_dword(adapter->pdev, A_PCICFG_INTR_CAUSE,
 				       pcix_cause);
-		t1_fatal_err(adapter);    /* PCI errors are fatal */
+		/* PCI errors are fatal */
+		t1_interrupts_disable(adapter);
+		adapter->pending_thread_intr |= F_PL_INTR_SGE_ERR;
+		pr_alert("%s: PCI error encountered.\n", adapter->name);
+		return true;
 	}
-	return 0;
+	return false;
 }
 
 #ifdef CONFIG_CHELSIO_T1_1G
@@ -210,13 +214,16 @@ static int fpga_phy_intr_handler(adapter_t *adapter)
 /*
  * Slow path interrupt handler for FPGAs.
  */
-static int fpga_slow_intr(adapter_t *adapter)
+static irqreturn_t fpga_slow_intr(adapter_t *adapter)
 {
 	u32 cause = readl(adapter->regs + A_PL_CAUSE);
+	irqreturn_t ret = IRQ_NONE;
 
 	cause &= ~F_PL_INTR_SGE_DATA;
-	if (cause & F_PL_INTR_SGE_ERR)
-		t1_sge_intr_error_handler(adapter->sge);
+	if (cause & F_PL_INTR_SGE_ERR) {
+		if (t1_sge_intr_error_handler(adapter->sge))
+			ret = IRQ_WAKE_THREAD;
+	}
 
 	if (cause & FPGA_PCIX_INTERRUPT_GMAC)
 		fpga_phy_intr_handler(adapter);
@@ -231,14 +238,19 @@ static int fpga_slow_intr(adapter_t *adapter)
 		/* Clear TP interrupt */
 		writel(tp_cause, adapter->regs + FPGA_TP_ADDR_INTERRUPT_CAUSE);
 	}
-	if (cause & FPGA_PCIX_INTERRUPT_PCIX)
-		t1_pci_intr_handler(adapter);
+	if (cause & FPGA_PCIX_INTERRUPT_PCIX) {
+		if (t1_pci_intr_handler(adapter))
+			ret = IRQ_WAKE_THREAD;
+	}
 
 	/* Clear the interrupts just processed. */
 	if (cause)
 		writel(cause, adapter->regs + A_PL_CAUSE);
 
-	return cause != 0;
+	if (ret != IRQ_NONE)
+		return ret;
+
+	return cause == 0 ? IRQ_NONE : IRQ_HANDLED;
 }
 #endif
 
@@ -842,31 +854,45 @@ void t1_interrupts_clear(adapter_t* adapter)
 /*
  * Slow path interrupt handler for ASICs.
  */
-static int asic_slow_intr(adapter_t *adapter)
+static irqreturn_t asic_slow_intr(adapter_t *adapter)
 {
 	u32 cause = readl(adapter->regs + A_PL_CAUSE);
+	irqreturn_t ret = IRQ_HANDLED;
 
 	cause &= adapter->slow_intr_mask;
 	if (!cause)
-		return 0;
-	if (cause & F_PL_INTR_SGE_ERR)
-		t1_sge_intr_error_handler(adapter->sge);
+		return IRQ_NONE;
+	if (cause & F_PL_INTR_SGE_ERR) {
+		if (t1_sge_intr_error_handler(adapter->sge))
+			ret = IRQ_WAKE_THREAD;
+	}
 	if (cause & F_PL_INTR_TP)
 		t1_tp_intr_handler(adapter->tp);
 	if (cause & F_PL_INTR_ESPI)
 		t1_espi_intr_handler(adapter->espi);
-	if (cause & F_PL_INTR_PCIX)
-		t1_pci_intr_handler(adapter);
-	if (cause & F_PL_INTR_EXT)
-		t1_elmer0_ext_intr(adapter);
+	if (cause & F_PL_INTR_PCIX) {
+		if (t1_pci_intr_handler(adapter))
+			ret = IRQ_WAKE_THREAD;
+	}
+	if (cause & F_PL_INTR_EXT) {
+		/* Wake the threaded interrupt to handle external interrupts as
+		 * we require a process context. We disable EXT interrupts in
+		 * the interim and let the thread reenable them when it's done.
+		 */
+		adapter->pending_thread_intr |= F_PL_INTR_EXT;
+		adapter->slow_intr_mask &= ~F_PL_INTR_EXT;
+		writel(adapter->slow_intr_mask | F_PL_INTR_SGE_DATA,
+		       adapter->regs + A_PL_ENABLE);
+		ret = IRQ_WAKE_THREAD;
+	}
 
 	/* Clear the interrupts just processed. */
 	writel(cause, adapter->regs + A_PL_CAUSE);
 	readl(adapter->regs + A_PL_CAUSE); /* flush writes */
-	return 1;
+	return ret;
 }
 
-int t1_slow_intr_handler(adapter_t *adapter)
+irqreturn_t t1_slow_intr_handler(adapter_t *adapter)
 {
 #ifdef CONFIG_CHELSIO_T1_1G
 	if (!t1_is_asic(adapter))
diff --git a/drivers/net/ethernet/dlink/sundance.c b/drivers/net/ethernet/dlink/sundance.c
index e3a8858915b3..df0eab479d51 100644
--- a/drivers/net/ethernet/dlink/sundance.c
+++ b/drivers/net/ethernet/dlink/sundance.c
@@ -963,7 +963,7 @@ static void tx_timeout(struct net_device *dev, unsigned int txqueue)
 	unsigned long flag;
 
 	netif_stop_queue(dev);
-	tasklet_disable(&np->tx_tasklet);
+	tasklet_disable_in_atomic(&np->tx_tasklet);
 	iowrite16(0, ioaddr + IntrEnable);
 	printk(KERN_WARNING "%s: Transmit timed out, TxStatus %2.2x "
 		   "TxFrameId %2.2x,"
diff --git a/drivers/net/ethernet/jme.c b/drivers/net/ethernet/jme.c
index e9efe074edc1..f1b9284e0bea 100644
--- a/drivers/net/ethernet/jme.c
+++ b/drivers/net/ethernet/jme.c
@@ -1265,9 +1265,9 @@ jme_stop_shutdown_timer(struct jme_adapter *jme)
 	jwrite32f(jme, JME_APMC, apmc);
 }
 
-static void jme_link_change_tasklet(struct tasklet_struct *t)
+static void jme_link_change_work(struct work_struct *work)
 {
-	struct jme_adapter *jme = from_tasklet(jme, t, linkch_task);
+	struct jme_adapter *jme = container_of(work, struct jme_adapter, linkch_task);
 	struct net_device *netdev = jme->dev;
 	int rc;
 
@@ -1510,7 +1510,7 @@ jme_intr_msi(struct jme_adapter *jme, u32 intrstat)
 		 * all other events are ignored
 		 */
 		jwrite32(jme, JME_IEVE, intrstat);
-		tasklet_schedule(&jme->linkch_task);
+		schedule_work(&jme->linkch_task);
 		goto out_reenable;
 	}
 
@@ -1832,7 +1832,6 @@ jme_open(struct net_device *netdev)
 	jme_clear_pm_disable_wol(jme);
 	JME_NAPI_ENABLE(jme);
 
-	tasklet_setup(&jme->linkch_task, jme_link_change_tasklet);
 	tasklet_setup(&jme->txclean_task, jme_tx_clean_tasklet);
 	tasklet_setup(&jme->rxclean_task, jme_rx_clean_tasklet);
 	tasklet_setup(&jme->rxempty_task, jme_rx_empty_tasklet);
@@ -1920,7 +1919,7 @@ jme_close(struct net_device *netdev)
 
 	JME_NAPI_DISABLE(jme);
 
-	tasklet_kill(&jme->linkch_task);
+	cancel_work_sync(&jme->linkch_task);
 	tasklet_kill(&jme->txclean_task);
 	tasklet_kill(&jme->rxclean_task);
 	tasklet_kill(&jme->rxempty_task);
@@ -3035,6 +3034,7 @@ jme_init_one(struct pci_dev *pdev,
 	atomic_set(&jme->rx_empty, 1);
 
 	tasklet_setup(&jme->pcc_task, jme_pcc_tasklet);
+	INIT_WORK(&jme->linkch_task, jme_link_change_work);
 	jme->dpi.cur = PCC_P1;
 
 	jme->reg_ghc = 0;
diff --git a/drivers/net/ethernet/jme.h b/drivers/net/ethernet/jme.h
index a2c3b00d939d..2af76329b4a2 100644
--- a/drivers/net/ethernet/jme.h
+++ b/drivers/net/ethernet/jme.h
@@ -411,7 +411,7 @@ struct jme_adapter {
 	struct tasklet_struct	rxempty_task;
 	struct tasklet_struct	rxclean_task;
 	struct tasklet_struct	txclean_task;
-	struct tasklet_struct	linkch_task;
+	struct work_struct	linkch_task;
 	struct tasklet_struct	pcc_task;
 	unsigned long		flags;
 	u32			reg_txcs;
diff --git a/drivers/net/ethernet/ti/cpsw.c b/drivers/net/ethernet/ti/cpsw.c
index b0f00b4edd94..0205de2bc4d9 100644
--- a/drivers/net/ethernet/ti/cpsw.c
+++ b/drivers/net/ethernet/ti/cpsw.c
@@ -135,9 +135,9 @@ static void cpsw_set_promiscious(struct net_device *ndev, bool enable)
 						     ALE_PORT_NO_SA_UPDATE, 1);
 			}
 
-			/* Clear All Untouched entries */
-			cpsw_ale_control_set(ale, 0, ALE_AGEOUT, 1);
 			do {
+				/* Clear All Untouched entries */
+				cpsw_ale_control_set(ale, 0, ALE_AGEOUT, 1);
 				cpu_relax();
 				if (cpsw_ale_control_get(ale, 0, ALE_AGEOUT))
 					break;
@@ -150,10 +150,12 @@ static void cpsw_set_promiscious(struct net_device *ndev, bool enable)
 
 			/* Flood All Unicast Packets to Host port */
 			cpsw_ale_control_set(ale, 0, ALE_P0_UNI_FLOOD, 1);
+			cpsw_ale_control_set(ale, 0, ALE_BYPASS, 1);
 			dev_dbg(&ndev->dev, "promiscuity enabled\n");
 		} else {
 			/* Don't Flood All Unicast Packets to Host port */
 			cpsw_ale_control_set(ale, 0, ALE_P0_UNI_FLOOD, 0);
+			cpsw_ale_control_set(ale, 0, ALE_BYPASS, 0);
 
 			/* Enable Learn for all ports (host is port 0 and slaves are port 1 and up */
 			for (i = 0; i <= cpsw->data.slaves; i++) {
@@ -311,6 +313,11 @@ static void cpsw_ndo_set_rx_mode(struct net_device *ndev)
 	struct cpsw_common *cpsw = priv->cpsw;
 	int slave_port = -1;
 
+	if (priv->netdev_flags == ndev->flags)
+		return;
+
+	priv->netdev_flags = ndev->flags;
+
 	if (cpsw->data.dual_emac)
 		slave_port = priv->emac_port + 1;
 
diff --git a/drivers/net/ethernet/ti/cpsw_priv.h b/drivers/net/ethernet/ti/cpsw_priv.h
index 7b7f3596b20d..0b0d0ebf867b 100644
--- a/drivers/net/ethernet/ti/cpsw_priv.h
+++ b/drivers/net/ethernet/ti/cpsw_priv.h
@@ -89,10 +89,18 @@ do {								\
 
 #define CPSW_POLL_WEIGHT	64
 #define CPSW_RX_VLAN_ENCAP_HDR_SIZE		4
+
+#ifdef CONFIG_TI_CPSW_SWITCHDEV
 #define CPSW_MIN_PACKET_SIZE	(VLAN_ETH_ZLEN)
 #define CPSW_MAX_PACKET_SIZE	(VLAN_ETH_FRAME_LEN +\
 				 ETH_FCS_LEN +\
 				 CPSW_RX_VLAN_ENCAP_HDR_SIZE)
+#else
+#define CPSW_MIN_PACKET_SIZE	(ETH_ZLEN)
+#define CPSW_MAX_PACKET_SIZE	(VLAN_ETH_FRAME_LEN +\
+				 ETH_FCS_LEN +\
+				 CPSW_RX_VLAN_ENCAP_HDR_SIZE)
+#endif
 
 #define RX_PRIORITY_MAPPING	0x76543210
 #define TX_PRIORITY_MAPPING	0x33221100
@@ -380,6 +388,7 @@ struct cpsw_priv {
 	u32 emac_port;
 	struct cpsw_common *cpsw;
 	int offload_fwd_mark;
+	int netdev_flags;
 };
 
 #define ndev_to_cpsw(ndev) (((struct cpsw_priv *)netdev_priv(ndev))->cpsw)
diff --git a/drivers/net/ethernet/ti/davinci_emac.c b/drivers/net/ethernet/ti/davinci_emac.c
index c7031e1960d4..8398a490290b 100644
--- a/drivers/net/ethernet/ti/davinci_emac.c
+++ b/drivers/net/ethernet/ti/davinci_emac.c
@@ -1894,16 +1894,24 @@ static int davinci_emac_probe(struct platform_device *pdev)
 	}
 	ndev->irq = res->start;
 
-	rc = davinci_emac_try_get_mac(pdev, res_ctrl ? 0 : 1, priv->mac_addr);
-	if (!rc)
-		ether_addr_copy(ndev->dev_addr, priv->mac_addr);
 
+	/* if no mac addr was passed first try to get it from HW,
+	 * if it fails get a random addr
+	 */
 	if (!is_valid_ether_addr(priv->mac_addr)) {
-		/* Use random MAC if still none obtained. */
-		eth_hw_addr_random(ndev);
-		memcpy(priv->mac_addr, ndev->dev_addr, ndev->addr_len);
-		dev_warn(&pdev->dev, "using random MAC addr: %pM\n",
-			 priv->mac_addr);
+		rc = davinci_emac_try_get_mac(pdev, res_ctrl ? 0 : 1,
+								priv->mac_addr);
+		if (rc || !is_valid_ether_addr(priv->mac_addr)) {
+			/* Use random MAC if none passed */
+			eth_hw_addr_random(ndev);
+			memcpy(priv->mac_addr, ndev->dev_addr, ndev->addr_len);
+			dev_warn(&pdev->dev,
+				 "using random MAC addr: %pM\n",
+				 priv->mac_addr);
+		} else {
+			/* set MAC address from HW */
+			ether_addr_copy(ndev->dev_addr, priv->mac_addr);
+		}
 	}
 
 	ndev->netdev_ops = &emac_netdev_ops;
diff --git a/drivers/net/mdio/Kconfig b/drivers/net/mdio/Kconfig
index a10cc460d7cf..a155e764a94a 100644
--- a/drivers/net/mdio/Kconfig
+++ b/drivers/net/mdio/Kconfig
@@ -69,6 +69,16 @@ config MDIO_BITBANG
 
 	  If in doubt, say N.
 
+if MDIO_BITBANG
+	config MICREL_KSZ8863_EXTENDED_REGS
+		bool "Enable ksz8863 extended reg access"
+		default y
+		help
+		  This config enables special treatment for ksz8863
+		  extended register access
+
+endif
+
 config MDIO_BCM_IPROC
 	tristate "Broadcom iProc MDIO bus controller"
 	depends on ARCH_BCM_IPROC || COMPILE_TEST
diff --git a/drivers/net/mdio/mdio-bitbang.c b/drivers/net/mdio/mdio-bitbang.c
index 5136275c8e73..8dd4c4c9ec77 100644
--- a/drivers/net/mdio/mdio-bitbang.c
+++ b/drivers/net/mdio/mdio-bitbang.c
@@ -22,11 +22,18 @@
 #define MDIO_READ 2
 #define MDIO_WRITE 1
 
-#define MDIO_C45 (1<<15)
+#define MDIO_C45 BIT(15)
 #define MDIO_C45_ADDR (MDIO_C45 | 0)
 #define MDIO_C45_READ (MDIO_C45 | 3)
 #define MDIO_C45_WRITE (MDIO_C45 | 1)
 
+#ifdef CONFIG_MICREL_KSZ8863_EXTENDED_REGS
+/* Special treatment for ksz8863 SMI access */
+#define MDIO_KSZ BIT(14)
+#define MDIO_KSZ_WR_BIT BIT(3)
+#define MDIO_KSZ_READ (MDIO_KSZ | 0)
+#define MDIO_KSZ_WRITE (MDIO_KSZ | MDIO_KSZ_WR_BIT)
+#endif
 #define MDIO_SETUP_TIME 10
 #define MDIO_HOLD_TIME 10
 
@@ -121,6 +128,20 @@ static void mdiobb_cmd(struct mdiobb_ctrl *ctrl, int op, u8 phy, u8 reg)
 	mdiobb_send_bit(ctrl, (op >> 1) & 1);
 	mdiobb_send_bit(ctrl, (op >> 0) & 1);
 
+#ifdef CONFIG_MICREL_KSZ8863_EXTENDED_REGS
+	/* Special treatment for ksz8863 SMI access */
+	if (op & MDIO_KSZ) {
+		/* SMI Frame Format:
+		 *       Preamble | SB | OP |  PHY  |  REG  | TA | Data Bits [15:0] | Z
+		 *    R:   32x1   | 01 | 00 | 1xRRR | RRRRR | Z0 | 00000000DDDDDDDD | Z
+		 *    W:   32x1   | 01 | 00 | 0xRRR | RRRRR | 10 | XXXXXXXXDDDDDDDD | Z
+		 */
+		phy = ((reg >> 5) & 0x7);
+		if (!(op & MDIO_KSZ_WR_BIT))
+			phy |= BIT(4);
+		reg &= 0x1f;
+	}
+#endif
 	mdiobb_send_num(ctrl, phy, 5);
 	mdiobb_send_num(ctrl, reg, 5);
 }
@@ -157,6 +178,10 @@ static int mdiobb_read(struct mii_bus *bus, int phy, int reg)
 	if (reg & MII_ADDR_C45) {
 		reg = mdiobb_cmd_addr(ctrl, phy, reg);
 		mdiobb_cmd(ctrl, MDIO_C45_READ, phy, reg);
+#ifdef CONFIG_MICREL_KSZ8863_EXTENDED_REGS
+	} else if (reg & MII_ADDR_KSZ) {
+		mdiobb_cmd(ctrl, MDIO_KSZ_READ, phy, reg);
+#endif
 	} else
 		mdiobb_cmd(ctrl, MDIO_READ, phy, reg);
 
@@ -178,6 +203,7 @@ static int mdiobb_read(struct mii_bus *bus, int phy, int reg)
 
 	ret = mdiobb_get_num(ctrl, 16);
 	mdiobb_get_bit(ctrl);
+
 	return ret;
 }
 
@@ -188,6 +214,10 @@ static int mdiobb_write(struct mii_bus *bus, int phy, int reg, u16 val)
 	if (reg & MII_ADDR_C45) {
 		reg = mdiobb_cmd_addr(ctrl, phy, reg);
 		mdiobb_cmd(ctrl, MDIO_C45_WRITE, phy, reg);
+#ifdef CONFIG_MICREL_KSZ8863_EXTENDED_REGS
+	} else if (reg & MII_ADDR_KSZ) {
+		mdiobb_cmd(ctrl, MDIO_KSZ_WRITE, phy, reg);
+#endif
 	} else
 		mdiobb_cmd(ctrl, MDIO_WRITE, phy, reg);
 
diff --git a/drivers/net/phy/Kconfig b/drivers/net/phy/Kconfig
index 698bea312adc..f5a0399cd1fc 100644
--- a/drivers/net/phy/Kconfig
+++ b/drivers/net/phy/Kconfig
@@ -25,6 +25,13 @@ menuconfig PHYLIB
 
 if PHYLIB
 
+config SWCONFIG
+	tristate "Switch configuration API"
+	help
+	  Switch configuration API using netlink. This allows
+	  you to configure the VLAN features of certain switches.
+
+
 config SWPHY
 	bool
 
@@ -319,3 +326,11 @@ endif # PHYLIB
 config MICREL_KS8995MA
 	tristate "Micrel KS8995MA 5-ports 10/100 managed Ethernet switch"
 	depends on SPI
+
+config SWCFG_KSZ8863
+	tristate "KSZ8863 SWCONFIG Configuration Interface via netlink"
+	depends on SWCONFIG
+
+config SWCFG_MV88E6321
+	tristate "MV88E6321 SWCONFIG Configuration Interface via netlink"
+	depends on SWCONFIG
diff --git a/drivers/net/phy/Makefile b/drivers/net/phy/Makefile
index a13e402074cf..9418c9c9b1fe 100644
--- a/drivers/net/phy/Makefile
+++ b/drivers/net/phy/Makefile
@@ -80,3 +80,7 @@ obj-$(CONFIG_STE10XP)		+= ste10Xp.o
 obj-$(CONFIG_TERANETICS_PHY)	+= teranetics.o
 obj-$(CONFIG_VITESSE_PHY)	+= vitesse.o
 obj-$(CONFIG_XILINX_GMII2RGMII) += xilinx_gmii2rgmii.o
+
+obj-$(CONFIG_SWCONFIG)		+= swconfig.o
+obj-$(CONFIG_SWCFG_KSZ8863)	+= ksz8863-cfg.o
+obj-$(CONFIG_SWCFG_MV88E6321)	+= mv88e6321-cfg.o
diff --git a/drivers/net/phy/intel-xway.c b/drivers/net/phy/intel-xway.c
index b7875b36097f..b963e531bd73 100644
--- a/drivers/net/phy/intel-xway.c
+++ b/drivers/net/phy/intel-xway.c
@@ -148,8 +148,6 @@
 static int xway_gphy_config_init(struct phy_device *phydev)
 {
 	int err;
-	u32 ledxh;
-	u32 ledxl;
 
 	/* Mask all interrupts */
 	err = phy_write(phydev, XWAY_MDIO_IMASK, 0);
@@ -160,28 +158,29 @@ static int xway_gphy_config_init(struct phy_device *phydev)
 	phy_read(phydev, XWAY_MDIO_ISTAT);
 
 	phy_write_mmd(phydev, MDIO_MMD_VEND2, XWAY_MMD_LEDCH,
-		      XWAY_MMD_LEDCH_NACS_NONE |
-		      XWAY_MMD_LEDCH_SBF_F02HZ |
+		      XWAY_MMD_LEDCH_NACS_NONE | XWAY_MMD_LEDCH_SBF_F02HZ |
 		      XWAY_MMD_LEDCH_FBF_F16HZ);
 	phy_write_mmd(phydev, MDIO_MMD_VEND2, XWAY_MMD_LEDCL,
-		      XWAY_MMD_LEDCH_CBLINK_NONE |
-		      XWAY_MMD_LEDCH_SCAN_NONE);
+		      XWAY_MMD_LEDCH_CBLINK_NONE | XWAY_MMD_LEDCH_SCAN_NONE);
 
-	/**
-	 * In most cases only one LED is connected to this phy, so
-	 * configure them all to constant on and pulse mode. LED3 is
-	 * only available in some packages, leave it in its reset
-	 * configuration.
-	 */
-	ledxh = XWAY_MMD_LEDxH_BLINKF_NONE | XWAY_MMD_LEDxH_CON_LINK10XX;
-	ledxl = XWAY_MMD_LEDxL_PULSE_TXACT | XWAY_MMD_LEDxL_PULSE_RXACT |
-		XWAY_MMD_LEDxL_BLINKS_NONE;
-	phy_write_mmd(phydev, MDIO_MMD_VEND2, XWAY_MMD_LED0H, ledxh);
-	phy_write_mmd(phydev, MDIO_MMD_VEND2, XWAY_MMD_LED0L, ledxl);
-	phy_write_mmd(phydev, MDIO_MMD_VEND2, XWAY_MMD_LED1H, ledxh);
-	phy_write_mmd(phydev, MDIO_MMD_VEND2, XWAY_MMD_LED1L, ledxl);
-	phy_write_mmd(phydev, MDIO_MMD_VEND2, XWAY_MMD_LED2H, ledxh);
-	phy_write_mmd(phydev, MDIO_MMD_VEND2, XWAY_MMD_LED2L, ledxl);
+	// LNK GREEN
+	phy_write_mmd(phydev, MDIO_MMD_VEND2, XWAY_MMD_LED0H,
+		      XWAY_MMD_LEDxH_BLINKF_NONE | XWAY_MMD_LEDxH_CON_LINK10XX);
+	phy_write_mmd(phydev, MDIO_MMD_VEND2, XWAY_MMD_LED0L,
+		      XWAY_MMD_LEDxL_PULSE_NONE | XWAY_MMD_LEDxL_BLINKS_NONE);
+
+	// LNK ORANGE
+	phy_write_mmd(phydev, MDIO_MMD_VEND2, XWAY_MMD_LED1H,
+		      XWAY_MMD_LEDxH_BLINKF_NONE | XWAY_MMD_LEDxH_CON_LINK1000);
+	phy_write_mmd(phydev, MDIO_MMD_VEND2, XWAY_MMD_LED1L,
+		      XWAY_MMD_LEDxL_PULSE_NONE | XWAY_MMD_LEDxL_BLINKS_NONE);
+
+	// ACT
+	phy_write_mmd(phydev, MDIO_MMD_VEND2, XWAY_MMD_LED2H,
+		      XWAY_MMD_LEDxH_BLINKF_NONE | XWAY_MMD_LEDxH_CON_NONE);
+	phy_write_mmd(phydev, MDIO_MMD_VEND2, XWAY_MMD_LED2L,
+		      XWAY_MMD_LEDxL_PULSE_TXACT | XWAY_MMD_LEDxL_PULSE_RXACT |
+		      XWAY_MMD_LEDxL_BLINKS_NONE);
 
 	return 0;
 }
@@ -201,6 +200,20 @@ static int xway_gphy14_config_aneg(struct phy_device *phydev)
 	return genphy_config_aneg(phydev);
 }
 
+static int xway_gphy11_config_aneg(struct phy_device *phydev)
+{
+	int err;
+
+	err = genphy_config_aneg(phydev);
+	if (!err) {
+		genphy_suspend(phydev);
+		usleep_range(100000, 300000);
+		genphy_resume(phydev);
+	}
+
+	return err;
+}
+
 static int xway_gphy_ack_interrupt(struct phy_device *phydev)
 {
 	int reg;
@@ -282,6 +295,7 @@ static struct phy_driver xway_gphy[] = {
 		.name		= "Intel XWAY PHY11G (PEF 7071/PEF 7072) v1.5 / v1.6",
 		/* PHY_GBIT_FEATURES */
 		.config_init	= xway_gphy_config_init,
+		.config_aneg	= xway_gphy11_config_aneg,
 		.ack_interrupt	= xway_gphy_ack_interrupt,
 		.did_interrupt	= xway_gphy_did_interrupt,
 		.config_intr	= xway_gphy_config_intr,
diff --git a/drivers/net/phy/ksz8863-cfg.c b/drivers/net/phy/ksz8863-cfg.c
new file mode 100644
index 000000000000..73c250ac648f
--- /dev/null
+++ b/drivers/net/phy/ksz8863-cfg.c
@@ -0,0 +1,1122 @@
+/*
+ * Micrel ksz8863 dsa switch support
+ *
+ * Copyright (c) 2019 WAGO Kontakttechnik GmbH & Co. KG
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ */
+
+#include <linux/module.h>
+#include <linux/platform_device.h>
+#include <linux/switch.h>
+#include <linux/of_mdio.h>
+#include <linux/ksz8863.h>
+
+#define DRIVER_NAME "ksz8863-cfg"
+
+#define KSZ8863_CFG_VAL_BUF_SIZE 200
+#define KSZ8863_CFG_TBL_BUF_SIZE 1024
+
+struct ksz8863_cfg {
+	struct switch_dev swdev;
+	struct device *dev;
+	u32 sw_addr;
+	struct ksz8863_chip *chip;
+	struct mii_bus *mii_bus;
+	char buf_ratelimit[KSZ8863_CFG_VAL_BUF_SIZE];
+	char buf_dyn_mac[KSZ8863_CFG_TBL_BUF_SIZE];
+	char buf_static_mac[KSZ8863_CFG_TBL_BUF_SIZE];
+};
+
+#define get_ksz8863_cfg(_dev) container_of((_dev), struct ksz8863_cfg, swdev)
+
+static int ksz8863_cfg_smi_read(struct ksz8863_cfg *cfg, int reg, u8 *val)
+{
+	return cfg->chip->mii_ops->read(cfg->chip, reg, val);
+}
+
+static int ksz8863_cfg_smi_write(struct ksz8863_cfg *cfg, int reg, u8 val)
+{
+	return cfg->chip->mii_ops->write(cfg->chip, reg, val);
+}
+
+static inline void ksz8863_reg_lock(struct ksz8863_cfg *cfg)
+{
+	mutex_lock(&cfg->chip->reg_lock);
+}
+
+static inline void ksz8863_reg_unlock(struct ksz8863_cfg *cfg)
+{
+	mutex_unlock(&cfg->chip->reg_lock);
+}
+
+static void __printf(4, 5)
+	ksz8863_cfg_set_str_val(struct switch_val *val, char buf[], size_t size,
+				const char *fmt, ...)
+{
+	va_list args;
+
+	va_start(args, fmt);
+	val->len = vsnprintf(buf, size, fmt, args);
+	va_end(args);
+
+	val->value.s = buf;
+}
+
+static inline int ksz8863_cfg_set_bit(struct ksz8863_cfg *cfg, int reg, u8 mask,
+				      bool val, bool lock)
+{
+	int err;
+	u8 rv;
+
+	if (lock)
+		ksz8863_reg_lock(cfg);
+
+	err = ksz8863_cfg_smi_read(cfg, reg, &rv);
+	if (err)
+		goto out_unlock;
+
+	rv &= ~mask;
+	if (val)
+		rv |= mask;
+
+	err = ksz8863_cfg_smi_write(cfg, reg, rv);
+
+out_unlock:
+	if (lock)
+		ksz8863_reg_unlock(cfg);
+
+	return err;
+}
+
+static bool is_ksz8863_cfg_10bt(struct ksz8863_cfg *cfg)
+{
+	int err;
+	u8 val;
+
+	err = ksz8863_cfg_smi_read(cfg, KSZ8863_REG_GL_CTRL4, &val);
+	if (err)
+		return false;
+
+	return val & KSZ8863_REG_GL_CTRL4_10_MBIT;
+}
+
+static int ksz8863_cfg_set_bcast_rate(struct ksz8863_cfg *cfg,
+				      int frames_per_interval)
+{
+	int err;
+	u8 gl_ctrl4_val;
+	u8 gl_ctrl5_val;
+
+	err = ksz8863_cfg_smi_read(cfg, KSZ8863_REG_GL_CTRL4, &gl_ctrl4_val);
+	if (err)
+		return err;
+
+	gl_ctrl4_val &= ~KSZ8863_REG_GL_CTRL4_BCAST_STORM_RATE_HI_MASK;
+	gl_ctrl4_val |= (frames_per_interval >> 8) &
+			KSZ8863_REG_GL_CTRL4_BCAST_STORM_RATE_HI_MASK;
+	gl_ctrl5_val = frames_per_interval &
+		       KSZ8863_REG_GL_CTRL5_BCAST_STORM_RATE_LO_MASK;
+
+	err = ksz8863_cfg_smi_write(cfg, KSZ8863_REG_GL_CTRL5, gl_ctrl5_val);
+	if (err)
+		return err;
+
+	return ksz8863_cfg_smi_write(cfg, KSZ8863_REG_GL_CTRL4, gl_ctrl4_val);
+}
+
+static int ksz8863_cfg_get_bcast_rate(struct ksz8863_cfg *cfg,
+				      int *frames_per_interval)
+{
+	int err;
+	u8 gl_ctrl4_val;
+	u8 gl_ctrl5_val;
+
+	err = ksz8863_cfg_smi_read(cfg, KSZ8863_REG_GL_CTRL4, &gl_ctrl4_val);
+	if (err)
+		return err;
+
+	err = ksz8863_cfg_smi_read(cfg, KSZ8863_REG_GL_CTRL5, &gl_ctrl5_val);
+	if (err)
+		return err;
+
+	*frames_per_interval = gl_ctrl4_val;
+	*frames_per_interval &= KSZ8863_REG_GL_CTRL4_BCAST_STORM_RATE_HI_MASK;
+	*frames_per_interval <<= 8;
+	*frames_per_interval |= gl_ctrl5_val;
+
+	return 0;
+}
+
+static int ksz8863_cfg_set_bcast_protect(struct switch_dev *dev,
+					 const struct switch_attr *attr,
+					 struct switch_val *val)
+{
+	int rate, port, err;
+	struct ksz8863_cfg *cfg;
+	u8 rv[KSZ8863_EXT_PORTS];
+
+	cfg = get_ksz8863_cfg(dev);
+
+	ksz8863_reg_lock(cfg);
+
+	rate = val->value.i;
+
+	for (port = 0; port < KSZ8863_EXT_PORTS; port++) {
+		int pb = KSZ8863_REG_PORTS_BASE * port;
+
+		err = ksz8863_cfg_smi_read(cfg, pb + KSZ8863_REG_PORT1_CTRL0,
+					   &rv[port]);
+		if (err)
+			goto out_unlock;
+	}
+
+	if (!rate) {
+		for (port = 0; port < KSZ8863_EXT_PORTS; port++)
+			rv[port] &= ~KSZ8863_REG_PORT_CTRL0_BROADCAST_STORM;
+		dev_info(
+			cfg->dev,
+			"disable broadcast storm protection on port 1 and port 2");
+	} else {
+		bool mode_10bt;
+		int frames_per_sec, ms_per_interval, frames_per_interval;
+		int div, num;
+
+		if (rate > 20) {
+			dev_info(
+				cfg->dev,
+				"rate (%d) is too high, throttle to max. (20%%)",
+				rate);
+			rate = 20;
+		}
+
+		if (is_ksz8863_cfg_10bt(cfg)) {
+			mode_10bt = true;
+			frames_per_sec = 148800 / 10;
+			ms_per_interval = 500;
+		} else {
+			mode_10bt = false;
+			frames_per_sec = 148800;
+			ms_per_interval = 67;
+		}
+
+		div = 1000 * 100;
+		num = frames_per_sec * ms_per_interval * rate;
+		frames_per_interval = num / div;
+
+		if ((num % div) > (div / 2))
+			frames_per_interval++;
+
+		err = ksz8863_cfg_set_bcast_rate(cfg, frames_per_interval);
+		if (err)
+			goto out_unlock;
+
+		for (port = 0; port < KSZ8863_EXT_PORTS; port++)
+			rv[port] |= KSZ8863_REG_PORT_CTRL0_BROADCAST_STORM;
+
+		dev_info(
+			cfg->dev,
+			"enable broadcast storm protection on port 1 and port 2 "
+			"(%s: rate %d, %d frames / interval",
+			mode_10bt ? "10BT" : "100BT", rate,
+			frames_per_interval);
+	}
+
+	for (port = 0; port < KSZ8863_EXT_PORTS; port++) {
+		int pb = KSZ8863_REG_PORTS_BASE * port;
+
+		err = ksz8863_cfg_smi_write(cfg, pb + KSZ8863_REG_PORT1_CTRL0,
+					    rv[port]);
+		if (err)
+			goto out_unlock;
+	}
+
+out_unlock:
+	ksz8863_reg_unlock(cfg);
+
+	return err;
+}
+
+static int ksz8863_cfg_get_bcast_protect(struct switch_dev *dev,
+					 const struct switch_attr *attr,
+					 struct switch_val *val)
+{
+	int rate, port, err;
+	bool mode_10bt, bcast_en;
+	int frames_per_sec, ms_per_interval, frames_per_interval, div, num;
+	struct ksz8863_cfg *cfg;
+
+	cfg = get_ksz8863_cfg(dev);
+
+	ksz8863_reg_lock(cfg);
+
+	err = ksz8863_cfg_get_bcast_rate(cfg, &frames_per_interval);
+	if (err)
+		goto out_unlock;
+
+	if (is_ksz8863_cfg_10bt(cfg)) {
+		mode_10bt = true;
+		frames_per_sec = 148800 / 10;
+		ms_per_interval = 500;
+	} else {
+		mode_10bt = false;
+		frames_per_sec = 148800;
+		ms_per_interval = 67;
+	}
+
+	div = frames_per_sec * ms_per_interval;
+	num = frames_per_interval * 1000 * 100;
+
+	rate = num / div;
+
+	if ((num % div) > (div / 2))
+		rate++;
+
+	bcast_en = true;
+
+	for (port = 0; port < KSZ8863_EXT_PORTS; port++) {
+		int pb = KSZ8863_REG_PORTS_BASE * port;
+		u8 val;
+
+		err = ksz8863_cfg_smi_read(cfg, pb + KSZ8863_REG_PORT1_CTRL0,
+					   &val);
+		if (err)
+			goto out_unlock;
+
+		if (!(val & KSZ8863_REG_PORT_CTRL0_BROADCAST_STORM))
+			bcast_en = false;
+	}
+
+	dev_info(cfg->dev,
+		 "broadcast storm protection is %s on port 1 and port 2 "
+		 "(%s: calculated rate %d, %d frames / interval )",
+		 bcast_en ? "enabled" : "disabled",
+		 mode_10bt ? "10BT" : "100BT", rate, frames_per_interval);
+
+	val->value.i = bcast_en ? rate : 0;
+
+out_unlock:
+	ksz8863_reg_unlock(cfg);
+
+	return err;
+}
+
+static int ksz8863_cfg_parse_ratelimit(const char *value, int *rate_limit,
+				       bool *mode_mbps)
+{
+	char modestr[5] = { 0 };
+
+	if (!strcmp(value, "off")) {
+		*rate_limit = -1;
+		return 0;
+	}
+
+	if (sscanf(value, "%d.%4s", rate_limit, modestr) != 2)
+		return -EINVAL;
+
+	if (!strcmp(modestr, "mbps"))
+		*mode_mbps = true;
+	else if (!strcmp(modestr, "kbps"))
+		*mode_mbps = false;
+	else
+		return -EINVAL;
+
+	return 0;
+}
+
+static int ksz8863_cfg_get_egress_rate(struct ksz8863_cfg *cfg, bool mode_mbps,
+				       int rate_limit, u8 *reg)
+{
+	bool mode_10bt;
+	int rate_start, rate_stop;
+
+	mode_10bt = is_ksz8863_cfg_10bt(cfg);
+
+	if (rate_limit < 0) {
+		*reg = 0;
+	} else if (mode_mbps) {
+		rate_start = 0;
+		rate_stop = mode_10bt ? 10 : 100;
+
+		if (rate_limit < rate_start || rate_limit > rate_stop)
+			return -EINVAL;
+
+		*reg = rate_limit;
+	} else {
+		const int rate_step = 64;
+
+		rate_start = rate_step;
+		rate_stop = 960;
+
+		if (rate_limit < rate_start || rate_limit > rate_stop ||
+		    (rate_limit % rate_step))
+			return -EINVAL;
+
+		*reg = 0x64 + (rate_limit / rate_step);
+	}
+
+	return 0;
+}
+
+static int ksz8863_cfg_set_ratelimit(struct switch_dev *dev,
+				     const struct switch_attr *attr,
+				     struct switch_val *val)
+{
+	int err;
+	u8 reg;
+	bool mode_mbps;
+	int rate_limit;
+	u8 p3_erl_q;
+	struct ksz8863_cfg *cfg;
+
+	cfg = get_ksz8863_cfg(dev);
+
+	err = ksz8863_cfg_parse_ratelimit(val->value.s, &rate_limit,
+					  &mode_mbps);
+	if (err)
+		return err;
+
+	if (rate_limit < 0) {
+		/* disable ratelimit */
+		err = ksz8863_cfg_set_bit(cfg, KSZ8863_REG_PORT3_EGRESS_RL_Q0,
+					  KSZ8863_REG_PORT_EGRESS_RL_Q0_ENABLE,
+					  false, true);
+
+		if (!err)
+			dev_info(cfg->dev, "disable rate limiting");
+
+		return err;
+	}
+
+	ksz8863_reg_lock(cfg);
+
+	err = ksz8863_cfg_get_egress_rate(cfg, mode_mbps, rate_limit, &reg);
+	if (err)
+		goto out_unlock;
+
+	for (p3_erl_q = KSZ8863_REG_PORT3_EGRESS_RL_Q3;
+	     p3_erl_q != KSZ8863_REG_PORT3_EGRESS_RL_Q1; p3_erl_q--) {
+		err = ksz8863_cfg_smi_write(cfg, p3_erl_q, reg);
+		if (err)
+			goto out_unlock;
+	}
+
+	err = ksz8863_cfg_smi_write(cfg, KSZ8863_REG_PORT3_EGRESS_RL_Q0,
+				    reg | KSZ8863_REG_PORT_EGRESS_RL_Q0_ENABLE);
+
+	if (!err)
+		dev_info(cfg->dev, "enable rate limiting");
+
+out_unlock:
+	ksz8863_reg_unlock(cfg);
+
+	return err;
+}
+
+static int ksz8863_cfg_get_ratelimit(struct switch_dev *dev,
+				     const struct switch_attr *attr,
+				     struct switch_val *val)
+{
+	int err;
+	u8 reg, rate_reg;
+	struct ksz8863_cfg *cfg;
+	int rate = -1;
+	bool mode_10bt;
+	int rate_stop;
+	char *modestr = "none";
+
+	cfg = get_ksz8863_cfg(dev);
+
+	ksz8863_reg_lock(cfg);
+
+	/* we take just the first queue of port 3 */
+	err = ksz8863_cfg_smi_read(cfg, KSZ8863_REG_PORT3_EGRESS_RL_Q0, &reg);
+	if (err)
+		goto out_unlock;
+
+	if (!(reg & KSZ8863_REG_PORT_EGRESS_RL_Q0_ENABLE)) {
+		ksz8863_cfg_set_str_val(val, cfg->buf_ratelimit,
+					sizeof(cfg->buf_ratelimit), "disabled");
+		ksz8863_reg_unlock(cfg);
+		return 0;
+	}
+
+	rate_reg = reg & KSZ8863_REG_PORT_EGRESS_RL_MASK;
+
+	mode_10bt = is_ksz8863_cfg_10bt(cfg);
+
+	rate_stop = mode_10bt ? 10 : 100;
+
+	if (rate_reg <= rate_stop) {
+		modestr = "mbps";
+		/* 0 is equal to 10/100 mbps */
+		if (!rate_reg || rate_reg == rate_stop)
+			rate = rate_stop;
+		else
+			rate = rate_reg;
+	} else if (rate_reg <= 0x73) {
+		modestr = "kbps";
+		rate = 64 * (rate_reg - 0x64);
+	}
+
+	if (!strcmp(modestr, "none") || rate < 0) {
+		ksz8863_cfg_set_str_val(val, cfg->buf_ratelimit,
+					sizeof(cfg->buf_ratelimit), "invalid");
+		err = -EINVAL;
+		goto out_unlock;
+	}
+
+	ksz8863_cfg_set_str_val(val, cfg->buf_ratelimit,
+				sizeof(cfg->buf_ratelimit), "%d.%s", rate,
+				modestr);
+
+out_unlock:
+	ksz8863_reg_unlock(cfg);
+
+	return err;
+}
+
+static int ksz8863_cfg_flush_dyn_mac(struct ksz8863_cfg *cfg)
+{
+	if (cfg->chip && cfg->chip->ops.flush_dyn_mac_table)
+		return cfg->chip->ops.flush_dyn_mac_table(cfg->chip);
+	return -EOPNOTSUPP;
+}
+
+static int ksz8863_cfg_set_fastageing(struct switch_dev *dev,
+				      const struct switch_attr *attr,
+				      struct switch_val *val)
+{
+	int err, port;
+	bool enable;
+	struct ksz8863_cfg *cfg;
+
+	cfg = get_ksz8863_cfg(dev);
+
+	enable = val->value.i;
+
+	/* set fast ageing over global ctrl1 bit[1] register is
+	 * not working correctly. The workaround is to disable
+	 * learning on all ports, so that the dynamic table will
+	 * not be built.
+	 */
+	for (port = 0; port < KSZ8863_NUM_PORTS; port++) {
+		int pb = KSZ8863_REG_PORTS_BASE * port;
+
+		err = ksz8863_cfg_set_bit(cfg, pb + KSZ8863_REG_PORT1_CTRL2,
+					  KSZ8863_REG_PORT_CTRL2_LEARN_DISABLE,
+					  enable, true);
+		if (err)
+			return err;
+	}
+
+	if (enable) {
+		err = ksz8863_cfg_flush_dyn_mac(cfg);
+		if (err)
+			return err;
+	}
+
+	dev_info(cfg->dev, "fast aging %s", enable ? "enabled" : "disabled");
+
+	return 0;
+}
+
+static int ksz8863_cfg_get_fastageing(struct switch_dev *dev,
+				      const struct switch_attr *attr,
+				      struct switch_val *val)
+{
+	int err, port;
+	u8 rv[KSZ8863_NUM_PORTS];
+	struct ksz8863_cfg *cfg;
+
+	cfg = get_ksz8863_cfg(dev);
+
+	if (!cfg->chip->switched) {
+		/* if switch is in separated mode then
+		 * fast ageing is always disabled. In
+		 * separated mode it makes no sense.
+		 */
+		val->value.i = 0;
+		return 0;
+	}
+
+	ksz8863_reg_lock(cfg);
+
+	for (port = 0; port < KSZ8863_NUM_PORTS; ++port) {
+		int pb = KSZ8863_REG_PORTS_BASE * port;
+
+		err = ksz8863_cfg_smi_read(cfg, pb + KSZ8863_REG_PORT1_CTRL2,
+					   &rv[port]);
+		if (err)
+			goto out_unlock;
+	}
+
+	ksz8863_reg_unlock(cfg);
+
+	/*
+	 * when learning is disabled on all ports,
+	 * fast ageing is enabled
+	 */
+	val->value.i = ((rv[0] & rv[1] & rv[2]) &
+			KSZ8863_REG_PORT_CTRL2_LEARN_DISABLE) ?
+			       1 :
+			       0;
+
+	return 0;
+
+out_unlock:
+	ksz8863_reg_unlock(cfg);
+
+	return err;
+}
+
+static int ksz8863_cfg_set_port_mirror(struct switch_dev *dev,
+				       const struct switch_attr *attr,
+				       struct switch_val *val)
+{
+	int err, port, mirror;
+	u8 pc1v[KSZ8863_NUM_PORTS];
+	struct ksz8863_cfg *cfg;
+
+	cfg = get_ksz8863_cfg(dev);
+
+	mirror = val->value.i;
+
+	if (mirror < 0 || mirror > 3)
+		return -EINVAL;
+
+	ksz8863_reg_lock(cfg);
+
+	for (port = 0; port < KSZ8863_NUM_PORTS; port++) {
+		int pb = KSZ8863_REG_PORTS_BASE * port;
+
+		err = ksz8863_cfg_smi_read(cfg, pb + KSZ8863_REG_PORT1_CTRL1,
+					   &pc1v[port]);
+		if (err)
+			goto out_unlock;
+
+		pc1v[port] &= ~KSZ8863_REG_PORT_CTRL1_MIRROR_MASK;
+	}
+
+	switch (mirror) {
+	case 0:
+		// disable
+		break;
+	case 1:
+		// to port 1
+		pc1v[0] |= KSZ8863_REG_PORT_CTRL1_MIRROR_SNIFFER;
+
+		pc1v[1] |= KSZ8863_REG_PORT_CTRL1_MIRROR_RX;
+
+		/* cpu port is alway mirrored */
+		pc1v[2] |= KSZ8863_REG_PORT_CTRL1_MIRROR_RX;
+		break;
+
+	case 2:
+		// to port 2
+		pc1v[1] |= KSZ8863_REG_PORT_CTRL1_MIRROR_SNIFFER;
+
+		pc1v[0] |= KSZ8863_REG_PORT_CTRL1_MIRROR_RX;
+
+		pc1v[2] |= KSZ8863_REG_PORT_CTRL1_MIRROR_RX;
+		break;
+	case 3:
+		// to cpu
+		pc1v[2] |= KSZ8863_REG_PORT_CTRL1_MIRROR_SNIFFER;
+
+		pc1v[1] |= KSZ8863_REG_PORT_CTRL1_MIRROR_RX;
+		pc1v[0] |= KSZ8863_REG_PORT_CTRL1_MIRROR_RX;
+		break;
+	}
+
+	for (port = 0; port < KSZ8863_NUM_PORTS; port++) {
+		int pb = KSZ8863_REG_PORTS_BASE * port;
+
+		err = ksz8863_cfg_smi_write(cfg, pb + KSZ8863_REG_PORT1_CTRL1,
+					    pc1v[port]);
+		if (err)
+			goto out_unlock;
+	}
+
+	ksz8863_cfg_flush_dyn_mac(cfg);
+
+	if (mirror != 0)
+		dev_info(cfg->dev, "set mirror port to %d\n", mirror);
+	else
+		dev_info(cfg->dev, "port mirroring disabled\n");
+
+out_unlock:
+	ksz8863_reg_unlock(cfg);
+
+	return err;
+}
+
+static int ksz8863_cfg_get_port_mirror(struct switch_dev *dev,
+				       const struct switch_attr *attr,
+				       struct switch_val *val)
+{
+	int err, port, mirror;
+	u8 pc1v[KSZ8863_NUM_PORTS], gc3v;
+	struct ksz8863_cfg *cfg;
+
+	cfg = get_ksz8863_cfg(dev);
+
+	ksz8863_reg_lock(cfg);
+
+	err = ksz8863_cfg_smi_read(cfg, KSZ8863_REG_GL_CTRL3, &gc3v);
+	if (err)
+		goto out_unlock;
+
+	for (port = 0; port < KSZ8863_NUM_PORTS; port++) {
+		int pb = KSZ8863_REG_PORTS_BASE * port;
+
+		err = ksz8863_cfg_smi_read(cfg, pb + KSZ8863_REG_PORT1_CTRL1,
+					   &pc1v[port]);
+		if (err)
+			goto out_unlock;
+	}
+
+	if (pc1v[0] & KSZ8863_REG_PORT_CTRL1_MIRROR_SNIFFER)
+		mirror = 1;
+	else if (pc1v[1] & KSZ8863_REG_PORT_CTRL1_MIRROR_SNIFFER)
+		mirror = 2;
+	else if (pc1v[2] & KSZ8863_REG_PORT_CTRL1_MIRROR_SNIFFER)
+		mirror = 3;
+	else
+		mirror = 0;
+
+	val->value.i = mirror;
+
+	if (mirror)
+		dev_info(cfg->dev, "mirror-port: X%d", mirror);
+	else
+		dev_info(cfg->dev, "port mirroring is disabled");
+
+out_unlock:
+	ksz8863_reg_unlock(cfg);
+
+	return err;
+}
+
+static inline int
+ksz8863_cfg_get_dyn_mac_entry(struct ksz8863_cfg *cfg, u8 idx,
+			      struct ksz8863_dyn_mac_tbl_entry *entry)
+{
+	return cfg->chip->mii_ops->read_table(cfg->chip, KSZ8863_TBL_DYN_MAC,
+					      idx, (u8 *)entry, sizeof(*entry));
+}
+
+static int ksz8863_cfg_get_dyn_macs(struct switch_dev *dev,
+				    const struct switch_attr *attr,
+				    struct switch_val *val)
+{
+	struct ksz8863_dyn_mac_tbl_entry entry;
+	int i, cnt, err;
+	struct ksz8863_cfg *cfg = get_ksz8863_cfg(dev);
+	char *buffer = cfg->buf_dyn_mac;
+	const size_t buffer_size = sizeof(cfg->buf_dyn_mac);
+
+	err = ksz8863_cfg_get_dyn_mac_entry(cfg, 0, &entry);
+	if (err)
+		return err;
+
+	*buffer = '\0';
+	val->value.s = buffer;
+	val->len = 0;
+
+	if (entry.empty)
+		return 0;
+
+	cnt = entry.valid_nb + 1;
+	for (i = 0; i < cnt && buffer_size > val->len + 64; i++) {
+		err = ksz8863_cfg_get_dyn_mac_entry(cfg, i, &entry);
+		if (err)
+			return err;
+
+		val->len += snprintf(buffer + val->len, buffer_size - val->len,
+				     "\n[%d: %pMR %d %02d %d]", i, entry.mac,
+				     entry.port, entry.fid, entry.time);
+	}
+
+	if (i < cnt)
+		dev_warn(cfg->dev, "%d dynamic MACs not displayed", cnt - i);
+
+	return 0;
+}
+
+static inline int
+ksz8863_cfg_get_static_mac_entry(struct ksz8863_cfg *cfg, u8 idx,
+				 struct ksz8863_static_mac_tbl_entry *entry)
+{
+	return cfg->chip->mii_ops->read_table(cfg->chip, KSZ8863_TBL_STATIC_MAC,
+					      idx, (u8 *)entry, sizeof(*entry));
+}
+
+static inline int
+ksz8863_cfg_set_static_mac_entry(struct ksz8863_cfg *cfg, u8 idx,
+				 struct ksz8863_static_mac_tbl_entry *entry)
+{
+	return cfg->chip->mii_ops->write_table(cfg->chip,
+					       KSZ8863_TBL_STATIC_MAC, idx,
+					       (u8 *)entry, sizeof(*entry));
+}
+
+static int ksz8863_cfg_add_or_update_static_mac_entry(
+	struct ksz8863_cfg *cfg, struct ksz8863_static_mac_tbl_entry *entry)
+{
+	int err, i;
+	int idx = -1;
+	struct ksz8863_static_mac_tbl_entry current_entry;
+
+	for (i = 0; i < KSZ8863_SMAC_MAX_ENTRIES; i++) {
+		err = ksz8863_cfg_get_static_mac_entry(cfg, i, &current_entry);
+		if (err)
+			return err;
+
+		if (idx == -1 && !current_entry.valid) {
+			idx = i;
+			pr_debug("found unused smac entry: %d:%pMR\n", idx,
+				 current_entry.mac);
+		}
+
+		if (!memcmp(entry->mac, current_entry.mac,
+			    sizeof(current_entry.mac))) {
+			idx = i;
+			pr_debug("found existing smac entry: %d:%pMR\n", idx,
+				 current_entry.mac);
+			break;
+		}
+	}
+
+	return idx != -1 ? ksz8863_cfg_set_static_mac_entry(cfg, idx, entry) :
+			   -ENOSPC;
+}
+
+static int ksz8863_cfg_get_static_macs(struct switch_dev *dev,
+				       const struct switch_attr *attr,
+				       struct switch_val *val)
+{
+	struct ksz8863_static_mac_tbl_entry entry;
+	int i, err;
+	struct ksz8863_cfg *cfg = get_ksz8863_cfg(dev);
+	const size_t buffer_size = sizeof(cfg->buf_static_mac);
+	char *buffer = cfg->buf_static_mac;
+
+	*buffer = '\0';
+	val->value.s = buffer;
+	val->len = 0;
+
+	for (i = 0; i < KSZ8863_SMAC_MAX_ENTRIES; i++) {
+		err = ksz8863_cfg_get_static_mac_entry(cfg, i, &entry);
+		if (err)
+			return err;
+
+		pr_debug("%d [%d: %pMR 0x%x %02d %d]\n", entry.valid, i,
+			 entry.mac, entry.forward_ports,
+			 (entry.use_fid) ? entry.fid : -1, entry.override);
+
+		if (!entry.valid)
+			continue;
+
+		val->len += snprintf(buffer + val->len, buffer_size - val->len,
+				     "\n[%d: %pMR 0x%x %02d %d]", i, entry.mac,
+				     entry.forward_ports,
+				     (entry.use_fid) ? entry.fid : -1,
+				     entry.override);
+	}
+
+	return 0;
+}
+
+#define MAC_FMT "%hhx:%hhx:%hhx:%hhx:%hhx:%hhx"
+static int ksz8863_cfg_set_static_macs(struct switch_dev *dev,
+				       const struct switch_attr *attr,
+				       struct switch_val *val)
+{
+	struct ksz8863_static_mac_tbl_entry entry = { 0 };
+	u8 *mac = entry.mac;
+	u8 idx; // index unused, kept for api compatibility
+	u8 values[5];
+
+	/* scan data
+	 * command syntax should be <0,00:30:DE:FF:00:C0,0x4,1,1,0,0>2
+	 */
+	if (sscanf(val->value.s, "%hhd," MAC_FMT ",%hhx,%hhd,%hhd,%hhd,%hhd",
+		   &idx, &mac[5], &mac[4], &mac[3], &mac[2], &mac[1], &mac[0],
+		   &values[0], &values[1], &values[2], &values[3],
+		   &values[4]) != 12) {
+		pr_info("Invalid argument: %s", val->value.s);
+		return -EINVAL;
+	}
+
+	entry.forward_ports = values[0] & 0x7;
+	entry.valid = values[1] & 0x1;
+	entry.override = values[2] & 0x1;
+	entry.use_fid = values[3] & 0x1;
+	entry.fid = values[4] & 0xf;
+
+	pr_debug("%s:%pMR,0x%x,%d,%d,%d,%d\n", __func__, mac,
+		 entry.forward_ports, entry.valid, entry.override,
+		 entry.use_fid, entry.fid);
+
+	return ksz8863_cfg_add_or_update_static_mac_entry(get_ksz8863_cfg(dev),
+							  &entry);
+}
+
+static struct switch_attr ksz8863_cfg_global_attrs[] = {
+	{
+		.type = SWITCH_TYPE_INT,
+		.name = "bcast_protect",
+		.description =
+			"Sets the broadcast storm protection rate limit"
+			" (0..20) for Port 1 & 2. '0' will disable protection",
+		.set = ksz8863_cfg_set_bcast_protect,
+		.get = ksz8863_cfg_get_bcast_protect,
+	},
+	{
+		.type = SWITCH_TYPE_STRING,
+		.name = "rate_limit",
+		.description =
+			"Enables rate limiting on egress"
+			" Port3(CPU-Ingress), format='50.mbps'/'64.kbps'. 'off'"
+			" will disable rate limiting.",
+		.set = ksz8863_cfg_set_ratelimit,
+		.get = ksz8863_cfg_get_ratelimit,
+	},
+	{
+		.type = SWITCH_TYPE_INT,
+		.name = "port_mirror",
+		.description = "Sets the mirror port"
+			       " [0 = disable port mirroring]"
+			       " [1 = mirror port X1 to port X2]"
+			       " [2 = mirror port X2 to port X1]",
+		.set = ksz8863_cfg_set_port_mirror,
+		.get = ksz8863_cfg_get_port_mirror,
+	},
+	{
+		.type = SWITCH_TYPE_INT,
+		.name = "fast_aging",
+		.description = "Sets fast aging [0 = disable] [1 = enable]",
+		.set = ksz8863_cfg_set_fastageing,
+		.get = ksz8863_cfg_get_fastageing,
+	},
+	{
+		.type = SWITCH_TYPE_STRING,
+		.name = "dynmacs",
+		.description = "Show dynamic MAC address table"
+			       " - read: 'mac, port, fid, ageing'",
+		.get = ksz8863_cfg_get_dyn_macs,
+	},
+	{
+		.type = SWITCH_TYPE_STRING,
+		.name = "statmacs",
+		.description =
+			"Read/Write static MAC address table"
+			" - read: 'mac, forward ports, fid, override'\n"
+			" - write: 'index,mac,forward ports,valid,override,"
+			"use_fid,fid'\n"
+			"\tindex: unused\n"
+			"\tmac: mac address\n"
+			"\tforward ports: bitfield 001 (1) port 1, 010 (2) port 2, 100 (4) port 3 (host)\n"
+			"\t\t 7 -> all ports except ingress, 4 to host only, etc\n"
+			"\tvalid: forward entry active\n"
+			"\toverride: forward even when port-transmit or recvieve is disabled\n"
+			"\tuse_fid: use mac and vlan for static lookup\n"
+			"\tfid: filter vlan id\n",
+		.set = ksz8863_cfg_set_static_macs,
+		.get = ksz8863_cfg_get_static_macs,
+	}
+};
+
+// clang-format off
+struct switch_dev_ops ksz8863_cfg_switch_dev_ops = {
+	.attr_global = {
+			.attr = ksz8863_cfg_global_attrs,
+			.n_attr = ARRAY_SIZE(ksz8863_cfg_global_attrs),
+		},
+};
+
+// clang-format on
+
+static struct ksz8863_cfg *ksz8863_cfg_alloc(struct device *dev)
+{
+	struct ksz8863_cfg *cfg;
+
+	cfg = devm_kzalloc(dev, sizeof(*cfg), GFP_KERNEL);
+	if (!cfg)
+		return NULL;
+
+	cfg->dev = dev;
+	cfg->swdev.ops = &ksz8863_cfg_switch_dev_ops;
+
+	return cfg;
+}
+
+static void ksz8863_cfg_free(struct ksz8863_cfg *cfg)
+{
+	if (cfg)
+		devm_kfree(cfg->dev, cfg);
+}
+
+static int ksz8863_cfg_of_get_data(struct ksz8863_cfg *cfg)
+{
+	struct device *dev = cfg->dev;
+	struct device_node *np = dev->of_node;
+	struct device_node *mdio_node;
+	struct device_node *switch_node;
+
+	if (!np)
+		return -ENODATA;
+
+	cfg->swdev.name = np->name;
+
+	if (of_property_read_string(np, "swcfg,alias", &cfg->swdev.alias)) {
+		dev_err(dev, "no switch alias given");
+		return -EINVAL;
+	}
+
+	if (of_property_read_u32(np, "swcfg,cpu_port", &cfg->swdev.cpu_port)) {
+		dev_err(dev, "no cpu port given");
+		return -EINVAL;
+	}
+
+	if (of_property_read_u32(np, "swcfg,ports", &cfg->swdev.ports)) {
+		dev_err(dev, "no ports given");
+		return -EINVAL;
+	}
+
+	if (of_property_read_u32(np, "swcfg,vlans", &cfg->swdev.vlans)) {
+		dev_err(dev, "no vlans given");
+		return -EINVAL;
+	}
+
+	switch_node = of_parse_phandle(np, "swcfg,switch", 0);
+	if (!switch_node) {
+		dev_err(dev, "no switch node given");
+		return -EINVAL;
+	}
+
+	if (of_property_read_u32(switch_node, "reg", &cfg->sw_addr)) {
+		dev_err(dev, "no switch mdio address given");
+		return -EINVAL;
+	}
+
+	mdio_node = of_parse_phandle(np, "swcfg,mii-bus", 0);
+	if (!mdio_node) {
+		dev_err(dev, "can not find node 'swcfg,mii-bus'");
+		return -EINVAL;
+	}
+
+	cfg->mii_bus = of_mdio_find_bus(mdio_node);
+	if (!cfg->mii_bus) {
+		dev_err(dev, "can not find mii bus device");
+		return -EPROBE_DEFER;
+	}
+
+	cfg->swdev.of_node = np;
+
+	return 0;
+}
+
+static int ksz8863_cfg_get_chip(struct ksz8863_cfg *cfg)
+{
+	struct mdio_device *mdio_dev;
+	struct dsa_switch *ds;
+
+	mdio_dev = cfg->mii_bus->mdio_map[cfg->sw_addr];
+	if (!mdio_dev)
+		return -EPROBE_DEFER;
+
+	ds = dev_get_drvdata(&mdio_dev->dev);
+
+	if (!ds)
+		return -EPROBE_DEFER;
+
+	if (!ds->priv) {
+		dev_err(cfg->dev, "ksz8863 chip structure is not available");
+		return -EINVAL;
+	}
+
+	cfg->chip = ds->priv;
+
+	return 0;
+}
+
+static int ksz8863_cfg_probe(struct platform_device *pdev)
+{
+	int err;
+	struct ksz8863_cfg *cfg;
+
+	cfg = ksz8863_cfg_alloc(&pdev->dev);
+	if (!cfg) {
+		err = -ENOMEM;
+		goto err;
+	}
+
+	err = ksz8863_cfg_of_get_data(cfg);
+	if (err)
+		goto err_free;
+
+	err = ksz8863_cfg_get_chip(cfg);
+	if (err)
+		goto err_free;
+
+	err = register_switch(&cfg->swdev, NULL);
+	if (err) {
+		dev_err(cfg->dev, "switch registration failed");
+		goto err_free;
+	}
+
+	pdev->dev.platform_data = cfg;
+
+	dev_info(&pdev->dev, "driver probed successful");
+	return 0;
+
+err_free:
+	ksz8863_cfg_free(cfg);
+err:
+	return err;
+}
+
+static int ksz8863_cfg_remove(struct platform_device *pdev)
+{
+	struct ksz8863_cfg *cfg = platform_get_drvdata(pdev);
+
+	if (!cfg)
+		return 0;
+
+	unregister_switch(&cfg->swdev);
+	ksz8863_cfg_free(cfg);
+
+	return 0;
+}
+
+// clang-format off
+#ifdef CONFIG_OF
+static const struct of_device_id ksz8863_of_match[] = {
+	{ .compatible = "swcfg,ksz8863" },
+	{ /* sentinel */ },
+};
+MODULE_DEVICE_TABLE(of, ksz8863_of_match);
+#endif
+
+static struct platform_driver ksz8863_driver = {
+	.probe  = ksz8863_cfg_probe,
+	.remove = ksz8863_cfg_remove,
+	.driver = {
+		.name   = DRIVER_NAME,
+		.owner  = THIS_MODULE,
+#ifdef CONFIG_OF
+		.of_match_table = ksz8863_of_match,
+#endif
+	},
+};
+
+// clang-format on
+module_platform_driver(ksz8863_driver);
+
+MODULE_AUTHOR("andreas.schmidt@wago.com>");
+MODULE_LICENSE("GPL");
diff --git a/drivers/net/phy/marvell.c b/drivers/net/phy/marvell.c
index 5aec673a0120..1ae4cb3bcf21 100644
--- a/drivers/net/phy/marvell.c
+++ b/drivers/net/phy/marvell.c
@@ -32,6 +32,7 @@
 #include <linux/marvell_phy.h>
 #include <linux/bitfield.h>
 #include <linux/of.h>
+#include <linux/mutex.h>
 
 #include <linux/io.h>
 #include <asm/irq.h>
@@ -160,6 +161,11 @@
 #define MII_88E3016_DISABLE_SCRAMBLER	0x0200
 #define MII_88E3016_AUTO_MDIX_CROSSOVER	0x0030
 
+#define MII_88E6321_PHY_SPEC_CTRL	0x10
+#define MII_88E6321_PHY_SCR_MDI		0x0000
+#define MII_88E6321_PHY_SCR_MDI_X	0x0020
+#define MII_88E6321_PHY_SCR_AUTO_CROSS	0x0060
+
 #define MII_88E1510_GEN_CTRL_REG_1		0x14
 #define MII_88E1510_GEN_CTRL_REG_1_MODE_MASK	0x7
 #define MII_88E1510_GEN_CTRL_REG_1_MODE_SGMII	0x1	/* SGMII to copper */
@@ -814,6 +820,73 @@ static int m88e1111_config_init_rtbi(struct phy_device *phydev)
 		MII_M1111_HWCFG_FIBER_COPPER_AUTO);
 }
 
+static int mv88e6321_config_init(struct phy_device *phydev)
+{
+	int reg, ret;
+	int phy_mdix;
+
+	reg = phy_read(phydev, MII_88E6321_PHY_SPEC_CTRL);
+	if (reg < 0)
+		return reg;
+
+	phy_mdix = reg & MII_88E6321_PHY_SCR_AUTO_CROSS;
+
+	if (phy_mdix == MII_88E6321_PHY_SCR_MDI)
+		phydev->mdix = ETH_TP_MDI;
+	else if (phy_mdix == MII_88E6321_PHY_SCR_MDI_X)
+		phydev->mdix = ETH_TP_MDI_X;
+	else if (phy_mdix == MII_88E6321_PHY_SCR_AUTO_CROSS)
+		phydev->mdix = ETH_TP_MDI_AUTO;
+
+	ret = genphy_read_abilities(phydev);
+	if(ret < 0)
+		return ret;
+
+	of_set_phy_supported(phydev);
+
+	return 0;
+}
+
+static int mv88e6321_config_aneg(struct phy_device *phydev)
+{
+	int reg, ret;
+	int phy_mdix;
+	int mdix_changed = 0;
+
+	reg = phy_read(phydev, MII_88E6321_PHY_SPEC_CTRL);
+	if (reg < 0)
+		return reg;
+
+	phy_mdix = reg & MII_88E6321_PHY_SCR_AUTO_CROSS;
+
+	reg &= ~MII_88E6321_PHY_SCR_AUTO_CROSS;
+
+	if (phydev->mdix == ETH_TP_MDI && phy_mdix != MII_88E6321_PHY_SCR_MDI) {
+		mdix_changed = 1;
+		reg |= MII_88E6321_PHY_SCR_MDI;
+	} else if (phydev->mdix == ETH_TP_MDI_X &&
+		   phy_mdix != MII_88E6321_PHY_SCR_MDI_X) {
+		mdix_changed = 1;
+		reg |= MII_88E6321_PHY_SCR_MDI_X;
+	} else if (phydev->mdix == ETH_TP_MDI_AUTO &&
+		   phy_mdix != MII_88E6321_PHY_SCR_AUTO_CROSS) {
+		mdix_changed = 1;
+		reg |= MII_88E6321_PHY_SCR_AUTO_CROSS;
+	}
+
+	if (mdix_changed) {
+		ret = phy_write(phydev, MII_88E6321_PHY_SPEC_CTRL, reg);
+		if (ret < 0)
+			return ret;
+
+		ret = genphy_soft_reset(phydev);
+		if (ret)
+			return ret;
+	}
+
+	return genphy_config_aneg(phydev);
+}
+
 static int m88e1111_config_init(struct phy_device *phydev)
 {
 	int err;
@@ -2981,6 +3054,25 @@ static struct phy_driver marvell_drivers[] = {
 		.get_tunable = m88e1540_get_tunable,
 		.set_tunable = m88e1540_set_tunable,
 	},
+	{
+		.phy_id = MARVELL_PHY_ID_88E6321,
+		.phy_id_mask = MARVELL_PHY_ID_MASK,
+		.name = "Marvell 88E6321",
+		.features = PHY_BASIC_FEATURES,
+		.probe = marvell_probe,
+		.config_aneg = &mv88e6321_config_aneg,
+		.config_init = &mv88e6321_config_init,
+		.aneg_done = &marvell_aneg_done,
+		.read_status = &marvell_read_status,
+		.ack_interrupt = &marvell_ack_interrupt,
+		.config_intr = &marvell_config_intr,
+		.did_interrupt = &m88e1121_did_interrupt,
+		.resume = &genphy_resume,
+		.suspend = &genphy_suspend,
+		.get_sset_count = marvell_get_sset_count,
+		.get_strings = marvell_get_strings,
+		.get_stats = marvell_get_stats,
+	},
 };
 
 module_phy_driver(marvell_drivers);
diff --git a/drivers/net/phy/mv88e6321-cfg.c b/drivers/net/phy/mv88e6321-cfg.c
new file mode 100644
index 000000000000..5f6890f301b5
--- /dev/null
+++ b/drivers/net/phy/mv88e6321-cfg.c
@@ -0,0 +1,357 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+
+#define pr_fmt(fmt) "mv88e6321-cfg: " fmt
+
+#include <linux/module.h>
+#include <linux/string.h>
+#include <linux/delay.h>
+#include <linux/mutex.h>
+#include <linux/miscdevice.h>
+#include <linux/fs.h>
+#include <linux/ioctl.h>
+#include <linux/uaccess.h>
+#include <linux/phy.h>
+#include <linux/switch.h>
+#include <net/switchdev.h>
+#include <linux/of_fdt.h>
+#include <linux/of_gpio.h>
+#include <linux/of_mdio.h>
+#include <linux/platform_device.h>
+#include <linux/kernel.h>
+#include <linux/list.h>
+
+#include "../dsa/mv88e6xxx/chip.h"
+
+#define DRIVER_NAME "mv88e6321-cfg"
+
+struct mv88e6321_cfg {
+	struct switch_dev swdev;
+	struct device *dev;
+	u32 sw_addr;
+	struct mv88e6xxx_chip *chip;
+	struct mii_bus *mii_bus;
+	u8 current_reg;
+	u8 current_phy_reg;
+};
+
+#define get_mv88e6321_cfg(_dev)                                                \
+	container_of((_dev), struct mv88e6321_cfg, swdev)
+
+static int mv88e6321_sw_set_mdio_phy(struct switch_dev *dev,
+				     const struct switch_attr *attr,
+				     struct switch_val *val)
+{
+	struct mv88e6321_cfg *cfg = get_mv88e6321_cfg(dev);
+
+	cfg->current_phy_reg = val->value.i;
+	return 0;
+}
+
+static int mv88e6321_sw_set_mdio_reg(struct switch_dev *dev,
+				     const struct switch_attr *attr,
+				     struct switch_val *val)
+{
+	struct mv88e6321_cfg *cfg = get_mv88e6321_cfg(dev);
+
+	cfg->current_reg = val->value.i;
+	return 0;
+}
+
+static int mv88e6321_sw_get_mdio_phy(struct switch_dev *dev,
+				     const struct switch_attr *attr,
+				     struct switch_val *val)
+{
+	struct mv88e6321_cfg *cfg = get_mv88e6321_cfg(dev);
+
+	val->value.i = cfg->current_phy_reg;
+	return 0;
+}
+
+static int mv88e6321_sw_get_mdio_reg(struct switch_dev *dev,
+				     const struct switch_attr *attr,
+				     struct switch_val *val)
+{
+	struct mv88e6321_cfg *cfg = get_mv88e6321_cfg(dev);
+
+	val->value.i = cfg->current_reg;
+	return 0;
+}
+
+static int mv88e6321_sw_set_mdio_write(struct switch_dev *dev,
+				       const struct switch_attr *attr,
+				       struct switch_val *val)
+{
+	struct mv88e6321_cfg *cfg = get_mv88e6321_cfg(dev);
+	u16 v = val->value.i;
+
+	pr_debug("mdiobus_write: phy 0x%02x, reg 0x%02x, val 0x%04x\n",
+		 cfg->current_phy_reg, cfg->current_reg, v);
+
+	return mdiobus_write(cfg->mii_bus, cfg->current_phy_reg,
+			     cfg->current_reg, v);
+}
+
+static int mv88e6321_sw_get_mdio_read(struct switch_dev *dev,
+				      const struct switch_attr *attr,
+				      struct switch_val *val)
+{
+	struct mv88e6321_cfg *cfg = get_mv88e6321_cfg(dev);
+
+	val->value.i = mdiobus_read(cfg->mii_bus, cfg->current_phy_reg,
+				    cfg->current_reg);
+
+	pr_debug("mdiobus_read: phy 0x%02x, reg 0x%02x, val 0x%04x\n",
+		 cfg->current_phy_reg, cfg->current_reg, val->value.i);
+	return 0;
+}
+
+static int mv88e6321_cfg_port_fdb_add(struct switch_dev *dev,
+				      const struct switch_attr *attr,
+				      struct switch_val *val)
+{
+	struct mv88e6321_cfg *cfg = get_mv88e6321_cfg(dev);
+	struct dsa_switch *ds = cfg->chip->ds;
+	u8 addr[ETH_ALEN];
+	int port;
+	u16 vid;
+
+	if (sscanf(val->value.s, "%hhx:%hhx:%hhx:%hhx:%hhx:%hhx,%d,%hu",
+		   &addr[0], &addr[1], &addr[2], &addr[3], &addr[4], &addr[5],
+		   &port, &vid) != 8) {
+		return -EINVAL;
+	}
+
+	if (port < 0 || port > DSA_MAX_PORTS)
+		return -EINVAL;
+
+	pr_debug("%s: mac %pM, port %d, vid %hu\n", __func__, addr, port, vid);
+
+	ds->ops->port_fdb_add(ds, port, addr, vid);
+
+	return 0;
+}
+
+static struct switch_attr mv88e6321_global_attrs[] = {
+	{
+		.type = SWITCH_TYPE_INT,
+		.name = "mdio_phy",
+		.description = "Select phy register for mdio rw operation",
+		.set = mv88e6321_sw_set_mdio_phy,
+		.get = mv88e6321_sw_get_mdio_phy,
+	},
+	{
+		.type = SWITCH_TYPE_INT,
+		.name = "mdio_reg",
+		.description = "Select register for mdio rw operation",
+		.set = mv88e6321_sw_set_mdio_reg,
+		.get = mv88e6321_sw_get_mdio_reg,
+	},
+	{
+		.type = SWITCH_TYPE_INT,
+		.name = "mdio_rw_mdio_generic",
+		.description = "Read/Write with selected mdio registers",
+		.set = mv88e6321_sw_set_mdio_write,
+		.get = mv88e6321_sw_get_mdio_read,
+	},
+	{
+		.type = SWITCH_TYPE_STRING,
+		.name = "port_fdb_add",
+		.description = "add mac to fdb <MAC,port,vid>",
+		.set = mv88e6321_cfg_port_fdb_add,
+		.get = NULL,
+	},
+};
+
+struct switch_dev_ops mv88e6321_switch_dev_ops = {
+	.attr_global = {
+		.attr = mv88e6321_global_attrs,
+		.n_attr = ARRAY_SIZE(mv88e6321_global_attrs),
+	},
+	.attr_port = {
+		.attr = NULL,
+		.n_attr = 0,
+	},
+	.attr_vlan = {
+		.attr = NULL,
+		.n_attr = 0,
+	},
+};
+
+static int mv88e6321_cfg_get_chip(struct mv88e6321_cfg *cfg)
+{
+	struct mdio_device *mdio_dev;
+	struct dsa_switch *ds;
+
+	mdio_dev = cfg->mii_bus->mdio_map[cfg->sw_addr];
+	if (!mdio_dev)
+		return -EPROBE_DEFER;
+
+	ds = dev_get_drvdata(&mdio_dev->dev);
+
+	if (!ds)
+		return -EPROBE_DEFER;
+
+	if (!ds->priv) {
+		dev_err(cfg->dev, "mv88e6321 chip structure is not available");
+		return -EINVAL;
+	}
+
+	cfg->chip = ds->priv;
+
+	return 0;
+}
+
+static int mv88e6321_cfg_of_get_data(struct mv88e6321_cfg *cfg)
+{
+	struct device *dev = cfg->dev;
+	struct device_node *np = dev->of_node;
+	struct device_node *mdio_node;
+	struct device_node *switch_node;
+
+	if (!np)
+		return -ENODATA;
+
+	cfg->swdev.name = np->name;
+
+	if (of_property_read_string(np, "swcfg,alias", &cfg->swdev.alias)) {
+		dev_err(dev, "No switch alias given\n");
+		return -EINVAL;
+	}
+
+	if (of_property_read_u32(np, "swcfg,cpu_port", &cfg->swdev.cpu_port)) {
+		dev_err(dev, "No cpu port given\n");
+		return -EINVAL;
+	}
+
+	if (of_property_read_u32(np, "swcfg,ports", &cfg->swdev.ports)) {
+		dev_err(dev, "No ports given\n");
+		return -EINVAL;
+	}
+
+	if (of_property_read_u32(np, "swcfg,vlans", &cfg->swdev.vlans)) {
+		dev_err(dev, "No vlans given\n");
+		return -EINVAL;
+	}
+
+	switch_node = of_parse_phandle(np, "swcfg,switch", 0);
+	if (!switch_node) {
+		dev_err(dev, "no switch node given");
+		return -EINVAL;
+	}
+
+	if (of_property_read_u32(switch_node, "reg", &cfg->sw_addr)) {
+		dev_err(dev, "no switch mdio address given");
+		return -EINVAL;
+	}
+
+	mdio_node = of_parse_phandle(np, "swcfg,mii-bus", 0);
+	if (!mdio_node) {
+		dev_err(dev, "can't find node 'swcfg,mii-bus'\n");
+		return -ENODEV;
+	}
+
+	cfg->mii_bus = of_mdio_find_bus(mdio_node);
+	if (!cfg->mii_bus) {
+		dev_err(dev, "can't find mii bus device\n");
+		return -EPROBE_DEFER;
+	}
+
+	cfg->swdev.of_node = np;
+
+	return 0;
+}
+
+static struct mv88e6321_cfg *mv88e6321_cfg_alloc(struct device *dev)
+{
+	struct mv88e6321_cfg *cfg;
+
+	cfg = devm_kzalloc(dev, sizeof(*cfg), GFP_KERNEL);
+	if (!cfg)
+		return NULL;
+
+	cfg->dev = dev;
+	cfg->swdev.ops = &mv88e6321_switch_dev_ops;
+
+	return cfg;
+}
+
+static void mv88e6321_cfg_free(struct mv88e6321_cfg *cfg)
+{
+	if (cfg)
+		devm_kfree(cfg->dev, cfg);
+}
+
+static int mv88e6321_cfg_probe(struct platform_device *pdev)
+{
+	int err;
+	struct mv88e6321_cfg *cfg;
+
+	cfg = mv88e6321_cfg_alloc(&pdev->dev);
+	if (!cfg) {
+		err = -ENOMEM;
+		goto err;
+	}
+
+	err = mv88e6321_cfg_of_get_data(cfg);
+	if (err)
+		goto err_free;
+
+	err = mv88e6321_cfg_get_chip(cfg);
+	if (err)
+		goto err_free;
+
+	err = register_switch(&cfg->swdev, NULL);
+	if (err) {
+		dev_err(cfg->dev, "switch registration failed");
+		goto err_free;
+	}
+
+	pdev->dev.platform_data = cfg;
+
+	dev_info(&pdev->dev, "driver probed\n");
+	return 0;
+
+err_free:
+	mv88e6321_cfg_free(cfg);
+err:
+	return err;
+}
+
+static int mv88e6321_cfg_remove(struct platform_device *pdev)
+{
+	struct mv88e6321_cfg *cfg = pdev->dev.platform_data;
+
+	if (!cfg)
+		return 0;
+
+	unregister_switch(&cfg->swdev);
+	mv88e6321_cfg_free(cfg);
+	return 0;
+}
+
+#ifdef CONFIG_OF
+static const struct of_device_id mv88e6321_dt_ids[] = {
+	{
+		.compatible = "swcfg,mv88e6321",
+	},
+	{ /* sentinel */ },
+};
+MODULE_DEVICE_TABLE(of, mv88e6321_dt_ids);
+#endif
+
+static struct platform_driver mv88e6321_driver = {
+	.probe = mv88e6321_cfg_probe,
+	.remove = mv88e6321_cfg_remove,
+	.driver = {
+		.name = DRIVER_NAME,
+		.owner = THIS_MODULE,
+#ifdef CONFIG_OF
+		.of_match_table = mv88e6321_dt_ids,
+#endif
+	},
+};
+
+module_platform_driver(mv88e6321_driver);
+
+MODULE_AUTHOR("heinrich.toews@wago.com>");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/net/phy/phy_device.c b/drivers/net/phy/phy_device.c
index dd1f711140c3..f3670c9887b9 100644
--- a/drivers/net/phy/phy_device.c
+++ b/drivers/net/phy/phy_device.c
@@ -1404,7 +1404,6 @@ int phy_attach_direct(struct net_device *dev, struct phy_device *phydev,
 	if (err)
 		return err;
 
-	phy_resume(phydev);
 	phy_led_triggers_register(phydev);
 
 	return err;
diff --git a/drivers/net/phy/swconfig.c b/drivers/net/phy/swconfig.c
new file mode 100644
index 000000000000..6fc45ad2c275
--- /dev/null
+++ b/drivers/net/phy/swconfig.c
@@ -0,0 +1,1273 @@
+// SPDX-License-Identifier: GPL-2.0+
+/* swconfig.c: Switch configuration API
+ *
+ * Copyright (C) 2008 Felix Fietkau <nbd@nbd.name>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version 2
+ * of the License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include <linux/types.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/list.h>
+#include <linux/if.h>
+#include <linux/if_ether.h>
+#include <linux/capability.h>
+#include <linux/skbuff.h>
+#include <linux/switch.h>
+#include <linux/of.h>
+#include <linux/version.h>
+#include <uapi/linux/mii.h>
+
+#define SWCONFIG_DEVNAME "switch%d"
+
+MODULE_AUTHOR("Felix Fietkau <nbd@nbd.name>");
+MODULE_LICENSE("GPL");
+
+static int swdev_id;
+static struct list_head swdevs;
+static DEFINE_MUTEX(swdevs_lock);
+struct swconfig_callback;
+
+struct swconfig_callback {
+	struct sk_buff *msg;
+	struct genlmsghdr *hdr;
+	struct genl_info *info;
+	int cmd;
+
+	/* callback for filling in the message data */
+	int (*fill)(struct swconfig_callback *cb, void *arg);
+
+	/* callback for closing the message before sending it */
+	int (*close)(struct swconfig_callback *cb, void *arg);
+
+	struct nlattr *nest[4];
+	int args[4];
+};
+
+/* defaults */
+
+static int swconfig_get_vlan_ports(struct switch_dev *dev,
+				   const struct switch_attr *attr,
+				   struct switch_val *val)
+{
+	int ret;
+	if (val->port_vlan >= dev->vlans)
+		return -EINVAL;
+
+	if (!dev->ops->get_vlan_ports)
+		return -EOPNOTSUPP;
+
+	ret = dev->ops->get_vlan_ports(dev, val);
+	return ret;
+}
+
+static int swconfig_set_vlan_ports(struct switch_dev *dev,
+				   const struct switch_attr *attr,
+				   struct switch_val *val)
+{
+	struct switch_port *ports = val->value.ports;
+	const struct switch_dev_ops *ops = dev->ops;
+	int i;
+
+	if (val->port_vlan >= dev->vlans)
+		return -EINVAL;
+
+	/* validate ports */
+	if (val->len > dev->ports)
+		return -EINVAL;
+
+	if (!ops->set_vlan_ports)
+		return -EOPNOTSUPP;
+
+	for (i = 0; i < val->len; i++) {
+		if (ports[i].id >= dev->ports)
+			return -EINVAL;
+
+		if (ops->set_port_pvid &&
+		    !(ports[i].flags & (1 << SWITCH_PORT_FLAG_TAGGED)))
+			ops->set_port_pvid(dev, ports[i].id, val->port_vlan);
+	}
+
+	return ops->set_vlan_ports(dev, val);
+}
+
+static int swconfig_set_pvid(struct switch_dev *dev,
+			     const struct switch_attr *attr,
+			     struct switch_val *val)
+{
+	if (val->port_vlan >= dev->ports)
+		return -EINVAL;
+
+	if (!dev->ops->set_port_pvid)
+		return -EOPNOTSUPP;
+
+	return dev->ops->set_port_pvid(dev, val->port_vlan, val->value.i);
+}
+
+static int swconfig_get_pvid(struct switch_dev *dev,
+			     const struct switch_attr *attr,
+			     struct switch_val *val)
+{
+	if (val->port_vlan >= dev->ports)
+		return -EINVAL;
+
+	if (!dev->ops->get_port_pvid)
+		return -EOPNOTSUPP;
+
+	return dev->ops->get_port_pvid(dev, val->port_vlan, &val->value.i);
+}
+
+static int swconfig_set_link(struct switch_dev *dev,
+			     const struct switch_attr *attr,
+			     struct switch_val *val)
+{
+	if (!dev->ops->set_port_link)
+		return -EOPNOTSUPP;
+
+	return dev->ops->set_port_link(dev, val->port_vlan, val->value.link);
+}
+
+static int swconfig_get_link(struct switch_dev *dev,
+			     const struct switch_attr *attr,
+			     struct switch_val *val)
+{
+	struct switch_port_link *link = val->value.link;
+
+	if (val->port_vlan >= dev->ports)
+		return -EINVAL;
+
+	if (!dev->ops->get_port_link)
+		return -EOPNOTSUPP;
+
+	memset(link, 0, sizeof(*link));
+	return dev->ops->get_port_link(dev, val->port_vlan, link);
+}
+
+static int swconfig_apply_config(struct switch_dev *dev,
+				 const struct switch_attr *attr,
+				 struct switch_val *val)
+{
+	/* don't complain if not supported by the switch driver */
+	if (!dev->ops->apply_config)
+		return 0;
+
+	return dev->ops->apply_config(dev);
+}
+
+static int swconfig_reset_switch(struct switch_dev *dev,
+				 const struct switch_attr *attr,
+				 struct switch_val *val)
+{
+	/* don't complain if not supported by the switch driver */
+	if (!dev->ops->reset_switch)
+		return 0;
+
+	return dev->ops->reset_switch(dev);
+}
+
+enum global_defaults {
+	GLOBAL_APPLY,
+	GLOBAL_RESET,
+};
+
+enum vlan_defaults {
+	VLAN_PORTS,
+};
+
+enum port_defaults {
+	PORT_PVID,
+	PORT_LINK,
+};
+
+static struct switch_attr default_global[] = {
+	[GLOBAL_APPLY] = {
+		.type = SWITCH_TYPE_NOVAL,
+		.name = "apply",
+		.description = "Activate changes in the hardware",
+		.set = swconfig_apply_config,
+	},
+	[GLOBAL_RESET] = {
+		.type = SWITCH_TYPE_NOVAL,
+		.name = "reset",
+		.description = "Reset the switch",
+		.set = swconfig_reset_switch,
+	}
+};
+
+static struct switch_attr default_port[] = {
+	[PORT_PVID] = {
+		.type = SWITCH_TYPE_INT,
+		.name = "pvid",
+		.description = "Primary VLAN ID",
+		.set = swconfig_set_pvid,
+		.get = swconfig_get_pvid,
+	},
+	[PORT_LINK] = {
+		.type = SWITCH_TYPE_LINK,
+		.name = "link",
+		.description = "Get port link information",
+		.set = swconfig_set_link,
+		.get = swconfig_get_link,
+	}
+};
+
+static struct switch_attr default_vlan[] = {
+	[VLAN_PORTS] = {
+		.type = SWITCH_TYPE_PORTS,
+		.name = "ports",
+		.description = "VLAN port mapping",
+		.set = swconfig_set_vlan_ports,
+		.get = swconfig_get_vlan_ports,
+	},
+};
+
+static const struct switch_attr *
+swconfig_find_attr_by_name(const struct switch_attrlist *alist,
+			   const char *name)
+{
+	int i;
+
+	for (i = 0; i < alist->n_attr; i++)
+		if (strcmp(name, alist->attr[i].name) == 0)
+			return &alist->attr[i];
+
+	return NULL;
+}
+
+static void swconfig_defaults_init(struct switch_dev *dev)
+{
+	const struct switch_dev_ops *ops = dev->ops;
+
+	dev->def_global = 0;
+	dev->def_vlan = 0;
+	dev->def_port = 0;
+
+	if (ops->get_vlan_ports || ops->set_vlan_ports)
+		set_bit(VLAN_PORTS, &dev->def_vlan);
+
+	if (ops->get_port_pvid || ops->set_port_pvid)
+		set_bit(PORT_PVID, &dev->def_port);
+
+	if (ops->get_port_link &&
+	    !swconfig_find_attr_by_name(&ops->attr_port, "link"))
+		set_bit(PORT_LINK, &dev->def_port);
+
+	/* always present, can be no-op */
+	set_bit(GLOBAL_APPLY, &dev->def_global);
+	set_bit(GLOBAL_RESET, &dev->def_global);
+}
+
+static struct genl_family switch_fam;
+
+static const struct nla_policy switch_policy[SWITCH_ATTR_MAX + 1] = {
+	[SWITCH_ATTR_ID] = { .type = NLA_U32 },
+	[SWITCH_ATTR_OP_ID] = { .type = NLA_U32 },
+	[SWITCH_ATTR_OP_PORT] = { .type = NLA_U32 },
+	[SWITCH_ATTR_OP_VLAN] = { .type = NLA_U32 },
+	[SWITCH_ATTR_OP_VALUE_INT] = { .type = NLA_U32 },
+	[SWITCH_ATTR_OP_VALUE_STR] = { .type = NLA_NUL_STRING },
+	[SWITCH_ATTR_OP_VALUE_PORTS] = { .type = NLA_NESTED },
+	[SWITCH_ATTR_TYPE] = { .type = NLA_U32 },
+};
+
+static const struct nla_policy port_policy[SWITCH_PORT_ATTR_MAX + 1] = {
+	[SWITCH_PORT_ID] = { .type = NLA_U32 },
+	[SWITCH_PORT_FLAG_TAGGED] = { .type = NLA_FLAG },
+};
+
+static struct nla_policy link_policy[SWITCH_LINK_ATTR_MAX] = {
+	[SWITCH_LINK_FLAG_DUPLEX] = { .type = NLA_FLAG },
+	[SWITCH_LINK_FLAG_ANEG] = { .type = NLA_FLAG },
+	[SWITCH_LINK_SPEED] = { .type = NLA_U32 },
+};
+
+static inline void swconfig_lock(void)
+{
+	mutex_lock(&swdevs_lock);
+}
+
+static inline void swconfig_unlock(void)
+{
+	mutex_unlock(&swdevs_lock);
+}
+
+static struct switch_dev *swconfig_get_dev(struct genl_info *info)
+{
+	struct switch_dev *dev = NULL;
+	struct switch_dev *p;
+	int id;
+
+	if (!info->attrs[SWITCH_ATTR_ID])
+		goto done;
+
+	id = nla_get_u32(info->attrs[SWITCH_ATTR_ID]);
+	swconfig_lock();
+	list_for_each_entry(p, &swdevs, dev_list) {
+		if (id != p->id)
+			continue;
+
+		dev = p;
+		break;
+	}
+	if (dev)
+		mutex_lock(&dev->sw_mutex);
+	else
+		pr_debug("device %d not found\n", id);
+	swconfig_unlock();
+done:
+	return dev;
+}
+
+static inline void swconfig_put_dev(struct switch_dev *dev)
+{
+	mutex_unlock(&dev->sw_mutex);
+}
+
+static int swconfig_dump_attr(struct swconfig_callback *cb, void *arg)
+{
+	struct switch_attr *op = arg;
+	struct genl_info *info = cb->info;
+	struct sk_buff *msg = cb->msg;
+	int id = cb->args[0];
+	void *hdr;
+
+	hdr = genlmsg_put(msg, info->snd_portid, info->snd_seq, &switch_fam,
+			  NLM_F_MULTI, SWITCH_CMD_NEW_ATTR);
+	if (IS_ERR(hdr))
+		return -1;
+
+	if (nla_put_u32(msg, SWITCH_ATTR_OP_ID, id))
+		goto nla_put_failure;
+	if (nla_put_u32(msg, SWITCH_ATTR_OP_TYPE, op->type))
+		goto nla_put_failure;
+	if (nla_put_string(msg, SWITCH_ATTR_OP_NAME, op->name))
+		goto nla_put_failure;
+	if (op->description)
+		if (nla_put_string(msg, SWITCH_ATTR_OP_DESCRIPTION,
+				   op->description))
+			goto nla_put_failure;
+
+	genlmsg_end(msg, hdr);
+	return msg->len;
+nla_put_failure:
+	genlmsg_cancel(msg, hdr);
+	return -EMSGSIZE;
+}
+
+/* spread multipart messages across multiple message buffers */
+static int swconfig_send_multipart(struct swconfig_callback *cb, void *arg)
+{
+	struct genl_info *info = cb->info;
+	int restart = 0;
+	int err;
+
+	do {
+		if (!cb->msg) {
+			cb->msg = nlmsg_new(NLMSG_GOODSIZE, GFP_KERNEL);
+			if (cb->msg == NULL)
+				goto error;
+		}
+
+		if (!(cb->fill(cb, arg) < 0))
+			break;
+
+		/* fill failed, check if this was already the second attempt */
+		if (restart)
+			goto error;
+
+		/* try again in a new message, send the current one */
+		restart = 1;
+		if (cb->close) {
+			if (cb->close(cb, arg) < 0)
+				goto error;
+		}
+		err = genlmsg_reply(cb->msg, info);
+		cb->msg = NULL;
+		if (err < 0)
+			goto error;
+
+	} while (restart);
+
+	return 0;
+
+error:
+	if (cb->msg)
+		nlmsg_free(cb->msg);
+	return -1;
+}
+
+static int swconfig_list_attrs(struct sk_buff *skb, struct genl_info *info)
+{
+	struct genlmsghdr *hdr = nlmsg_data(info->nlhdr);
+	const struct switch_attrlist *alist;
+	struct switch_dev *dev;
+	struct swconfig_callback cb;
+	int err = -EINVAL;
+	int i;
+
+	/* defaults */
+	struct switch_attr *def_list;
+	unsigned long *def_active;
+	int n_def;
+
+	dev = swconfig_get_dev(info);
+	if (!dev)
+		return -EINVAL;
+
+	switch (hdr->cmd) {
+	case SWITCH_CMD_LIST_GLOBAL:
+		alist = &dev->ops->attr_global;
+		def_list = default_global;
+		def_active = &dev->def_global;
+		n_def = ARRAY_SIZE(default_global);
+		break;
+	case SWITCH_CMD_LIST_VLAN:
+		alist = &dev->ops->attr_vlan;
+		def_list = default_vlan;
+		def_active = &dev->def_vlan;
+		n_def = ARRAY_SIZE(default_vlan);
+		break;
+	case SWITCH_CMD_LIST_PORT:
+		alist = &dev->ops->attr_port;
+		def_list = default_port;
+		def_active = &dev->def_port;
+		n_def = ARRAY_SIZE(default_port);
+		break;
+	default:
+		WARN_ON(1);
+		goto out;
+	}
+
+	memset(&cb, 0, sizeof(cb));
+	cb.info = info;
+	cb.fill = swconfig_dump_attr;
+	for (i = 0; i < alist->n_attr; i++) {
+		if (alist->attr[i].disabled)
+			continue;
+		cb.args[0] = i;
+		err = swconfig_send_multipart(&cb, (void *)&alist->attr[i]);
+		if (err < 0)
+			goto error;
+	}
+
+	/* defaults */
+	for (i = 0; i < n_def; i++) {
+		if (!test_bit(i, def_active))
+			continue;
+		cb.args[0] = SWITCH_ATTR_DEFAULTS_OFFSET + i;
+		err = swconfig_send_multipart(&cb, (void *)&def_list[i]);
+		if (err < 0)
+			goto error;
+	}
+	swconfig_put_dev(dev);
+
+	if (!cb.msg)
+		return 0;
+
+	return genlmsg_reply(cb.msg, info);
+
+error:
+	if (cb.msg)
+		nlmsg_free(cb.msg);
+out:
+	swconfig_put_dev(dev);
+	return err;
+}
+
+static const struct switch_attr *swconfig_lookup_attr(struct switch_dev *dev,
+						      struct genl_info *info,
+						      struct switch_val *val)
+{
+	struct genlmsghdr *hdr = nlmsg_data(info->nlhdr);
+	const struct switch_attrlist *alist;
+	const struct switch_attr *attr = NULL;
+	unsigned int attr_id;
+
+	/* defaults */
+	struct switch_attr *def_list;
+	unsigned long *def_active;
+	int n_def;
+
+	if (!info->attrs[SWITCH_ATTR_OP_ID])
+		goto done;
+
+	switch (hdr->cmd) {
+	case SWITCH_CMD_SET_GLOBAL:
+	case SWITCH_CMD_GET_GLOBAL:
+		alist = &dev->ops->attr_global;
+		def_list = default_global;
+		def_active = &dev->def_global;
+		n_def = ARRAY_SIZE(default_global);
+		break;
+	case SWITCH_CMD_SET_VLAN:
+	case SWITCH_CMD_GET_VLAN:
+		alist = &dev->ops->attr_vlan;
+		def_list = default_vlan;
+		def_active = &dev->def_vlan;
+		n_def = ARRAY_SIZE(default_vlan);
+		if (!info->attrs[SWITCH_ATTR_OP_VLAN])
+			goto done;
+		val->port_vlan = nla_get_u32(info->attrs[SWITCH_ATTR_OP_VLAN]);
+		if (val->port_vlan >= dev->vlans)
+			goto done;
+		break;
+	case SWITCH_CMD_SET_PORT:
+	case SWITCH_CMD_GET_PORT:
+		alist = &dev->ops->attr_port;
+		def_list = default_port;
+		def_active = &dev->def_port;
+		n_def = ARRAY_SIZE(default_port);
+		if (!info->attrs[SWITCH_ATTR_OP_PORT])
+			goto done;
+		val->port_vlan = nla_get_u32(info->attrs[SWITCH_ATTR_OP_PORT]);
+		if (val->port_vlan >= dev->ports)
+			goto done;
+		break;
+	default:
+		WARN_ON(1);
+		goto done;
+	}
+
+	if (!alist)
+		goto done;
+
+	attr_id = nla_get_u32(info->attrs[SWITCH_ATTR_OP_ID]);
+	if (attr_id >= SWITCH_ATTR_DEFAULTS_OFFSET) {
+		attr_id -= SWITCH_ATTR_DEFAULTS_OFFSET;
+		if (attr_id >= n_def)
+			goto done;
+		if (!test_bit(attr_id, def_active))
+			goto done;
+		attr = &def_list[attr_id];
+	} else {
+		if (attr_id >= alist->n_attr)
+			goto done;
+		attr = &alist->attr[attr_id];
+	}
+
+	if (attr->disabled)
+		attr = NULL;
+
+done:
+	if (!attr)
+		pr_debug("attribute lookup failed\n");
+	val->attr = attr;
+	return attr;
+}
+
+static int swconfig_parse_ports(struct sk_buff *msg, struct nlattr *head,
+				struct switch_val *val, int max)
+{
+	struct nlattr *nla;
+	int rem;
+
+	val->len = 0;
+	nla_for_each_nested(nla, head, rem) {
+		struct nlattr *tb[SWITCH_PORT_ATTR_MAX + 1];
+		struct switch_port *port;
+
+		if (val->len >= max)
+			return -EINVAL;
+
+		port = &val->value.ports[val->len];
+
+#if KERNEL_VERSION(5, 2, 0) <= LINUX_VERSION_CODE
+		if (nla_parse_nested_deprecated(tb, SWITCH_PORT_ATTR_MAX, nla,
+						port_policy, NULL))
+#else
+		if (nla_parse_nested(tb, SWITCH_PORT_ATTR_MAX, nla, port_policy,
+				     NULL))
+#endif
+			return -EINVAL;
+
+		if (!tb[SWITCH_PORT_ID])
+			return -EINVAL;
+
+		port->id = nla_get_u32(tb[SWITCH_PORT_ID]);
+		if (tb[SWITCH_PORT_FLAG_TAGGED])
+			port->flags |= (1 << SWITCH_PORT_FLAG_TAGGED);
+		val->len++;
+	}
+
+	return 0;
+}
+
+static int swconfig_parse_link(struct sk_buff *msg, struct nlattr *nla,
+			       struct switch_port_link *link)
+{
+	struct nlattr *tb[SWITCH_LINK_ATTR_MAX + 1];
+
+#if KERNEL_VERSION(5, 2, 0) <= LINUX_VERSION_CODE
+	if (nla_parse_nested_deprecated(tb, SWITCH_LINK_ATTR_MAX, nla,
+					link_policy, NULL))
+#else
+	if (nla_parse_nested(tb, SWITCH_LINK_ATTR_MAX, nla, link_policy, NULL))
+#endif
+		return -EINVAL;
+
+	link->duplex = !!tb[SWITCH_LINK_FLAG_DUPLEX];
+	link->aneg = !!tb[SWITCH_LINK_FLAG_ANEG];
+	link->speed = nla_get_u32(tb[SWITCH_LINK_SPEED]);
+
+	return 0;
+}
+
+static int swconfig_set_attr(struct sk_buff *skb, struct genl_info *info)
+{
+	const struct switch_attr *attr;
+	struct switch_dev *dev;
+	struct switch_val val;
+	int err = -EINVAL;
+
+	if (!capable(CAP_NET_ADMIN))
+		return -EPERM;
+
+	dev = swconfig_get_dev(info);
+	if (!dev)
+		return -EINVAL;
+
+	memset(&val, 0, sizeof(val));
+	attr = swconfig_lookup_attr(dev, info, &val);
+	if (!attr || !attr->set)
+		goto error;
+
+	val.attr = attr;
+	switch (attr->type) {
+	case SWITCH_TYPE_NOVAL:
+		break;
+	case SWITCH_TYPE_INT:
+		if (!info->attrs[SWITCH_ATTR_OP_VALUE_INT])
+			goto error;
+		val.value.i =
+			nla_get_u32(info->attrs[SWITCH_ATTR_OP_VALUE_INT]);
+		break;
+	case SWITCH_TYPE_STRING:
+		if (!info->attrs[SWITCH_ATTR_OP_VALUE_STR])
+			goto error;
+		val.value.s = nla_data(info->attrs[SWITCH_ATTR_OP_VALUE_STR]);
+		break;
+	case SWITCH_TYPE_PORTS:
+		val.value.ports = dev->portbuf;
+		memset(dev->portbuf, 0,
+		       sizeof(struct switch_port) * dev->ports);
+
+		/* TODO: implement multipart? */
+		if (info->attrs[SWITCH_ATTR_OP_VALUE_PORTS]) {
+			err = swconfig_parse_ports(
+				skb, info->attrs[SWITCH_ATTR_OP_VALUE_PORTS],
+				&val, dev->ports);
+			if (err < 0)
+				goto error;
+		} else {
+			val.len = 0;
+			err = 0;
+		}
+		break;
+	case SWITCH_TYPE_LINK:
+		val.value.link = &dev->linkbuf;
+		memset(&dev->linkbuf, 0, sizeof(struct switch_port_link));
+
+		if (info->attrs[SWITCH_ATTR_OP_VALUE_LINK]) {
+			err = swconfig_parse_link(
+				skb, info->attrs[SWITCH_ATTR_OP_VALUE_LINK],
+				val.value.link);
+			if (err < 0)
+				goto error;
+		} else {
+			val.len = 0;
+			err = 0;
+		}
+		break;
+	default:
+		goto error;
+	}
+
+	err = attr->set(dev, attr, &val);
+error:
+	swconfig_put_dev(dev);
+	return err;
+}
+
+static int swconfig_close_portlist(struct swconfig_callback *cb, void *arg)
+{
+	if (cb->nest[0])
+		nla_nest_end(cb->msg, cb->nest[0]);
+	return 0;
+}
+
+static int swconfig_send_port(struct swconfig_callback *cb, void *arg)
+{
+	const struct switch_port *port = arg;
+	struct nlattr *p = NULL;
+
+	if (!cb->nest[0]) {
+		cb->nest[0] = nla_nest_start(cb->msg, cb->cmd);
+		if (!cb->nest[0])
+			return -1;
+	}
+
+	p = nla_nest_start(cb->msg, SWITCH_ATTR_PORT);
+	if (!p)
+		goto error;
+
+	if (nla_put_u32(cb->msg, SWITCH_PORT_ID, port->id))
+		goto nla_put_failure;
+	if (port->flags & (1 << SWITCH_PORT_FLAG_TAGGED)) {
+		if (nla_put_flag(cb->msg, SWITCH_PORT_FLAG_TAGGED))
+			goto nla_put_failure;
+	}
+
+	nla_nest_end(cb->msg, p);
+	return 0;
+
+nla_put_failure:
+	nla_nest_cancel(cb->msg, p);
+error:
+	nla_nest_cancel(cb->msg, cb->nest[0]);
+	return -1;
+}
+
+static int swconfig_send_ports(struct sk_buff **msg, struct genl_info *info,
+			       int attr, const struct switch_val *val)
+{
+	struct swconfig_callback cb;
+	int err = 0;
+	int i;
+
+	if (!val->value.ports)
+		return -EINVAL;
+
+	memset(&cb, 0, sizeof(cb));
+	cb.cmd = attr;
+	cb.msg = *msg;
+	cb.info = info;
+	cb.fill = swconfig_send_port;
+	cb.close = swconfig_close_portlist;
+
+	cb.nest[0] = nla_nest_start(cb.msg, cb.cmd);
+	for (i = 0; i < val->len; i++) {
+		err = swconfig_send_multipart(&cb, &val->value.ports[i]);
+		if (err)
+			goto done;
+	}
+	err = val->len;
+	swconfig_close_portlist(&cb, NULL);
+	*msg = cb.msg;
+
+done:
+	return err;
+}
+
+static int swconfig_send_link(struct sk_buff *msg, struct genl_info *info,
+			      int attr, const struct switch_port_link *link)
+{
+	struct nlattr *p = NULL;
+	int err = 0;
+
+	p = nla_nest_start(msg, attr);
+	if (link->link) {
+		if (nla_put_flag(msg, SWITCH_LINK_FLAG_LINK))
+			goto nla_put_failure;
+	}
+	if (link->duplex) {
+		if (nla_put_flag(msg, SWITCH_LINK_FLAG_DUPLEX))
+			goto nla_put_failure;
+	}
+	if (link->aneg) {
+		if (nla_put_flag(msg, SWITCH_LINK_FLAG_ANEG))
+			goto nla_put_failure;
+	}
+	if (link->tx_flow) {
+		if (nla_put_flag(msg, SWITCH_LINK_FLAG_TX_FLOW))
+			goto nla_put_failure;
+	}
+	if (link->rx_flow) {
+		if (nla_put_flag(msg, SWITCH_LINK_FLAG_RX_FLOW))
+			goto nla_put_failure;
+	}
+	if (nla_put_u32(msg, SWITCH_LINK_SPEED, link->speed))
+		goto nla_put_failure;
+	if (link->eee & ADVERTISED_100baseT_Full) {
+		if (nla_put_flag(msg, SWITCH_LINK_FLAG_EEE_100BASET))
+			goto nla_put_failure;
+	}
+	if (link->eee & ADVERTISED_1000baseT_Full) {
+		if (nla_put_flag(msg, SWITCH_LINK_FLAG_EEE_1000BASET))
+			goto nla_put_failure;
+	}
+	nla_nest_end(msg, p);
+
+	return err;
+
+nla_put_failure:
+	nla_nest_cancel(msg, p);
+	return -1;
+}
+
+static int swconfig_get_attr(struct sk_buff *skb, struct genl_info *info)
+{
+	struct genlmsghdr *hdr = nlmsg_data(info->nlhdr);
+	const struct switch_attr *attr;
+	struct switch_dev *dev;
+	struct sk_buff *msg = NULL;
+	struct switch_val val;
+	int err = -EINVAL;
+	int cmd = hdr->cmd;
+
+	dev = swconfig_get_dev(info);
+	if (!dev)
+		return -EINVAL;
+
+	memset(&val, 0, sizeof(val));
+	attr = swconfig_lookup_attr(dev, info, &val);
+	if (!attr || !attr->get)
+		goto error;
+
+	if (attr->type == SWITCH_TYPE_PORTS) {
+		val.value.ports = dev->portbuf;
+		memset(dev->portbuf, 0,
+		       sizeof(struct switch_port) * dev->ports);
+	} else if (attr->type == SWITCH_TYPE_LINK) {
+		val.value.link = &dev->linkbuf;
+		memset(&dev->linkbuf, 0, sizeof(struct switch_port_link));
+	}
+
+	err = attr->get(dev, attr, &val);
+	if (err)
+		goto error;
+
+	msg = nlmsg_new(NLMSG_GOODSIZE, GFP_KERNEL);
+	if (!msg)
+		goto error;
+
+	hdr = genlmsg_put(msg, info->snd_portid, info->snd_seq, &switch_fam, 0,
+			  cmd);
+	if (IS_ERR(hdr))
+		goto nla_put_failure;
+
+	switch (attr->type) {
+	case SWITCH_TYPE_INT:
+		if (nla_put_u32(msg, SWITCH_ATTR_OP_VALUE_INT, val.value.i))
+			goto nla_put_failure;
+		break;
+	case SWITCH_TYPE_STRING:
+		if (nla_put_string(msg, SWITCH_ATTR_OP_VALUE_STR, val.value.s))
+			goto nla_put_failure;
+		break;
+	case SWITCH_TYPE_PORTS:
+		err = swconfig_send_ports(&msg, info,
+					  SWITCH_ATTR_OP_VALUE_PORTS, &val);
+		if (err < 0)
+			goto nla_put_failure;
+		break;
+	case SWITCH_TYPE_LINK:
+		err = swconfig_send_link(msg, info, SWITCH_ATTR_OP_VALUE_LINK,
+					 val.value.link);
+		if (err < 0)
+			goto nla_put_failure;
+		break;
+	default:
+		pr_debug("invalid type in attribute\n");
+		err = -EINVAL;
+		goto nla_put_failure;
+	}
+	genlmsg_end(msg, hdr);
+	err = msg->len;
+	if (err < 0)
+		goto nla_put_failure;
+
+	swconfig_put_dev(dev);
+	return genlmsg_reply(msg, info);
+
+nla_put_failure:
+	if (msg)
+		nlmsg_free(msg);
+error:
+	swconfig_put_dev(dev);
+	if (!err)
+		err = -ENOMEM;
+	return err;
+}
+
+static int swconfig_send_switch(struct sk_buff *msg, u32 pid, u32 seq,
+				int flags, const struct switch_dev *dev)
+{
+	struct nlattr *p = NULL, *m = NULL;
+	void *hdr;
+	int i;
+
+	hdr = genlmsg_put(msg, pid, seq, &switch_fam, flags,
+			  SWITCH_CMD_NEW_ATTR);
+	if (IS_ERR(hdr))
+		return -1;
+
+	if (nla_put_u32(msg, SWITCH_ATTR_ID, dev->id))
+		goto nla_put_failure;
+	if (nla_put_string(msg, SWITCH_ATTR_DEV_NAME, dev->devname))
+		goto nla_put_failure;
+	if (nla_put_string(msg, SWITCH_ATTR_ALIAS, dev->alias))
+		goto nla_put_failure;
+	if (nla_put_string(msg, SWITCH_ATTR_NAME, dev->name))
+		goto nla_put_failure;
+	if (nla_put_u32(msg, SWITCH_ATTR_VLANS, dev->vlans))
+		goto nla_put_failure;
+	if (nla_put_u32(msg, SWITCH_ATTR_PORTS, dev->ports))
+		goto nla_put_failure;
+	if (nla_put_u32(msg, SWITCH_ATTR_CPU_PORT, dev->cpu_port))
+		goto nla_put_failure;
+
+	m = nla_nest_start(msg, SWITCH_ATTR_PORTMAP);
+	if (!m)
+		goto nla_put_failure;
+	for (i = 0; i < dev->ports; i++) {
+		p = nla_nest_start(msg, SWITCH_ATTR_PORTS);
+		if (!p)
+			continue;
+		if (dev->portmap[i].s) {
+			if (nla_put_string(msg, SWITCH_PORTMAP_SEGMENT,
+					   dev->portmap[i].s))
+				goto nla_put_failure;
+			if (nla_put_u32(msg, SWITCH_PORTMAP_VIRT,
+					dev->portmap[i].virt))
+				goto nla_put_failure;
+		}
+		nla_nest_end(msg, p);
+	}
+	nla_nest_end(msg, m);
+	genlmsg_end(msg, hdr);
+	return msg->len;
+nla_put_failure:
+	genlmsg_cancel(msg, hdr);
+	return -EMSGSIZE;
+}
+
+static int swconfig_dump_switches(struct sk_buff *skb,
+				  struct netlink_callback *cb)
+{
+	struct switch_dev *dev;
+	int start = cb->args[0];
+	int idx = 0;
+
+	swconfig_lock();
+	list_for_each_entry(dev, &swdevs, dev_list) {
+		if (++idx <= start)
+			continue;
+		if (swconfig_send_switch(skb, NETLINK_CB(cb->skb).portid,
+					 cb->nlh->nlmsg_seq, NLM_F_MULTI,
+					 dev) < 0)
+			break;
+	}
+	swconfig_unlock();
+	cb->args[0] = idx;
+
+	return skb->len;
+}
+
+static int swconfig_done(struct netlink_callback *cb)
+{
+	return 0;
+}
+
+static struct genl_ops swconfig_ops[] = {
+	{
+		.cmd = SWITCH_CMD_LIST_GLOBAL,
+#if KERNEL_VERSION(5, 2, 0) <= LINUX_VERSION_CODE
+		.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,
+#endif
+		.doit = swconfig_list_attrs,
+#if KERNEL_VERSION(5, 2, 0) > LINUX_VERSION_CODE
+		.policy = switch_policy,
+#endif
+	},
+	{
+		.cmd = SWITCH_CMD_LIST_VLAN,
+#if KERNEL_VERSION(5, 2, 0) <= LINUX_VERSION_CODE
+		.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,
+#endif
+		.doit = swconfig_list_attrs,
+#if KERNEL_VERSION(5, 2, 0) > LINUX_VERSION_CODE
+		.policy = switch_policy,
+#endif
+	},
+	{
+		.cmd = SWITCH_CMD_LIST_PORT,
+#if KERNEL_VERSION(5, 2, 0) <= LINUX_VERSION_CODE
+		.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,
+#endif
+		.doit = swconfig_list_attrs,
+#if KERNEL_VERSION(5, 2, 0) > LINUX_VERSION_CODE
+		.policy = switch_policy,
+#endif
+	},
+	{
+		.cmd = SWITCH_CMD_GET_GLOBAL,
+#if KERNEL_VERSION(5, 2, 0) <= LINUX_VERSION_CODE
+		.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,
+#endif
+		.doit = swconfig_get_attr,
+#if KERNEL_VERSION(5, 2, 0) > LINUX_VERSION_CODE
+		.policy = switch_policy,
+#endif
+	},
+	{
+		.cmd = SWITCH_CMD_GET_VLAN,
+#if KERNEL_VERSION(5, 2, 0) <= LINUX_VERSION_CODE
+		.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,
+#endif
+		.doit = swconfig_get_attr,
+#if KERNEL_VERSION(5, 2, 0) > LINUX_VERSION_CODE
+		.policy = switch_policy,
+#endif
+	},
+	{
+		.cmd = SWITCH_CMD_GET_PORT,
+#if KERNEL_VERSION(5, 2, 0) <= LINUX_VERSION_CODE
+		.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,
+#endif
+		.doit = swconfig_get_attr,
+#if KERNEL_VERSION(5, 2, 0) > LINUX_VERSION_CODE
+		.policy = switch_policy,
+#endif
+	},
+	{
+		.cmd = SWITCH_CMD_SET_GLOBAL,
+#if KERNEL_VERSION(5, 2, 0) <= LINUX_VERSION_CODE
+		.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,
+#endif
+		.flags = GENL_ADMIN_PERM,
+		.doit = swconfig_set_attr,
+#if KERNEL_VERSION(5, 2, 0) > LINUX_VERSION_CODE
+		.policy = switch_policy,
+#endif
+	},
+	{
+		.cmd = SWITCH_CMD_SET_VLAN,
+#if KERNEL_VERSION(5, 2, 0) <= LINUX_VERSION_CODE
+		.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,
+#endif
+		.flags = GENL_ADMIN_PERM,
+		.doit = swconfig_set_attr,
+#if KERNEL_VERSION(5, 2, 0) > LINUX_VERSION_CODE
+		.policy = switch_policy,
+#endif
+	},
+	{
+		.cmd = SWITCH_CMD_SET_PORT,
+#if KERNEL_VERSION(5, 2, 0) <= LINUX_VERSION_CODE
+		.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,
+#endif
+		.flags = GENL_ADMIN_PERM,
+		.doit = swconfig_set_attr,
+#if KERNEL_VERSION(5, 2, 0) > LINUX_VERSION_CODE
+		.policy = switch_policy,
+#endif
+	},
+	{
+		.cmd = SWITCH_CMD_GET_SWITCH,
+#if KERNEL_VERSION(5, 2, 0) <= LINUX_VERSION_CODE
+		.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,
+#endif
+		.dumpit = swconfig_dump_switches,
+#if KERNEL_VERSION(5, 2, 0) > LINUX_VERSION_CODE
+		.policy = switch_policy,
+#endif
+		.done = swconfig_done,
+	}
+};
+
+static struct genl_family switch_fam = {
+	.name = "switch",
+	.hdrsize = 0,
+	.version = 1,
+	.maxattr = SWITCH_ATTR_MAX,
+#if KERNEL_VERSION(5, 2, 0) <= LINUX_VERSION_CODE
+	.policy = switch_policy,
+#endif
+	.module = THIS_MODULE,
+	.ops = swconfig_ops,
+	.n_ops = ARRAY_SIZE(swconfig_ops),
+};
+
+#ifdef CONFIG_OF
+void of_switch_load_portmap(struct switch_dev *dev)
+{
+	struct device_node *port;
+
+	if (!dev->of_node)
+		return;
+
+	for_each_child_of_node(dev->of_node, port) {
+		const __be32 *prop;
+		const char *segment;
+		int size, phys;
+
+		if (!of_device_is_compatible(port, "swconfig,port"))
+			continue;
+
+		if (of_property_read_string(port, "swconfig,segment", &segment))
+			continue;
+
+		prop = of_get_property(port, "swconfig,portmap", &size);
+		if (!prop)
+			continue;
+
+		if (size != (2 * sizeof(*prop))) {
+			pr_err("%s: failed to parse port mapping\n",
+			       port->name);
+			continue;
+		}
+
+		phys = be32_to_cpup(prop++);
+		if ((phys < 0) | (phys >= dev->ports)) {
+			pr_err("%s: physical port index out of range\n",
+			       port->name);
+			continue;
+		}
+
+		dev->portmap[phys].s = kstrdup(segment, GFP_KERNEL);
+		dev->portmap[phys].virt = be32_to_cpup(prop);
+		pr_debug("Found port: %s, physical: %d, virtual: %d\n", segment,
+			 phys, dev->portmap[phys].virt);
+	}
+}
+#endif
+
+int register_switch(struct switch_dev *dev, struct net_device *netdev)
+{
+	struct switch_dev *sdev;
+	const int max_switches = 8 * sizeof(unsigned long);
+	unsigned long in_use = 0;
+	int i;
+
+	INIT_LIST_HEAD(&dev->dev_list);
+	if (netdev) {
+		dev->netdev = netdev;
+		if (!dev->alias)
+			dev->alias = netdev->name;
+	}
+	BUG_ON(!dev->alias);
+
+	/* Make sure swdev_id doesn't overflow */
+	if (swdev_id == INT_MAX) {
+		return -ENOMEM;
+	}
+
+	if (dev->ports > 0) {
+		dev->portbuf = kcalloc(dev->ports, sizeof(struct switch_port),
+				       GFP_KERNEL);
+		if (!dev->portbuf)
+			return -ENOMEM;
+		dev->portmap = kcalloc(dev->ports,
+				       sizeof(struct switch_portmap),
+				       GFP_KERNEL);
+		if (!dev->portmap) {
+			kfree(dev->portbuf);
+			return -ENOMEM;
+		}
+	}
+	swconfig_defaults_init(dev);
+	mutex_init(&dev->sw_mutex);
+	swconfig_lock();
+	dev->id = ++swdev_id;
+
+	list_for_each_entry(sdev, &swdevs, dev_list) {
+		if (!sscanf(sdev->devname, SWCONFIG_DEVNAME, &i))
+			continue;
+		if (i < 0 || i > max_switches)
+			continue;
+
+		set_bit(i, &in_use);
+	}
+	i = find_first_zero_bit(&in_use, max_switches);
+
+	if (i == max_switches) {
+		swconfig_unlock();
+		return -ENFILE;
+	}
+
+#ifdef CONFIG_OF
+	if (dev->ports)
+		of_switch_load_portmap(dev);
+#endif
+
+	/* fill device name */
+	snprintf(dev->devname, IFNAMSIZ, SWCONFIG_DEVNAME, i);
+
+	list_add_tail(&dev->dev_list, &swdevs);
+	swconfig_unlock();
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(register_switch);
+
+void unregister_switch(struct switch_dev *dev)
+{
+	kfree(dev->portbuf);
+	mutex_lock(&dev->sw_mutex);
+	swconfig_lock();
+	list_del(&dev->dev_list);
+	swconfig_unlock();
+	mutex_unlock(&dev->sw_mutex);
+}
+EXPORT_SYMBOL_GPL(unregister_switch);
+
+int switch_generic_set_link(struct switch_dev *dev, int port,
+			    struct switch_port_link *link)
+{
+	if (WARN_ON(!dev->ops->phy_write16))
+		return -EOPNOTSUPP;
+
+	/* Generic implementation */
+	if (link->aneg) {
+		dev->ops->phy_write16(dev, port, MII_BMCR, 0x0000);
+		dev->ops->phy_write16(dev, port, MII_BMCR,
+				      BMCR_ANENABLE | BMCR_ANRESTART);
+	} else {
+		u16 bmcr = 0;
+
+		if (link->duplex)
+			bmcr |= BMCR_FULLDPLX;
+
+		switch (link->speed) {
+		case SWITCH_PORT_SPEED_10:
+			break;
+		case SWITCH_PORT_SPEED_100:
+			bmcr |= BMCR_SPEED100;
+			break;
+		case SWITCH_PORT_SPEED_1000:
+			bmcr |= BMCR_SPEED1000;
+			break;
+		default:
+			return -EOPNOTSUPP;
+		}
+
+		dev->ops->phy_write16(dev, port, MII_BMCR, bmcr);
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(switch_generic_set_link);
+
+static int __init swconfig_init(void)
+{
+	INIT_LIST_HEAD(&swdevs);
+
+	return genl_register_family(&switch_fam);
+}
+
+static void __exit swconfig_exit(void)
+{
+	genl_unregister_family(&switch_fam);
+}
+
+module_init(swconfig_init);
+module_exit(swconfig_exit);
diff --git a/drivers/net/wireless/ath/ath9k/beacon.c b/drivers/net/wireless/ath/ath9k/beacon.c
index 71e2ada86793..72e2e71aac0e 100644
--- a/drivers/net/wireless/ath/ath9k/beacon.c
+++ b/drivers/net/wireless/ath/ath9k/beacon.c
@@ -251,7 +251,7 @@ void ath9k_beacon_ensure_primary_slot(struct ath_softc *sc)
 	int first_slot = ATH_BCBUF;
 	int slot;
 
-	tasklet_disable(&sc->bcon_tasklet);
+	tasklet_disable_in_atomic(&sc->bcon_tasklet);
 
 	/* Find first taken slot. */
 	for (slot = 0; slot < ATH_BCBUF; slot++) {
diff --git a/drivers/pci/controller/pci-hyperv.c b/drivers/pci/controller/pci-hyperv.c
index 03ed5cb1c4b2..7370cdc1abdb 100644
--- a/drivers/pci/controller/pci-hyperv.c
+++ b/drivers/pci/controller/pci-hyperv.c
@@ -1458,7 +1458,7 @@ static void hv_compose_msi_msg(struct irq_data *data, struct msi_msg *msg)
 	 * Prevents hv_pci_onchannelcallback() from running concurrently
 	 * in the tasklet.
 	 */
-	tasklet_disable(&channel->callback_event);
+	tasklet_disable_in_atomic(&channel->callback_event);
 
 	/*
 	 * Since this function is called with IRQ locks held, can't
diff --git a/drivers/pinctrl/stm32/pinctrl-stm32.c b/drivers/pinctrl/stm32/pinctrl-stm32.c
index 7d9bdedcd71b..9ee26dcab814 100644
--- a/drivers/pinctrl/stm32/pinctrl-stm32.c
+++ b/drivers/pinctrl/stm32/pinctrl-stm32.c
@@ -308,13 +308,7 @@ static const struct gpio_chip stm32_gpio_template = {
 
 static void stm32_gpio_irq_trigger(struct irq_data *d)
 {
-	struct stm32_gpio_bank *bank = d->domain->host_data;
-	int level;
-
-	/* If level interrupt type then retrig */
-	level = stm32_gpio_get(&bank->gpio_chip, d->hwirq);
-	if ((level == 0 && bank->irq_type[d->hwirq] == IRQ_TYPE_LEVEL_LOW) ||
-	    (level == 1 && bank->irq_type[d->hwirq] == IRQ_TYPE_LEVEL_HIGH))
+	if (irqd_is_level_type(d))
 		irq_chip_retrigger_hierarchy(d);
 }
 
diff --git a/drivers/regulator/tps65218-regulator.c b/drivers/regulator/tps65218-regulator.c
index fa263545a70e..4e067184f431 100644
--- a/drivers/regulator/tps65218-regulator.c
+++ b/drivers/regulator/tps65218-regulator.c
@@ -101,7 +101,7 @@ static int tps65218_pmic_enable(struct regulator_dev *dev)
 	struct tps65218 *tps = rdev_get_drvdata(dev);
 	int rid = rdev_get_id(dev);
 
-	if (rid < TPS65218_DCDC_1 || rid > TPS65218_LDO_1)
+	if (!TPS65218_REG_ID_VALID(rid))
 		return -EINVAL;
 
 	/* Enable the regulator and password protection is level 1 */
@@ -115,7 +115,7 @@ static int tps65218_pmic_disable(struct regulator_dev *dev)
 	struct tps65218 *tps = rdev_get_drvdata(dev);
 	int rid = rdev_get_id(dev);
 
-	if (rid < TPS65218_DCDC_1 || rid > TPS65218_LDO_1)
+	if (!TPS65218_REG_ID_VALID(rid))
 		return -EINVAL;
 
 	/* Disable the regulator and password protection is level 1 */
@@ -126,9 +126,9 @@ static int tps65218_pmic_disable(struct regulator_dev *dev)
 static int tps65218_pmic_set_suspend_enable(struct regulator_dev *dev)
 {
 	struct tps65218 *tps = rdev_get_drvdata(dev);
-	unsigned int rid = rdev_get_id(dev);
+	int rid = rdev_get_id(dev);
 
-	if (rid > TPS65218_LDO_1)
+	if (!TPS65218_REG_ID_VALID(rid))
 		return -EINVAL;
 
 	return tps65218_clear_bits(tps, dev->desc->bypass_reg,
@@ -139,9 +139,9 @@ static int tps65218_pmic_set_suspend_enable(struct regulator_dev *dev)
 static int tps65218_pmic_set_suspend_disable(struct regulator_dev *dev)
 {
 	struct tps65218 *tps = rdev_get_drvdata(dev);
-	unsigned int rid = rdev_get_id(dev);
+	int rid = rdev_get_id(dev);
 
-	if (rid > TPS65218_LDO_1)
+	if (!TPS65218_REG_ID_VALID(rid))
 		return -EINVAL;
 
 	/*
diff --git a/drivers/rtc/rtc-rs5c372.c b/drivers/rtc/rtc-rs5c372.c
index 3bd6eaa0dcf6..f29959b2a685 100644
--- a/drivers/rtc/rtc-rs5c372.c
+++ b/drivers/rtc/rtc-rs5c372.c
@@ -346,6 +346,43 @@ static int rs5c372_get_trim(struct i2c_client *client, int *osc, int *trim)
 
 	return 0;
 }
+
+/*
+ * dev_bit: Bit DEV
+ * dec: bit F6 (a '1' will cause an increment of 1-second time count)
+ * trim: bits F5..F0 (adjustment factor)
+ * */
+static int rs5c372_set_trim(struct i2c_client *client, u8 dev_bit, u8 dec, int trim)
+{
+	struct rs5c372 *rs5c372 = i2c_get_clientdata(client);
+	unsigned char	buf[1];
+	int		addr;
+	int             tmp = 0;
+
+	dev_dbg(&client->dev, "%s: set trim to %d (0x%x).\n",
+		__func__, (trim & 0x3f), (trim & 0x3f));
+
+	if (dev_bit)
+		tmp |= 0x80;	/* set DEV Bit */
+
+	if (dec)
+		tmp |= 0x40;	/* set F6 Bit */
+
+	tmp |= (trim & 0x3f);
+
+	addr   = RS5C_ADDR(RS5C372_REG_TRIM);
+/* 	buf[0] = bin2bcd(tmp); */
+	buf[0] = tmp;
+
+	if (i2c_smbus_write_i2c_block_data(client, addr, sizeof(buf), buf) < 0) {
+		dev_err(&client->dev, "%s: write error\n", __func__);
+		return -EIO;
+	}
+
+	rs5c_get_regs(rs5c372);
+
+	return 0;
+}
 #endif
 
 static int rs5c_rtc_alarm_irq_enable(struct device *dev, unsigned int enabled)
@@ -698,6 +735,39 @@ static int rs5c372_probe(struct i2c_client *client,
 		goto exit;
 	}
 
+#ifdef	NEED_TRIM
+	{
+		struct device_node *node = client->dev.of_node;
+		u8 dev_bit, dec;
+		int trim;
+		u32 trim_data[3];
+
+		if (node) {
+			int ret = of_property_read_u32_array(node, "trim-data", trim_data,
+						ARRAY_SIZE(trim_data));
+			if (!ret) {
+				dev_bit = (u8) trim_data[0];
+				dec = (u8) trim_data[1];
+				trim = (int) trim_data[2];
+
+				dev_info(&client->dev, "set osc adjustment to 0x%x (%d)\n",
+						trim, trim);
+
+				/* Calculation is based on an empirical drift value of +32.47 ppm.
+				 * Set trim value to (32768.00 * 32.47 ppm * 10) + 1 = 11.6398 ~ 12.
+				 * Measurements provide better values for an adjustment value of 13
+				 * which results in a drift value of -1.41 ppm. Also Hardware measurements
+				 * of osc frequency indicate the need of a higher value. So we set 13 here.
+				 * See documentation of r2221 on page 34.
+				 */
+				rs5c372_set_trim(client, dev_bit, dec, trim);
+			} else {
+				dev_info(&client->dev, "skip osc adjustment\n");
+			}
+		}
+	}
+#endif
+
 	dev_info(&client->dev, "%s found, %s\n",
 			({ char *s; switch (rs5c372->type) {
 			case rtc_r2025sd:	s = "r2025sd"; break;
diff --git a/drivers/scsi/fcoe/fcoe.c b/drivers/scsi/fcoe/fcoe.c
index 0f9274960dc6..dc97e4f1f4ad 100644
--- a/drivers/scsi/fcoe/fcoe.c
+++ b/drivers/scsi/fcoe/fcoe.c
@@ -1452,11 +1452,11 @@ static int fcoe_rcv(struct sk_buff *skb, struct net_device *netdev,
 static int fcoe_alloc_paged_crc_eof(struct sk_buff *skb, int tlen)
 {
 	struct fcoe_percpu_s *fps;
-	int rc;
+	int rc, cpu = get_cpu_light();
 
-	fps = &get_cpu_var(fcoe_percpu);
+	fps = &per_cpu(fcoe_percpu, cpu);
 	rc = fcoe_get_paged_crc_eof(skb, tlen, fps);
-	put_cpu_var(fcoe_percpu);
+	put_cpu_light();
 
 	return rc;
 }
@@ -1641,11 +1641,11 @@ static inline int fcoe_filter_frames(struct fc_lport *lport,
 		return 0;
 	}
 
-	stats = per_cpu_ptr(lport->stats, get_cpu());
+	stats = per_cpu_ptr(lport->stats, get_cpu_light());
 	stats->InvalidCRCCount++;
 	if (stats->InvalidCRCCount < 5)
 		printk(KERN_WARNING "fcoe: dropping frame with CRC error\n");
-	put_cpu();
+	put_cpu_light();
 	return -EINVAL;
 }
 
@@ -1686,7 +1686,7 @@ static void fcoe_recv_frame(struct sk_buff *skb)
 	 */
 	hp = (struct fcoe_hdr *) skb_network_header(skb);
 
-	stats = per_cpu_ptr(lport->stats, get_cpu());
+	stats = per_cpu_ptr(lport->stats, get_cpu_light());
 	if (unlikely(FC_FCOE_DECAPS_VER(hp) != FC_FCOE_VER)) {
 		if (stats->ErrorFrames < 5)
 			printk(KERN_WARNING "fcoe: FCoE version "
@@ -1718,13 +1718,13 @@ static void fcoe_recv_frame(struct sk_buff *skb)
 		goto drop;
 
 	if (!fcoe_filter_frames(lport, fp)) {
-		put_cpu();
+		put_cpu_light();
 		fc_exch_recv(lport, fp);
 		return;
 	}
 drop:
 	stats->ErrorFrames++;
-	put_cpu();
+	put_cpu_light();
 	kfree_skb(skb);
 }
 
diff --git a/drivers/scsi/fcoe/fcoe_ctlr.c b/drivers/scsi/fcoe/fcoe_ctlr.c
index 5ea426effa60..0d6b9acc7cf8 100644
--- a/drivers/scsi/fcoe/fcoe_ctlr.c
+++ b/drivers/scsi/fcoe/fcoe_ctlr.c
@@ -828,7 +828,7 @@ static unsigned long fcoe_ctlr_age_fcfs(struct fcoe_ctlr *fip)
 
 	INIT_LIST_HEAD(&del_list);
 
-	stats = per_cpu_ptr(fip->lp->stats, get_cpu());
+	stats = per_cpu_ptr(fip->lp->stats, get_cpu_light());
 
 	list_for_each_entry_safe(fcf, next, &fip->fcfs, list) {
 		deadline = fcf->time + fcf->fka_period + fcf->fka_period / 2;
@@ -864,7 +864,7 @@ static unsigned long fcoe_ctlr_age_fcfs(struct fcoe_ctlr *fip)
 				sel_time = fcf->time;
 		}
 	}
-	put_cpu();
+	put_cpu_light();
 
 	list_for_each_entry_safe(fcf, next, &del_list, list) {
 		/* Removes fcf from current list */
diff --git a/drivers/scsi/libfc/fc_exch.c b/drivers/scsi/libfc/fc_exch.c
index a50f1eef0e0c..0b2acad7c354 100644
--- a/drivers/scsi/libfc/fc_exch.c
+++ b/drivers/scsi/libfc/fc_exch.c
@@ -826,10 +826,10 @@ static struct fc_exch *fc_exch_em_alloc(struct fc_lport *lport,
 	}
 	memset(ep, 0, sizeof(*ep));
 
-	cpu = get_cpu();
+	cpu = get_cpu_light();
 	pool = per_cpu_ptr(mp->pool, cpu);
 	spin_lock_bh(&pool->lock);
-	put_cpu();
+	put_cpu_light();
 
 	/* peek cache of free slot */
 	if (pool->left != FC_XID_UNKNOWN) {
diff --git a/drivers/spi/Kconfig b/drivers/spi/Kconfig
index aadaea052f51..9d8f57c443bf 100644
--- a/drivers/spi/Kconfig
+++ b/drivers/spi/Kconfig
@@ -573,12 +573,28 @@ config SPI_TI_QSPI
 	  This device supports single, dual and quad read support, while
 	  it only supports single write mode.
 
+config SPI_KBUS_OMAP_EXTENSION
+	bool "KBUS Extension for OMAP MCSPI Driver (Read Notes!)"
+	depends on SPI_OMAP24XX
+	help
+	  KBUS Extension for the McSPI OMAP driver.
+	  It implements the communication protocol for the infineon XE164 Chip
+ 	  which does the communication with the KBUS logic.
+ 	  ATTENTION: This disbles the use of a worker thread (work queue).
+	  In its current state only one userspace process is allowed.
+
 config SPI_OMAP_100K
 	tristate "OMAP SPI 100K"
 	depends on ARCH_OMAP850 || ARCH_OMAP730 || COMPILE_TEST
 	help
 	  OMAP SPI 100K master controller for omap7xx boards.
 
+config SPI_KBUS
+	select SPI_KBUS_OMAP_EXTENSION
+	tristate "Wago KBUS Driver"
+	help
+	  This is driver provides access to the KBUS interface.
+
 config SPI_ORION
 	tristate "Orion SPI master"
 	depends on PLAT_ORION || ARCH_MVEBU || COMPILE_TEST
diff --git a/drivers/spi/Makefile b/drivers/spi/Makefile
index 6fea5821662e..05e170eb056b 100644
--- a/drivers/spi/Makefile
+++ b/drivers/spi/Makefile
@@ -131,6 +131,8 @@ obj-$(CONFIG_SPI_XTENSA_XTFPGA)		+= spi-xtensa-xtfpga.o
 obj-$(CONFIG_SPI_ZYNQ_QSPI)		+= spi-zynq-qspi.o
 obj-$(CONFIG_SPI_ZYNQMP_GQSPI)		+= spi-zynqmp-gqspi.o
 obj-$(CONFIG_SPI_AMD)			+= spi-amd.o
+obj-$(CONFIG_SPI_KBUS)			+= spi-kbus.o
+obj-$(CONFIG_SPI_OMAP24XX)		+= spi-omap2-mcspi.o
 
 # SPI slave protocol handlers
 obj-$(CONFIG_SPI_SLAVE_TIME)		+= spi-slave-time.o
diff --git a/drivers/spi/spi-kbus.c b/drivers/spi/spi-kbus.c
new file mode 100644
index 000000000000..72ab4f856c16
--- /dev/null
+++ b/drivers/spi/spi-kbus.c
@@ -0,0 +1,1279 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/version.h>
+#include <linux/init.h>
+#include <linux/uaccess.h>
+#include <linux/module.h>
+#include <linux/fs.h>
+#include <linux/cdev.h>
+#include <linux/poll.h>
+#include <linux/gpio.h>
+#include <linux/interrupt.h>
+#include <linux/device.h>
+#include <linux/sched.h>
+#include <uapi/linux/sched/types.h>
+#include <linux/file.h>
+#include <linux/dma-mapping.h>
+
+#include <linux/of.h>
+#include <linux/of_address.h>
+#include <linux/of_device.h>
+#include <linux/of_gpio.h>
+#include <linux/of_irq.h>
+
+#include <linux/delay.h>
+#include <linux/spi/spi.h>
+#include <linux/spi/spidev.h>
+#include <linux/spi/kbus.h>
+
+#include <misc/wago-tests.h>
+
+#define PXC_SPI_KBUS_TRACER
+#define CREATE_TRACE_POINTS
+#include <trace/events/pxc.h>
+
+#define KBUS_DUMMY_BIT_PER_WORD 8
+#define KBUS_DUMMY_SPEED 1000000
+
+#if KBUS_TESTING
+struct wago_trace_data wago_ktest;
+#endif
+
+static dev_t kbus_dev;
+static struct cdev kbus_cdev;
+
+/* sysfs */
+extern struct class *wsysinit_sysfs_class;
+extern struct device *wsysinit_sysfs_device;
+static struct device *kbus_device;
+
+int kbus_wait_for_gpio(int gpio)
+{
+	unsigned long timeout;
+
+	timeout = jiffies + msecs_to_jiffies(1); /* 1000 ? */
+	while (gpio_get_value(gpio)) { /* active low */
+		if (time_after(jiffies, timeout))
+			return -1;
+		cpu_relax();
+	}
+	return 0;
+}
+
+int kbus_error(void)
+{
+	struct kbus_drv_data *kdrvdata = dev_get_drvdata(kbus_device);
+
+	/*
+	 * read error code from gpios
+	 */
+	kdrvdata->kbus_err = !gpiod_get_value(kdrvdata->gpio_nerr);
+	return kdrvdata->kbus_err ? -1 : 0;
+}
+
+int kbus_wait_for_irq(void)
+{
+	struct kbus_drv_data *kdrvdata = dev_get_drvdata(kbus_device);
+	unsigned long timeout;
+
+	timeout = jiffies + msecs_to_jiffies(1000);
+	while (!kdrvdata->kbus_irq_state) {
+		if (time_after(jiffies, timeout))
+			return -1;
+		cpu_relax();
+	}
+	kdrvdata->kbus_irq_state = 0;
+	return 0;
+}
+
+int kbus_wait_for_event(int *event)
+{
+	unsigned long timeout;
+
+	timeout = jiffies + msecs_to_jiffies(1000);
+	while (!(*event)) {
+		if (time_after(jiffies, timeout))
+			return -1;
+		cpu_relax();
+	}
+	*event = 0;
+	return 0;
+}
+
+static irqreturn_t kbus_isr(int irq, void *dev)
+{
+	struct kbus_drv_data *kdrvdata = dev_get_drvdata(kbus_device);
+
+	trace_pxc_kbus(__func__, "kbus-irq:in");
+	kdrvdata->kbus_irq_state = 0; /* attention: changed polarisation */
+
+	/* wake_up_interruptible(&kdrvdata->kbus_irq_wq); */
+	wake_up(&kdrvdata->kbus_irq_wq);
+
+	trace_pxc_kbus(__func__, "kbus-irq:out");
+	return IRQ_HANDLED;
+}
+
+static ssize_t kbus_write(struct file *filp, const char __user *buf,
+			  size_t count, loff_t *f_pos)
+{
+	struct kbus_drv_data *kdrvdata = dev_get_drvdata(kbus_device);
+	u32 irq_value = 0;
+	ssize_t status = 0;
+
+	status = __get_user(irq_value, buf);
+	if (status == 0) {
+		if (irq_value) {
+			if (!kdrvdata->kbus_irq_enabled)
+				KBUS_ENABLE_IRQ(kdrvdata->kbus_irq);
+			kdrvdata->kbus_irq_enabled = 1;
+			trace_pxc_kbus(__func__, "turned-on irqs!");
+		} else {
+			if (kdrvdata->kbus_irq_enabled)
+				KBUS_DISABLE_IRQ(kdrvdata->kbus_irq);
+			kdrvdata->kbus_irq_enabled = 0;
+			trace_pxc_kbus(__func__, "turned-off irqs!");
+		}
+	}
+
+	return status;
+}
+
+static struct task_struct *find_dma_task(void)
+{
+	struct task_struct *p, *found = NULL;
+	struct kbus_drv_data *kdrvdata = dev_get_drvdata(kbus_device);
+	struct device_node *dma_node;
+	int irq = 0;
+	char task_name[16];
+
+	dma_node = of_find_node_by_name(NULL, "dma");
+	if (!dma_node) {
+		dma_node = of_find_node_by_name(NULL, "dma-controller");
+		if (!dma_node)
+			return NULL;
+		irq = of_irq_get(dma_node, 1);
+	} else
+		irq = of_irq_get(dma_node, 0);
+	of_node_put(dma_node);
+	if (irq <= 0)
+		return NULL;
+
+	snprintf(task_name, 16, "irq/%d-%s", irq,
+		kdrvdata->kbus_dma_boost_irq_thread);
+
+	read_lock(&tasklist_lock);
+	for_each_process(p) {
+		if (p->flags & PF_KTHREAD) {
+			if ((strcmp(p->comm, task_name) == 0)) {
+				found = p;
+				break;
+			}
+		}
+	}
+	read_unlock(&tasklist_lock);
+
+	return found;
+}
+
+void kbus_boost_dma_task(u8 enable)
+{
+	struct kbus_drv_data *kdrvdata = dev_get_drvdata(kbus_device);
+	struct sched_param boost_param = {
+		.sched_priority = enable ? kdrvdata->kbus_dma_boost_prio :
+					   kdrvdata->kbus_dma_normal_prio
+	};
+
+	if (kdrvdata->dma_task) {
+		if ((enable && !kdrvdata->kbus_dma_boost_en) ||
+		    (!enable && kdrvdata->kbus_dma_boost_en)) {
+			kdrvdata->kbus_dma_boost_en =
+				!kdrvdata->kbus_dma_boost_en;
+			sched_setscheduler(kdrvdata->dma_task, SCHED_FIFO,
+					   &boost_param);
+		}
+	}
+}
+
+static int kbus_open(struct inode *node, struct file *file)
+{
+	struct kbus_drv_data *kdrvdata = dev_get_drvdata(kbus_device);
+
+#if KBUS_TESTING
+	wago_tests_init(&wago_ktest, 1);
+#endif
+	/* get pid of irq/12-DMA here
+	 * and set it in kdrvdata
+	 */
+	if (!kdrvdata->dma_task) {
+		kdrvdata->dma_task = find_dma_task();
+		if (kdrvdata->dma_task)
+			pr_info("dma-task pid is %u.\n",
+				kdrvdata->dma_task->pid);
+		else
+			printk("dma-task not found!\n");
+	}
+
+	return 0;
+}
+
+static int kbus_close(struct inode *node, struct file *file)
+{
+#if KBUS_TESTING
+	wago_tests_deinit();
+#endif
+	return 0;
+}
+
+static void kbus_complete(void *arg)
+{
+	kbus_wago_mpoint(); /* MX */
+	trace_pxc_kbus(__func__, "jump to complete()");
+	complete(arg);
+}
+
+static int kbus_spi_sync(struct spi_device *spi, struct spi_message *msg)
+{
+	DECLARE_COMPLETION_ONSTACK(done);
+	int status;
+
+	msg->complete = kbus_complete;
+	msg->context = &done;
+
+	trace_pxc_kbus(__func__, "jump to spi_async");
+	status = spi_async(spi, msg);
+	if (status == 0) {
+		wait_for_completion(&done);
+		status = msg->status;
+		if (status == 0)
+			status = msg->actual_length;
+	}
+
+	return status;
+}
+
+static int kbus_spi_message(struct spi_ioc_transfer *u_xfers, unsigned n_xfers)
+{
+	struct spi_message msg;
+	struct spi_transfer *k_xfers;
+	struct spi_transfer *k_tmp;
+	struct spi_ioc_transfer *u_tmp;
+	unsigned int n, total;
+	u8 *buf;
+	int status = -EFAULT;
+	u32 bufsiz = KBUS__MAX_BUF_LEN;
+	struct kbus_drv_data *kdrvdata = dev_get_drvdata(kbus_device);
+
+	spi_message_init(&msg);
+	k_xfers = kcalloc(n_xfers, sizeof(*k_tmp), GFP_KERNEL);
+	if (k_xfers == NULL)
+		return -ENOMEM;
+
+	/* Construct spi_message, copying any tx data to bounce buffer.
+	 * We walk the array of user-provided transfers, using each one
+	 * to initialize a kernel version of the same transfer.
+	 */
+	buf = kdrvdata->tx_buf; /* use in this case the buffer for tx and rx */
+	total = 0;
+	for (n = n_xfers, k_tmp = k_xfers, u_tmp = u_xfers; n;
+	     n--, k_tmp++, u_tmp++) {
+		k_tmp->len = u_tmp->len;
+
+		total += k_tmp->len;
+		if (total > bufsiz) {
+			status = -EMSGSIZE;
+			goto done;
+		}
+
+		if (u_tmp->rx_buf) {
+			k_tmp->rx_buf = buf;
+			if (!access_ok((u8 __user *)(uintptr_t)u_tmp->rx_buf,
+				       u_tmp->len))
+				goto done;
+		}
+		if (u_tmp->tx_buf) {
+			k_tmp->tx_buf = buf;
+			if (copy_from_user(
+				    buf,
+				    (const u8 __user *)(uintptr_t)u_tmp->tx_buf,
+				    u_tmp->len))
+				goto done;
+		}
+		buf += k_tmp->len;
+
+		k_tmp->cs_change = !!u_tmp->cs_change;
+		k_tmp->bits_per_word = u_tmp->bits_per_word;
+		k_tmp->delay_usecs = u_tmp->delay_usecs;
+		k_tmp->speed_hz = u_tmp->speed_hz;
+
+		spi_message_add_tail(k_tmp, &msg);
+	}
+
+	status = kbus_spi_sync(kdrvdata->spi, &msg);
+	if (status < 0)
+		goto done;
+
+	/* copy any rx data out of bounce buffer */
+	buf = kdrvdata->tx_buf;
+	for (n = n_xfers, u_tmp = u_xfers; n; n--, u_tmp++) {
+		if (u_tmp->rx_buf) {
+			if (__copy_to_user((u8 __user *)(uintptr_t)u_tmp->rx_buf,
+					   buf, u_tmp->len)) {
+				status = -EFAULT;
+				goto done;
+			}
+		}
+		buf += u_tmp->len;
+	}
+	status = total;
+
+done:
+	kfree(k_xfers);
+	return status;
+}
+
+static void kbus_dump(char *prefix, char *buf, int len)
+{
+	int i;
+
+	for (i = 0; i < len; i += 32) {
+		pr_info("DATADUMP(%s) copylen %4d buf %p"
+			"[%.2x-%.2x-%.2x-%.2x-%.2x-%.2x-%.2x-%.2x"
+			"-%.2x-%.2x-%.2x-%.2x-%.2x-%.2x-%.2x-%.2x"
+			"-%.2x-%.2x-%.2x-%.2x-%.2x-%.2x-%.2x-%.2x"
+			"-%.2x-%.2x-%.2x-%.2x-%.2x-%.2x-%.2x-%.2x]\n",
+			prefix, (len - i), buf, buf[i + 0], buf[i + 1],
+			buf[i + 2], buf[i + 3], buf[i + 4], buf[i + 5],
+			buf[i + 6], buf[i + 7], buf[i + 8], buf[i + 9],
+			buf[i + 10], buf[i + 11], buf[i + 12], buf[i + 13],
+			buf[i + 14], buf[i + 15], buf[i + 16], buf[i + 17],
+			buf[i + 18], buf[i + 19], buf[i + 20], buf[i + 21],
+			buf[i + 22], buf[i + 23], buf[i + 24], buf[i + 25],
+			buf[i + 26], buf[i + 27], buf[i + 28], buf[i + 29],
+			buf[i + 30], buf[i + 31]);
+	}
+}
+
+static int kbus_data_txrx(struct kbus_data *kdata)
+{
+	struct kbus_drv_data *kdrvdata = dev_get_drvdata(kbus_device);
+	struct spi_message msg;
+	struct spi_transfer spi_t[] = {
+		{
+			.tx_buf = kdrvdata->tx_buf,
+			.len = kdata->byte_len,
+			.bits_per_word = KBUS_DUMMY_BIT_PER_WORD,
+			.speed_hz = KBUS_DUMMY_SPEED,
+		},
+		{
+			.rx_buf = kdrvdata->rx_buf,
+			.len = kdata->byte_len,
+			.bits_per_word = KBUS_DUMMY_BIT_PER_WORD,
+			.speed_hz = KBUS_DUMMY_SPEED,
+		},
+	};
+	int status;
+
+	trace_pxc_kbus(__func__, "enter");
+
+	if (!kdata->byte_len || kdata->byte_len > KBUS__MAX_BUF_LEN)
+		return -EINVAL;
+
+	if (kdata->timeout_ms)
+		kdrvdata->timeout_ms = kdata->timeout_ms;
+	else
+		kdrvdata->timeout_ms = KBUS_IRQ_TIMEOUT;
+
+	/* get the userspace data */
+	if (copy_from_user(kdrvdata->tx_buf, kdata->tx_buf, kdata->byte_len))
+		return -EFAULT;
+
+	if (trace_pxc_buf32_enabled())
+		kbus_dump("KTX", kdrvdata->tx_buf, kdata->byte_len);
+
+	/* clear rx buf */
+	memset(kdrvdata->rx_buf, 0, kdata->byte_len);
+
+	/*
+	 * set some valid dummy data.
+	 * This configuration will not be valid during transfer.
+	 * It is only set by kbus_spi_config().
+	 *
+	 */
+
+	kbus_wago_mpoint(); /* M1 */
+
+	/* create message and add transfers to it */
+	spi_message_init(&msg);
+
+	/* setup dma */
+	if (kdrvdata->use_dma) {
+		trace_pxc_kbus(__func__, "dma transfer enabled");
+		spi_t[0].tx_dma = kdrvdata->tx_buf_dma;
+		spi_t[1].rx_dma = kdrvdata->rx_buf_dma;
+		msg.is_dma_mapped = 1;
+	}
+
+	spi_message_add_tail(&spi_t[0], &msg);
+	spi_message_add_tail(&spi_t[1], &msg);
+
+	trace_pxc_kbusmsg(__func__, &msg, "");
+	status = kbus_spi_sync(kdrvdata->spi, &msg);
+	if (status > 0)
+		if (copy_to_user(kdata->rx_buf, kdrvdata->rx_buf,
+				 kdata->byte_len))
+			return -EFAULT;
+	trace_pxc_kbusmsg(__func__, &msg, "");
+
+	/* tell the user about the error, if occured */
+	if (status < 0 && kdrvdata->kbus_err) {
+		if (__put_user(kdrvdata->kbus_err, kdata->err))
+			return -EFAULT;
+	}
+
+	kbus_wago_mpoint(); /* M5 */
+
+	if (trace_pxc_buf32_enabled())
+		kbus_dump("KRX", kdrvdata->rx_buf, kdata->byte_len);
+
+	trace_pxc_kbus(__func__, "leave");
+
+	return status;
+}
+
+static int kbus_binary_txrx(struct kbus_data *kbinary)
+{
+	struct kbus_drv_data *kdrvdata = dev_get_drvdata(kbus_device);
+	struct spi_message msg;
+	struct spi_transfer spi_t[] = {
+		{
+			.len = kbinary->byte_len,
+			.bits_per_word = kdrvdata->spi->bits_per_word,
+			.speed_hz = kdrvdata->spi->max_speed_hz,
+		},
+	};
+	int status;
+
+	trace_pxc_kbus(__func__, "enter");
+
+	if (!kbinary->byte_len || kbinary->byte_len > KBUS__MAX_BUF_LEN)
+		return -EINVAL;
+
+	if (!kbinary->tx_buf && !kbinary->rx_buf)
+		return -EINVAL;
+
+	if (kbinary->tx_buf) {
+		spi_t[0].tx_buf = kdrvdata->tx_buf;
+		/* get the userspace data */
+		if (copy_from_user(kdrvdata->tx_buf, kbinary->tx_buf,
+				   kbinary->byte_len))
+			return -EFAULT;
+	}
+
+	if (kbinary->rx_buf) {
+		spi_t[0].rx_buf = kdrvdata->rx_buf;
+		/* clear rx buf */
+		memset(kdrvdata->rx_buf, 0, kbinary->byte_len);
+	}
+
+	/* create message and add transfer to it */
+	spi_message_init(&msg);
+	spi_message_add_tail(&spi_t[0], &msg);
+
+	trace_pxc_kbusmsg(__func__, &msg, "");
+	status = kbus_spi_sync(kdrvdata->spi, &msg);
+	if (status && kbinary->rx_buf)
+		if (copy_to_user(kbinary->rx_buf, kdrvdata->rx_buf,
+				 kbinary->byte_len))
+			return -EFAULT;
+	trace_pxc_kbusmsg(__func__, &msg, "");
+
+	trace_pxc_kbus(__func__, "leave");
+	return status;
+}
+
+static int kbus_cmd_txrx(struct kbus_cmd *kcmd)
+{
+	struct kbus_drv_data *kdrvdata = dev_get_drvdata(kbus_device);
+	struct spi_message msg;
+	/*
+	 * set some valid dummy data for bits_per_word and speed_hz.
+	 * This configuration will not be valid during transfer.
+	 * It is only set by kbus_spi_config(). */
+	struct spi_transfer spi_t[] = {
+		{
+			/* TX */
+			.tx_buf = kdrvdata->tx_buf,
+			.len = kcmd->byte_len_tx,
+			.bits_per_word = KBUS_DUMMY_BIT_PER_WORD,
+			.speed_hz = KBUS_DUMMY_SPEED,
+		},
+		{
+			/* RX0 */
+			.rx_buf = kdrvdata->rx_buf,
+			.len = 6,
+			.bits_per_word = KBUS_DUMMY_BIT_PER_WORD,
+			.speed_hz = KBUS_DUMMY_SPEED,
+		},
+		{
+			/* RX1 */
+			.rx_buf = kdrvdata->rx_buf + 6,
+			.len = kcmd->byte_len_rx -
+			       6, /* set remaining max bytes.
+							  * RX0 will update it later. */
+			.bits_per_word = KBUS_DUMMY_BIT_PER_WORD,
+			.speed_hz = KBUS_DUMMY_SPEED,
+		},
+	};
+	int status;
+
+	trace_pxc_kbus(__func__, "enter");
+
+	if ((!kcmd->byte_len_tx && !kcmd->byte_len_rx) ||
+	    kcmd->byte_len_tx > KBUS__MAX_BUF_LEN ||
+	    kcmd->byte_len_rx > KBUS__MAX_BUF_LEN)
+		return -EINVAL;
+
+	if (kcmd->timeout_ms)
+		kdrvdata->timeout_ms = kcmd->timeout_ms;
+	else
+		kdrvdata->timeout_ms = KBUS_IRQ_TIMEOUT;
+
+	/* get the userspace data */
+	if (copy_from_user(kdrvdata->tx_buf, kcmd->tx_buf, kcmd->byte_len_tx))
+		return -EFAULT;
+
+	/* clear rx buf */
+	memset(kdrvdata->rx_buf, 0, kcmd->byte_len_rx);
+
+	kbus_wago_mpoint(); /* M1 */
+	trace_pxc_kbus(__func__, "M1");
+
+	/* create message and add transfers to it */
+	spi_message_init(&msg);
+
+	/* setup dma */
+	if (kdrvdata->use_dma) {
+		trace_pxc_kbus(__func__, "dma transfer enabled");
+		spi_t[0].tx_dma = kdrvdata->tx_buf_dma;
+		spi_t[1].rx_dma = kdrvdata->rx_buf_dma;
+		spi_t[2].rx_dma = kdrvdata->rx_buf_dma + 6;
+		msg.is_dma_mapped = 1;
+	}
+
+	spi_message_add_tail(&spi_t[0], &msg);
+	spi_message_add_tail(&spi_t[1], &msg);
+	spi_message_add_tail(&spi_t[2], &msg);
+
+	/* do the actual spi msg transfer */
+	trace_pxc_kbusmsg(__func__, &msg, "");
+	status = kbus_spi_sync(kdrvdata->spi, &msg);
+	if (status)
+		if (copy_to_user(kcmd->rx_buf, kdrvdata->rx_buf,
+				 spi_t[2].len + 6))
+			return -EFAULT;
+	trace_pxc_kbusmsg(__func__, &msg, "");
+
+	/* tell the user about the error, if occured */
+	if (status < 0 && kdrvdata->kbus_err) {
+		if (__put_user(kdrvdata->kbus_err, kcmd->err))
+			return -EFAULT;
+	}
+
+	kbus_wago_mpoint(); /* M5 */
+	trace_pxc_kbus(__func__, "leave");
+
+	return status;
+}
+
+static int kbus_spi_config(struct kbus_spi_config *kconfig)
+{
+	struct kbus_drv_data *kdrvdata = dev_get_drvdata(kbus_device);
+	int retval = 0;
+	u8 save = 0;
+
+	if (!kconfig->bits_per_word && !kconfig->mode)
+		return -EINVAL;
+
+	/* update values */
+	if (kconfig->bits_per_word) {
+		if (kconfig->bits_per_word != 16)
+			trace_pxc_kbus(__func__,
+				       "WARNING: bits_per_word != 16Bit.");
+		kdrvdata->spi->bits_per_word = kconfig->bits_per_word;
+	}
+	if (kconfig->max_speed_hz)
+		kdrvdata->spi->max_speed_hz = kconfig->max_speed_hz;
+	if (kconfig->mode) {
+		if (kconfig->mode & ~SPI_MODE_MASK)
+			return -EINVAL;
+		save = kdrvdata->spi->mode;
+		kconfig->mode |= kdrvdata->spi->mode & ~SPI_MODE_MASK;
+		kdrvdata->spi->mode = (u8)kconfig->mode;
+	}
+
+	/* do the actual spi setup */
+	retval = spi_setup(kdrvdata->spi);
+	if (retval < 0) { /* restore mode if changed */
+		if (save)
+			kdrvdata->spi->mode = save;
+	} else
+		kbus_dbg("%s: spi mode is updated: %02x\n", __func__,
+			 kconfig->mode);
+
+	return retval;
+}
+
+static long kbus_ioctl(struct file *file, uint cmd, ulong arg)
+{
+	long ret = 0;
+	struct kbus_drv_data *kdrvdata = dev_get_drvdata(kbus_device);
+	struct spi_device *spi = kdrvdata->spi;
+
+	switch (cmd) {
+	case KBUS_IOC_CMD: /* FIXME */
+	{
+		struct kbus_cmd kcmd;
+		struct kbus_cmd __user *kcmd_user;
+
+		trace_pxc_kbus(__func__,
+			       "KCMD: Enter: Set kbus_irq_state to 1.");
+		kbus_wago_mpoint(); /* M0 */
+
+		spi_set_drvdata(spi, kdrvdata);
+		kdrvdata->kbus_irq_state =
+			1; /* will be set to 0 by kbus_isr() */
+
+		gpiod_set_value(kdrvdata->gpio_cmdsel, 0);
+		kdrvdata->cmdsel = 1;
+		/* Indicate Transfer Start */
+		gpiod_set_value(kdrvdata->gpio_nirq, 0);
+
+		kcmd_user = (struct kbus_cmd __user *)arg;
+		if (copy_from_user(&kcmd, kcmd_user, sizeof(kcmd))) {
+			/* release the irq pin */
+			gpiod_set_value(kdrvdata->gpio_nirq, 1);
+			return -EFAULT;
+		}
+
+		ret = kbus_cmd_txrx(&kcmd);
+		if (ret < 0) {
+			/* release the irq pin */
+			gpiod_set_value(kdrvdata->gpio_nirq, 1);
+			trace_pxc_kbus(
+				__func__,
+				"KCMD: kbus_cmd_txrx() returned with error!");
+		}
+
+		/* FIXME: JUST FOR TESTING */
+		gpiod_set_value(kdrvdata->gpio_cmdsel, 1);
+
+		kbus_wago_mpoint(); /* M6 */
+		trace_pxc_kbus(__func__, "KCMD: Leave");
+	} break;
+	case KBUS_IOC_DATA: {
+		struct kbus_data kdata;
+		struct kbus_data __user *kdata_user;
+
+		trace_pxc_kbus(__func__,
+			       "KDATA: Enter: Set kbus_irq_state to 1.");
+		kbus_wago_mpoint(); /* M0 */
+
+		spi_set_drvdata(spi, kdrvdata);
+		kdrvdata->kbus_irq_state =
+			1; /* will be set to 0 by kbus_isr() */
+
+		gpiod_set_value(kdrvdata->gpio_cmdsel, 1);
+		kdrvdata->cmdsel = 0;
+		/* Indicate Transfer Start */
+		gpiod_set_value(kdrvdata->gpio_nirq, 0);
+
+		kdata_user = (struct kbus_data __user *)arg;
+
+		if (copy_from_user(&kdata, kdata_user, sizeof(kdata))) {
+			/* release the irq pin */
+			gpiod_set_value(kdrvdata->gpio_nirq, 1);
+			return -EFAULT;
+		}
+
+		ret = kbus_data_txrx(&kdata);
+		if (ret < 0) {
+			/* release the irq pin */
+			gpiod_set_value(kdrvdata->gpio_nirq, 1);
+			trace_pxc_kbus(
+				__func__,
+				"KDATA: kbus_data_txrx() returned with error!");
+		}
+
+		/* FIXME: JUST FOR TESTING */
+		gpiod_set_value(kdrvdata->gpio_cmdsel, 0);
+
+		kbus_wago_mpoint(); /* M6 */
+		trace_pxc_kbus(__func__, "KDATA: Leave");
+	} break;
+	case KBUS_IOC_CONFIG: {
+		struct kbus_spi_config kconfig;
+		struct kbus_spi_config __user *kconfig_user;
+
+		trace_pxc_kbus(__func__, "KCONFIG: Enter");
+
+		spi_set_drvdata(spi, kdrvdata);
+		kconfig_user = (struct kbus_spi_config __user *)arg;
+
+		if (copy_from_user(&kconfig, kconfig_user, sizeof(kconfig)))
+			return -EFAULT;
+
+		ret = kbus_spi_config(&kconfig);
+		if (ret < 0)
+			trace_pxc_kbus(
+				__func__,
+				"KCONFIG: kbus_spi_config() returned with error!");
+
+		trace_pxc_kbus(__func__, "KCONFIG: Leave");
+	} break;
+	case KBUS_IOC_BINARY: {
+		struct kbus_data kbinary;
+		struct kbus_data __user *kbinary_user;
+
+		trace_pxc_kbus(__func__, "KBINARY: Enter.");
+
+		spi_set_drvdata(
+			spi,
+			NULL); /* don't use kbus algorithm in spi-omap2-mcspi.c */
+		kbinary_user = (struct kbus_data __user *)arg;
+		if (copy_from_user(&kbinary, kbinary_user, sizeof(kbinary)))
+			return -EFAULT;
+
+		ret = kbus_binary_txrx(&kbinary);
+		if (ret < 0)
+			trace_pxc_kbus(
+				__func__,
+				"KBINARY: kbus_binary_txrx() returned with error!");
+
+		trace_pxc_kbus(__func__, "KBINARY: Leave");
+	} break;
+	default: /* FIXME: make it possible to use the spidev-way of communication. Not yet tested! */
+	{
+		u32 tmp;
+		unsigned int n_ioc;
+		struct spi_ioc_transfer *ioc;
+
+		spi_set_drvdata(
+			spi,
+			NULL); /* don't use kbus algorithm in spi-omap2-mcspi.c */
+		/* segmented and/or full-duplex I/O request */
+		if (_IOC_NR(cmd) != _IOC_NR(SPI_IOC_MESSAGE(0)) ||
+		    _IOC_DIR(cmd) != _IOC_WRITE) {
+			ret = -ENOTTY;
+			break;
+		}
+
+		tmp = _IOC_SIZE(cmd);
+		if ((tmp % sizeof(struct spi_ioc_transfer)) != 0) {
+			ret = -EINVAL;
+			break;
+		}
+		n_ioc = tmp / sizeof(struct spi_ioc_transfer);
+		if (n_ioc == 0)
+			break;
+
+		/* copy into scratch area */
+		ioc = kmalloc(tmp, GFP_KERNEL);
+		if (!ioc) {
+			ret = -ENOMEM;
+			break;
+		}
+		if (__copy_from_user(ioc, (void __user *)arg, tmp)) {
+			kfree(ioc);
+			ret = -EFAULT;
+			break;
+		}
+
+		/* translate to spi_message, execute */
+		ret = kbus_spi_message(ioc, n_ioc);
+		kfree(ioc);
+		break;
+	}
+	}
+
+	return ret;
+}
+
+static struct file_operations kbus_fops = {
+	.owner = THIS_MODULE,
+	.unlocked_ioctl = kbus_ioctl,
+	.write = kbus_write,
+	.open = kbus_open,
+	.release = kbus_close,
+};
+
+static ssize_t kbus_sysfs_prio_show(struct device *dev,
+				    struct device_attribute *attr, char *buf);
+static ssize_t kbus_sysfs_prio_set(struct device *dev,
+				   struct device_attribute *attr,
+				   const char *buf, size_t count);
+static ssize_t kbus_sysfs_trig_reset(struct device *dev,
+				     struct device_attribute *attr,
+				     const char *buf, size_t count);
+
+DEVICE_ATTR(kbus_dma_boost_prio, 0600, kbus_sysfs_prio_show,
+	    kbus_sysfs_prio_set);
+
+DEVICE_ATTR(kbus_dma_normal_prio, 0600, kbus_sysfs_prio_show,
+	    kbus_sysfs_prio_set);
+
+DEVICE_ATTR(kbus_trig_reset, 0200, NULL, kbus_sysfs_trig_reset);
+
+static ssize_t kbus_sysfs_prio_show(struct device *dev,
+				    struct device_attribute *attr, char *buf)
+{
+	struct kbus_drv_data *kdrvdata = dev_get_drvdata(kbus_device);
+	u8 prio = 0;
+
+	if (attr == &dev_attr_kbus_dma_boost_prio)
+		prio = kdrvdata->kbus_dma_boost_prio; /* boost */
+	else if (attr == &dev_attr_kbus_dma_normal_prio)
+		prio = kdrvdata->kbus_dma_normal_prio; /* normal */
+
+	return sprintf(buf, "%d\n", (int)prio);
+}
+
+static ssize_t kbus_sysfs_prio_set(struct device *dev,
+				   struct device_attribute *attr,
+				   const char *buf, size_t count)
+{
+	struct kbus_drv_data *kdrvdata = dev_get_drvdata(kbus_device);
+	u32 tmp = simple_strtoul(buf, NULL, 10);
+
+	/* check if value is valid */
+	if (tmp < 1 || tmp > 99)
+		return -EINVAL;
+
+	if (attr == &dev_attr_kbus_dma_boost_prio) {
+		pr_info("dma-boost prio changed from %d to %d.\n",
+			(int)kdrvdata->kbus_dma_boost_prio, (int)tmp);
+		kdrvdata->kbus_dma_boost_prio = (u8)tmp; /* boost */
+	} else if (attr == &dev_attr_kbus_dma_normal_prio) {
+		pr_info("dma-normal prio changed from %d to %d.\n",
+			(int)kdrvdata->kbus_dma_normal_prio, (int)tmp);
+		kdrvdata->kbus_dma_normal_prio = (u8)tmp; /* normal */
+	}
+
+	return count;
+}
+
+static int kbus_trig_reset(struct kbus_drv_data *kdrvdata)
+{
+	/* check if value is valid */
+	if (kdrvdata == NULL)
+		return -EINVAL;
+
+	/* reset kbus slave cpu (Infineon XE164) */
+	gpiod_set_value_cansleep(kdrvdata->gpio_nrst, 1);
+	udelay(100);
+	gpiod_set_value_cansleep(kdrvdata->gpio_nrst, 0);
+
+	pr_info("PFCxxx-KBUS: Kbus Slave CPU Reset.\n");
+
+	return 0;
+}
+
+static ssize_t kbus_sysfs_trig_reset(struct device *dev,
+				     struct device_attribute *attr,
+				     const char *buf, size_t count)
+{
+	struct kbus_drv_data *kdrvdata = dev_get_drvdata(kbus_device);
+	u32 val = simple_strtoul(buf, NULL, 10);
+
+	/* check if value is valid */
+	if (val != 1)
+		return -EINVAL;
+
+	kbus_trig_reset(kdrvdata);
+
+	return count;
+}
+
+static ssize_t kbus_sysfs_boost_en_show(struct device *dev,
+					struct device_attribute *attr,
+					char *buf)
+{
+	struct kbus_drv_data *kdrvdata = dev_get_drvdata(kbus_device);
+
+	return sprintf(buf, "%d\n", (int)kdrvdata->kbus_dma_boost_en);
+}
+
+static ssize_t kbus_sysfs_boost_en_set(struct device *dev,
+				       struct device_attribute *attr,
+				       const char *buf, size_t count)
+{
+	struct kbus_drv_data *kdrvdata = dev_get_drvdata(kbus_device);
+	u32 tmp = simple_strtoul(buf, NULL, 10);
+
+	/* check if valid */
+	if (tmp > 1)
+		return -EINVAL;
+
+	/* get pid of irq/12-DMA here
+	 * and set it in kdrvdata
+	 */
+	if (!kdrvdata->dma_task) {
+		kdrvdata->dma_task = find_dma_task();
+		if (kdrvdata->dma_task)
+			pr_info("dma-task pid is %u.\n",
+				kdrvdata->dma_task->pid);
+		else
+			pr_info("dma-task not found!\n");
+	}
+
+	kbus_boost_dma_task((u8)tmp);
+	pr_info("dma-boost %s.\n", tmp ? "enabled" : "disabled");
+
+	return count;
+}
+DEVICE_ATTR(kbus_dma_boost_en, 0600, kbus_sysfs_boost_en_show,
+	    kbus_sysfs_boost_en_set);
+
+static ssize_t kbus_sysfs_tty_device_name_show(struct device *dev,
+					       struct device_attribute *attr,
+					       char *buf)
+{
+	struct kbus_drv_data *kdrvdata = dev_get_drvdata(kbus_device);
+
+	return sprintf(buf, "%s\n", kdrvdata->kbus_tty_device_name);
+}
+DEVICE_ATTR(kbus_tty_device_name, 0444, kbus_sysfs_tty_device_name_show, NULL);
+
+static struct kbus_drv_data *kbus_probe_dt(struct spi_device *spi)
+{
+	struct kbus_drv_data *kdrvdata;
+	struct device_node *np = spi->dev.of_node;
+	u32 val;
+	int ret;
+
+	pr_debug("%s: probing device tree node (%s)\n", __func__, np->name);
+
+	/* alloc kernel space buffers */
+	kdrvdata = devm_kmalloc(&spi->dev, sizeof(struct kbus_drv_data),
+				GFP_KERNEL);
+
+	if (!kdrvdata)
+		return ERR_PTR(-ENOMEM);
+
+	kdrvdata->dma_task = NULL;
+
+	kdrvdata->use_dma = of_property_read_bool(np, "kbus,use-dma-always");
+	kdrvdata->kbus_dma_boost_en =
+		of_property_read_bool(np, "kbus,dma-boost");
+	if (kdrvdata->kbus_dma_boost_en) {
+		ret = of_property_read_u32(np, "kbus,dma-boost-prio", &val);
+		if (ret < 0) {
+			pr_err("%s: dt: dma-boost-prio must be set\n",
+			       __func__);
+			goto out_err;
+		}
+		kdrvdata->kbus_dma_boost_prio = (u8)val;
+		ret = of_property_read_u32(np, "kbus,dma-default-prio", &val);
+		if (ret < 0) {
+			pr_err("%s: dt: dma-default-prio must be set\n",
+			       __func__);
+			goto out_err;
+		}
+		kdrvdata->kbus_dma_normal_prio = (u8)val;
+	}
+
+	ret = of_property_read_string(np, "kbus,tty-device",
+				      &kdrvdata->kbus_tty_device_name);
+	if (ret < 0) {
+		pr_err("%s: dt: kbus tty-device must be set (e.g. ttyO4)\n",
+		       __func__);
+		goto out_err;
+	}
+
+	ret = of_property_read_string(np, "kbus,dma-boost-irq-thread",
+				      &kdrvdata->kbus_dma_boost_irq_thread);
+	if (ret < 0) {
+		pr_err("%s: dt: kbus dma-irq-thread must be set (e.g. irq/19-edma)\n",
+		       __func__);
+		goto out_err;
+	}
+
+	/* get gpios from device tree */
+	kdrvdata->gpio_nrst =
+		devm_gpiod_get(&spi->dev, "kbus,nrst", GPIOD_OUT_HIGH);
+	if (IS_ERR(kdrvdata->gpio_nrst)) {
+		ret = PTR_ERR(kdrvdata->gpio_nrst);
+		goto out_err;
+	}
+
+	kdrvdata->gpio_nsync =
+		devm_gpiod_get(&spi->dev, "kbus,nsync", GPIOD_IN);
+	if (IS_ERR(kdrvdata->gpio_nsync)) {
+		ret = PTR_ERR(kdrvdata->gpio_nsync);
+		goto out_gpio_nrst;
+	}
+
+	kdrvdata->gpio_cmdsel =
+		devm_gpiod_get(&spi->dev, "kbus,cmdsel", GPIOD_OUT_HIGH);
+	if (IS_ERR(kdrvdata->gpio_cmdsel)) {
+		ret = PTR_ERR(kdrvdata->gpio_cmdsel);
+		goto out_gpio_nsync;
+	}
+
+	kdrvdata->gpio_nirq =
+		devm_gpiod_get(&spi->dev, "kbus,nirq", GPIOD_OUT_HIGH);
+	if (IS_ERR(kdrvdata->gpio_nirq)) {
+		ret = PTR_ERR(kdrvdata->gpio_nirq);
+		goto out_gpio_cmdsel;
+	}
+
+	kdrvdata->gpio_nerr =
+		devm_gpiod_get(&spi->dev, "kbus,nerr", GPIOD_IN);
+	if (IS_ERR(kdrvdata->gpio_nerr)) {
+		ret = PTR_ERR(kdrvdata->gpio_nerr);
+		goto out_gpio_nirq;
+	}
+
+	/* reset kbus slave cpu (Infineon XE164) */
+	if (of_property_read_bool(np, "kbus,reset-on-boot"))
+		kbus_trig_reset(kdrvdata);
+
+	/* get irq pin */
+	kdrvdata->gpio_nrdy =
+		devm_gpiod_get(&spi->dev, "kbus,nrdy", GPIOD_IN);
+	ret = IS_ERR(kdrvdata->gpio_nrdy) ? PTR_ERR(kdrvdata->gpio_nrdy) : 0;
+	if (ret == -EPROBE_DEFER)
+		goto out_gpio_nerr;
+	else if (ret < 0) {
+		kdrvdata->kbus_irq = irq_of_parse_and_map(np, 0);
+		if (!kdrvdata->kbus_irq) {
+			pr_err("KBUS Probe: failed to get irq pin!\n");
+			goto out_gpio_nerr;
+		}
+	}
+
+	kdrvdata->kbus_irq = gpiod_to_irq(kdrvdata->gpio_nrdy);
+	kdrvdata->spi = spi;
+
+	return kdrvdata;
+
+out_gpio_nerr:
+	devm_gpiod_put(&spi->dev, kdrvdata->gpio_nerr);
+out_gpio_nirq:
+	devm_gpiod_put(&spi->dev, kdrvdata->gpio_nirq);
+out_gpio_cmdsel:
+	devm_gpiod_put(&spi->dev, kdrvdata->gpio_cmdsel);
+out_gpio_nsync:
+	devm_gpiod_put(&spi->dev, kdrvdata->gpio_nsync);
+out_gpio_nrst:
+	devm_gpiod_put(&spi->dev, kdrvdata->gpio_nrst);
+out_err:
+	if (ret != -EPROBE_DEFER)
+		pr_err("%s: failed to probe kbus oftree (%d)\n", __func__, ret);
+	devm_kfree(&spi->dev, kdrvdata);
+	return ERR_PTR(ret);
+}
+
+static int kbus_probe(struct spi_device *spi)
+{
+	int ret = 0;
+	u8 save = 0;
+	struct kbus_drv_data *kdrvdata;
+
+	if (!spi->dev.of_node) {
+		pr_err("Wago KBUS Driver: No DT node found!\n");
+		return -EFAULT;
+	}
+
+	kdrvdata = kbus_probe_dt(spi);
+	if (IS_ERR(kdrvdata))
+		return PTR_ERR(kdrvdata);
+
+	init_waitqueue_head(&kdrvdata->kbus_irq_wq);
+
+	ret = request_irq(kdrvdata->kbus_irq, kbus_isr,
+			  IRQF_SHARED | IRQ_TYPE_EDGE_BOTH |
+				  IRQF_THREAD_TBL_LOOKUP,
+			  "kbus", &kbus_dev);
+	if (ret) {
+		pr_err("%s: could not request irq %d: ret=%d\n", __func__,
+		       kdrvdata->kbus_irq, ret);
+		goto out3;
+	}
+
+	/* disable kbus irq per default */
+	KBUS_DISABLE_IRQ(kdrvdata->kbus_irq);
+	kdrvdata->kbus_irq_enabled = 0;
+
+	/* allocate memory */
+	if (kdrvdata->use_dma) {
+		spi->dev.coherent_dma_mask = ~0; /* why that? */
+
+		/*
+		 * Minimum coherent DMA allocation is PAGE_SIZE, so allocate
+		 * that much and share it between Tx and Rx DMA buffers.
+		 */
+		kdrvdata->tx_buf = dma_alloc_coherent(
+			&spi->dev, PAGE_SIZE, &kdrvdata->tx_buf_dma, GFP_DMA);
+
+		if (kdrvdata->tx_buf) {
+			kdrvdata->rx_buf =
+				(u8 *)(kdrvdata->tx_buf + (PAGE_SIZE / 2));
+			/* set bus address based on allocated space */
+			kdrvdata->rx_buf_dma = (dma_addr_t)(
+				kdrvdata->tx_buf_dma + (PAGE_SIZE / 2));
+			kbus_dbg("%s: allocated dma space (%lu).", __func__,
+				 PAGE_SIZE);
+		} else {
+			/* Fall back to non-DMA */
+			kdrvdata->use_dma = 0;
+			kbus_dbg("%s: failed to allocate dma space (%lu).",
+				 __func__, PAGE_SIZE);
+		}
+	}
+
+	if (!kdrvdata->use_dma) {
+		kdrvdata->tx_buf = kmalloc(KBUS__MAX_BUF_LEN, GFP_KERNEL);
+		kdrvdata->rx_buf = kmalloc(KBUS__MAX_BUF_LEN, GFP_KERNEL);
+	}
+
+	/* create device node in /dev */
+	if (!wsysinit_sysfs_class) {
+		pr_err("PFCXXX: Wago SYSFS class not defined!\n");
+		ret = -EFAULT;
+		goto out1;
+	}
+
+	kbus_device = device_create(wsysinit_sysfs_class, NULL, kbus_dev, NULL,
+				    "kbus%d", MINOR(kbus_dev));
+	dev_set_drvdata(kbus_device, kdrvdata);
+	spi_set_drvdata(spi, kdrvdata);
+
+	/* create sysfs entries for dma boost support */
+	device_create_file(wsysinit_sysfs_device,
+			   &dev_attr_kbus_dma_normal_prio);
+	device_create_file(wsysinit_sysfs_device,
+			   &dev_attr_kbus_dma_boost_prio);
+	device_create_file(wsysinit_sysfs_device, &dev_attr_kbus_dma_boost_en);
+	device_create_file(wsysinit_sysfs_device, &dev_attr_kbus_trig_reset);
+
+	/* create sysfs entrie for tty device-name */
+	device_create_file(wsysinit_sysfs_device,
+			   &dev_attr_kbus_tty_device_name);
+
+	/* do the initial spi setup - it can be updated through kbus_spi_config() */
+	kdrvdata->spi->bits_per_word = 16;
+	kdrvdata->spi->max_speed_hz = KBUS__DEFAULT_SPEED;
+	save = kdrvdata->spi->mode;
+	kdrvdata->spi->mode &= (u8)~SPI_MODE_MASK;
+	kdrvdata->spi->mode |= SPI_CPHA;
+	ret = spi_setup(kdrvdata->spi);
+	if (ret < 0)
+		kdrvdata->spi->mode = save;
+	else
+		pr_info("%s: spi mode set to: %02x\n", __func__,
+			kdrvdata->spi->mode);
+
+	pr_info("probe (%d)\n", ret);
+
+	return ret;
+
+out1:
+	/* clean up */
+	if (kdrvdata->use_dma) {
+		dma_free_coherent(&spi->dev, PAGE_SIZE, kdrvdata->tx_buf,
+				  kdrvdata->tx_buf_dma);
+	} else {
+		kfree(kdrvdata->rx_buf);
+		kfree(kdrvdata->tx_buf);
+	}
+
+out3:
+	kfree(kdrvdata);
+
+	return ret;
+}
+
+static int kbus_remove(struct spi_device *spi)
+{
+	struct kbus_drv_data *kdrvdata = dev_get_drvdata(kbus_device);
+
+	device_destroy(wsysinit_sysfs_class, kbus_dev);
+	free_irq(kdrvdata->kbus_irq, &kbus_dev);
+
+	devm_gpiod_put(&spi->dev, kdrvdata->gpio_cmdsel);
+	devm_gpiod_put(&spi->dev, kdrvdata->gpio_nerr);
+	devm_gpiod_put(&spi->dev, kdrvdata->gpio_nirq);
+	devm_gpiod_put(&spi->dev, kdrvdata->gpio_nrdy);
+	devm_gpiod_put(&spi->dev, kdrvdata->gpio_nrst);
+	devm_gpiod_put(&spi->dev, kdrvdata->gpio_nsync);
+
+	if (kdrvdata->use_dma) {
+		dma_free_coherent(&spi->dev, PAGE_SIZE, kdrvdata->tx_buf,
+				  kdrvdata->tx_buf_dma);
+	} else {
+		kfree(kdrvdata->rx_buf);
+		kfree(kdrvdata->tx_buf);
+	}
+
+	dev_set_drvdata(kbus_device, NULL);
+	spi_set_drvdata(spi, NULL);
+
+	return 0;
+}
+
+#ifdef CONFIG_OF
+static const struct of_device_id kbus_spi_dt_ids[] = {
+	{ .compatible = "wago,spi-kbus" },
+	{}
+};
+MODULE_DEVICE_TABLE(of, kbus_spi_dt_ids);
+#endif
+
+struct spi_driver kbus_driver = {
+	.driver = {
+		.name	= "kbus-cpu",
+		.owner	= THIS_MODULE,
+		.of_match_table = of_match_ptr(kbus_spi_dt_ids),
+	},
+	.probe		= kbus_probe,
+	.remove		= kbus_remove,
+};
+
+static int __init kbus_init(void)
+{
+	int ret;
+
+	kbus_dbg("%s ...\n", __func__);
+
+	kbus_dev = MKDEV(KBUS_DRIVER_MAJOR, 0);
+	if ((ret = register_chrdev_region(kbus_dev, 1, "kbus")) < 0) {
+		pr_err("%s: register_chrdev_region(): ret=%d\n", __func__, ret);
+		return ret;
+	}
+
+	cdev_init(&kbus_cdev, &kbus_fops);
+	if ((ret = cdev_add(&kbus_cdev, kbus_dev, 1)) < 0) {
+		pr_err("%s: cdev_add(): ret=%d\n", __func__, ret);
+		unregister_chrdev_region(kbus_dev, 1);
+		return ret;
+	}
+
+	return spi_register_driver(&kbus_driver);
+}
+
+static void __exit kbus_exit(void)
+{
+	spi_unregister_driver(&kbus_driver);
+	cdev_del(&kbus_cdev);
+	unregister_chrdev_region(kbus_dev, 1);
+}
+
+module_init(kbus_init);
+module_exit(kbus_exit);
+
+MODULE_DESCRIPTION("WAGO KBUS SPI Driver");
+MODULE_AUTHOR("Heinrich Toews <heinrich.toews@wago.com>");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/spi/spi-omap2-mcspi.c b/drivers/spi/spi-omap2-mcspi.c
index d4c9510af393..5584cf77328e 100644
--- a/drivers/spi/spi-omap2-mcspi.c
+++ b/drivers/spi/spi-omap2-mcspi.c
@@ -29,6 +29,20 @@
 
 #include <linux/platform_data/spi-omap2-mcspi.h>
 
+#include <linux/errno.h>
+
+#ifdef CONFIG_SPI_KBUS_OMAP_EXTENSION
+#include <linux/sched.h>
+#endif
+
+#ifdef CONFIG_SPI_KBUS_OMAP_EXTENSION
+#include <linux/spi/kbus.h>
+#include <misc/wago-tests.h>
+#define PXC_SPI_KBUS_TRACER
+#include <trace/events/pxc.h>
+static int kbusdelay = 0;	/* KBUS Inter-Frame-Delay in nanoseconds */
+#endif
+
 #define OMAP2_MCSPI_MAX_FREQ		48000000
 #define OMAP2_MCSPI_MAX_DIVIDER		4096
 #define OMAP2_MCSPI_MAX_FIFODEPTH	64
@@ -289,6 +303,25 @@ static void omap2_mcspi_set_mode(struct spi_master *master)
 	ctx->modulctrl = l;
 }
 
+#ifdef CONFIG_SPI_KBUS_OMAP_SET_SPIDAT_DIR
+static void omap2_mcspi_set_spidat_direction(struct spi_master *master)
+{
+	u32 l;
+
+#define OMAP2_MCSPI_SYST_SPIDATDIR0_INPUT_EN      BIT(8)
+#define OMAP2_MCSPI_SYST_SPIDATDIR1_INPUT_EN      BIT(9)
+	l = mcspi_read_reg(master, OMAP2_MCSPI_SYST);
+	pr_info("%s: read-OMAP2_MCSPI_SYST: 0x%x\n", __func__, l);
+	l &= ~(OMAP2_MCSPI_SYST_SPIDATDIR0_INPUT_EN | OMAP2_MCSPI_SYST_SPIDATDIR1_INPUT_EN);
+	l |= OMAP2_MCSPI_SYST_SPIDATDIR0_INPUT_EN;
+	pr_info("%s: write-OMAP2_MCSPI_SYST: 0x%x\n", __func__, l);
+	mcspi_write_reg(master, OMAP2_MCSPI_SYST, l);
+
+	l = mcspi_read_reg(master, OMAP2_MCSPI_SYST);
+	pr_info("%s: (update) read-OMAP2_MCSPI_SYST: 0x%x\n", __func__, l);
+}
+#endif
+
 static void omap2_mcspi_set_fifo(const struct spi_device *spi,
 				struct spi_transfer *t, int enable)
 {
@@ -385,6 +418,11 @@ static void omap2_mcspi_rx_callback(void *data)
 	/* We must disable the DMA RX request */
 	omap2_mcspi_set_dma_req(spi, 1, 0);
 
+#ifdef CONFIG_SPI_KBUS_OMAP_EXTENSION
+	if (spi->dev.driver == &kbus_driver.driver)
+		trace_pxc_kbus(__func__, "DMA: RX completed!");
+#endif
+
 	complete(&mcspi_dma->dma_rx_completion);
 }
 
@@ -397,6 +435,11 @@ static void omap2_mcspi_tx_callback(void *data)
 	/* We must disable the DMA TX request */
 	omap2_mcspi_set_dma_req(spi, 0, 0);
 
+#ifdef CONFIG_SPI_KBUS_OMAP_EXTENSION
+	if (spi->dev.driver == &kbus_driver.driver)
+		trace_pxc_kbus(__func__, "DMA: TX completed!");
+#endif
+
 	complete(&mcspi_dma->dma_tx_completion);
 }
 
@@ -521,6 +564,11 @@ omap2_mcspi_rx_dma(struct spi_device *spi, struct spi_transfer *xfer,
 		return 0;
 	}
 
+#ifdef CONFIG_SPI_KBUS_OMAP_EXTENSION
+	if (spi->dev.driver == &kbus_driver.driver)
+		if (xfer->rx_buf && ((char *) xfer->rx_buf)[96-1] == 0x66)
+			trace_pxc_kbus(__func__, "DMA: RX Data MATCH (0x66)");
+#endif
 	for (x = 0; x < nb_sizes; x++)
 		kfree(sg_out[x]);
 
@@ -591,6 +639,12 @@ omap2_mcspi_txrx_dma(struct spi_device *spi, struct spi_transfer *xfer)
 	void __iomem            *irqstat_reg;
 	int			wait_res;
 
+#ifdef CONFIG_SPI_KBUS_OMAP_EXTENSION
+	trace_pxc_kbus(__func__, "enter");
+
+	kbus_boost_dma_task(1);
+#endif
+
 	mcspi = spi_master_get_devdata(spi->master);
 	mcspi_dma = &mcspi->dma_channels[spi->chip_select];
 
@@ -681,6 +735,11 @@ omap2_mcspi_txrx_dma(struct spi_device *spi, struct spi_transfer *xfer)
 				dev_err(&spi->dev, "EOT timed out\n");
 		}
 	}
+
+#ifdef CONFIG_SPI_KBUS_OMAP_EXTENSION
+	trace_pxc_kbus(__func__, "leave");
+#endif
+
 	return count;
 }
 
@@ -695,11 +754,20 @@ omap2_mcspi_txrx_pio(struct spi_device *spi, struct spi_transfer *xfer)
 	void __iomem		*rx_reg;
 	void __iomem		*chstat_reg;
 	int			word_len;
+	struct omap2_mcspi_device_config *cd;
 
+	cd = spi->controller_data;
 	count = xfer->len;
 	c = count;
 	word_len = cs->word_len;
 
+#ifdef CONFIG_SPI_KBUS_OMAP_EXTENSION
+	trace_pxc_kbus(__func__, "enter");
+	kbus_dbg("%s[%d]: count: %d\n", __func__,__LINE__, count);
+
+	kbus_boost_dma_task(0);
+#endif
+
 	l = mcspi_cached_chconf0(spi);
 
 	/* We store the pre-calculated register addresses on stack to speed
@@ -711,6 +779,10 @@ omap2_mcspi_txrx_pio(struct spi_device *spi, struct spi_transfer *xfer)
 	if (c < (word_len>>3))
 		return 0;
 
+	dev_dbg(&spi->dev, "xx: %s-%d %d %s:%s\n", xfer->tx_buf ? "tx" : "rx",
+		word_len, count, cd->turbo_mode ? "turbo" : "-",
+		(l & OMAP2_MCSPI_CHCONF_TURBO) ? "1" : "-");
+
 	if (word_len <= 8) {
 		u8		*rx;
 		const u8	*tx;
@@ -721,6 +793,9 @@ omap2_mcspi_txrx_pio(struct spi_device *spi, struct spi_transfer *xfer)
 		do {
 			c -= 1;
 			if (tx != NULL) {
+#ifdef CONFIG_SPI_KBUS_OMAP_EXTENSION
+				kbus_wago_mpoint(); /* MX */
+#endif
 				if (mcspi_wait_for_reg_bit(chstat_reg,
 						OMAP2_MCSPI_CHSTAT_TXS) < 0) {
 					dev_err(&spi->dev, "TXS timed out\n");
@@ -728,9 +803,15 @@ omap2_mcspi_txrx_pio(struct spi_device *spi, struct spi_transfer *xfer)
 				}
 				dev_vdbg(&spi->dev, "write-%d %02x\n",
 						word_len, *tx);
+#ifdef CONFIG_SPI_KBUS_OMAP_EXTENSION
+				trace_pxc_kbusdump(__func__, "write", word_len, *tx);
+#endif
 				writel_relaxed(*tx++, tx_reg);
 			}
 			if (rx != NULL) {
+#ifdef CONFIG_SPI_KBUS_OMAP_EXTENSION
+				kbus_wago_mpoint(); /* MX */
+#endif
 				if (mcspi_wait_for_reg_bit(chstat_reg,
 						OMAP2_MCSPI_CHSTAT_RXS) < 0) {
 					dev_err(&spi->dev, "RXS timed out\n");
@@ -743,6 +824,9 @@ omap2_mcspi_txrx_pio(struct spi_device *spi, struct spi_transfer *xfer)
 					*rx++ = readl_relaxed(rx_reg);
 					dev_vdbg(&spi->dev, "read-%d %02x\n",
 						    word_len, *(rx - 1));
+#ifdef CONFIG_SPI_KBUS_OMAP_EXTENSION
+					trace_pxc_kbusdump(__func__, "readtb", word_len, *(rx - 1));
+#endif
 					if (mcspi_wait_for_reg_bit(chstat_reg,
 						OMAP2_MCSPI_CHSTAT_RXS) < 0) {
 						dev_err(&spi->dev,
@@ -755,8 +839,14 @@ omap2_mcspi_txrx_pio(struct spi_device *spi, struct spi_transfer *xfer)
 				}
 
 				*rx++ = readl_relaxed(rx_reg);
+#ifdef CONFIG_SPI_KBUS_OMAP_EXTENSION
+				kbus_wago_mpoint(); /* MX */
+#endif
 				dev_vdbg(&spi->dev, "read-%d %02x\n",
 						word_len, *(rx - 1));
+#ifdef CONFIG_SPI_KBUS_OMAP_EXTENSION
+				trace_pxc_kbusdump(__func__, "read", word_len, *(rx - 1));
+#endif
 			}
 		} while (c);
 	} else if (word_len <= 16) {
@@ -765,22 +855,34 @@ omap2_mcspi_txrx_pio(struct spi_device *spi, struct spi_transfer *xfer)
 
 		rx = xfer->rx_buf;
 		tx = xfer->tx_buf;
+#ifdef CONFIG_SPI_KBUS_OMAP_EXTENSION
+		kbus_wago_mpoint(); /* MX */
+#endif
 		do {
 			c -= 2;
 			if (tx != NULL) {
 				if (mcspi_wait_for_reg_bit(chstat_reg,
 						OMAP2_MCSPI_CHSTAT_TXS) < 0) {
+#ifdef CONFIG_SPI_KBUS_OMAP_EXTENSION
+					trace_pxc_kbus(__func__, "TXS timed out");
+#endif
 					dev_err(&spi->dev, "TXS timed out\n");
 					goto out;
 				}
 				dev_vdbg(&spi->dev, "write-%d %04x\n",
 						word_len, *tx);
+#ifdef CONFIG_SPI_KBUS_OMAP_EXTENSION
+				trace_pxc_kbusdump(__func__, "write", word_len, *tx);
+#endif
 				writel_relaxed(*tx++, tx_reg);
 			}
 			if (rx != NULL) {
 				if (mcspi_wait_for_reg_bit(chstat_reg,
 						OMAP2_MCSPI_CHSTAT_RXS) < 0) {
 					dev_err(&spi->dev, "RXS timed out\n");
+#ifdef CONFIG_SPI_KBUS_OMAP_EXTENSION
+					trace_pxc_kbus(__func__, "RXS timed out");
+#endif
 					goto out;
 				}
 
@@ -790,6 +892,9 @@ omap2_mcspi_txrx_pio(struct spi_device *spi, struct spi_transfer *xfer)
 					*rx++ = readl_relaxed(rx_reg);
 					dev_vdbg(&spi->dev, "read-%d %04x\n",
 						    word_len, *(rx - 1));
+#ifdef CONFIG_SPI_KBUS_OMAP_EXTENSION
+					trace_pxc_kbusdump(__func__, "readtb", word_len, *(rx - 1));
+#endif
 					if (mcspi_wait_for_reg_bit(chstat_reg,
 						OMAP2_MCSPI_CHSTAT_RXS) < 0) {
 						dev_err(&spi->dev,
@@ -801,11 +906,28 @@ omap2_mcspi_txrx_pio(struct spi_device *spi, struct spi_transfer *xfer)
 					omap2_mcspi_set_enable(spi, 0);
 				}
 
+				/* INFO:
+				 *    We have a timing problem here!!!
+				 *    During extensive spi traffic some bytes were lost
+				 *    during read. Some tests also showed that the Infineon
+				 *    needs some more time between the spi words.
+				 */
+#ifdef CONFIG_SPI_KBUS_OMAP_EXTENSION
+				if (kbusdelay)
+					ndelay(kbusdelay); /* 400: With this delay we did a test over 14h successfully! */
+#endif
+
 				*rx++ = readl_relaxed(rx_reg);
 				dev_vdbg(&spi->dev, "read-%d %04x\n",
 						word_len, *(rx - 1));
+#ifdef CONFIG_SPI_KBUS_OMAP_EXTENSION
+				trace_pxc_kbusdump(__func__, "read", word_len, *(rx - 1));
+#endif
 			}
 		} while (c >= 2);
+#ifdef CONFIG_SPI_KBUS_OMAP_EXTENSION
+		kbus_wago_mpoint(); /* MX */
+#endif
 	} else if (word_len <= 32) {
 		u32		*rx;
 		const u32	*tx;
@@ -835,6 +957,14 @@ omap2_mcspi_txrx_pio(struct spi_device *spi, struct spi_transfer *xfer)
 				    (l & OMAP2_MCSPI_CHCONF_TURBO)) {
 					omap2_mcspi_set_enable(spi, 0);
 					*rx++ = readl_relaxed(rx_reg);
+
+					/* For some reason while beeing in
+					 * turbo mode we need a short delay
+					 * here. Otherwise it will hang if we
+					 * try to disable and enable
+					 * turbo mode again */
+					ndelay(1);
+
 					dev_vdbg(&spi->dev, "read-%d %08x\n",
 						    word_len, *(rx - 1));
 					if (mcspi_wait_for_reg_bit(chstat_reg,
@@ -872,6 +1002,10 @@ omap2_mcspi_txrx_pio(struct spi_device *spi, struct spi_transfer *xfer)
 	}
 out:
 	omap2_mcspi_set_enable(spi, 1);
+#ifdef CONFIG_SPI_KBUS_OMAP_EXTENSION
+	kbus_wago_mpoint(); /* MX */
+	trace_pxc_kbus(__func__, "leave");
+#endif
 	return count - c;
 }
 
@@ -928,6 +1062,9 @@ static int omap2_mcspi_setup_transfer(struct spi_device *spi,
 		l &= ~OMAP2_MCSPI_CHCONF_IS;
 		l &= ~OMAP2_MCSPI_CHCONF_DPE1;
 		l |= OMAP2_MCSPI_CHCONF_DPE0;
+#ifdef CONFIG_SPI_KBUS_OMAP_SET_SPIDAT_DIR
+		omap2_mcspi_set_spidat_direction(spi->master);
+#endif
 	} else {
 		l |= OMAP2_MCSPI_CHCONF_IS;
 		l |= OMAP2_MCSPI_CHCONF_DPE1;
@@ -1039,6 +1176,12 @@ static int omap2_mcspi_setup(struct spi_device *spi)
 	struct omap2_mcspi_regs	*ctx = &mcspi->ctx;
 	struct omap2_mcspi_cs	*cs = spi->controller_state;
 
+#ifdef CONFIG_SPI_KBUS_OMAP_EXTENSION
+	if (spi->max_speed_hz < (OMAP2_MCSPI_MAX_FREQ >> 15) ||
+	    spi->max_speed_hz > OMAP2_MCSPI_MAX_FREQ)
+		return -EINVAL;
+#endif
+
 	if (!cs) {
 		cs = kzalloc(sizeof *cs, GFP_KERNEL);
 		if (!cs)
@@ -1126,15 +1269,29 @@ static int omap2_mcspi_transfer_one(struct spi_master *master,
 	struct omap2_mcspi_dma		*mcspi_dma;
 	struct omap2_mcspi_cs		*cs;
 	struct omap2_mcspi_device_config *cd;
-	int				par_override = 0;
+#ifndef CONFIG_SPI_KBUS_OMAP_EXTENSION
+	int                             par_override = 0;
+#endif
 	int				status = 0;
 	u32				chconf;
 
+#ifdef CONFIG_SPI_KBUS_OMAP_EXTENSION
+	struct kbus_drv_data            *kdrvdata = NULL;
+
+	trace_pxc_kbus(__func__, "enter");
+	kbus_wago_mpoint(); /* M4 */
+#endif
+
 	mcspi = spi_master_get_devdata(master);
 	mcspi_dma = mcspi->dma_channels + spi->chip_select;
 	cs = spi->controller_state;
 	cd = spi->controller_data;
 
+#ifdef CONFIG_SPI_KBUS_OMAP_EXTENSION
+	if (spi->dev.driver == &kbus_driver.driver)
+		kdrvdata = spi_get_drvdata(spi);
+#endif
+#ifndef CONFIG_SPI_KBUS_OMAP_EXTENSION
 	/*
 	 * The slave driver could have changed spi->mode in which case
 	 * it will be different from cs->mode (the current hardware setup).
@@ -1144,12 +1301,14 @@ static int omap2_mcspi_transfer_one(struct spi_master *master,
 	 */
 	if (spi->mode != cs->mode)
 		par_override = 1;
+#endif
 
 	omap2_mcspi_set_enable(spi, 0);
 
 	if (spi->cs_gpiod)
 		omap2_mcspi_set_cs(spi, spi->mode & SPI_CS_HIGH);
 
+#ifndef CONFIG_SPI_KBUS_OMAP_EXTENSION
 	if (par_override ||
 	    (t->speed_hz != spi->max_speed_hz) ||
 	    (t->bits_per_word != spi->bits_per_word)) {
@@ -1161,6 +1320,8 @@ static int omap2_mcspi_transfer_one(struct spi_master *master,
 		    t->bits_per_word == spi->bits_per_word)
 			par_override = 0;
 	}
+#endif
+
 	if (cd && cd->cs_per_word) {
 		chconf = mcspi->ctx.modulctrl;
 		chconf &= ~OMAP2_MCSPI_MODULCTRL_SINGLE;
@@ -1188,11 +1349,19 @@ static int omap2_mcspi_transfer_one(struct spi_master *master,
 
 	if (t->len) {
 		unsigned	count;
-
+#ifdef CONFIG_SPI_KBUS_OMAP_EXTENSION
+		int                 i;
+#endif
+
+#ifndef CONFIG_SPI_KBUS_OMAP_EXTENSION
+		/* FIXME
+		   At this moment keep fifo disabled due to some issues
+		   that were coming up with large kbus nodes. */
 		if ((mcspi_dma->dma_rx && mcspi_dma->dma_tx) &&
 		    master->cur_msg_mapped &&
 		    master->can_dma(master, spi, t))
 			omap2_mcspi_set_fifo(spi, t, 1);
+#endif
 
 		omap2_mcspi_set_enable(spi, 1);
 
@@ -1201,10 +1370,47 @@ static int omap2_mcspi_transfer_one(struct spi_master *master,
 			writel_relaxed(0, cs->base
 					+ OMAP2_MCSPI_TX0);
 
+#ifdef CONFIG_SPI_KBUS_OMAP_EXTENSION
+		/*
+		 * kbus:
+		 *  release irq and check the sync signal
+		 *  before sending data
+		 */
+		if (kdrvdata && t->tx_buf) {
+			/*
+			 * XE164 should be ready a long time ago (several us).
+			 * Otherwise something is wrong with the controller!
+			 */
+			for (i = 0; i < PAC_KBUS_SYNC_CYCLES; i++) {
+				if (gpiod_get_value(kdrvdata->gpio_nsync)) /* active low */
+					continue;
+				break;
+			}
+
+			if (i >= PAC_KBUS_SYNC_CYCLES) {
+				trace_pxc_kbus(__func__, "err: sync pin is always high [-EBUSY(-16)]!");
+				status = -EBUSY;
+				goto out;
+			}
+
+			/* release the irq pin */
+			gpiod_set_value(kdrvdata->gpio_nirq, 1);
+
+		}
+#endif
+
 		if ((mcspi_dma->dma_rx && mcspi_dma->dma_tx) &&
 		    master->cur_msg_mapped &&
-		    master->can_dma(master, spi, t))
+#ifdef CONFIG_SPI_KBUS_OMAP_EXTENSION
+		    master->can_dma(master, spi, t)) {
+			if (kdrvdata)
+				trace_pxc_kbus(__func__, "DMA: TXRX: Trigger DMA Transfer.");
+#endif
+#ifndef CONFIG_SPI_KBUS_OMAP_EXTENSION
+		    master->can_dma(master, spi, t)) {
+#endif
 			count = omap2_mcspi_txrx_dma(spi, t);
+		}
 		else
 			count = omap2_mcspi_txrx_pio(spi, t);
 
@@ -1219,12 +1425,97 @@ static int omap2_mcspi_transfer_one(struct spi_master *master,
 	if (mcspi->fifo_depth > 0)
 		omap2_mcspi_set_fifo(spi, t, 0);
 
+#ifdef CONFIG_SPI_KBUS_OMAP_EXTENSION
+	/*
+	 * Special KBUS Treatment
+	 *
+	 */
+	if (kdrvdata) {
+		static u16 *kcmd_txbuf;
+		int tmp_ret;
+
+		kbus_wago_mpoint(); /* MX */
+		if (t->tx_buf) { /* after tx transfer */
+			kcmd_txbuf = (u16 *) t->tx_buf;
+			/* wait for READYn IRQ from xe164 */
+
+			trace_pxc_kbus(__func__, "TX0");
+			kbus_dbg("%s: irq timeout is %dms\n",
+				 __func__, kdrvdata->timeout_ms);
+			tmp_ret = wait_event_interruptible_timeout(kdrvdata->kbus_irq_wq,
+								   kdrvdata->kbus_irq_state == 0,
+								   msecs_to_jiffies(kdrvdata->timeout_ms));
+
+			if (tmp_ret == 0 && kdrvdata->kbus_irq_state) {
+				status = -ETIMEDOUT;
+				trace_pxc_kbus(__func__, "TX0: IRQ timeout!");
+				goto out;
+			}
+			if (kbus_error() < 0) {
+				status = -ENODATA;
+				trace_pxc_kbus(__func__, "TX0: KBUS ERROR.");
+				goto out;
+			}
+			trace_pxc_kbus(__func__, "TX0: IRQ received");
+		}
+
+		if (kdrvdata->cmdsel) { /* special treatment in command mode */
+			/* We're getting the data in two nibbles.
+			 * First: 6 bytes header where we get the length for the rest.
+			 * Second: The remaining N bytes.
+			 */
+			if (t->rx_buf == kdrvdata->rx_buf) { /* This RX0 */
+				u16 *kcmd_hdr = (u16 *) t->rx_buf;
+				u8 lb_cmd = kcmd_hdr[0] & 0xff;
+				u8 hb_cmd_inv = ~(kcmd_hdr[0] >> 8) & 0xff;
+				u8 lb_wlen = kcmd_hdr[2] & 0xff;
+				u8 hb_wlen_inv = ~(kcmd_hdr[2] >> 8) & 0xff;
+				unsigned int byte_len;
+				struct spi_transfer *kcmd_tnext;
+
+				trace_pxc_kbus(__func__, "RX0");
+
+				/* validate header data here */
+				if (lb_cmd != (kcmd_txbuf[0] & 0xff) ||
+				    hb_cmd_inv != lb_cmd ||
+				    hb_wlen_inv != lb_wlen) {
+					trace_pxc_kbus(__func__, "RX0: RX0 HDR not valid.");
+					kbus_dbg("%s[%d]: RX0 HDR not valid: 0x%.4x(TX:0x%.2x)|0x%.4x|0x%.4x\n",
+						 __func__, __LINE__, kcmd_hdr[0], kcmd_txbuf[0] & 0xff,
+						 kcmd_hdr[1], kcmd_hdr[2]);
+					status = -EPROTO;
+					goto out;
+				}
+
+				/* get next transfer entry */
+				kcmd_tnext = list_entry(t->transfer_list.next,
+							struct spi_transfer, transfer_list);
+
+				/* regard word (16bit) count */
+				byte_len = (kcmd_hdr[2] & 0xff) << 1;
+				if (byte_len < kcmd_tnext->len) {
+					kcmd_tnext->len = byte_len;
+					trace_pxc_kbus(__func__, "RX0: RX1 len updated.");
+				}
+
+				kbus_dbg("%s[%d]: RX1 len set to: %d\n",
+					 __func__, __LINE__, kcmd_tnext->len);
+			} else if (t->rx_buf != NULL) {
+				trace_pxc_kbus(__func__, "RX1");
+			}
+		}
+		kbus_wago_mpoint(); /* MX */
+	}
+#endif
+
 out:
+#ifndef CONFIG_SPI_KBUS_OMAP_EXTENSION
 	/* Restore defaults if they were overriden */
 	if (par_override) {
 		par_override = 0;
 		status = omap2_mcspi_setup_transfer(spi, NULL);
 	}
+#endif
 
 	if (cd && cd->cs_per_word) {
 		chconf = mcspi->ctx.modulctrl;
@@ -1493,8 +1784,8 @@ static int omap2_mcspi_probe(struct platform_device *pdev)
 	}
 	init_completion(&mcspi->txdone);
 	status = devm_request_irq(&pdev->dev, status,
-				  omap2_mcspi_irq_handler, 0, pdev->name,
-				  mcspi);
+				  omap2_mcspi_irq_handler,
+				  IRQF_THREAD_TBL_LOOKUP, pdev->name, mcspi);
 	if (status) {
 		dev_err(&pdev->dev, "Cannot request IRQ");
 		goto free_master;
@@ -1512,6 +1803,16 @@ static int omap2_mcspi_probe(struct platform_device *pdev)
 	if (status < 0)
 		goto disable_pm;
 
+#ifdef CONFIG_SPI_KBUS_OMAP_EXTENSION
+	dev_dbg(&pdev->dev, "kbusdelay=%d, %s interframe gap delay.\n",
+		kbusdelay, kbusdelay ? "using" : "NOT using");
+#endif
+
+#if 0 /* #ifdef CONFIG_SPI_KBUS_OMAP_EXTENSION */
+	if (omap2_mcspi_enable_clocks(mcspi) < 0)
+		goto free_master;
+#endif
+
 	return status;
 
 disable_pm:
@@ -1596,4 +1897,7 @@ static struct platform_driver omap2_mcspi_driver = {
 };
 
 module_platform_driver(omap2_mcspi_driver);
+#ifdef CONFIG_SPI_KBUS_OMAP_EXTENSION
+core_param(kbusdelay, kbusdelay, int, 0000);
+#endif
 MODULE_LICENSE("GPL");
diff --git a/drivers/spi/spi.c b/drivers/spi/spi.c
index 4257a2d368f7..5155e64bebd4 100644
--- a/drivers/spi/spi.c
+++ b/drivers/spi/spi.c
@@ -34,6 +34,10 @@
 #include <linux/idr.h>
 #include <linux/platform_data/x86/apple.h>
 
+#ifdef CONFIG_IRQ_PRIORITY_TABLE
+#include <linux/wsysinit-prio.h>
+#endif
+
 #define CREATE_TRACE_POINTS
 #include <trace/events/spi.h>
 EXPORT_TRACEPOINT_SYMBOL(spi_transfer_start);
@@ -1655,6 +1659,10 @@ static int spi_init_queue(struct spi_controller *ctlr)
 		return PTR_ERR(ctlr->kworker);
 	}
 
+#ifdef CONFIG_IRQ_PRIORITY_TABLE
+	wsysinit_set_fifo_nocheck(ctlr->kworker->task);
+#endif
+
 	kthread_init_work(&ctlr->pump_messages, spi_pump_messages);
 
 	/*
diff --git a/drivers/tty/n_tty.c b/drivers/tty/n_tty.c
index e4f4b2186bce..229360715805 100644
--- a/drivers/tty/n_tty.c
+++ b/drivers/tty/n_tty.c
@@ -789,8 +789,11 @@ static void commit_echoes(struct tty_struct *tty)
 	echoed = __process_echoes(tty);
 	mutex_unlock(&ldata->output_lock);
 
-	if (echoed && tty->ops->flush_chars)
-		tty->ops->flush_chars(tty);
+	if (echoed && tty->ops->flush_chars) {
+                /* flush chars if not prohibited by the driver */
+                if (!(tty->driver && (tty->driver->flags & TTY_DRIVER_IGNORE_FLUSH)))
+                        tty->ops->flush_chars(tty);
+        }
 }
 
 static void process_echoes(struct tty_struct *tty)
@@ -806,8 +809,11 @@ static void process_echoes(struct tty_struct *tty)
 	echoed = __process_echoes(tty);
 	mutex_unlock(&ldata->output_lock);
 
-	if (echoed && tty->ops->flush_chars)
-		tty->ops->flush_chars(tty);
+	if (echoed && tty->ops->flush_chars) {
+                /* flush chars if not prohibited by the driver */
+                if (!(tty->driver && (tty->driver->flags & TTY_DRIVER_IGNORE_FLUSH)))
+                        tty->ops->flush_chars(tty);
+        }
 }
 
 /* NB: echo_mark and echo_head should be equivalent here */
@@ -1219,10 +1225,13 @@ static void n_tty_receive_parity_error(struct tty_struct *tty, unsigned char c)
 {
 	struct n_tty_data *ldata = tty->disc_data;
 
+	n_tty_trace("%d: char 0x%x\n", __LINE__, c);
+
 	if (I_INPCK(tty)) {
 		if (I_IGNPAR(tty))
 			return;
 		if (I_PARMRK(tty)) {
+			n_tty_trace("%d: I_PARMRK: char 0x%x\n", __LINE__, c);
 			put_tty_queue('\377', ldata);
 			put_tty_queue('\0', ldata);
 			put_tty_queue(c, ldata);
@@ -1641,8 +1650,11 @@ static void __receive_buf(struct tty_struct *tty, const unsigned char *cp,
 			n_tty_receive_buf_standard(tty, cp, fp, count);
 
 		flush_echoes(tty);
-		if (tty->ops->flush_chars)
-			tty->ops->flush_chars(tty);
+                if (tty->ops->flush_chars) {
+                        /* flush chars if not prohibited by the driver */
+                        if (!(tty->driver && (tty->driver->flags & TTY_DRIVER_IGNORE_FLUSH)))
+                                tty->ops->flush_chars(tty);
+                }
 	}
 
 	if (ldata->icanon && !L_EXTPROC(tty))
@@ -1847,6 +1859,7 @@ static void n_tty_set_termios(struct tty_struct *tty, struct ktermios *old)
 		clear_bit(__DISABLED_CHAR, ldata->char_map);
 		ldata->raw = 0;
 		ldata->real_raw = 0;
+                n_tty_trace("%d: real_raw = 0\n", __LINE__);
 	} else {
 		ldata->raw = 1;
 		if ((I_IGNBRK(tty) || (!I_BRKINT(tty) && !I_PARMRK(tty))) &&
@@ -2362,8 +2375,11 @@ static ssize_t n_tty_write(struct tty_struct *tty, struct file *file,
 					break;
 				b++; nr--;
 			}
-			if (tty->ops->flush_chars)
-				tty->ops->flush_chars(tty);
+                        if (tty->ops->flush_chars) {
+                                /* flush chars if not prohibited by the driver */
+                                if (!(tty->driver && (tty->driver->flags & TTY_DRIVER_IGNORE_FLUSH)))
+                                        tty->ops->flush_chars(tty);
+                        }
 		} else {
 			struct n_tty_data *ldata = tty->disc_data;
 
diff --git a/drivers/tty/serial/8250/8250.h b/drivers/tty/serial/8250/8250.h
index 52bb21205bb6..5cbcaafbb4aa 100644
--- a/drivers/tty/serial/8250/8250.h
+++ b/drivers/tty/serial/8250/8250.h
@@ -130,12 +130,55 @@ static inline void serial_dl_write(struct uart_8250_port *up, int value)
 	up->dl_write(up, value);
 }
 
+static inline void serial8250_set_IER(struct uart_8250_port *up,
+				      unsigned char ier)
+{
+	struct uart_port *port = &up->port;
+	unsigned int flags;
+	bool is_console;
+
+	is_console = uart_console(port);
+
+	if (is_console)
+		console_atomic_lock(&flags);
+
+	serial_out(up, UART_IER, ier);
+
+	if (is_console)
+		console_atomic_unlock(flags);
+}
+
+static inline unsigned char serial8250_clear_IER(struct uart_8250_port *up)
+{
+	struct uart_port *port = &up->port;
+	unsigned int clearval = 0;
+	unsigned int prior;
+	unsigned int flags;
+	bool is_console;
+
+	is_console = uart_console(port);
+
+	if (up->capabilities & UART_CAP_UUE)
+		clearval = UART_IER_UUE;
+
+	if (is_console)
+		console_atomic_lock(&flags);
+
+	prior = serial_port_in(port, UART_IER);
+	serial_port_out(port, UART_IER, clearval);
+
+	if (is_console)
+		console_atomic_unlock(flags);
+
+	return prior;
+}
+
 static inline bool serial8250_set_THRI(struct uart_8250_port *up)
 {
 	if (up->ier & UART_IER_THRI)
 		return false;
 	up->ier |= UART_IER_THRI;
-	serial_out(up, UART_IER, up->ier);
+	serial8250_set_IER(up, up->ier);
 	return true;
 }
 
@@ -144,7 +187,7 @@ static inline bool serial8250_clear_THRI(struct uart_8250_port *up)
 	if (!(up->ier & UART_IER_THRI))
 		return false;
 	up->ier &= ~UART_IER_THRI;
-	serial_out(up, UART_IER, up->ier);
+	serial8250_set_IER(up, up->ier);
 	return true;
 }
 
diff --git a/drivers/tty/serial/8250/8250_core.c b/drivers/tty/serial/8250/8250_core.c
index cae61d1ebec5..47dd23056271 100644
--- a/drivers/tty/serial/8250/8250_core.c
+++ b/drivers/tty/serial/8250/8250_core.c
@@ -274,10 +274,8 @@ static void serial8250_backup_timeout(struct timer_list *t)
 	 * Must disable interrupts or else we risk racing with the interrupt
 	 * based handler.
 	 */
-	if (up->port.irq) {
-		ier = serial_in(up, UART_IER);
-		serial_out(up, UART_IER, 0);
-	}
+	if (up->port.irq)
+		ier = serial8250_clear_IER(up);
 
 	iir = serial_in(up, UART_IIR);
 
@@ -300,7 +298,7 @@ static void serial8250_backup_timeout(struct timer_list *t)
 		serial8250_tx_chars(up);
 
 	if (up->port.irq)
-		serial_out(up, UART_IER, ier);
+		serial8250_set_IER(up, ier);
 
 	spin_unlock_irqrestore(&up->port.lock, flags);
 
@@ -578,6 +576,14 @@ serial8250_register_ports(struct uart_driver *drv, struct device *dev)
 
 #ifdef CONFIG_SERIAL_8250_CONSOLE
 
+static void univ8250_console_write_atomic(struct console *co, const char *s,
+					  unsigned int count)
+{
+	struct uart_8250_port *up = &serial8250_ports[co->index];
+
+	serial8250_console_write_atomic(up, s, count);
+}
+
 static void univ8250_console_write(struct console *co, const char *s,
 				   unsigned int count)
 {
@@ -671,6 +677,7 @@ static int univ8250_console_match(struct console *co, char *name, int idx,
 
 static struct console univ8250_console = {
 	.name		= "ttyS",
+	.write_atomic	= univ8250_console_write_atomic,
 	.write		= univ8250_console_write,
 	.device		= uart_console_device,
 	.setup		= univ8250_console_setup,
diff --git a/drivers/tty/serial/8250/8250_fsl.c b/drivers/tty/serial/8250/8250_fsl.c
index fbcc90c31ca1..b33cb454ce03 100644
--- a/drivers/tty/serial/8250/8250_fsl.c
+++ b/drivers/tty/serial/8250/8250_fsl.c
@@ -60,9 +60,18 @@ int fsl8250_handle_irq(struct uart_port *port)
 
 	/* Stop processing interrupts on input overrun */
 	if ((orig_lsr & UART_LSR_OE) && (up->overrun_backoff_time_ms > 0)) {
+		unsigned int ca_flags;
 		unsigned long delay;
+		bool is_console;
 
+		is_console = uart_console(port);
+
+		if (is_console)
+			console_atomic_lock(&ca_flags);
 		up->ier = port->serial_in(port, UART_IER);
+		if (is_console)
+			console_atomic_unlock(ca_flags);
+
 		if (up->ier & (UART_IER_RLSI | UART_IER_RDI)) {
 			port->ops->stop_rx(port);
 		} else {
diff --git a/drivers/tty/serial/8250/8250_ingenic.c b/drivers/tty/serial/8250/8250_ingenic.c
index 988bf6bcce42..bcd26d672539 100644
--- a/drivers/tty/serial/8250/8250_ingenic.c
+++ b/drivers/tty/serial/8250/8250_ingenic.c
@@ -146,6 +146,8 @@ OF_EARLYCON_DECLARE(x1000_uart, "ingenic,x1000-uart",
 
 static void ingenic_uart_serial_out(struct uart_port *p, int offset, int value)
 {
+	unsigned int flags;
+	bool is_console;
 	int ier;
 
 	switch (offset) {
@@ -167,7 +169,12 @@ static void ingenic_uart_serial_out(struct uart_port *p, int offset, int value)
 		 * If we have enabled modem status IRQs we should enable
 		 * modem mode.
 		 */
+		is_console = uart_console(p);
+		if (is_console)
+			console_atomic_lock(&flags);
 		ier = p->serial_in(p, UART_IER);
+		if (is_console)
+			console_atomic_unlock(flags);
 
 		if (ier & UART_IER_MSI)
 			value |= UART_MCR_MDCE | UART_MCR_FCM;
diff --git a/drivers/tty/serial/8250/8250_mtk.c b/drivers/tty/serial/8250/8250_mtk.c
index f7d3023f860f..8133713dcf5e 100644
--- a/drivers/tty/serial/8250/8250_mtk.c
+++ b/drivers/tty/serial/8250/8250_mtk.c
@@ -213,12 +213,37 @@ static void mtk8250_shutdown(struct uart_port *port)
 
 static void mtk8250_disable_intrs(struct uart_8250_port *up, int mask)
 {
-	serial_out(up, UART_IER, serial_in(up, UART_IER) & (~mask));
+	struct uart_port *port = &up->port;
+	unsigned int flags;
+	unsigned int ier;
+	bool is_console;
+
+	is_console = uart_console(port);
+
+	if (is_console)
+		console_atomic_lock(&flags);
+
+	ier = serial_in(up, UART_IER);
+	serial_out(up, UART_IER, ier & (~mask));
+
+	if (is_console)
+		console_atomic_unlock(flags);
 }
 
 static void mtk8250_enable_intrs(struct uart_8250_port *up, int mask)
 {
-	serial_out(up, UART_IER, serial_in(up, UART_IER) | mask);
+	struct uart_port *port = &up->port;
+	unsigned int flags;
+	unsigned int ier;
+
+	if (uart_console(port))
+		console_atomic_lock(&flags);
+
+	ier = serial_in(up, UART_IER);
+	serial_out(up, UART_IER, ier | mask);
+
+	if (uart_console(port))
+		console_atomic_unlock(flags);
 }
 
 static void mtk8250_set_flow_ctrl(struct uart_8250_port *up, int mode)
diff --git a/drivers/tty/serial/8250/8250_port.c b/drivers/tty/serial/8250/8250_port.c
index b0af13074cd3..b05f8c34b291 100644
--- a/drivers/tty/serial/8250/8250_port.c
+++ b/drivers/tty/serial/8250/8250_port.c
@@ -757,7 +757,7 @@ static void serial8250_set_sleep(struct uart_8250_port *p, int sleep)
 			serial_out(p, UART_EFR, UART_EFR_ECB);
 			serial_out(p, UART_LCR, 0);
 		}
-		serial_out(p, UART_IER, sleep ? UART_IERX_SLEEP : 0);
+		serial8250_set_IER(p, sleep ? UART_IERX_SLEEP : 0);
 		if (p->capabilities & UART_CAP_EFR) {
 			serial_out(p, UART_LCR, UART_LCR_CONF_MODE_B);
 			serial_out(p, UART_EFR, efr);
@@ -1429,7 +1429,7 @@ static void serial8250_stop_rx(struct uart_port *port)
 
 	up->ier &= ~(UART_IER_RLSI | UART_IER_RDI);
 	up->port.read_status_mask &= ~UART_LSR_DR;
-	serial_port_out(port, UART_IER, up->ier);
+	serial8250_set_IER(up, up->ier);
 
 	serial8250_rpm_put(up);
 }
@@ -1459,7 +1459,7 @@ void serial8250_em485_stop_tx(struct uart_8250_port *p)
 		serial8250_clear_and_reinit_fifos(p);
 
 		p->ier |= UART_IER_RLSI | UART_IER_RDI;
-		serial_port_out(&p->port, UART_IER, p->ier);
+		serial8250_set_IER(p, p->ier);
 	}
 }
 EXPORT_SYMBOL_GPL(serial8250_em485_stop_tx);
@@ -1687,7 +1687,7 @@ static void serial8250_disable_ms(struct uart_port *port)
 	mctrl_gpio_disable_ms(up->gpios);
 
 	up->ier &= ~UART_IER_MSI;
-	serial_port_out(port, UART_IER, up->ier);
+	serial8250_set_IER(up, up->ier);
 }
 
 static void serial8250_enable_ms(struct uart_port *port)
@@ -1703,7 +1703,7 @@ static void serial8250_enable_ms(struct uart_port *port)
 	up->ier |= UART_IER_MSI;
 
 	serial8250_rpm_get(up);
-	serial_port_out(port, UART_IER, up->ier);
+	serial8250_set_IER(up, up->ier);
 	serial8250_rpm_put(up);
 }
 
@@ -2118,14 +2118,7 @@ static void serial8250_put_poll_char(struct uart_port *port,
 	struct uart_8250_port *up = up_to_u8250p(port);
 
 	serial8250_rpm_get(up);
-	/*
-	 *	First save the IER then disable the interrupts
-	 */
-	ier = serial_port_in(port, UART_IER);
-	if (up->capabilities & UART_CAP_UUE)
-		serial_port_out(port, UART_IER, UART_IER_UUE);
-	else
-		serial_port_out(port, UART_IER, 0);
+	ier = serial8250_clear_IER(up);
 
 	wait_for_xmitr(up, BOTH_EMPTY);
 	/*
@@ -2138,7 +2131,7 @@ static void serial8250_put_poll_char(struct uart_port *port,
 	 *	and restore the IER
 	 */
 	wait_for_xmitr(up, BOTH_EMPTY);
-	serial_port_out(port, UART_IER, ier);
+	serial8250_set_IER(up, ier);
 	serial8250_rpm_put(up);
 }
 
@@ -2441,7 +2434,7 @@ void serial8250_do_shutdown(struct uart_port *port)
 	 */
 	spin_lock_irqsave(&port->lock, flags);
 	up->ier = 0;
-	serial_port_out(port, UART_IER, 0);
+	serial8250_set_IER(up, 0);
 	spin_unlock_irqrestore(&port->lock, flags);
 
 	synchronize_irq(port->irq);
@@ -2771,7 +2764,7 @@ serial8250_do_set_termios(struct uart_port *port, struct ktermios *termios,
 	if (up->capabilities & UART_CAP_RTOIE)
 		up->ier |= UART_IER_RTOIE;
 
-	serial_port_out(port, UART_IER, up->ier);
+	serial8250_set_IER(up, up->ier);
 
 	if (up->capabilities & UART_CAP_EFR) {
 		unsigned char efr = 0;
@@ -3237,7 +3230,7 @@ EXPORT_SYMBOL_GPL(serial8250_set_defaults);
 
 #ifdef CONFIG_SERIAL_8250_CONSOLE
 
-static void serial8250_console_putchar(struct uart_port *port, int ch)
+static void serial8250_console_putchar_locked(struct uart_port *port, int ch)
 {
 	struct uart_8250_port *up = up_to_u8250p(port);
 
@@ -3245,6 +3238,18 @@ static void serial8250_console_putchar(struct uart_port *port, int ch)
 	serial_port_out(port, UART_TX, ch);
 }
 
+static void serial8250_console_putchar(struct uart_port *port, int ch)
+{
+	struct uart_8250_port *up = up_to_u8250p(port);
+	unsigned int flags;
+
+	wait_for_xmitr(up, UART_LSR_THRE);
+
+	console_atomic_lock(&flags);
+	serial8250_console_putchar_locked(port, ch);
+	console_atomic_unlock(flags);
+}
+
 /*
  *	Restore serial console when h/w power-off detected
  */
@@ -3266,6 +3271,32 @@ static void serial8250_console_restore(struct uart_8250_port *up)
 	serial8250_out_MCR(up, UART_MCR_DTR | UART_MCR_RTS);
 }
 
+void serial8250_console_write_atomic(struct uart_8250_port *up,
+				     const char *s, unsigned int count)
+{
+	struct uart_port *port = &up->port;
+	unsigned int flags;
+	unsigned int ier;
+
+	console_atomic_lock(&flags);
+
+	touch_nmi_watchdog();
+
+	ier = serial8250_clear_IER(up);
+
+	if (atomic_fetch_inc(&up->console_printing)) {
+		uart_console_write(port, "\n", 1,
+				   serial8250_console_putchar_locked);
+	}
+	uart_console_write(port, s, count, serial8250_console_putchar_locked);
+	atomic_dec(&up->console_printing);
+
+	wait_for_xmitr(up, BOTH_EMPTY);
+	serial8250_set_IER(up, ier);
+
+	console_atomic_unlock(flags);
+}
+
 /*
  *	Print a string to the serial port trying not to disturb
  *	any possible real use of the port...
@@ -3282,24 +3313,12 @@ void serial8250_console_write(struct uart_8250_port *up, const char *s,
 	struct uart_port *port = &up->port;
 	unsigned long flags;
 	unsigned int ier;
-	int locked = 1;
 
 	touch_nmi_watchdog();
 
-	if (oops_in_progress)
-		locked = spin_trylock_irqsave(&port->lock, flags);
-	else
-		spin_lock_irqsave(&port->lock, flags);
-
-	/*
-	 *	First save the IER then disable the interrupts
-	 */
-	ier = serial_port_in(port, UART_IER);
+	spin_lock_irqsave(&port->lock, flags);
 
-	if (up->capabilities & UART_CAP_UUE)
-		serial_port_out(port, UART_IER, UART_IER_UUE);
-	else
-		serial_port_out(port, UART_IER, 0);
+	ier = serial8250_clear_IER(up);
 
 	/* check scratch reg to see if port powered off during system sleep */
 	if (up->canary && (up->canary != serial_port_in(port, UART_SCR))) {
@@ -3313,7 +3332,9 @@ void serial8250_console_write(struct uart_8250_port *up, const char *s,
 		mdelay(port->rs485.delay_rts_before_send);
 	}
 
+	atomic_inc(&up->console_printing);
 	uart_console_write(port, s, count, serial8250_console_putchar);
+	atomic_dec(&up->console_printing);
 
 	/*
 	 *	Finally, wait for transmitter to become empty
@@ -3326,8 +3347,7 @@ void serial8250_console_write(struct uart_8250_port *up, const char *s,
 		if (em485->tx_stopped)
 			up->rs485_stop_tx(up);
 	}
-
-	serial_port_out(port, UART_IER, ier);
+	serial8250_set_IER(up, ier);
 
 	/*
 	 *	The receive handling will happen properly because the
@@ -3339,8 +3359,7 @@ void serial8250_console_write(struct uart_8250_port *up, const char *s,
 	if (up->msr_saved_flags)
 		serial8250_modem_status(up);
 
-	if (locked)
-		spin_unlock_irqrestore(&port->lock, flags);
+	spin_unlock_irqrestore(&port->lock, flags);
 }
 
 static unsigned int probe_baud(struct uart_port *port)
@@ -3360,6 +3379,7 @@ static unsigned int probe_baud(struct uart_port *port)
 
 int serial8250_console_setup(struct uart_port *port, char *options, bool probe)
 {
+	struct uart_8250_port *up = up_to_u8250p(port);
 	int baud = 9600;
 	int bits = 8;
 	int parity = 'n';
@@ -3369,6 +3389,8 @@ int serial8250_console_setup(struct uart_port *port, char *options, bool probe)
 	if (!port->iobase && !port->membase)
 		return -ENODEV;
 
+	atomic_set(&up->console_printing, 0);
+
 	if (options)
 		uart_parse_options(options, &baud, &parity, &bits, &flow);
 	else if (probe)
diff --git a/drivers/tty/serial/Kconfig b/drivers/tty/serial/Kconfig
index 28f22e58639c..8e7406706447 100644
--- a/drivers/tty/serial/Kconfig
+++ b/drivers/tty/serial/Kconfig
@@ -1004,6 +1004,16 @@ config SERIAL_OMAP
 	  with the omap-serial driver. DMA support can be enabled from platform
 	  data.
 
+config SERIAL_OMAP_RTU
+	tristate "OMAP serial port support (RTU)"
+	depends on ARCH_OMAP2PLUS
+	select SERIAL_CORE
+	help
+         This driver is actually the SERIAL_OMAP driver from kernel 3.6.11
+         which was enhanced by RS485 and Modbus RTU functionallity.
+         Because this driver was tested and is stable this one is used for
+         fieldbus communication.
+
 config SERIAL_OMAP_CONSOLE
 	bool "Console on OMAP serial port"
 	depends on SERIAL_OMAP=y
@@ -1044,6 +1054,13 @@ config SERIAL_SIFIVE_CONSOLE
 	  your boot loader about how to pass options to the kernel at
 	  boot time.)
 
+config SERIAL_OMAP_MODBUS
+        bool "Modbus RTU support for OMAP"
+        default y
+        help
+          Select this option to enable Modbus RTU support.
+          This will allow a timing-based frame determination and address-based frame filtering.
+
 config SERIAL_LANTIQ
 	tristate "Lantiq serial driver"
 	depends on (LANTIQ || X86) || COMPILE_TEST
diff --git a/drivers/tty/serial/Makefile b/drivers/tty/serial/Makefile
index caf167f0c10a..143dab07b48c 100644
--- a/drivers/tty/serial/Makefile
+++ b/drivers/tty/serial/Makefile
@@ -58,6 +58,8 @@ obj-$(CONFIG_SERIAL_UARTLITE) += uartlite.o
 obj-$(CONFIG_SERIAL_MSM) += msm_serial.o
 obj-$(CONFIG_SERIAL_QCOM_GENI) += qcom_geni_serial.o
 obj-$(CONFIG_SERIAL_OMAP) += omap-serial.o
+obj-$(CONFIG_SERIAL_OMAP_MODBUS) += pfc-modbus-rtu.o
+obj-$(CONFIG_SERIAL_OMAP_RTU) += omap-serial-rtu.o
 obj-$(CONFIG_SERIAL_ALTERA_UART) += altera_uart.o
 obj-$(CONFIG_SERIAL_ST_ASC) += st-asc.o
 obj-$(CONFIG_SERIAL_QE) += ucc_uart.o
diff --git a/drivers/tty/serial/amba-pl011.c b/drivers/tty/serial/amba-pl011.c
index 87dc3fc15694..c529a804b6fc 100644
--- a/drivers/tty/serial/amba-pl011.c
+++ b/drivers/tty/serial/amba-pl011.c
@@ -2201,18 +2201,24 @@ pl011_console_write(struct console *co, const char *s, unsigned int count)
 {
 	struct uart_amba_port *uap = amba_ports[co->index];
 	unsigned int old_cr = 0, new_cr;
-	unsigned long flags;
+	unsigned long flags = 0;
 	int locked = 1;
 
 	clk_enable(uap->clk);
 
-	local_irq_save(flags);
+	/*
+	 * local_irq_save(flags);
+	 *
+	 * This local_irq_save() is nonsense. If we come in via sysrq
+	 * handling then interrupts are already disabled. Aside of
+	 * that the port.sysrq check is racy on SMP regardless.
+	*/
 	if (uap->port.sysrq)
 		locked = 0;
 	else if (oops_in_progress)
-		locked = spin_trylock(&uap->port.lock);
+		locked = spin_trylock_irqsave(&uap->port.lock, flags);
 	else
-		spin_lock(&uap->port.lock);
+		spin_lock_irqsave(&uap->port.lock, flags);
 
 	/*
 	 *	First save the CR then disable the interrupts
@@ -2238,8 +2244,7 @@ pl011_console_write(struct console *co, const char *s, unsigned int count)
 		pl011_write(old_cr, uap, REG_CR);
 
 	if (locked)
-		spin_unlock(&uap->port.lock);
-	local_irq_restore(flags);
+		spin_unlock_irqrestore(&uap->port.lock, flags);
 
 	clk_disable(uap->clk);
 }
diff --git a/drivers/tty/serial/omap-serial-rtu.c b/drivers/tty/serial/omap-serial-rtu.c
new file mode 100644
index 000000000000..aebe95f6114e
--- /dev/null
+++ b/drivers/tty/serial/omap-serial-rtu.c
@@ -0,0 +1,2100 @@
+/*
+ * Driver for OMAP-UART controller.
+ * Based on drivers/serial/8250.c
+ *
+ * Copyright (C) 2010 Texas Instruments.
+ *
+ * Authors:
+ *	Govindraj R	<govindraj.raja@ti.com>
+ *	Thara Gopinath	<thara@ti.com>
+ *
+ * RS485 support is based on
+ * http://code.google.com/p/meta-igep/source/browse/recipes-kernel/linux/
+ * linux-3.2/rs485/0010-omap-serial-add-RS-485-standard-support.patch
+ * by Javier Martinez Canillas
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * Note: This driver is made separate from 8250 driver as we cannot
+ * over load 8250 driver with omap platform specific configuration for
+ * features like DMA, it makes easier to implement features like DMA and
+ * hardware flow control and software flow control configuration with
+ * this driver as required for the omap-platform.
+ */
+#undef DEBUG
+
+#if defined(CONFIG_SERIAL_OMAP_CONSOLE) && defined(CONFIG_MAGIC_SYSRQ)
+#define SUPPORT_SYSRQ
+#endif
+
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/console.h>
+#include "serial_reg_rtu.h"
+#include <linux/delay.h>
+#include <linux/slab.h>
+#include <linux/tty.h>
+#include <linux/tty_flip.h>
+#include <linux/platform_device.h>
+#include <linux/io.h>
+#include <linux/clk.h>
+#include <linux/serial_core.h>
+#include <linux/irq.h>
+#include <linux/interrupt.h>
+#include <linux/uaccess.h>
+#include <linux/pm_runtime.h>
+#include <linux/of.h>
+#include <linux/gpio.h>
+#include <linux/pinctrl/consumer.h>
+#include <linux/of_gpio.h>
+#include <linux/of_irq.h>
+
+#include <linux/platform_data/pfc-modbus-rtu.h>
+#include "omap-serial-rtu.h"
+
+#define to_uart_omap_port(p)	((container_of((p), struct uart_omap_port, port)))
+
+static struct uart_omap_port *ui[OMAP_MAX_HSUART_PORTS];
+
+/* Forward declaration of functions */
+static void serial_omap_mdr1_errataset(struct uart_omap_port *up, u8 mdr1);
+static inline void serial_omap_enable_ier_thri(struct uart_omap_port *up);
+
+static inline void serial_omap_irq_enable(struct uart_omap_port * up);
+static inline void serial_omap_irq_disable(struct uart_omap_port *up);
+
+static struct workqueue_struct *serial_omap_uart_wq;
+
+extern int __wsysinit_boot_id;
+
+inline void serial_omap_irq_disable(struct uart_omap_port * up)
+{
+	if(up->port.irq != -1)
+		disable_irq(up->port.irq);
+}
+
+inline void serial_omap_irq_enable(struct uart_omap_port * up)
+{
+	if(up->port.irq != -1)
+		enable_irq(up->port.irq);
+
+}
+
+inline unsigned int serial_in_rtu(struct uart_omap_port *up, int offset)
+{
+	offset <<= up->port.regshift;
+	return readw(up->port.membase + offset);
+}
+
+inline void serial_out_rtu(struct uart_omap_port *up, int offset, int value)
+{
+	pr_debug("%s: %s: offset = 0x%08x, value 0x%08x\n",
+		 up->name, __func__, offset, value);
+
+	offset <<= up->port.regshift;
+	writew(value, up->port.membase + offset);
+}
+
+static inline void serial_omap_clear_fifos(struct uart_omap_port *up)
+{
+	serial_out_rtu(up, UART_FCR, UART_FCR_ENABLE_FIFO);
+	serial_out_rtu(up, UART_FCR, UART_FCR_ENABLE_FIFO |
+		       UART_FCR_CLEAR_RCVR | UART_FCR_CLEAR_XMIT);
+	serial_out_rtu(up, UART_FCR, 0);
+}
+
+static int serial_omap_get_context_loss_count(struct uart_omap_port *up)
+{
+	struct omap_uart_port_info *pdata = up->dev->platform_data;
+
+	if (!pdata || !pdata->get_context_loss_count)
+		return 0;
+
+	return pdata->get_context_loss_count(up->dev);
+}
+
+static void serial_omap_set_forceidle(struct uart_omap_port *up)
+{
+	struct omap_uart_port_info *pdata = up->dev->platform_data;
+
+	if (!pdata || !pdata->set_forceidle)
+		return;
+
+	pdata->set_forceidle(up->dev);
+}
+
+static void serial_omap_set_noidle(struct uart_omap_port *up)
+{
+	struct omap_uart_port_info *pdata = up->dev->platform_data;
+
+	if (!pdata || !pdata->set_noidle)
+		return;
+
+	pdata->set_noidle(up->dev);
+}
+
+static void serial_omap_enable_wakeup(struct uart_omap_port *up, bool enable)
+{
+	struct omap_uart_port_info *pdata = up->dev->platform_data;
+
+	if (!pdata || !pdata->enable_wakeup)
+		return;
+
+	pdata->enable_wakeup(up->dev, enable);
+}
+
+/*
+ * serial_omap_baud_is_mode16 - check if baud rate is MODE16X
+ * @port: uart port info
+ * @baud: baudrate for which mode needs to be determined
+ *
+ * Returns true if baud rate is MODE16X and false if MODE13X
+ * Original table in OMAP TRM named "UART Mode Baud Rates, Divisor Values,
+ * and Error Rates" determines modes not for all common baud rates.
+ * E.g. for 1000000 baud rate mode must be 16x, but according to that
+ * table it's determined as 13x.
+ */
+static bool
+serial_omap_baud_is_mode16(struct uart_port *port, unsigned int baud)
+{
+	unsigned int n13 = port->uartclk / (13 * baud);
+	unsigned int n16 = port->uartclk / (16 * baud);
+	int baudAbsDiff13 = baud - (port->uartclk / (13 * n13));
+	int baudAbsDiff16 = baud - (port->uartclk / (16 * n16));
+	if(baudAbsDiff13 < 0)
+		baudAbsDiff13 = -baudAbsDiff13;
+	if(baudAbsDiff16 < 0)
+		baudAbsDiff16 = -baudAbsDiff16;
+
+	return (baudAbsDiff13 > baudAbsDiff16);
+}
+
+#ifdef CONFIG_SERIAL_OMAP_RS485
+static inline int rts_on_send(struct uart_omap_port *up)
+{
+	return up->rs485.flags & SER_RS485_RTS_ON_SEND;
+}
+
+inline void serial_omap_disable_ier_thri(struct uart_omap_port *up)
+{
+	up->ier &= ~UART_IER_THRI;
+	serial_out_rtu(up, UART_IER, up->ier);
+}
+
+/*
++ * Switch transmit interrupt mode (to transmit last data chunk before
++ * end of transmit)
++ */
+static inline void serial_omap_thri_mode(struct uart_omap_port *up)
+{
+	unsigned char scr = serial_in_rtu(up, UART_OMAP_SCR);
+
+	if(up->tx_wait_end)
+                /* wait for remaining data to be sent: interrupt when FIFO empty */
+		scr |= UART_OMAP_SCR_TX_EMPTY_CTL_IT;
+	else
+		/*
+		 * default IRQ mode: interrupt when FIFO not full (reset by filling
+		 * it again)
+		 */
+		scr &= ~UART_OMAP_SCR_TX_EMPTY_CTL_IT;
+
+	serial_out_rtu(up, UART_OMAP_SCR, scr);
+}
+
+/*
+ *  Toggle RTS pin according to configuration settings (RTS high/low on send?)
+ */
+static inline void serial_omap_update_rts(struct uart_omap_port *up)
+{
+	pr_debug("%s: %s: enter\n", up->name, __func__);
+
+	if(up->rs485.flags & SER_RS485_ENABLED) {
+
+		if(up->tx_in_progress) {
+			pr_debug("%s: %s: deassert RTS (~UART_MCR_RTS)\n", up->name, __func__);
+			/* See AM35x docs 14.6.2.10 MCR_REG: 0x0 -> RTS high, 0x1 -> RTS low */
+			up->mcr &= ~UART_MCR_RTS;
+		} else {
+			pr_debug("%s: %s: assert RTS (UART_MCR_RTS)\n", up->name, __func__);
+			up->mcr |= UART_MCR_RTS;
+		}
+
+		serial_out_rtu(up, UART_MCR, up->mcr);
+	}
+}
+
+static void serial_omap_config_rs485(struct uart_omap_port *up, struct serial_rs485 *rs485conf)
+{
+	if (gpio_is_valid(up->rs485en_gpio)) {
+		int val;
+
+		/* way from uart_port to tty_port: ugly :( */
+		struct tty_struct *tty = up->port.state->port.tty;
+
+		up->rs485 = *rs485conf;
+
+		val = up->rs485.flags & SER_RS485_ENABLED;
+
+		/*
+		 * WAGO: PARMRK is set by CODESYS when parity is enabled.
+		 * 		This way CODESYS libraries are notified about parity errors.
+		 * 		N_TTY line discipline disables raw connection mode with PARMRK
+		 * 		active and triggers a flush_chars() on each RX operation.
+		 * 		flush_chars() activates TX during a running RX operation, leading
+		 * 		to framing and parity errors (RS485 is half-duplex!)
+		 * 		That's why we deactivate flush_chars() for RS485 mode
+		 */
+
+
+		if(val) { /* Enable RS485 */
+                        tty->driver->flags |= TTY_DRIVER_IGNORE_FLUSH;
+		} else { /* Enable RS232 */
+			up->tx_in_progress = 0;
+			up->tx_wait_end = 0;
+
+                        tty->driver->flags &= ~TTY_DRIVER_IGNORE_FLUSH;
+		}
+
+		gpio_set_value(up->rs485en_gpio, up->rs485en_alow ? !val : val);
+
+		pr_info("%s: %s mode enabled\n", __func__, val ? "rs485" : "rs232");
+                pr_info("tty driver flags: 0x%08lx", tty->driver->flags);
+	}
+}
+
+#endif
+
+/*
+ * serial_omap_get_divisor - calculate divisor value
+ * @port: uart port info
+ * @baud: baudrate for which divisor needs to be calculated.
+ */
+static unsigned int
+serial_omap_get_divisor(struct uart_port *port, unsigned int baud)
+{
+	unsigned int divisor;
+
+	if (!serial_omap_baud_is_mode16(port, baud))
+		divisor = 13;
+	else
+		divisor = 16;
+	return port->uartclk/(baud * divisor);
+}
+
+static void serial_omap_enable_ms(struct uart_port *port)
+{
+	struct uart_omap_port *up = to_uart_omap_port(port);
+
+	dev_dbg(up->port.dev, "serial_omap_enable_ms+%d\n", up->port.line);
+
+	pm_runtime_get_sync(up->dev);
+	up->ier |= UART_IER_MSI;
+	serial_out_rtu(up, UART_IER, up->ier);
+	pm_runtime_mark_last_busy(up->dev);
+	pm_runtime_put_autosuspend(up->dev);
+}
+
+void serial_omap_stop_tx(struct uart_port *port)
+{
+	struct uart_omap_port *up = to_uart_omap_port(port);
+
+	pm_runtime_get_sync(up->dev);
+
+#ifdef CONFIG_SERIAL_OMAP_RS485
+	if(up->rs485.flags & SER_RS485_ENABLED){
+		up->tx_in_progress = 0;
+		up->tx_wait_end = 1;
+		serial_omap_thri_mode(up);
+		serial_omap_enable_ier_thri(up);
+	} else
+#endif
+	serial_omap_disable_ier_thri(up);
+
+	serial_omap_set_forceidle(up);
+
+	pm_runtime_mark_last_busy(up->dev);
+	pm_runtime_put_autosuspend(up->dev);
+}
+
+static void serial_omap_stop_rx(struct uart_port *port)
+{
+	struct uart_omap_port *up = to_uart_omap_port(port);
+
+	pm_runtime_get_sync(up->dev);
+	up->ier &= ~UART_IER_RLSI;
+	up->port.read_status_mask &= ~UART_LSR_DR;
+	serial_out_rtu(up, UART_IER, up->ier);
+	pm_runtime_mark_last_busy(up->dev);
+	pm_runtime_put_autosuspend(up->dev);
+}
+
+static inline unsigned int __serial_omap_tx_empty(struct uart_omap_port *up){
+	return serial_in_rtu(up, UART_LSR) & UART_LSR_TEMT ? TIOCSER_TEMT : 0;
+}
+
+#ifdef CONFIG_SERIAL_OMAP_RS485
+// TCe: new
+inline int serial_omap_end_tx(struct uart_omap_port *up)
+{
+	if((up->rs485.flags & SER_RS485_ENABLED) && up->tx_wait_end &&
+	   __serial_omap_tx_empty(up)){
+
+		up->tx_wait_end = 0;
+		up->tx_in_progress = 0;
+
+		if(rts_on_send(up))
+			serial_omap_update_rts(up);
+
+		/* rts change causes a break condition, just remove it */
+		serial_in_rtu(up, UART_RX);
+
+		/* Enable RX interrupt */
+		up->ier = UART_IER_RLSI | UART_IER_RDI;
+		up->port.read_status_mask |= UART_LSR_DR;
+		serial_out_rtu(up, UART_IER, up->ier);
+
+		serial_out_rtu(up, UART_OMAP_SCR, up->scr);
+
+		return 1;
+	}
+
+	return 0;
+}
+#endif
+
+void transmit_chars_rtu(struct uart_omap_port *up, unsigned int lsr)
+{
+	struct circ_buf *xmit = &up->port.state->xmit;
+	int count;
+
+#ifdef CONFIG_SERIAL_OMAP_RS485
+	if(up->rs485.flags & SER_RS485_ENABLED){
+		if(!up->tx_in_progress){
+			up->tx_in_progress = 1;
+
+                        /* Clear LSR (error/status bits, especially Data Ready bit) */
+                        serial_in_rtu(up, UART_LSR);
+
+                        /* Disable RX */
+                        up->ier &= ~(UART_IER_RLSI | UART_IER_RDI);
+                        up->port.read_status_mask &= ~UART_LSR_DR;
+	                serial_out_rtu(up, UART_IER, up->ier);
+
+			if(rts_on_send(up))
+				serial_omap_update_rts(up);
+		}
+
+		if(up->tx_wait_end) {
+			up->tx_wait_end = 0;
+			serial_omap_thri_mode(up);
+			serial_omap_disable_ier_thri(up); /* it gets reenabled below */
+		}
+	}
+#endif
+
+	if (up->port.x_char) {
+		serial_out_rtu(up, UART_TX, up->port.x_char);
+		up->port.icount.tx++;
+		up->port.x_char = 0;
+		return;
+	}
+	if (uart_circ_empty(xmit) || uart_tx_stopped(&up->port)) {
+		serial_omap_stop_tx(&up->port);
+		return;
+	}
+	count = up->port.fifosize / 4;
+
+	do {
+		serial_out_rtu(up, UART_TX, xmit->buf[xmit->tail]);
+		xmit->tail = (xmit->tail + 1) & (UART_XMIT_SIZE - 1);
+		up->port.icount.tx++;
+		if (uart_circ_empty(xmit))
+			break;
+	} while (--count > 0);
+
+	if (uart_circ_chars_pending(xmit) < WAKEUP_CHARS) {
+		spin_unlock(&up->port.lock);
+		uart_write_wakeup(&up->port);
+		spin_lock(&up->port.lock);
+	}
+
+	if (uart_circ_empty(xmit))
+		serial_omap_stop_tx(&up->port);
+	else
+		serial_omap_start_tx(&up->port);
+}
+
+static inline void serial_omap_enable_ier_thri(struct uart_omap_port *up)
+{
+	if (!(up->ier & UART_IER_THRI)) {
+		up->ier |= UART_IER_THRI;
+		serial_out_rtu(up, UART_IER, up->ier);
+	}
+}
+
+void serial_omap_start_tx(struct uart_port *port)
+{
+	struct uart_omap_port *up = to_uart_omap_port(port);
+
+	pm_runtime_get_sync(up->dev);
+	serial_omap_enable_ier_thri(up);
+	serial_omap_set_noidle(up);
+	pm_runtime_mark_last_busy(up->dev);
+	pm_runtime_put_autosuspend(up->dev);
+}
+
+static void serial_omap_throttle(struct uart_port *port)
+{
+	struct uart_omap_port *up = to_uart_omap_port(port);
+	unsigned long flags;
+
+	pm_runtime_get_sync(up->dev);
+	serial_omap_irq_disable(up);
+	spin_lock_irqsave(&up->port.lock, flags);
+	up->ier &= ~(UART_IER_RLSI | UART_IER_RDI);
+	serial_out_rtu(up, UART_IER, up->ier);
+	spin_unlock_irqrestore(&up->port.lock, flags);
+	serial_omap_irq_enable(up);
+	pm_runtime_mark_last_busy(up->dev);
+	pm_runtime_put_autosuspend(up->dev);
+}
+
+static void serial_omap_unthrottle(struct uart_port *port)
+{
+	struct uart_omap_port *up = to_uart_omap_port(port);
+	unsigned long flags;
+
+	pm_runtime_get_sync(up->dev);
+	serial_omap_irq_disable(up);
+	spin_lock_irqsave(&up->port.lock, flags);
+	up->ier |= UART_IER_RLSI | UART_IER_RDI;
+	serial_out_rtu(up, UART_IER, up->ier);
+	spin_unlock_irqrestore(&up->port.lock, flags);
+	serial_omap_irq_enable(up);
+	pm_runtime_mark_last_busy(up->dev);
+	pm_runtime_put_autosuspend(up->dev);
+}
+
+static unsigned int check_modem_status(struct uart_omap_port *up)
+{
+	unsigned int status;
+
+	status = serial_in_rtu(up, UART_MSR);
+	status |= up->msr_saved_flags;
+	up->msr_saved_flags = 0;
+	if ((status & UART_MSR_ANY_DELTA) == 0)
+		return status;
+
+	if (status & UART_MSR_ANY_DELTA && up->ier & UART_IER_MSI &&
+	    up->port.state != NULL) {
+		if (status & UART_MSR_TERI)
+			up->port.icount.rng++;
+		if (status & UART_MSR_DDSR)
+			up->port.icount.dsr++;
+		if (status & UART_MSR_DDCD)
+			uart_handle_dcd_change
+				(&up->port, status & UART_MSR_DCD);
+		if (status & UART_MSR_DCTS)
+			uart_handle_cts_change
+				(&up->port, status & UART_MSR_CTS);
+		wake_up_interruptible(&up->port.state->port.delta_msr_wait);
+	}
+
+	return status;
+}
+
+#define SET_LSR(val, lsr) ((val) |= ((lsr) << 8))
+#define GET_LSR(val) ((val) >> 8)
+
+#define SET_CH(val, ch) ((val) |= (ch))
+#define GET_CH(val) ((val) & 0xff)
+
+void inline static serial_omap_do_rx(struct uart_omap_port *up, unsigned char ch, unsigned int lsr)
+{
+	unsigned int flag = TTY_NORMAL;
+
+	up->port.icount.rx++;
+
+	if (uart_handle_sysrq_char(&up->port, ch))
+		return;
+
+	uart_insert_char(&up->port, lsr, UART_LSR_OE, ch, flag);
+}
+
+static void serial_omap_rlsi(struct uart_omap_port *up, unsigned int lsr)
+{
+	uint16_t val = 0;
+	unsigned char ch = 0;
+
+	/* clear byte in rx fifo */
+	if (likely(lsr & UART_LSR_DR))
+		ch = serial_in_rtu(up, UART_RX);
+
+	SET_LSR(val, lsr);
+	/* error happened -> ignore ch */
+	kfifo_put(&up->rx_fifo, val);
+	tasklet_hi_schedule(&up->rx_tsklt);
+}
+
+// serial_omap_rlsi() bottom half
+void inline static serial_omap_rx_handle_errors(struct uart_omap_port *up, unsigned int lsr)
+{
+	unsigned int flag = TTY_NORMAL;
+
+	up->port.icount.rx++;
+
+	if (lsr & UART_LSR_BI) {
+		flag = TTY_BREAK;
+		lsr &= ~(UART_LSR_FE | UART_LSR_PE);
+		up->port.icount.brk++;
+		/*
+		 * We do the SysRQ and SAK checking
+		 * here because otherwise the break
+		 * may get masked by ignore_status_mask
+		 * or read_status_mask.
+		 */
+		if (uart_handle_break(&up->port))
+			return;
+
+	}
+
+	if (lsr & UART_LSR_PE) {
+		flag = TTY_PARITY;
+		up->port.icount.parity++;
+	}
+
+	if (lsr & UART_LSR_FE) {
+		flag = TTY_FRAME;
+		up->port.icount.frame++;
+	}
+
+	if (lsr & UART_LSR_OE)
+		up->port.icount.overrun++;
+
+#ifdef CONFIG_SERIAL_OMAP_CONSOLE
+	if (up->port.line == up->port.cons->index) {
+		/* Recover the break flag from console xmit */
+		lsr |= up->lsr_break_flag;
+	}
+#endif
+	uart_insert_char(&up->port, lsr, UART_LSR_OE, 0, flag);
+}
+
+static void serial_omap_rdi(struct uart_omap_port *up, unsigned int lsr)
+{
+	uint16_t val = 0;
+
+	if (!(lsr & UART_LSR_DR))
+		return;
+
+	SET_CH(val, serial_in_rtu(up, UART_RX));
+	SET_LSR(val, lsr);
+
+	kfifo_put(&up->rx_fifo, val);
+
+	tasklet_hi_schedule(&up->rx_tsklt);
+}
+
+/* BUGME: version for PSV was lacking pm calls here! */
+void serial_omap_modem_status_bh(unsigned long dev_id)
+{
+	struct uart_omap_port *up = (void*) dev_id;
+
+	pm_runtime_get_sync(up->dev);
+
+	spin_lock_bh(&up->port.lock);
+	check_modem_status(up);
+	spin_unlock_bh(&up->port.lock);
+
+	pm_runtime_mark_last_busy(up->dev);
+	pm_runtime_put_autosuspend(up->dev);
+
+}
+
+
+/*
+ * Wrapper to load off transmit_chars into a tasklet. Identical to modbus_omap_transmit()
+ */
+void serial_omap_transmit_bh(unsigned long dev_id)
+{
+	struct uart_omap_port *up = (void*) dev_id;
+
+	pm_runtime_get_sync(up->dev);
+
+	spin_lock(&up->port.lock);
+	transmit_chars_rtu(up, serial_in_rtu(up, UART_LSR));
+	spin_unlock(&up->port.lock);
+
+	serial_omap_set_noidle(up);
+	pm_runtime_mark_last_busy(up->dev);
+	pm_runtime_put_autosuspend(up->dev);
+}
+
+#define UART_LSR_ERROR_MASK (UART_LSR_BI | UART_LSR_FE | UART_LSR_PE |UART_LSR_OE)
+
+void serial_omap_receive_bh(unsigned long dev_id)
+{
+	struct uart_omap_port *up = (void*) dev_id;
+	uint16_t val = 0;
+	unsigned char ch = 0;
+	unsigned int lsr, nr_processed;
+
+	pm_runtime_get_sync(up->dev);
+	spin_lock_bh(&up->port.lock);
+
+	while(!kfifo_is_empty(&up->rx_fifo)) {
+
+		nr_processed = kfifo_get(&up->rx_fifo, &val);
+
+		if((lsr = GET_LSR(val)) & UART_LSR_ERROR_MASK)
+			serial_omap_rx_handle_errors(up, lsr);
+		else
+		{
+			ch = GET_CH(val);
+			/* regular rx; should be last statement in while (because of handle_sysrq) */
+			serial_omap_do_rx(up, ch, lsr);
+		}
+	}
+
+	spin_unlock_bh(&up->port.lock);
+
+	/* stuff from vanilla irq handler that may hold port.lock */
+	tty_flip_buffer_push(&up->port.state->port);
+
+	pm_runtime_mark_last_busy(up->dev);
+	pm_runtime_put_autosuspend(up->dev);
+}
+
+/**
+ * serial_omap_irq_rtu() - This handles the interrupt from one port
+ * @irq: uart port irq number
+ * @dev_id: uart port info
+ */
+irqreturn_t serial_omap_irq_rtu(int irq, void *dev_id)
+{
+	struct uart_omap_port *up = dev_id;
+	unsigned int iir, lsr;
+	unsigned int type;
+	irqreturn_t ret = IRQ_NONE;
+	int max_count = 256;
+
+	do {
+		iir = serial_in_rtu(up, UART_IIR);
+		if (iir & UART_IIR_NO_INT)
+			break;
+
+		ret = IRQ_HANDLED;
+		lsr = serial_in_rtu(up, UART_LSR);
+
+		/* extract IRQ type from IIR register */
+		type = iir & 0x3e;
+
+		switch (type) {
+		case UART_IIR_MSI:
+			/*
+			 * This should be safe towards process context functions because
+			 * we don't clear any existing bits, only adding new ones.
+			 */
+			up->msr_saved_flags |= serial_in_rtu(up, UART_MSR);
+			tasklet_hi_schedule(&up->mdm_status_tsklt);
+			break;
+		case UART_IIR_THRI:
+			/*
+			 * THR irq is cleared by writing to TX FIFO in tasklet
+			 * It gets reenabled there
+			 */
+			serial_omap_disable_ier_thri(up);
+
+#ifdef CONFIG_SERIAL_OMAP_RS485
+			if(!serial_omap_end_tx(up))
+				tasklet_hi_schedule(&up->tx_tsklt);
+#else
+			tasklet_hi_schedule(&up->tx_tsklt);
+#endif
+			break;
+		case UART_IIR_RX_TIMEOUT:
+			/* FALLTHROUGH */
+		case UART_IIR_RDI:
+			serial_omap_rdi(up, lsr);
+			break;
+		case UART_IIR_RLSI:
+			serial_omap_rlsi(up, lsr);
+			break;
+		case UART_IIR_CTS_RTS_DSR:
+			/* simply try again */
+			break;
+		case UART_IIR_XOFF:
+			/* FALLTHROUGH */
+		default:
+			break;
+		}
+	} while (!(iir & UART_IIR_NO_INT) && max_count--);
+
+	up->port_activity = jiffies;
+
+	return ret;
+}
+
+static unsigned int serial_omap_tx_empty(struct uart_port *port)
+{
+	struct uart_omap_port *up = to_uart_omap_port(port);
+	unsigned long flags = 0;
+	unsigned int ret = 0;
+
+	pm_runtime_get_sync(up->dev);
+	dev_dbg(up->port.dev, "serial_omap_tx_empty+%d\n", up->port.line);
+
+	serial_omap_irq_disable(up);
+	spin_lock_irqsave(&up->port.lock, flags);
+	//FIXME: use __serial_omap_tx_empty instead?
+	ret = serial_in_rtu(up, UART_LSR) & UART_LSR_TEMT ? TIOCSER_TEMT : 0;
+	spin_unlock_irqrestore(&up->port.lock, flags);
+	serial_omap_irq_enable(up);
+
+	pm_runtime_mark_last_busy(up->dev);
+	pm_runtime_put_autosuspend(up->dev);
+	return ret;
+}
+
+static unsigned int serial_omap_get_mctrl(struct uart_port *port)
+{
+	struct uart_omap_port *up = to_uart_omap_port(port);
+	unsigned int status;
+	unsigned int ret = 0;
+
+	pm_runtime_get_sync(up->dev);
+	status = check_modem_status(up);
+	pm_runtime_mark_last_busy(up->dev);
+	pm_runtime_put_autosuspend(up->dev);
+
+	dev_dbg(up->port.dev, "serial_omap_get_mctrl+%d\n", up->port.line);
+
+	if (status & UART_MSR_DCD)
+		ret |= TIOCM_CAR;
+	if (status & UART_MSR_RI)
+		ret |= TIOCM_RNG;
+	if (status & UART_MSR_DSR)
+		ret |= TIOCM_DSR;
+	if (status & UART_MSR_CTS)
+		ret |= TIOCM_CTS;
+	return ret;
+}
+
+static void serial_omap_set_mctrl(struct uart_port *port, unsigned int mctrl)
+{
+	struct uart_omap_port *up = to_uart_omap_port(port);
+	unsigned char mcr = 0, old_mcr;
+
+	dev_dbg(up->port.dev, "serial_omap_set_mctrl+%d\n", up->port.line);
+	if (mctrl & TIOCM_RTS)
+		mcr |= UART_MCR_RTS;
+	if (mctrl & TIOCM_DTR)
+		mcr |= UART_MCR_DTR;
+	if (mctrl & TIOCM_OUT1)
+		mcr |= UART_MCR_OUT1;
+	if (mctrl & TIOCM_OUT2)
+		mcr |= UART_MCR_OUT2;
+	if (mctrl & TIOCM_LOOP)
+		mcr |= UART_MCR_LOOP;
+
+	pm_runtime_get_sync(up->dev);
+	old_mcr = serial_in_rtu(up, UART_MCR);
+	old_mcr &= ~(UART_MCR_LOOP | UART_MCR_OUT2 | UART_MCR_OUT1 |
+		     UART_MCR_DTR | UART_MCR_RTS);
+	up->mcr = old_mcr | mcr;
+#ifdef CONFIG_SERIAL_OMAP_RS485
+	if(up->rs485.flags & SER_RS485_ENABLED && rts_on_send(up))
+		serial_omap_update_rts(up);
+#endif
+	serial_out_rtu(up, UART_MCR, up->mcr);
+	pm_runtime_mark_last_busy(up->dev);
+	pm_runtime_put_autosuspend(up->dev);
+
+}
+
+static void serial_omap_break_ctl(struct uart_port *port, int break_state)
+{
+	struct uart_omap_port *up = to_uart_omap_port(port);
+	unsigned long flags = 0;
+
+	dev_dbg(up->port.dev, "serial_omap_break_ctl+%d\n", up->port.line);
+	pm_runtime_get_sync(up->dev);
+	serial_omap_irq_disable(up);
+	spin_lock_irqsave(&up->port.lock, flags);
+	if (break_state == -1)
+		up->lcr |= UART_LCR_SBC;
+	else
+		up->lcr &= ~UART_LCR_SBC;
+	serial_out_rtu(up, UART_LCR, up->lcr);
+	spin_unlock_irqrestore(&up->port.lock, flags);
+	serial_omap_irq_enable(up);
+	pm_runtime_mark_last_busy(up->dev);
+	pm_runtime_put_autosuspend(up->dev);
+}
+
+static int serial_omap_startup(struct uart_port *port)
+{
+	struct uart_omap_port *up = to_uart_omap_port(port);
+	unsigned long flags = 0;
+	int retval;
+
+	/*
+	 * Allocate the IRQ
+	 */
+	retval = request_irq(up->port.irq, serial_omap_irq_rtu, up->port.irqflags | IRQF_NO_THREAD,
+				up->name, up);
+	if (retval)
+		return retval;
+
+	dev_dbg(up->port.dev, "serial_omap_startup+%d\n", up->port.line);
+
+	pm_runtime_get_sync(up->dev);
+	/*
+	 * Clear the FIFO buffers and disable them.
+	 * (they will be reenabled in set_termios())
+	 */
+	serial_omap_clear_fifos(up);
+	/* For Hardware flow control */
+	serial_out_rtu(up, UART_MCR, UART_MCR_RTS);
+
+	/*
+	 * Clear the interrupt registers.
+	 */
+	(void) serial_in_rtu(up, UART_LSR);
+	if (serial_in_rtu(up, UART_LSR) & UART_LSR_DR)
+		(void) serial_in_rtu(up, UART_RX);
+	(void) serial_in_rtu(up, UART_IIR);
+	(void) serial_in_rtu(up, UART_MSR);
+
+	/*
+	 * Now, initialize the UART
+	 */
+	serial_out_rtu(up, UART_LCR, UART_LCR_WLEN8);
+	serial_omap_irq_disable(up);
+	spin_lock_irqsave(&up->port.lock, flags);
+	/*
+	 * Most PC uarts need OUT2 raised to enable interrupts.
+	 */
+	up->port.mctrl |= TIOCM_OUT2;
+	serial_omap_set_mctrl(&up->port, up->port.mctrl);
+	spin_unlock_irqrestore(&up->port.lock, flags);
+	serial_omap_irq_enable(up);
+
+	up->msr_saved_flags = 0;
+
+	retval = kfifo_alloc(&up->rx_fifo, 128, 0);
+
+	if(retval) {
+		dev_err(up->dev, "Unable to allocate memory (%d)\n", retval);
+		return retval;
+	}
+
+	tasklet_init(&up->tx_tsklt, serial_omap_transmit_bh, (unsigned long)up);
+	tasklet_init(&up->rx_tsklt, serial_omap_receive_bh, (unsigned long)up);
+	tasklet_init(&up->mdm_status_tsklt, serial_omap_modem_status_bh, (unsigned long)up);
+
+	/*
+	 * Finally, enable interrupts. Note: Modem status interrupts
+	 * are set via set_termios(), which will be occurring imminently
+	 * anyway, so we don't enable them here.
+	 */
+	up->ier = UART_IER_RLSI | UART_IER_RDI;
+	serial_out_rtu(up, UART_IER, up->ier);
+
+	/* Enable module level wake up */
+	serial_out_rtu(up, UART_OMAP_WER, OMAP_UART_WER_MOD_WKUP);
+
+#ifdef CONFIG_SERIAL_OMAP_MODBUS
+	modbus_omap_startup(up);
+#endif
+
+	pm_runtime_mark_last_busy(up->dev);
+	pm_runtime_put_autosuspend(up->dev);
+	up->port_activity = jiffies;
+
+	return 0;
+}
+
+static void serial_omap_shutdown(struct uart_port *port)
+{
+	struct uart_omap_port *up = to_uart_omap_port(port);
+	unsigned long flags = 0;
+
+	dev_dbg(up->port.dev, "serial_omap_shutdown+%d\n", up->port.line);
+
+	pm_runtime_get_sync(up->dev);
+
+#ifdef CONFIG_SERIAL_OMAP_MODBUS
+	modbus_omap_shutdown(up);
+#endif
+
+	/*
+	 * Disable interrupts from this port
+	 */
+	up->ier = 0;
+	serial_out_rtu(up, UART_IER, 0);
+
+	serial_omap_irq_disable(up);
+	spin_lock_irqsave(&up->port.lock, flags);
+	up->port.mctrl &= ~TIOCM_OUT2;
+	serial_omap_set_mctrl(&up->port, up->port.mctrl);
+	spin_unlock_irqrestore(&up->port.lock, flags);
+	serial_omap_irq_enable(up);
+
+	/*
+	 * Disable break condition and FIFOs
+	 */
+	serial_out_rtu(up, UART_LCR, serial_in_rtu(up, UART_LCR) & ~UART_LCR_SBC);
+	serial_omap_clear_fifos(up);
+
+	/*
+	 * Read data port to reset things, and then free the irq
+	 */
+	if (serial_in_rtu(up, UART_LSR) & UART_LSR_DR)
+		(void) serial_in_rtu(up, UART_RX);
+
+	pm_runtime_mark_last_busy(up->dev);
+	pm_runtime_put_autosuspend(up->dev);
+	free_irq(up->port.irq, up);
+
+	up->tx_in_progress = 0;
+	up->tx_wait_end = 0;
+
+	tasklet_kill(&up->tx_tsklt);
+	tasklet_kill(&up->rx_tsklt);
+	tasklet_kill(&up->mdm_status_tsklt);
+	kfifo_free(&up->rx_fifo);
+}
+
+static void serial_omap_uart_qos_work(struct work_struct *work)
+{
+	struct uart_omap_port *up = container_of(work, struct uart_omap_port,
+						qos_work);
+
+	cpu_latency_qos_update_request(&up->pm_qos_request, up->latency);
+}
+
+static void
+serial_omap_set_termios(struct uart_port *port, struct ktermios *termios,
+			struct ktermios *old)
+{
+	struct uart_omap_port *up = to_uart_omap_port(port);
+	unsigned char cval = 0;
+	unsigned int baud, quot;
+
+	switch (termios->c_cflag & CSIZE) {
+	case CS5:
+		cval = UART_LCR_WLEN5;
+		break;
+	case CS6:
+		cval = UART_LCR_WLEN6;
+		break;
+	case CS7:
+		cval = UART_LCR_WLEN7;
+		break;
+	default:
+	case CS8:
+		cval = UART_LCR_WLEN8;
+		break;
+	}
+
+	if (termios->c_cflag & CSTOPB)
+		cval |= UART_LCR_STOP;
+	if (termios->c_cflag & PARENB)
+		cval |= UART_LCR_PARITY;
+	if (!(termios->c_cflag & PARODD))
+		cval |= UART_LCR_EPAR;
+	if (termios->c_cflag & CMSPAR)
+		cval |= UART_LCR_SPAR;
+
+	/*
+	 * Ask the core to calculate the divisor for us.
+	 */
+
+	baud = uart_get_baud_rate(port, termios, old, 0, port->uartclk/13);
+	quot = serial_omap_get_divisor(port, baud);
+
+#ifdef CONFIG_SERIAL_OMAP_MODBUS
+	modbus_omap_calc_to(up, baud);
+#endif
+
+	/* calculate wakeup latency constraint */
+	up->calc_latency = (USEC_PER_SEC * up->port.fifosize) / (baud / 8);
+	up->latency = up->calc_latency;
+	schedule_work(&up->qos_work);
+
+	up->dll = quot & 0xff;
+	up->dlh = quot >> 8;
+	up->mdr1 = UART_OMAP_MDR1_DISABLE;
+
+	up->fcr = UART_FCR_R_TRIG_01 | UART_FCR_T_TRIG_01 |
+			UART_FCR_ENABLE_FIFO;
+
+	/*
+	 * Ok, we're now changing the port state. Do it with
+	 * interrupts disabled.
+	 */
+	pm_runtime_get_sync(up->dev);
+	serial_omap_irq_disable(up);
+	spin_lock(&up->port.lock);
+
+	/*
+	 * Update the per-port timeout.
+	 */
+	uart_update_timeout(port, termios->c_cflag, baud);
+
+	up->port.read_status_mask = UART_LSR_OE | UART_LSR_THRE | UART_LSR_DR;
+	if (termios->c_iflag & INPCK)
+		up->port.read_status_mask |= UART_LSR_FE | UART_LSR_PE;
+	if (termios->c_iflag & (BRKINT | PARMRK))
+		up->port.read_status_mask |= UART_LSR_BI;
+
+	/*
+	 * Characters to ignore
+	 */
+	up->port.ignore_status_mask = 0;
+	if (termios->c_iflag & IGNPAR)
+		up->port.ignore_status_mask |= UART_LSR_PE | UART_LSR_FE;
+	if (termios->c_iflag & IGNBRK) {
+		up->port.ignore_status_mask |= UART_LSR_BI;
+		/*
+		 * If we're ignoring parity and break indicators,
+		 * ignore overruns too (for real raw support).
+		 */
+		if (termios->c_iflag & IGNPAR)
+			up->port.ignore_status_mask |= UART_LSR_OE;
+	}
+
+	/*
+	 * ignore all characters if CREAD is not set
+	 */
+	if ((termios->c_cflag & CREAD) == 0)
+		up->port.ignore_status_mask |= UART_LSR_DR;
+
+	/*
+	 * Modem status interrupts
+	 */
+	up->ier &= ~UART_IER_MSI;
+	if (UART_ENABLE_MS(&up->port, termios->c_cflag))
+		up->ier |= UART_IER_MSI;
+	serial_out_rtu(up, UART_IER, up->ier);
+	serial_out_rtu(up, UART_LCR, cval);		/* reset DLAB */
+	up->lcr = cval;
+	up->scr = 0;
+
+	/* FIFOs and DMA Settings */
+
+	/* FCR can be changed only when the
+	 * baud clock is not running
+	 * DLL_REG and DLH_REG set to 0.
+	 */
+	serial_out_rtu(up, UART_LCR, UART_LCR_CONF_MODE_A);
+	serial_out_rtu(up, UART_DLL, 0);
+	serial_out_rtu(up, UART_DLM, 0);
+	serial_out_rtu(up, UART_LCR, 0);
+
+	serial_out_rtu(up, UART_LCR, UART_LCR_CONF_MODE_B);
+
+	up->efr = serial_in_rtu(up, UART_EFR) & ~UART_EFR_ECB;
+	up->efr &= ~UART_EFR_SCD;
+	serial_out_rtu(up, UART_EFR, up->efr | UART_EFR_ECB);
+
+	serial_out_rtu(up, UART_LCR, UART_LCR_CONF_MODE_A);
+	up->mcr = serial_in_rtu(up, UART_MCR) & ~UART_MCR_TCRTLR;
+	serial_out_rtu(up, UART_MCR, up->mcr | UART_MCR_TCRTLR);
+	/* FIFO ENABLE, DMA MODE */
+
+	up->scr |= OMAP_UART_SCR_RX_TRIG_GRANU1_MASK;
+	/*
+	 * NOTE: Setting OMAP_UART_SCR_RX_TRIG_GRANU1_MASK
+	 * sets Enables the granularity of 1 for TRIGGER RX
+	 * level. Along with setting RX FIFO trigger level
+	 * to 1 (as noted below, 16 characters) and TLR[3:0]
+	 * to zero this will result RX FIFO threshold level
+	 * to 1 character, instead of 16 as noted in comment
+	 * below.
+	 */
+
+	/* Set receive FIFO threshold to 16 characters and
+	 * transmit FIFO threshold to 16 spaces
+	 */
+	up->fcr &= ~OMAP_UART_FCR_RX_FIFO_TRIG_MASK;
+	up->fcr &= ~OMAP_UART_FCR_TX_FIFO_TRIG_MASK;
+	up->fcr |= UART_FCR6_R_TRIGGER_16 | UART_FCR6_T_TRIGGER_24 |
+		UART_FCR_ENABLE_FIFO;
+
+	serial_out_rtu(up, UART_FCR, up->fcr);
+	serial_out_rtu(up, UART_LCR, UART_LCR_CONF_MODE_B);
+
+	serial_out_rtu(up, UART_OMAP_SCR, up->scr);
+
+	/* Reset UART_MCR00CRTLR: this must be done with the EFR_ECB bit set */
+	serial_out_rtu(up, UART_LCR, UART_LCR_CONF_MODE_A);
+	serial_out_rtu(up, UART_MCR, up->mcr);
+	serial_out_rtu(up, UART_LCR, UART_LCR_CONF_MODE_B);
+	serial_out_rtu(up, UART_EFR, up->efr);
+	serial_out_rtu(up, UART_LCR, UART_LCR_CONF_MODE_A);
+
+	/* Protocol, Baud Rate, and Interrupt Settings */
+
+	if (up->errata & UART_ERRATA_i202_MDR1_ACCESS)
+		serial_omap_mdr1_errataset(up, up->mdr1);
+	else
+		serial_out_rtu(up, UART_OMAP_MDR1, up->mdr1);
+
+	serial_out_rtu(up, UART_LCR, UART_LCR_CONF_MODE_B);
+	serial_out_rtu(up, UART_EFR, up->efr | UART_EFR_ECB);
+
+	serial_out_rtu(up, UART_LCR, 0);
+	serial_out_rtu(up, UART_IER, 0);
+	serial_out_rtu(up, UART_LCR, UART_LCR_CONF_MODE_B);
+
+	serial_out_rtu(up, UART_DLL, up->dll);	/* LS of divisor */
+	serial_out_rtu(up, UART_DLM, up->dlh);	/* MS of divisor */
+
+	serial_out_rtu(up, UART_LCR, 0);
+	serial_out_rtu(up, UART_IER, up->ier);
+	serial_out_rtu(up, UART_LCR, UART_LCR_CONF_MODE_B);
+
+	serial_out_rtu(up, UART_EFR, up->efr);
+	serial_out_rtu(up, UART_LCR, cval);
+
+	if (!serial_omap_baud_is_mode16(port, baud))
+		up->mdr1 = UART_OMAP_MDR1_13X_MODE;
+	else
+		up->mdr1 = UART_OMAP_MDR1_16X_MODE;
+
+	if (up->errata & UART_ERRATA_i202_MDR1_ACCESS)
+		serial_omap_mdr1_errataset(up, up->mdr1);
+	else
+		serial_out_rtu(up, UART_OMAP_MDR1, up->mdr1);
+
+	/* Configure flow control */
+	serial_out_rtu(up, UART_LCR, UART_LCR_CONF_MODE_B);
+
+	/* XON1/XOFF1 accessible mode B, TCRTLR=0, ECB=0 */
+	serial_out_rtu(up, UART_XON1, termios->c_cc[VSTART]);
+	serial_out_rtu(up, UART_XOFF1, termios->c_cc[VSTOP]);
+
+	/* Enable access to TCR/TLR */
+	serial_out_rtu(up, UART_EFR, up->efr | UART_EFR_ECB);
+	serial_out_rtu(up, UART_LCR, UART_LCR_CONF_MODE_A);
+	serial_out_rtu(up, UART_MCR, up->mcr | UART_MCR_TCRTLR);
+
+	serial_out_rtu(up, UART_TI752_TCR, OMAP_UART_TCR_TRIG);
+
+	if (termios->c_cflag & CRTSCTS && up->port.flags & UPF_HARD_FLOW) {
+		/* Enable AUTORTS and AUTOCTS */
+		up->efr |= UART_EFR_CTS | UART_EFR_RTS;
+
+		/* Ensure MCR RTS is asserted */
+		up->mcr |= UART_MCR_RTS;
+	} else {
+		/* Disable AUTORTS and AUTOCTS */
+		up->efr &= ~(UART_EFR_CTS | UART_EFR_RTS);
+	}
+
+	if (up->port.flags & UPF_SOFT_FLOW) {
+		/* clear SW control mode bits */
+		up->efr &= OMAP_UART_SW_CLR;
+
+		/*
+		 * IXON Flag:
+		 * Enable XON/XOFF flow control on input.
+		 * Receiver compares XON1, XOFF1.
+		 */
+		if (termios->c_iflag & IXON)
+			up->efr |= OMAP_UART_SW_RX;
+
+		/*
+		 * IXOFF Flag:
+		 * Enable XON/XOFF flow control on output.
+		 * Transmit XON1, XOFF1
+		 */
+		if (termios->c_iflag & IXOFF)
+			up->efr |= OMAP_UART_SW_TX;
+
+		/*
+		 * IXANY Flag:
+		 * Enable any character to restart output.
+		 * Operation resumes after receiving any
+		 * character after recognition of the XOFF character
+		 */
+		if (termios->c_iflag & IXANY)
+			up->mcr |= UART_MCR_XONANY;
+		else
+			up->mcr &= ~UART_MCR_XONANY;
+	}
+	serial_out_rtu(up, UART_MCR, up->mcr);
+	serial_out_rtu(up, UART_LCR, UART_LCR_CONF_MODE_B);
+	serial_out_rtu(up, UART_EFR, up->efr);
+	serial_out_rtu(up, UART_LCR, up->lcr);
+
+	serial_omap_set_mctrl(&up->port, up->port.mctrl);
+
+	spin_unlock(&up->port.lock);
+	serial_omap_irq_enable(up);
+	pm_runtime_mark_last_busy(up->dev);
+	pm_runtime_put_autosuspend(up->dev);
+	dev_dbg(up->port.dev, "serial_omap_set_termios+%d\n", up->port.line);
+}
+
+/* static int serial_omap_set_wake(struct uart_port *port, unsigned int state) */
+/* { */
+/* 	struct uart_omap_port *up = to_uart_omap_port(port); */
+
+/* 	serial_omap_enable_wakeup(up, state); */
+
+/* 	return 0; */
+/* } */
+
+static void
+serial_omap_pm(struct uart_port *port, unsigned int state,
+	       unsigned int oldstate)
+{
+	struct uart_omap_port *up = to_uart_omap_port(port);
+	unsigned char efr;
+
+	dev_dbg(up->port.dev, "serial_omap_pm+%d\n", up->port.line);
+
+	pm_runtime_get_sync(up->dev);
+	serial_out_rtu(up, UART_LCR, UART_LCR_CONF_MODE_B);
+	efr = serial_in_rtu(up, UART_EFR);
+	serial_out_rtu(up, UART_EFR, efr | UART_EFR_ECB);
+	serial_out_rtu(up, UART_LCR, 0);
+
+	serial_out_rtu(up, UART_IER, (state != 0) ? UART_IERX_SLEEP : 0);
+	serial_out_rtu(up, UART_LCR, UART_LCR_CONF_MODE_B);
+	serial_out_rtu(up, UART_EFR, efr);
+	serial_out_rtu(up, UART_LCR, 0);
+
+	if (!device_may_wakeup(up->dev)) {
+		if (!state)
+			pm_runtime_forbid(up->dev);
+		else
+			pm_runtime_allow(up->dev);
+	}
+
+	pm_runtime_mark_last_busy(up->dev);
+	pm_runtime_put_autosuspend(up->dev);
+}
+
+static void serial_omap_release_port(struct uart_port *port)
+{
+	dev_dbg(port->dev, "serial_omap_release_port+\n");
+}
+
+static int serial_omap_request_port(struct uart_port *port)
+{
+	dev_dbg(port->dev, "serial_omap_request_port+\n");
+	return 0;
+}
+
+static void serial_omap_config_port(struct uart_port *port, int flags)
+{
+	struct uart_omap_port *up = to_uart_omap_port(port);
+
+	dev_dbg(up->port.dev, "serial_omap_config_port+%d\n",
+							up->port.line);
+	up->port.type = PORT_OMAP;
+	up->port.flags |= UPF_SOFT_FLOW | UPF_HARD_FLOW;
+}
+
+static int
+serial_omap_verify_port(struct uart_port *port, struct serial_struct *ser)
+{
+	/* we don't want the core code to modify any port params */
+	dev_dbg(port->dev, "serial_omap_verify_port+\n");
+	return -EINVAL;
+}
+
+static const char *
+serial_omap_type(struct uart_port *port)
+{
+	struct uart_omap_port *up = to_uart_omap_port(port);
+
+	dev_dbg(up->port.dev, "serial_omap_type+%d\n", up->port.line);
+	return up->name;
+}
+
+#define BOTH_EMPTY (UART_LSR_TEMT | UART_LSR_THRE)
+
+static inline void wait_for_xmitr(struct uart_omap_port *up)
+{
+	unsigned int status, tmout = 10000;
+
+	/* Wait up to 10ms for the character(s) to be sent. */
+	do {
+		status = serial_in_rtu(up, UART_LSR);
+
+		if (status & UART_LSR_BI)
+			up->lsr_break_flag = UART_LSR_BI;
+
+		if (--tmout == 0)
+			break;
+		udelay(1);
+	} while ((status & BOTH_EMPTY) != BOTH_EMPTY);
+
+	/* Wait up to 1s for flow control if necessary */
+	if (up->port.flags & UPF_CONS_FLOW) {
+		tmout = 1000000;
+		for (tmout = 1000000; tmout; tmout--) {
+			unsigned int msr = serial_in_rtu(up, UART_MSR);
+
+			up->msr_saved_flags |= msr & MSR_SAVE_FLAGS;
+			if (msr & UART_MSR_CTS)
+				break;
+
+			udelay(1);
+		}
+	}
+}
+
+#ifdef CONFIG_CONSOLE_POLL
+
+static void serial_omap_poll_put_char(struct uart_port *port, unsigned char ch)
+{
+	struct uart_omap_port *up = to_uart_omap_port(port);
+
+	pm_runtime_get_sync(up->dev);
+	wait_for_xmitr(up);
+	serial_out_rtu(up, UART_TX, ch);
+	pm_runtime_mark_last_busy(up->dev);
+	pm_runtime_put_autosuspend(up->dev);
+}
+
+static int serial_omap_poll_get_char(struct uart_port *port)
+{
+	struct uart_omap_port *up = to_uart_omap_port(port);
+	unsigned int status;
+
+	pm_runtime_get_sync(up->dev);
+	status = serial_in_rtu(up, UART_LSR);
+	if (!(status & UART_LSR_DR)) {
+		status = NO_POLL_CHAR;
+		goto out;
+	}
+
+	status = serial_in_rtu(up, UART_RX);
+
+out:
+	pm_runtime_mark_last_busy(up->dev);
+	pm_runtime_put_autosuspend(up->dev);
+
+	return status;
+}
+
+#endif /* CONFIG_CONSOLE_POLL */
+
+#ifdef CONFIG_SERIAL_OMAP_CONSOLE
+
+static struct uart_omap_port *serial_omap_console_ports[OMAP_MAX_HSUART_PORTS];
+
+static struct uart_driver serial_omap_reg;
+
+static void serial_omap_console_putchar(struct uart_port *port, int ch)
+{
+	struct uart_omap_port *up = to_uart_omap_port(port);
+
+	wait_for_xmitr(up);
+	serial_out_rtu(up, UART_TX, ch);
+}
+
+static void
+serial_omap_console_write(struct console *co, const char *s,
+		unsigned int count)
+{
+	struct uart_omap_port *up = serial_omap_console_ports[co->index];
+	unsigned long flags;
+	unsigned int ier;
+	int locked = 1;
+
+	pm_runtime_get_sync(up->dev);
+
+	serial_omap_irq_disable(up);
+	if (up->port.sysrq)
+		locked = 0;
+	else if (oops_in_progress)
+		locked = spin_trylock_irqsave(&up->port.lock, flags);
+	else
+		spin_lock_irqsave(&up->port.lock, flags);
+
+	/*
+	 * First save the IER then disable the interrupts
+	 */
+	ier = serial_in_rtu(up, UART_IER);
+	serial_out_rtu(up, UART_IER, 0);
+
+	uart_console_write(&up->port, s, count, serial_omap_console_putchar);
+
+	/*
+	 * Finally, wait for transmitter to become empty
+	 * and restore the IER
+	 */
+	wait_for_xmitr(up);
+	serial_out_rtu(up, UART_IER, ier);
+	/*
+	 * The receive handling will happen properly because the
+	 * receive ready bit will still be set; it is not cleared
+	 * on read.  However, modem control will not, we must
+	 * call it if we have saved something in the saved flags
+	 * while processing with interrupts off.
+	 */
+	if (up->msr_saved_flags)
+		check_modem_status(up);
+
+	pm_runtime_mark_last_busy(up->dev);
+	pm_runtime_put_autosuspend(up->dev);
+	if (locked)
+		spin_unlock_irqrestore(&up->port.lock, flags);
+
+	serial_omap_irq_enable(up);
+}
+
+#ifdef CONFIG_SERIAL_OMAP_RS485
+static int
+serial_omap_ioctl(struct uart_port *port, unsigned int cmd, unsigned long arg)
+{
+	struct uart_omap_port *up = (struct uart_omap_port *) port;
+	struct serial_rs485 rs485conf;
+	unsigned long flags;
+
+	switch(cmd) {
+	case TIOCSRS485:
+		if(copy_from_user(&rs485conf, (struct serial_rs485*) arg, sizeof(rs485conf)))
+			return -EFAULT;
+
+		serial_omap_irq_disable(up);
+		spin_lock_irqsave(&up->port.lock, flags);
+
+		serial_omap_config_rs485(up, &rs485conf);
+
+		if(rts_on_send(up))
+			serial_omap_update_rts(up);
+		serial_omap_thri_mode(up);
+
+		spin_unlock_irqrestore(&up->port.lock, flags);
+		serial_omap_irq_enable(up);
+
+		break;
+
+	case TIOCGRS485:
+		if(copy_to_user((struct serial_rs485 *) arg, &(up->rs485), sizeof(rs485conf)))
+			return -EFAULT;
+		break;
+#ifdef CONFIG_SERIAL_OMAP_MODBUS
+	case TIOCSMBRTU:
+		switch(arg)
+		{
+		case 0:
+			modbus_omap_disable(up);
+			break;
+		case 1:
+			modbus_omap_enable(up);
+			break;
+		default: break;
+		}
+		break;
+	case TIOCSMBRTUADDR:
+		dev_info(up->dev, "Setting slave ID to %lu", arg);
+		up->mb_port.slave_id = arg;
+		break;
+	case TIOCSMBRTUIFT:
+		dev_info(up->dev, "Setting interframe timeout to %luns %s",
+		         arg, arg ? "" : "(auto)");
+		up->mb_port.to_35_override = arg;
+		break;
+	case TIOCSMBRTUICT:
+		dev_info(up->dev, "Setting intercharacter timeout to %luns %s",
+		         arg, arg ? "" : "(auto)");
+		up->mb_port.to_15_override = arg;
+		break;
+	case TIOCSMBRTUTS:
+		switch(arg)
+		{
+		case 0:
+			modbus_omap_timestamp_disable(up);
+			break;
+		case 1:
+			modbus_omap_timestamp_enable(up);
+			break;
+		default: break;
+		}
+	break;
+#endif
+	default:
+		return -ENOIOCTLCMD;
+	}
+	return 0;
+}
+#endif
+
+static int __init
+serial_omap_console_setup(struct console *co, char *options)
+{
+	struct uart_omap_port *up;
+	int baud = 115200;
+	int bits = 8;
+	int parity = 'n';
+	int flow = 'n';
+
+	if (serial_omap_console_ports[co->index] == NULL)
+		return -ENODEV;
+	up = serial_omap_console_ports[co->index];
+
+	if (options)
+		uart_parse_options(options, &baud, &parity, &bits, &flow);
+
+	return uart_set_options(&up->port, co, baud, parity, bits, flow);
+}
+
+static struct console serial_omap_console = {
+	.name		= OMAP_SERIAL_NAME,
+	.write		= serial_omap_console_write,
+	.device		= uart_console_device,
+	.setup		= serial_omap_console_setup,
+	.flags		= CON_PRINTBUFFER,
+	.index		= -1,
+	.data		= &serial_omap_reg,
+};
+
+static void serial_omap_add_console_port(struct uart_omap_port *up)
+{
+	serial_omap_console_ports[up->port.line] = up;
+}
+
+#define OMAP_CONSOLE	(&serial_omap_console)
+
+#else
+
+#define OMAP_CONSOLE	NULL
+
+static inline void serial_omap_add_console_port(struct uart_omap_port *up)
+{}
+
+#endif
+
+static struct uart_ops serial_omap_pops = {
+	.tx_empty	= serial_omap_tx_empty,
+	.set_mctrl	= serial_omap_set_mctrl,
+	.get_mctrl	= serial_omap_get_mctrl,
+	.stop_tx	= serial_omap_stop_tx,
+	.start_tx	= serial_omap_start_tx,
+	.throttle	= serial_omap_throttle,
+	.unthrottle	= serial_omap_unthrottle,
+	.stop_rx	= serial_omap_stop_rx,
+	.enable_ms	= serial_omap_enable_ms,
+	.break_ctl	= serial_omap_break_ctl,
+	.startup	= serial_omap_startup,
+	.shutdown	= serial_omap_shutdown,
+	.set_termios	= serial_omap_set_termios,
+	.pm		= serial_omap_pm,
+	//	.set_wake	= serial_omap_set_wake,
+	.type		= serial_omap_type,
+	.release_port	= serial_omap_release_port,
+	.request_port	= serial_omap_request_port,
+	.config_port	= serial_omap_config_port,
+	.verify_port	= serial_omap_verify_port,
+#ifdef CONFIG_SERIAL_OMAP_RS485
+	.ioctl = serial_omap_ioctl,
+#endif
+#ifdef CONFIG_CONSOLE_POLL
+	.poll_put_char  = serial_omap_poll_put_char,
+	.poll_get_char  = serial_omap_poll_get_char,
+#endif
+};
+
+static struct uart_driver serial_omap_reg = {
+	.owner		= THIS_MODULE,
+	.driver_name	= "OMAP-SERIAL-RTU",
+	.dev_name	= OMAP_SERIAL_NAME,
+	.nr		= OMAP_MAX_HSUART_PORTS,
+	.cons		= OMAP_CONSOLE,
+};
+
+#ifdef CONFIG_PM_SLEEP
+static int serial_omap_suspend(struct device *dev)
+{
+	struct uart_omap_port *up = dev_get_drvdata(dev);
+
+	uart_suspend_port(&serial_omap_reg, &up->port);
+	flush_work(&up->qos_work);
+
+	return 0;
+}
+
+static int serial_omap_resume(struct device *dev)
+{
+	struct uart_omap_port *up = dev_get_drvdata(dev);
+
+	uart_resume_port(&serial_omap_reg, &up->port);
+
+	return 0;
+}
+#endif
+
+static void omap_serial_fill_features_erratas(struct uart_omap_port *up)
+{
+	u32 mvr, scheme;
+	u16 revision, major, minor;
+
+	mvr = serial_in_rtu(up, UART_OMAP_MVER);
+
+	/* Check revision register scheme */
+	scheme = mvr >> OMAP_UART_MVR_SCHEME_SHIFT;
+
+	switch (scheme) {
+	case 0: /* Legacy Scheme: OMAP2/3 */
+		/* MINOR_REV[0:4], MAJOR_REV[4:7] */
+		major = (mvr & OMAP_UART_LEGACY_MVR_MAJ_MASK) >>
+					OMAP_UART_LEGACY_MVR_MAJ_SHIFT;
+		minor = (mvr & OMAP_UART_LEGACY_MVR_MIN_MASK);
+		break;
+	case 1:
+		/* New Scheme: OMAP4+ */
+		/* MINOR_REV[0:5], MAJOR_REV[8:10] */
+		major = (mvr & OMAP_UART_MVR_MAJ_MASK) >>
+					OMAP_UART_MVR_MAJ_SHIFT;
+		minor = (mvr & OMAP_UART_MVR_MIN_MASK);
+		break;
+	default:
+		dev_warn(up->dev,
+			"Unknown %s revision, defaulting to highest\n",
+			up->name);
+		/* highest possible revision */
+		major = 0xff;
+		minor = 0xff;
+	}
+
+	/* normalize revision for the driver */
+	revision = UART_BUILD_REVISION(major, minor);
+
+	switch (revision) {
+	case OMAP_UART_REV_46:
+		up->errata |= (UART_ERRATA_i202_MDR1_ACCESS |
+				UART_ERRATA_i291_DMA_FORCEIDLE);
+		break;
+	case OMAP_UART_REV_52:
+		up->errata |= (UART_ERRATA_i202_MDR1_ACCESS |
+				UART_ERRATA_i291_DMA_FORCEIDLE);
+		break;
+	case OMAP_UART_REV_63:
+		up->errata |= UART_ERRATA_i202_MDR1_ACCESS;
+		break;
+	default:
+		break;
+	}
+}
+
+static struct omap_uart_port_info *of_get_uart_port_info(struct device *dev)
+{
+	struct omap_uart_port_info *omap_up_info;
+
+	omap_up_info = devm_kzalloc(dev, sizeof(*omap_up_info), GFP_KERNEL);
+	if (!omap_up_info)
+		return NULL; /* out of memory */
+
+	of_property_read_u32(dev->of_node, "clock-frequency",
+					 &omap_up_info->uartclk);
+	return omap_up_info;
+}
+
+static int of_probe_rs485(struct uart_omap_port *up,
+			  struct device_node *np)
+{
+	struct serial_rs485 *rs485conf = &up->rs485;
+	u32 rs485_delay[2];
+	enum of_gpio_flags flags;
+	int ret;
+
+	rs485conf->flags = 0;
+	up->rts_gpio = -EINVAL;
+
+	if (!np)
+		return 0;
+
+	if (of_property_read_bool(np, "rs485-rts-active-high"))
+		rs485conf->flags |= SER_RS485_RTS_ON_SEND;
+	else
+		rs485conf->flags |= SER_RS485_RTS_AFTER_SEND;
+
+#if 0				/* we have no rts_gpio, we toggle rts by register */
+	/* check for tx enable gpio */
+	up->rts_gpio = of_get_named_gpio_flags(np, "rts-gpio", 0, &flags);
+	if (gpio_is_valid(up->rts_gpio)) {
+		ret = devm_gpio_request(up->dev, up->rts_gpio, "omap-serial-rtu");
+		if (ret < 0)
+			return ret;
+		ret = gpio_direction_output(up->rts_gpio,
+				rs485conf->flags & SER_RS485_RTS_AFTER_SEND);
+		if (ret < 0)
+			return ret;
+	} else if (up->rts_gpio == -EPROBE_DEFER) {
+		return -EPROBE_DEFER;
+	} else {
+		up->rts_gpio = -EINVAL;
+	}
+#endif
+
+	/* check for tx enable gpio */
+	up->rs485en_gpio = of_get_named_gpio_flags(np, "rs485en-gpio", 0, &flags);
+	if (gpio_is_valid(up->rs485en_gpio)) {
+		ret = gpio_request(up->rs485en_gpio, "omap-serial-rtu-rs485en");
+		if (ret < 0)
+			return ret;
+	} else if (up->rs485en_gpio == -EPROBE_DEFER) {
+#if 0
+		devm_gpio_free(up->dev, up->rts_gpio);
+#endif
+		return -EPROBE_DEFER;
+	} else {
+		up->rs485en_gpio = -EINVAL;
+	}
+
+	/*
+	 * This is only used in newer omap-serial driver
+	 * but keep it here ...
+	 */
+	if (of_property_read_u32_array(np, "rs485-rts-delay",
+				rs485_delay, 2) == 0) {
+		rs485conf->delay_rts_before_send = rs485_delay[0];
+		rs485conf->delay_rts_after_send = rs485_delay[1];
+	}
+
+	if (of_property_read_bool(np, "rs485-rx-during-tx"))
+		rs485conf->flags |= SER_RS485_RX_DURING_TX;
+
+	if (of_property_read_bool(np, "linux,rs485-enabled-at-boot-time"))
+		rs485conf->flags |= SER_RS485_ENABLED;
+
+	if (gpio_is_valid(up->rs485en_gpio)) {
+		int val = rs485conf->flags & SER_RS485_ENABLED;
+
+		up->rs485en_alow = flags & OF_GPIO_ACTIVE_LOW;
+
+		pr_info("%s: rs485en-gpio is %s\n", __func__,
+				up->rs485en_alow ? "active_low" : "active_high");
+
+		/* turn on or off rs485 hw path */
+		ret = gpio_direction_output(up->rs485en_gpio, up->rs485en_alow ? !val : val);
+		if (ret < 0)
+			return ret;
+
+		pr_info("%s: %s mode enabled\n", __func__, val ? "rs485" : "rs232");
+	}
+
+	return 0;
+}
+
+static int serial_omap_probe(struct platform_device *pdev)
+{
+	struct uart_omap_port	*up;
+	struct resource		*mem;
+	struct omap_uart_port_info *omap_up_info = pdev->dev.platform_data;
+	int uartirq = 0;
+	int ret;
+
+	if (pdev->dev.of_node) {
+		uartirq = irq_of_parse_and_map(pdev->dev.of_node, 0);
+		if (!uartirq)
+			return -EPROBE_DEFER;
+
+		omap_up_info = of_get_uart_port_info(&pdev->dev);
+	} else {
+		uartirq = platform_get_irq(pdev, 0);
+		if (uartirq < 0)
+			return -EPROBE_DEFER;
+	}
+
+	mem = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!mem) {
+		dev_err(&pdev->dev, "no mem resource?\n");
+		return -ENODEV;
+	}
+
+	if (!devm_request_mem_region(&pdev->dev, mem->start, resource_size(mem),
+				pdev->dev.driver->name)) {
+		dev_err(&pdev->dev, "memory region already claimed\n");
+		return -EBUSY;
+	}
+
+	up = devm_kzalloc(&pdev->dev, sizeof(*up), GFP_KERNEL);
+	if (!up)
+		return -ENOMEM;
+
+	if (pdev->dev.of_node) {
+		ret = of_probe_rs485(up, pdev->dev.of_node);
+		if (ret == -EPROBE_DEFER) {
+			devm_kfree(&pdev->dev, up);
+			return -EPROBE_DEFER;
+		}
+	}
+
+	up->dev = &pdev->dev;
+	up->port.dev = &pdev->dev;
+	up->port.type = PORT_OMAP;
+	up->port.iotype = UPIO_MEM;
+	up->port.irq = uartirq;
+
+	up->port.regshift = 2;
+	up->port.fifosize = 64;
+	up->port.ops = &serial_omap_pops;
+
+	if (pdev->dev.of_node)
+		up->port.line = of_alias_get_id(pdev->dev.of_node, "serial");
+	else
+		up->port.line = pdev->id;
+
+	if (up->port.line < 0) {
+		dev_err(&pdev->dev, "failed to get alias/pdev id, errno %d\n",
+								up->port.line);
+		ret = -ENODEV;
+		goto err_port_line;
+	}
+
+	up->pins = devm_pinctrl_get_select_default(&pdev->dev);
+	if (IS_ERR(up->pins)) {
+		dev_warn(&pdev->dev, "did not get pins for uart%i error: %li\n",
+			 up->port.line, PTR_ERR(up->pins));
+		up->pins = NULL;
+	}
+
+	sprintf(up->name, "OMAP UART%d HWIRQ", up->port.line);
+	up->port.mapbase = mem->start;
+	up->port.membase = devm_ioremap(&pdev->dev, mem->start,
+						resource_size(mem));
+	if (!up->port.membase) {
+		dev_err(&pdev->dev, "can't ioremap UART\n");
+		ret = -ENOMEM;
+		goto err_ioremap;
+	}
+
+	up->port.flags = omap_up_info->flags;
+	up->port.uartclk = omap_up_info->uartclk;
+	if (!up->port.uartclk) {
+		up->port.uartclk = DEFAULT_CLK_SPEED;
+		dev_warn(&pdev->dev, "No clock speed specified: using default:"
+						"%d\n", DEFAULT_CLK_SPEED);
+	}
+
+	up->latency = PM_QOS_CPU_LATENCY_DEFAULT_VALUE;
+	up->calc_latency = PM_QOS_CPU_LATENCY_DEFAULT_VALUE;
+	cpu_latency_qos_add_request(&up->pm_qos_request, up->latency);
+	serial_omap_uart_wq = create_singlethread_workqueue(up->name);
+	INIT_WORK(&up->qos_work, serial_omap_uart_qos_work);
+
+	platform_set_drvdata(pdev, up);
+	pm_runtime_enable(&pdev->dev);
+	pm_runtime_use_autosuspend(&pdev->dev);
+	pm_runtime_set_autosuspend_delay(&pdev->dev,
+			omap_up_info->autosuspend_timeout);
+
+	pm_runtime_irq_safe(&pdev->dev);
+	pm_runtime_get_sync(&pdev->dev);
+
+	omap_serial_fill_features_erratas(up);
+
+	ui[up->port.line] = up;
+	serial_omap_add_console_port(up);
+
+	ret = uart_add_one_port(&serial_omap_reg, &up->port);
+	if (ret != 0)
+		goto err_add_port;
+
+	pm_runtime_mark_last_busy(up->dev);
+	pm_runtime_put_autosuspend(up->dev);
+
+	return 0;
+
+err_add_port:
+	pm_runtime_put(&pdev->dev);
+	pm_runtime_disable(&pdev->dev);
+err_ioremap:
+err_port_line:
+	dev_err(&pdev->dev, "[UART%d]: failure [%s]: %d\n",
+				pdev->id, __func__, ret);
+	return ret;
+}
+
+static int serial_omap_remove(struct platform_device *dev)
+{
+	struct uart_omap_port *up = platform_get_drvdata(dev);
+
+	pm_runtime_put_sync(up->dev);
+	pm_runtime_disable(up->dev);
+	uart_remove_one_port(&serial_omap_reg, &up->port);
+	cpu_latency_qos_remove_request(&up->pm_qos_request);
+
+	return 0;
+}
+
+/*
+ * Work Around for Errata i202 (2430, 3430, 3630, 4430 and 4460)
+ * The access to uart register after MDR1 Access
+ * causes UART to corrupt data.
+ *
+ * Need a delay =
+ * 5 L4 clock cycles + 5 UART functional clock cycle (@48MHz = ~0.2uS)
+ * give 10 times as much
+ */
+static void serial_omap_mdr1_errataset(struct uart_omap_port *up, u8 mdr1)
+{
+	u8 timeout = 255;
+
+	serial_out_rtu(up, UART_OMAP_MDR1, mdr1);
+	udelay(2);
+	serial_out_rtu(up, UART_FCR, up->fcr | UART_FCR_CLEAR_XMIT |
+			UART_FCR_CLEAR_RCVR);
+	/*
+	 * Wait for FIFO to empty: when empty, RX_FIFO_E bit is 0 and
+	 * TX_FIFO_E bit is 1.
+	 */
+	while (UART_LSR_THRE != (serial_in_rtu(up, UART_LSR) &
+				(UART_LSR_THRE | UART_LSR_DR))) {
+		timeout--;
+		if (!timeout) {
+			/* Should *never* happen. we warn and carry on */
+			dev_crit(up->dev, "Errata i202: timedout %x\n",
+						serial_in_rtu(up, UART_LSR));
+			break;
+		}
+		udelay(1);
+	}
+}
+
+#ifdef CONFIG_PM
+static void serial_omap_restore_context(struct uart_omap_port *up)
+{
+	if (up->errata & UART_ERRATA_i202_MDR1_ACCESS)
+		serial_omap_mdr1_errataset(up, UART_OMAP_MDR1_DISABLE);
+	else
+		serial_out_rtu(up, UART_OMAP_MDR1, UART_OMAP_MDR1_DISABLE);
+
+	serial_out_rtu(up, UART_LCR, UART_LCR_CONF_MODE_B); /* Config B mode */
+	serial_out_rtu(up, UART_EFR, UART_EFR_ECB);
+	serial_out_rtu(up, UART_LCR, 0x0); /* Operational mode */
+	serial_out_rtu(up, UART_IER, 0x0);
+	serial_out_rtu(up, UART_LCR, UART_LCR_CONF_MODE_B); /* Config B mode */
+	serial_out_rtu(up, UART_DLL, up->dll);
+	serial_out_rtu(up, UART_DLM, up->dlh);
+	serial_out_rtu(up, UART_LCR, 0x0); /* Operational mode */
+	serial_out_rtu(up, UART_IER, up->ier);
+	serial_out_rtu(up, UART_FCR, up->fcr);
+	serial_out_rtu(up, UART_LCR, UART_LCR_CONF_MODE_A);
+	serial_out_rtu(up, UART_MCR, up->mcr);
+	serial_out_rtu(up, UART_LCR, UART_LCR_CONF_MODE_B); /* Config B mode */
+	serial_out_rtu(up, UART_OMAP_SCR, up->scr);
+	serial_out_rtu(up, UART_EFR, up->efr);
+	serial_out_rtu(up, UART_LCR, up->lcr);
+	if (up->errata & UART_ERRATA_i202_MDR1_ACCESS)
+		serial_omap_mdr1_errataset(up, up->mdr1);
+	else
+		serial_out_rtu(up, UART_OMAP_MDR1, up->mdr1);
+}
+
+static int serial_omap_runtime_suspend(struct device *dev)
+{
+	struct uart_omap_port *up = dev_get_drvdata(dev);
+	struct omap_uart_port_info *pdata = dev->platform_data;
+
+	if (!up)
+		return -EINVAL;
+
+	if (!pdata)
+		return 0;
+
+	up->context_loss_cnt = serial_omap_get_context_loss_count(up);
+
+	if (device_may_wakeup(dev)) {
+		if (!up->wakeups_enabled) {
+			serial_omap_enable_wakeup(up, true);
+			up->wakeups_enabled = true;
+		}
+	} else {
+		if (up->wakeups_enabled) {
+			serial_omap_enable_wakeup(up, false);
+			up->wakeups_enabled = false;
+		}
+	}
+
+	up->latency = PM_QOS_CPU_LATENCY_DEFAULT_VALUE;
+	schedule_work(&up->qos_work);
+
+	return 0;
+}
+
+static int serial_omap_runtime_resume(struct device *dev)
+{
+	struct uart_omap_port *up = dev_get_drvdata(dev);
+
+	int loss_cnt = serial_omap_get_context_loss_count(up);
+
+	if (loss_cnt < 0) {
+		dev_err(dev, "serial_omap_get_context_loss_count failed : %d\n",
+			loss_cnt);
+		serial_omap_restore_context(up);
+	} else if (up->context_loss_cnt != loss_cnt) {
+		serial_omap_restore_context(up);
+	}
+	up->latency = up->calc_latency;
+	schedule_work(&up->qos_work);
+
+	return 0;
+}
+#endif
+
+static const struct dev_pm_ops serial_omap_dev_pm_ops = {
+	SET_SYSTEM_SLEEP_PM_OPS(serial_omap_suspend, serial_omap_resume)
+	SET_RUNTIME_PM_OPS(serial_omap_runtime_suspend,
+				serial_omap_runtime_resume, NULL)
+};
+
+#if defined(CONFIG_OF)
+static const struct of_device_id omap_serial_of_match[] = {
+	{ .compatible = "ti,omap2-uart-rtu" },
+	{ .compatible = "ti,omap3-uart-rtu" },
+	{ .compatible = "ti,omap4-uart-rtu" },
+	{},
+};
+MODULE_DEVICE_TABLE(of, omap_serial_of_match);
+#endif
+
+static struct platform_driver serial_omap_driver = {
+	.probe          = serial_omap_probe,
+	.remove         = serial_omap_remove,
+	.driver		= {
+		.name	= DRIVER_NAME,
+		.pm	= &serial_omap_dev_pm_ops,
+		.of_match_table = of_match_ptr(omap_serial_of_match),
+	},
+};
+
+static int __init serial_omap_init(void)
+{
+	int ret;
+
+	ret = uart_register_driver(&serial_omap_reg);
+	if (ret != 0)
+		return ret;
+	ret = platform_driver_register(&serial_omap_driver);
+	if (ret != 0)
+		uart_unregister_driver(&serial_omap_reg);
+	return ret;
+}
+
+static void __exit serial_omap_exit(void)
+{
+	platform_driver_unregister(&serial_omap_driver);
+	uart_unregister_driver(&serial_omap_reg);
+}
+
+module_init(serial_omap_init);
+module_exit(serial_omap_exit);
+
+MODULE_DESCRIPTION("OMAP High Speed UART driver");
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Texas Instruments Inc");
diff --git a/drivers/tty/serial/omap-serial-rtu.h b/drivers/tty/serial/omap-serial-rtu.h
new file mode 100644
index 000000000000..fc690d4ce60d
--- /dev/null
+++ b/drivers/tty/serial/omap-serial-rtu.h
@@ -0,0 +1,222 @@
+/*
+ * Driver for OMAP-UART controller.
+ * Based on drivers/serial/8250.c
+ *
+ * Copyright (C) 2010 Texas Instruments.
+ *
+ * Authors:
+ *	Govindraj R	<govindraj.raja@ti.com>
+ *	Thara Gopinath	<thara@ti.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ */
+
+#ifndef __OMAP_SERIAL_H__
+#define __OMAP_SERIAL_H__
+
+#include <linux/serial_core.h>
+#include <linux/platform_device.h>
+#include <linux/pm_qos.h>
+#include <linux/kfifo.h>
+
+//#include <plat/mux.h>
+//#include <plat/pfc-modbus-rtu.h>
+
+#define DRIVER_NAME	"omap_uart_rtu"
+
+/*
+ * Use tty device name as ttyO, [O -> OMAP]
+ * in bootargs we specify as console=ttyO0 if uart1
+ * is used as console uart.
+ */
+#define OMAP_SERIAL_NAME	"ttyO"
+
+#define OMAP_MODE13X_SPEED	230400
+
+/* WER = 0x7F
+ * Enable module level wakeup in WER reg
+ */
+#define OMAP_UART_WER_MOD_WKUP	0X7F
+
+#define OMAP_UART_SYSC_RESET	0X07
+#define OMAP_UART_FIFO_CLR	0X06
+
+#define OMAP_UART_DMA_CH_FREE	-1
+
+#define MSR_SAVE_FLAGS		UART_MSR_ANY_DELTA
+
+#define UART_ERRATA_i202_MDR1_ACCESS	BIT(0)
+#define UART_ERRATA_i291_DMA_FORCEIDLE	BIT(1)
+
+#define OMAP_MAX_HSUART_PORTS	6
+
+#define UART_BUILD_REVISION(x, y)	(((x) << 8) | (y))
+
+#define OMAP_UART_REV_42 0x0402
+#define OMAP_UART_REV_46 0x0406
+#define OMAP_UART_REV_52 0x0502
+#define OMAP_UART_REV_63 0x0603
+
+#define UART_ERRATA_i202_MDR1_ACCESS	BIT(0)
+#define UART_ERRATA_i291_DMA_FORCEIDLE	BIT(1)
+
+#define DEFAULT_CLK_SPEED 48000000 /* 48Mhz*/
+
+/* SCR register bitmasks */
+#define OMAP_UART_SCR_RX_TRIG_GRANU1_MASK		(1 << 7)
+#define OMAP_UART_SCR_TX_TRIG_GRANU1_MASK		(1 << 6)
+#define OMAP_UART_SCR_TX_EMPTY			(1 << 3)
+
+/* FCR register bitmasks */
+#define OMAP_UART_FCR_RX_FIFO_TRIG_MASK			(0x3 << 6)
+#define OMAP_UART_FCR_TX_FIFO_TRIG_MASK			(0x3 << 4)
+
+/* MVR register bitmasks */
+#define OMAP_UART_MVR_SCHEME_SHIFT	30
+
+#define OMAP_UART_LEGACY_MVR_MAJ_MASK	0xf0
+#define OMAP_UART_LEGACY_MVR_MAJ_SHIFT	4
+#define OMAP_UART_LEGACY_MVR_MIN_MASK	0x0f
+
+#define OMAP_UART_MVR_MAJ_MASK		0x700
+#define OMAP_UART_MVR_MAJ_SHIFT		8
+#define OMAP_UART_MVR_MIN_MASK		0x3f
+
+#define OMAP_UART_DMA_CH_FREE	-1
+
+#define MSR_SAVE_FLAGS		UART_MSR_ANY_DELTA
+#define OMAP_MODE13X_SPEED	230400
+
+/* WER = 0x7F
+ * Enable module level wakeup in WER reg
+ */
+#define OMAP_UART_WER_MOD_WKUP	0X7F
+
+/* Enable XON/XOFF flow control on output */
+#define OMAP_UART_SW_TX		0x08
+
+/* Enable XON/XOFF flow control on input */
+#define OMAP_UART_SW_RX		0x02
+
+#define OMAP_UART_SW_CLR	0xF0
+
+#define OMAP_UART_TCR_TRIG	0x0F
+
+
+struct omap_uart_port_info {
+	bool			dma_enabled;	/* To specify DMA Mode */
+	unsigned int		uartclk;	/* UART clock rate */
+	upf_t			flags;		/* UPF_* flags */
+	unsigned int		dma_rx_buf_size;
+	unsigned int		dma_rx_timeout;
+	unsigned int		autosuspend_timeout;
+	unsigned int		dma_rx_poll_rate;
+	int			DTR_gpio;
+	int			DTR_inverted;
+	int			DTR_present;
+
+  //	int                     disable_port;
+
+	int (*get_context_loss_count)(struct device *);
+	void (*set_forceidle)(struct device *);
+	void (*set_noidle)(struct device *);
+	void (*enable_wakeup)(struct device *, bool);
+};
+
+struct uart_omap_dma {
+	u8			uart_dma_tx;
+	u8			uart_dma_rx;
+	int			rx_dma_channel;
+	int			tx_dma_channel;
+	dma_addr_t		rx_buf_dma_phys;
+	dma_addr_t		tx_buf_dma_phys;
+	unsigned int		uart_base;
+	/*
+	 * Buffer for rx dma.It is not required for tx because the buffer
+	 * comes from port structure.
+	 */
+	unsigned char		*rx_buf;
+	unsigned int		prev_rx_dma_pos;
+	int			tx_buf_size;
+	int			tx_dma_used;
+	int			rx_dma_used;
+	spinlock_t		tx_lock;
+	spinlock_t		rx_lock;
+	/* timer to poll activity on rx dma */
+	struct timer_list	rx_timer;
+	unsigned int		rx_buf_size;
+	unsigned int		rx_poll_rate;
+	unsigned int		rx_timeout;
+};
+
+struct uart_omap_port {
+	struct uart_port	port;
+	struct uart_omap_dma	uart_dma;
+	struct device		*dev;
+
+#ifdef CONFIG_SERIAL_OMAP_MODBUS
+	struct modbus_port	mb_port;
+#endif
+
+	unsigned char		ier;
+	unsigned char		lcr;
+	unsigned char		mcr;
+	unsigned char		fcr;
+	unsigned char		efr;
+	unsigned char		dll;
+	unsigned char		dlh;
+	unsigned char		mdr1;
+	unsigned char		scr;
+
+	int			use_dma;
+	/*
+	 * Some bits in registers are cleared on a read, so they must
+	 * be saved whenever the register is read but the bits will not
+	 * be immediately processed.
+	 */
+	unsigned int		lsr_break_flag;
+	unsigned char		msr_saved_flags;
+	char			name[20];
+	unsigned long		port_activity;
+
+	struct serial_rs485     rs485;
+	unsigned int            tx_in_progress:1, tx_wait_end:1;
+	int			rts_gpio;
+	int 			rs485en_gpio;
+	u8 			rs485en_alow;
+
+	int			context_loss_cnt;
+	u32			errata;
+	u8			wakeups_enabled;
+
+	int			DTR_gpio;
+	int			DTR_inverted;
+	int			DTR_active;
+
+	struct pm_qos_request pm_qos_request;
+	u32			latency;
+	u32			calc_latency;
+	struct work_struct	qos_work;
+	struct pinctrl		*pins;
+
+       struct tasklet_struct   tx_tsklt;
+       struct tasklet_struct   rx_tsklt;
+       struct tasklet_struct   mdm_status_tsklt;
+       DECLARE_KFIFO_PTR(rx_fifo, uint16_t);
+};
+
+unsigned int serial_in(struct uart_omap_port *up, int offset);
+void serial_out(struct uart_omap_port *up, int offset, int value);
+void transmit_chars(struct uart_omap_port *up, unsigned int lsr);
+irqreturn_t serial_omap_irq(int irq, void *dev_id);
+void serial_omap_stop_tx(struct uart_port *port);
+void serial_omap_start_tx(struct uart_port *port);
+void serial_omap_disable_ier_thri(struct uart_omap_port *up);
+int serial_omap_end_tx(struct uart_omap_port *up);
+
+#define CONFIG_SERIAL_OMAP_RS485
+
+#endif /* __OMAP_SERIAL_H__ */
diff --git a/drivers/tty/serial/omap-serial.c b/drivers/tty/serial/omap-serial.c
index 76b94d0ff586..80371598efea 100644
--- a/drivers/tty/serial/omap-serial.c
+++ b/drivers/tty/serial/omap-serial.c
@@ -1301,13 +1301,10 @@ serial_omap_console_write(struct console *co, const char *s,
 
 	pm_runtime_get_sync(up->dev);
 
-	local_irq_save(flags);
-	if (up->port.sysrq)
-		locked = 0;
-	else if (oops_in_progress)
-		locked = spin_trylock(&up->port.lock);
+	if (up->port.sysrq || oops_in_progress)
+		locked = spin_trylock_irqsave(&up->port.lock, flags);
 	else
-		spin_lock(&up->port.lock);
+		spin_lock_irqsave(&up->port.lock, flags);
 
 	/*
 	 * First save the IER then disable the interrupts
@@ -1336,8 +1333,7 @@ serial_omap_console_write(struct console *co, const char *s,
 	pm_runtime_mark_last_busy(up->dev);
 	pm_runtime_put_autosuspend(up->dev);
 	if (locked)
-		spin_unlock(&up->port.lock);
-	local_irq_restore(flags);
+		spin_unlock_irqrestore(&up->port.lock, flags);
 }
 
 static int __init
diff --git a/drivers/tty/serial/pfc-modbus-rtu.c b/drivers/tty/serial/pfc-modbus-rtu.c
new file mode 100644
index 000000000000..5086944da9e9
--- /dev/null
+++ b/drivers/tty/serial/pfc-modbus-rtu.c
@@ -0,0 +1,483 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+
+/*
+ * WAGO Serial Modbus RTU Driver
+ *
+ * Copyright (C) 2013 Wago Kontakttechnik GmbH
+ *
+ * Author: Timur Celik <timur.celik@wago.com>
+ *
+ */
+
+#undef DEBUG
+
+#ifdef CONFIG_SERIAL_OMAP_MODBUS
+
+//#include <linux/platform_data/serial-omap.h>
+#include <linux/module.h>
+#include <linux/platform_data/pfc-modbus-rtu.h>
+#include "omap-serial-rtu.h"
+
+#include <uapi/linux/serial_reg.h>
+#include <linux/pm_runtime.h>
+#include <linux/tty_flip.h>
+#include <linux/time64.h>
+#include <asm/ioctls.h>
+
+static struct kfifo valid_frame;
+
+extern unsigned int serial_in_rtu(struct uart_omap_port *up, int offset);
+extern void serial_out_rtu(struct uart_omap_port *up, int offset, int value);
+
+extern void transmit_chars_rtu(struct uart_omap_port *up, unsigned int lsr);
+extern irqreturn_t serial_omap_irq_rtu(int irq, void *dev_id);
+
+/* extern void serial_omap_enable_ier_thri(struct uart_omap_port *up); */
+/* extern void serial_omap_disable_ier_thri(struct uart_omap_port *up); */
+
+static irqreturn_t modbus_omap_irq(int irq, void *dev_id)
+{
+	struct uart_omap_port *up = dev_id;
+	unsigned int iir, lsr;
+	unsigned int type;
+	irqreturn_t ret = IRQ_NONE;
+	int max_count = 256;
+
+	raw_spin_lock(&up->port.lock);
+
+	do {
+		iir = serial_in_rtu(up, UART_IIR);
+		if (iir & UART_IIR_NO_INT)
+			break;
+
+		ret = IRQ_HANDLED;
+		lsr = serial_in_rtu(up, UART_LSR);
+
+		/* extract IRQ type from IIR register */
+		type = iir & 0x3e;
+
+		switch (type) {
+		case UART_IIR_MSI:
+			up->mb_port.modem_status = serial_in_rtu(up, UART_MSR);
+			tasklet_hi_schedule(&up->mb_port.mdr_tsklt);
+			break;
+		case UART_IIR_THRI:
+			serial_omap_disable_ier_thri(up);
+			if(!serial_omap_end_tx(up)) {
+				tasklet_hi_schedule(&up->mb_port.tx_tsklt);
+			}
+			break;
+		case UART_IIR_RLSI:
+			modbus_omap_rlsi_handler(up, lsr);
+			break;
+		case UART_IIR_RX_TIMEOUT:
+			/* FALLTHROUGH */
+		case UART_IIR_RDI:
+			modbus_omap_rhr_handler(up);
+			break;
+		case UART_IIR_CTS_RTS_DSR:
+			/* simply try again */
+			break;
+		case UART_IIR_XOFF:
+			/* FALLTHROUGH */
+		default:
+			break;
+		}
+	} while (!(iir & UART_IIR_NO_INT) && max_count--);
+
+	raw_spin_unlock(&up->port.lock);
+
+	up->port_activity = jiffies;
+
+	return ret;
+}
+
+static void modbus_check_modem_status(unsigned long dev_id)
+{
+	struct uart_omap_port *up = (void*) dev_id;
+	unsigned int status;
+
+	status = up->mb_port.modem_status;
+	status |= up->msr_saved_flags;
+	up->msr_saved_flags = 0;
+	if ((status & UART_MSR_ANY_DELTA) == 0)
+		return;
+
+	if (status & UART_MSR_ANY_DELTA && up->ier & UART_IER_MSI &&
+	    up->port.state != NULL) {
+		if (status & UART_MSR_TERI)
+			up->port.icount.rng++;
+		if (status & UART_MSR_DDSR)
+			up->port.icount.dsr++;
+		if (status & UART_MSR_DDCD)
+			uart_handle_dcd_change
+				(&up->port, status & UART_MSR_DCD);
+		if (status & UART_MSR_DCTS)
+			uart_handle_cts_change
+				(&up->port, status & UART_MSR_CTS);
+		wake_up_interruptible(&up->port.state->port.delta_msr_wait);
+	}
+}
+
+static void modbus_omap_transmit(unsigned long dev_id)
+{
+	struct uart_omap_port *up = (void*) dev_id;
+
+	spin_lock(&up->port.lock);
+	transmit_chars_rtu(up, serial_in_rtu(up, UART_LSR));
+	spin_unlock(&up->port.lock);
+}
+
+void modbus_omap_process_frame(struct uart_omap_port *up)
+{
+	unsigned char ch, is_valid;
+	unsigned int cnt;
+	int frame_len = 0;
+
+	spin_lock(&up->port.lock);
+start:
+	is_valid = true;
+	if(!kfifo_get(&up->mb_port.length, &frame_len)) {
+		goto exit;
+	}
+	while((frame_len > 0) && (kfifo_get(&up->mb_port.frame, &ch))) {
+		frame_len--;
+		up->port.icount.rx++;
+		if(!kfifo_put(&valid_frame, ch)) {
+			dev_dbg(up->dev, "Buffer overrun\n");
+			is_valid = false;
+		}
+	}
+
+	atomic_dec(&up->mb_port.frame_cnt);
+
+	if(!is_valid)
+		goto exit;
+
+	cnt = kfifo_len(&valid_frame);
+	if(up->mb_port.timestamp_enabled) {
+		cnt -= MB_TIMESTAMP_LENGTH;
+	}
+	if(cnt<MB_FRAME_MIN || cnt>MB_FRAME_MAX) {
+		dev_dbg(up->dev, "Frame length invalid\n");
+		up->port.icount.frame++;
+		goto exit;
+	}
+
+	dev_dbg(up->dev, "Frame of %u bytes received\n", cnt);
+
+	if(kfifo_peek(&valid_frame, &ch) &&
+	   ch != up->mb_port.slave_id && up->mb_port.slave_id && ch) {
+		dev_dbg(up->dev, "Dropping frame with slave id=%d\n", ch);
+		goto exit;
+	}
+
+	// push frame to the uart layer
+	while(kfifo_get(&valid_frame, &ch))
+		uart_insert_char(&up->port, 0, UART_LSR_OE, ch, TTY_NORMAL);
+
+exit:
+	kfifo_reset(&valid_frame);
+	if(atomic_read(&up->mb_port.frame_cnt) > 0) {
+		dev_dbg(up->dev, "Processing another frame\n");
+		goto start;
+	}
+
+	spin_unlock(&up->port.lock);
+}
+
+static void modbus_omap_finish_frame(unsigned long dev_id)
+{
+	struct uart_omap_port *up = (void*) dev_id;
+
+	pm_runtime_get_sync(up->dev);
+
+	modbus_omap_process_frame(up);
+
+	if(up->port.state->port.tty)
+		tty_flip_buffer_push(&up->port.state->port);
+
+	pm_runtime_mark_last_busy(up->dev);
+	pm_runtime_put_autosuspend(up->dev);
+}
+
+enum hrtimer_restart modbus_omap_to_handler(struct hrtimer* hrt)
+{
+	enum hrtimer_restart ret = HRTIMER_NORESTART;
+	struct modbus_port *mp = container_of(hrt, struct modbus_port, timer);
+	struct uart_omap_port *up =
+			container_of(mp, struct uart_omap_port, mb_port);
+	int frame_len = 0;
+	u64 to_35 = mp->to_35_override ? mp->to_35_override : mp->to_35;
+	u64 to_15 = mp->to_15_override ? mp->to_15_override : mp->to_15;
+
+	switch(mp->state) {
+		case ERR:
+			dev_dbg(up->dev, "Timeout at 3.5 char\n");
+
+			if(up->mb_port.timestamp_enabled) {
+				struct timespec64 ts;
+				ktime_get_ts64(&ts);
+				// push timestamp after frame
+				kfifo_put(&up->mb_port.frame, (ts.tv_sec >> 24) & 0xFF);
+				kfifo_put(&up->mb_port.frame, (ts.tv_sec >> 16) & 0xFF);
+				kfifo_put(&up->mb_port.frame, (ts.tv_sec >> 8) & 0xFF);
+				kfifo_put(&up->mb_port.frame, ts.tv_sec & 0xFF);
+				kfifo_put(&up->mb_port.frame, (ts.tv_nsec >> 24) & 0xFF);
+				kfifo_put(&up->mb_port.frame, (ts.tv_nsec >> 16) & 0xFF);
+				kfifo_put(&up->mb_port.frame, (ts.tv_nsec >> 8) & 0xFF);
+				kfifo_put(&up->mb_port.frame, ts.tv_nsec & 0xFF);
+				frame_len = MB_TIMESTAMP_LENGTH;
+			}
+			// push frame length including 8 bytes for timestamp if activated
+			frame_len += atomic_read(&up->mb_port.ch_cnt);
+			kfifo_put(&up->mb_port.length, frame_len);
+			atomic_set(&up->mb_port.ch_cnt, 0);
+			atomic_inc(&up->mb_port.frame_cnt);
+			tasklet_hi_schedule(&up->mb_port.rx_tsklt);
+
+			mp->state = IF;
+			break;
+		case IC:
+			mp->state = ERR;
+			hrtimer_forward(hrt, hrtimer_get_expires(hrt),
+			                ns_to_ktime(to_35-to_15));
+			ret = HRTIMER_RESTART;
+			dev_dbg(up->dev, "Timeout at 1.5 char\n");
+			break;
+		case IF: break;
+		default: break;
+	}
+
+	return ret;
+}
+
+int modbus_omap_rhr_handler(struct uart_omap_port *up)
+{
+	unsigned int ch;
+	int err = false;
+	struct modbus_port *mp = &up->mb_port;
+	u64 to_15 = mp->to_15_override ? mp->to_15_override : mp->to_15;
+
+        hrtimer_start(&mp->timer, ns_to_ktime(to_15), HRTIMER_MODE_REL_HARD);
+
+	ch = serial_in_rtu(up, UART_RX);
+
+	if(kfifo_put(&up->mb_port.frame, ch) != 0)
+		atomic_inc(&up->mb_port.ch_cnt);
+
+	switch(mp->state) {
+		case ERR:
+			dev_dbg(up->dev, "Protocol timing incorrect\n");
+			atomic_set(&up->mb_port.ch_cnt, 0);
+			kfifo_reset(&up->mb_port.frame);
+			up->port.icount.frame++;
+			mp->state = IC;
+			err = true;
+			break;
+		case IC: break;
+		case IF:
+			mp->state = IC;
+			break;
+		default: break;
+	}
+
+	return err;
+};
+
+int modbus_omap_rlsi_handler(struct uart_omap_port *up, unsigned int lsr)
+{
+	int err = false;
+	struct modbus_port *mp = &up->mb_port;
+	u64 to_15 = mp->to_15_override ? mp->to_15_override : mp->to_15;
+
+	if((lsr & (UART_LSR_BI | UART_LSR_PE | UART_LSR_FE | UART_LSR_OE)) == 0)
+		return modbus_omap_rhr_handler(up);
+
+	hrtimer_start(&mp->timer, ns_to_ktime(to_15), HRTIMER_MODE_REL);
+
+	dev_dbg(up->dev, "Line status error\n");
+	serial_in_rtu(up, UART_RX);
+	// TODO: kfifo_reset(&mp->frame);
+
+	raw_spin_lock(&up->port.lock);
+	if (lsr & UART_LSR_BI)
+		up->port.icount.brk++;
+
+	if (lsr & UART_LSR_PE)
+		up->port.icount.parity++;
+
+	if (lsr & UART_LSR_FE)
+		up->port.icount.frame++;
+
+	if (lsr & UART_LSR_OE)
+		up->port.icount.overrun++;
+	raw_spin_unlock(&up->port.lock);
+
+	return err;
+};
+
+void modbus_omap_calc_to(struct uart_omap_port *up, unsigned int baud)
+{
+	struct modbus_port *mp = &up->mb_port;
+
+	u8 char_bits = 11; /* TODO: get the actual value from the uart */
+	u64 char_duration = USEC_PER_SEC * char_bits / baud;
+	char_duration *= 1000; /* workaround for bug in gcc */
+
+	/*
+	 * When calculating the timeouts, always add one char_duration. The IRQ
+	 * measures the time between rx_end and rx_end, but the time from rx_end
+	 * to rx_start is needed.
+	 */
+	if(baud <= 19000) {
+		mp->to_15 = char_duration * 5/2;
+		mp->to_35 = char_duration * 9/2;
+	} else {
+		mp->to_15 = char_duration + 750000UL;
+		mp->to_35 = char_duration + 1750000UL;
+	}
+
+	dev_info(up->dev, "Baudrate = %u, TO_15 = %lluns, TO_35 = %lluns\n",
+	         baud, mp->to_15, mp->to_35);
+};
+
+void modbus_omap_timestamp_enable(struct uart_omap_port *up)
+{
+	dev_info(up->dev, "Enabling Modbus timestamp\n");
+	up->mb_port.timestamp_enabled = true;
+}
+
+void modbus_omap_timestamp_disable(struct uart_omap_port *up)
+{
+	dev_info(up->dev, "Disabling Modbus timestamp\n");
+	up->mb_port.timestamp_enabled = false;
+}
+
+int modbus_omap_enable(struct uart_omap_port *up)
+{
+	int err;
+	u64 to_35;
+	struct modbus_port *mp = &up->mb_port;
+	struct hrtimer* hrt = &mp->timer;
+
+	if(mp->activated) {
+		err = modbus_omap_disable(up);
+		if(err) return err;
+	}
+
+	dev_info(up->dev, "Enabling Modbus mode\n");
+
+	atomic_set(&mp->ch_cnt, 0);
+	atomic_set(&mp->frame_cnt, 0);
+
+	err = kfifo_alloc(&mp->frame, 640, 0);
+	if(err) {
+		dev_err(up->dev, "Unable to allocate memory (%d)\n", err);
+		return err;
+	}
+	err = kfifo_alloc(&mp->length, 64, 0);
+	if(err) {
+		dev_err(up->dev, "Unable to allocate memory (%d)\n", err);
+		return err;
+	}
+	err = kfifo_alloc(&valid_frame, 512, 0);
+	if(err) {
+		dev_err(up->dev, "Unable to allocate memory (%d)\n", err);
+		return err;
+	}
+
+	hrtimer_init(hrt, CLOCK_MONOTONIC, HRTIMER_MODE_REL_HARD);
+	hrt->function = modbus_omap_to_handler;
+
+	if(!hrtimer_is_hres_active(hrt))
+		dev_warn(up->dev, "No hi-res timer available\n");
+
+	/* init a timer which measures the time in between two chars */
+	to_35 = mp->to_35_override ? mp->to_35_override : mp->to_35;
+	hrtimer_start(hrt, ns_to_ktime(to_35), HRTIMER_MODE_REL_HARD);
+
+	/* create a tasklet for stuff that wants to sleep */
+	tasklet_init(&mp->rx_tsklt, modbus_omap_finish_frame, (unsigned long)up);
+	tasklet_init(&mp->tx_tsklt, modbus_omap_transmit, (unsigned long)up);
+	tasklet_init(&mp->mdr_tsklt, modbus_check_modem_status, (unsigned long)up);
+
+	/* set the new ISR to not threaded */
+	free_irq(up->port.irq, up);
+	err = request_irq(up->port.irq, modbus_omap_irq, IRQF_NO_THREAD,
+	                  up->name, up);
+	if(err) {
+		dev_err(up->dev, "irq-%d could not claim: err %d\n",
+		        up->port.irq, err);
+		return -1;
+	}
+
+	mp->activated = true;
+
+	return 0;
+};
+
+int modbus_omap_disable(struct uart_omap_port *up)
+{
+	int err;
+	struct modbus_port *mp = &up->mb_port;
+
+	if(!mp->activated)
+		return -1;
+
+	dev_info(up->dev, "Disabling Modbus mode\n");
+
+	tasklet_kill(&mp->rx_tsklt);
+	tasklet_kill(&mp->tx_tsklt);
+	tasklet_kill(&mp->mdr_tsklt);
+
+	hrtimer_cancel(&mp->timer);
+
+	/* restore old isr */
+	free_irq(up->port.irq, up);
+	err = request_irq(up->port.irq, serial_omap_irq_rtu, up->port.irqflags | IRQF_NO_THREAD,
+			  up->name, up);
+	if(err) {
+		dev_err(up->dev, "irq-%d could not claim: err %d\n",
+		        up->port.irq, err);
+		return -1;
+	}
+
+	kfifo_free(&mp->frame);
+	kfifo_free(&mp->length);
+	kfifo_free(&valid_frame);
+
+	mp->activated = false;
+
+	return 0;
+};
+
+int modbus_omap_startup(struct uart_omap_port *up)
+{
+	struct modbus_port *mp = &up->mb_port;
+	mp->activated = false;
+	mp->state = ERR;
+	mp->to_15 = ULLONG_MAX;
+	mp->to_35 = ULLONG_MAX;
+	mp->to_15_override = 0;
+	mp->to_35_override = 0;
+	mp->slave_id = 0;
+	mp->modem_status = 0;
+
+	dev_info(up->dev, "Initializing Modbus driver\n");
+	dev_dbg(up->dev,
+	        "IOCTLs are TIOCSMBRTU (%u), TIOCSMBRTUADDR (%u), "
+	        "TIOCSMBRTUIFT (%u) and TIOCSMBRTUICT (%u)\n",
+	        TIOCSMBRTU, TIOCSMBRTUADDR, TIOCSMBRTUIFT, TIOCSMBRTUICT);
+
+	return 0;
+};
+
+void modbus_omap_shutdown(struct uart_omap_port *up)
+{
+};
+
+MODULE_AUTHOR("Timur Celik <timur.celik@wago.com>");
+MODULE_DESCRIPTION("KSZ8863 Switch Reset Driver");
+MODULE_LICENSE("GPL v2");
+#endif
diff --git a/drivers/tty/serial/serial_core.c b/drivers/tty/serial/serial_core.c
index 828f9ad1be49..3efec96fb9aa 100644
--- a/drivers/tty/serial/serial_core.c
+++ b/drivers/tty/serial/serial_core.c
@@ -1282,47 +1282,6 @@ static int uart_get_icount(struct tty_struct *tty,
 	return 0;
 }
 
-static int uart_get_rs485_config(struct uart_port *port,
-			 struct serial_rs485 __user *rs485)
-{
-	unsigned long flags;
-	struct serial_rs485 aux;
-
-	spin_lock_irqsave(&port->lock, flags);
-	aux = port->rs485;
-	spin_unlock_irqrestore(&port->lock, flags);
-
-	if (copy_to_user(rs485, &aux, sizeof(aux)))
-		return -EFAULT;
-
-	return 0;
-}
-
-static int uart_set_rs485_config(struct uart_port *port,
-			 struct serial_rs485 __user *rs485_user)
-{
-	struct serial_rs485 rs485;
-	int ret;
-	unsigned long flags;
-
-	if (!port->rs485_config)
-		return -ENOIOCTLCMD;
-
-	if (copy_from_user(&rs485, rs485_user, sizeof(*rs485_user)))
-		return -EFAULT;
-
-	spin_lock_irqsave(&port->lock, flags);
-	ret = port->rs485_config(port, &rs485);
-	spin_unlock_irqrestore(&port->lock, flags);
-	if (ret)
-		return ret;
-
-	if (copy_to_user(rs485_user, &port->rs485, sizeof(port->rs485)))
-		return -EFAULT;
-
-	return 0;
-}
-
 static int uart_get_iso7816_config(struct uart_port *port,
 				   struct serial_iso7816 __user *iso7816)
 {
@@ -1437,14 +1396,6 @@ uart_ioctl(struct tty_struct *tty, unsigned int cmd, unsigned long arg)
 		ret = uart_get_lsr_info(tty, state, uarg);
 		break;
 
-	case TIOCGRS485:
-		ret = uart_get_rs485_config(uport, uarg);
-		break;
-
-	case TIOCSRS485:
-		ret = uart_set_rs485_config(uport, uarg);
-		break;
-
 	case TIOCSISO7816:
 		ret = uart_set_iso7816_config(state->uart_port, uarg);
 		break;
diff --git a/drivers/tty/serial/serial_reg_rtu.h b/drivers/tty/serial/serial_reg_rtu.h
new file mode 100644
index 000000000000..3e6ca531acd5
--- /dev/null
+++ b/drivers/tty/serial/serial_reg_rtu.h
@@ -0,0 +1,390 @@
+/*
+ * include/linux/serial_reg.h
+ *
+ * Copyright (C) 1992, 1994 by Theodore Ts'o.
+ * 
+ * Redistribution of this file is permitted under the terms of the GNU 
+ * Public License (GPL)
+ * 
+ * These are the UART port assignments, expressed as offsets from the base
+ * register.  These assignments should hold for any serial port based on
+ * a 8250, 16450, or 16550(A).
+ */
+
+#ifndef _LINUX_SERIAL_REG_H
+#define _LINUX_SERIAL_REG_H
+
+/*
+ * DLAB=0
+ */
+#define UART_RX		0	/* In:  Receive buffer */
+#define UART_TX		0	/* Out: Transmit buffer */
+
+#define UART_IER	1	/* Out: Interrupt Enable Register */
+#define UART_IER_MSI		0x08 /* Enable Modem status interrupt */
+#define UART_IER_RLSI		0x04 /* Enable receiver line status interrupt */
+#define UART_IER_THRI		0x02 /* Enable Transmitter holding register int. */
+#define UART_IER_RDI		0x01 /* Enable receiver data interrupt */
+/*
+ * Sleep mode for ST16650 and TI16750.  For the ST16650, EFR[4]=1
+ */
+#define UART_IERX_SLEEP		0x10 /* Enable sleep mode */
+
+#define UART_IIR	2	/* In:  Interrupt ID Register */
+#define UART_IIR_NO_INT		0x01 /* No interrupts pending */
+#define UART_IIR_ID		0x06 /* Mask for the interrupt ID */
+#define UART_IIR_MSI		0x00 /* Modem status interrupt */
+#define UART_IIR_THRI		0x02 /* Transmitter holding register empty */
+#define UART_IIR_RDI		0x04 /* Receiver data interrupt */
+#define UART_IIR_RLSI		0x06 /* Receiver line status interrupt */
+
+#define UART_IIR_BUSY		0x07 /* DesignWare APB Busy Detect */
+#define UART_IIR_RX_TIMEOUT	0x0c /* OMAP RX Timeout interrupt */
+#define UART_IIR_XOFF		0x10 /* OMAP XOFF/Special Character */
+#define UART_IIR_CTS_RTS_DSR	0x20 /* OMAP CTS/RTS/DSR Change */
+
+#define UART_FCR	2	/* Out: FIFO Control Register */
+#define UART_FCR_ENABLE_FIFO	0x01 /* Enable the FIFO */
+#define UART_FCR_CLEAR_RCVR	0x02 /* Clear the RCVR FIFO */
+#define UART_FCR_CLEAR_XMIT	0x04 /* Clear the XMIT FIFO */
+#define UART_FCR_DMA_SELECT	0x08 /* For DMA applications */
+/*
+ * Note: The FIFO trigger levels are chip specific:
+ *	RX:76 = 00  01  10  11	TX:54 = 00  01  10  11
+ * PC16550D:	 1   4   8  14		xx  xx  xx  xx
+ * TI16C550A:	 1   4   8  14          xx  xx  xx  xx
+ * TI16C550C:	 1   4   8  14          xx  xx  xx  xx
+ * ST16C550:	 1   4   8  14		xx  xx  xx  xx
+ * ST16C650:	 8  16  24  28		16   8  24  30	PORT_16650V2
+ * NS16C552:	 1   4   8  14		xx  xx  xx  xx
+ * ST16C654:	 8  16  56  60		 8  16  32  56	PORT_16654
+ * TI16C750:	 1  16  32  56		xx  xx  xx  xx	PORT_16750
+ * TI16C752:	 8  16  56  60		 8  16  32  56
+ * Tegra:	 1   4   8  14		16   8   4   1	PORT_TEGRA
+ */
+#define UART_FCR_R_TRIG_00	0x00
+#define UART_FCR_R_TRIG_01	0x40
+#define UART_FCR_R_TRIG_10	0x80
+#define UART_FCR_R_TRIG_11	0xc0
+#define UART_FCR_T_TRIG_00	0x00
+#define UART_FCR_T_TRIG_01	0x10
+#define UART_FCR_T_TRIG_10	0x20
+#define UART_FCR_T_TRIG_11	0x30
+
+#define UART_FCR_TRIGGER_MASK	0xC0 /* Mask for the FIFO trigger range */
+#define UART_FCR_TRIGGER_1	0x00 /* Mask for trigger set at 1 */
+#define UART_FCR_TRIGGER_4	0x40 /* Mask for trigger set at 4 */
+#define UART_FCR_TRIGGER_8	0x80 /* Mask for trigger set at 8 */
+#define UART_FCR_TRIGGER_14	0xC0 /* Mask for trigger set at 14 */
+/* 16650 definitions */
+#define UART_FCR6_R_TRIGGER_8	0x00 /* Mask for receive trigger set at 1 */
+#define UART_FCR6_R_TRIGGER_16	0x40 /* Mask for receive trigger set at 4 */
+#define UART_FCR6_R_TRIGGER_24  0x80 /* Mask for receive trigger set at 8 */
+#define UART_FCR6_R_TRIGGER_28	0xC0 /* Mask for receive trigger set at 14 */
+#define UART_FCR6_T_TRIGGER_16	0x00 /* Mask for transmit trigger set at 16 */
+#define UART_FCR6_T_TRIGGER_8	0x10 /* Mask for transmit trigger set at 8 */
+#define UART_FCR6_T_TRIGGER_24  0x20 /* Mask for transmit trigger set at 24 */
+#define UART_FCR6_T_TRIGGER_30	0x30 /* Mask for transmit trigger set at 30 */
+#define UART_FCR7_64BYTE	0x20 /* Go into 64 byte mode (TI16C750) */
+
+#define UART_LCR	3	/* Out: Line Control Register */
+/*
+ * Note: if the word length is 5 bits (UART_LCR_WLEN5), then setting 
+ * UART_LCR_STOP will select 1.5 stop bits, not 2 stop bits.
+ */
+#define UART_LCR_DLAB		0x80 /* Divisor latch access bit */
+#define UART_LCR_SBC		0x40 /* Set break control */
+#define UART_LCR_SPAR		0x20 /* Stick parity (?) */
+#define UART_LCR_EPAR		0x10 /* Even parity select */
+#define UART_LCR_PARITY		0x08 /* Parity Enable */
+#define UART_LCR_STOP		0x04 /* Stop bits: 0=1 bit, 1=2 bits */
+#define UART_LCR_WLEN5		0x00 /* Wordlength: 5 bits */
+#define UART_LCR_WLEN6		0x01 /* Wordlength: 6 bits */
+#define UART_LCR_WLEN7		0x02 /* Wordlength: 7 bits */
+#define UART_LCR_WLEN8		0x03 /* Wordlength: 8 bits */
+
+/*
+ * Access to some registers depends on register access / configuration
+ * mode.
+ */
+#define UART_LCR_CONF_MODE_A	UART_LCR_DLAB	/* Configutation mode A */
+#define UART_LCR_CONF_MODE_B	0xBF		/* Configutation mode B */
+
+#define UART_MCR	4	/* Out: Modem Control Register */
+#define UART_MCR_CLKSEL		0x80 /* Divide clock by 4 (TI16C752, EFR[4]=1) */
+#define UART_MCR_TCRTLR		0x40 /* Access TCR/TLR (TI16C752, EFR[4]=1) */
+#define UART_MCR_XONANY		0x20 /* Enable Xon Any (TI16C752, EFR[4]=1) */
+#define UART_MCR_AFE		0x20 /* Enable auto-RTS/CTS (TI16C550C/TI16C750) */
+#define UART_MCR_LOOP		0x10 /* Enable loopback test mode */
+#define UART_MCR_OUT2		0x08 /* Out2 complement */
+#define UART_MCR_OUT1		0x04 /* Out1 complement */
+#define UART_MCR_RTS		0x02 /* RTS complement */
+#define UART_MCR_DTR		0x01 /* DTR complement */
+
+#define UART_LSR	5	/* In:  Line Status Register */
+#define UART_LSR_FIFOE		0x80 /* Fifo error */
+#define UART_LSR_TEMT		0x40 /* Transmitter empty */
+#define UART_LSR_THRE		0x20 /* Transmit-hold-register empty */
+#define UART_LSR_BI		0x10 /* Break interrupt indicator */
+#define UART_LSR_FE		0x08 /* Frame error indicator */
+#define UART_LSR_PE		0x04 /* Parity error indicator */
+#define UART_LSR_OE		0x02 /* Overrun error indicator */
+#define UART_LSR_DR		0x01 /* Receiver data ready */
+#define UART_LSR_BRK_ERROR_BITS	0x1E /* BI, FE, PE, OE bits */
+
+#define UART_MSR	6	/* In:  Modem Status Register */
+#define UART_MSR_DCD		0x80 /* Data Carrier Detect */
+#define UART_MSR_RI		0x40 /* Ring Indicator */
+#define UART_MSR_DSR		0x20 /* Data Set Ready */
+#define UART_MSR_CTS		0x10 /* Clear to Send */
+#define UART_MSR_DDCD		0x08 /* Delta DCD */
+#define UART_MSR_TERI		0x04 /* Trailing edge ring indicator */
+#define UART_MSR_DDSR		0x02 /* Delta DSR */
+#define UART_MSR_DCTS		0x01 /* Delta CTS */
+#define UART_MSR_ANY_DELTA	0x0F /* Any of the delta bits! */
+
+#define UART_SCR	7	/* I/O: Scratch Register */
+
+/*
+ * DLAB=1
+ */
+#define UART_DLL	0	/* Out: Divisor Latch Low */
+#define UART_DLM	1	/* Out: Divisor Latch High */
+
+/*
+ * LCR=0xBF (or DLAB=1 for 16C660)
+ */
+#define UART_EFR	2	/* I/O: Extended Features Register */
+#define UART_XR_EFR	9	/* I/O: Extended Features Register (XR17D15x) */
+#define UART_EFR_CTS		0x80 /* CTS flow control */
+#define UART_EFR_RTS		0x40 /* RTS flow control */
+#define UART_EFR_SCD		0x20 /* Special character detect */
+#define UART_EFR_ECB		0x10 /* Enhanced control bit */
+/*
+ * the low four bits control software flow control
+ */
+
+/*
+ * LCR=0xBF, TI16C752, ST16650, ST16650A, ST16654
+ */
+#define UART_XON1	4	/* I/O: Xon character 1 */
+#define UART_XON2	5	/* I/O: Xon character 2 */
+#define UART_XOFF1	6	/* I/O: Xoff character 1 */
+#define UART_XOFF2	7	/* I/O: Xoff character 2 */
+
+/*
+ * EFR[4]=1 MCR[6]=1, TI16C752
+ */
+#define UART_TI752_TCR	6	/* I/O: transmission control register */
+#define UART_TI752_TLR	7	/* I/O: trigger level register */
+
+/*
+ * LCR=0xBF, XR16C85x
+ */
+#define UART_TRG	0	/* FCTR bit 7 selects Rx or Tx
+				 * In: Fifo count
+				 * Out: Fifo custom trigger levels */
+/*
+ * These are the definitions for the Programmable Trigger Register
+ */
+#define UART_TRG_1		0x01
+#define UART_TRG_4		0x04
+#define UART_TRG_8		0x08
+#define UART_TRG_16		0x10
+#define UART_TRG_32		0x20
+#define UART_TRG_64		0x40
+#define UART_TRG_96		0x60
+#define UART_TRG_120		0x78
+#define UART_TRG_128		0x80
+
+#define UART_FCTR	1	/* Feature Control Register */
+#define UART_FCTR_RTS_NODELAY	0x00  /* RTS flow control delay */
+#define UART_FCTR_RTS_4DELAY	0x01
+#define UART_FCTR_RTS_6DELAY	0x02
+#define UART_FCTR_RTS_8DELAY	0x03
+#define UART_FCTR_IRDA		0x04  /* IrDa data encode select */
+#define UART_FCTR_TX_INT	0x08  /* Tx interrupt type select */
+#define UART_FCTR_TRGA		0x00  /* Tx/Rx 550 trigger table select */
+#define UART_FCTR_TRGB		0x10  /* Tx/Rx 650 trigger table select */
+#define UART_FCTR_TRGC		0x20  /* Tx/Rx 654 trigger table select */
+#define UART_FCTR_TRGD		0x30  /* Tx/Rx 850 programmable trigger select */
+#define UART_FCTR_SCR_SWAP	0x40  /* Scratch pad register swap */
+#define UART_FCTR_RX		0x00  /* Programmable trigger mode select */
+#define UART_FCTR_TX		0x80  /* Programmable trigger mode select */
+
+/*
+ * LCR=0xBF, FCTR[6]=1
+ */
+#define UART_EMSR	7	/* Extended Mode Select Register */
+#define UART_EMSR_FIFO_COUNT	0x01  /* Rx/Tx select */
+#define UART_EMSR_ALT_COUNT	0x02  /* Alternating count select */
+
+/*
+ * The Intel XScale on-chip UARTs define these bits
+ */
+#define UART_IER_DMAE	0x80	/* DMA Requests Enable */
+#define UART_IER_UUE	0x40	/* UART Unit Enable */
+#define UART_IER_NRZE	0x20	/* NRZ coding Enable */
+#define UART_IER_RTOIE	0x10	/* Receiver Time Out Interrupt Enable */
+
+#define UART_IIR_TOD	0x08	/* Character Timeout Indication Detected */
+
+#define UART_FCR_PXAR1	0x00	/* receive FIFO threshold = 1 */
+#define UART_FCR_PXAR8	0x40	/* receive FIFO threshold = 8 */
+#define UART_FCR_PXAR16	0x80	/* receive FIFO threshold = 16 */
+#define UART_FCR_PXAR32	0xc0	/* receive FIFO threshold = 32 */
+
+/*
+ * Intel MID on-chip HSU (High Speed UART) defined bits
+ */
+#define UART_FCR_HSU_64_1B	0x00	/* receive FIFO treshold = 1 */
+#define UART_FCR_HSU_64_16B	0x40	/* receive FIFO treshold = 16 */
+#define UART_FCR_HSU_64_32B	0x80	/* receive FIFO treshold = 32 */
+#define UART_FCR_HSU_64_56B	0xc0	/* receive FIFO treshold = 56 */
+
+#define UART_FCR_HSU_16_1B	0x00	/* receive FIFO treshold = 1 */
+#define UART_FCR_HSU_16_4B	0x40	/* receive FIFO treshold = 4 */
+#define UART_FCR_HSU_16_8B	0x80	/* receive FIFO treshold = 8 */
+#define UART_FCR_HSU_16_14B	0xc0	/* receive FIFO treshold = 14 */
+
+#define UART_FCR_HSU_64B_FIFO	0x20	/* chose 64 bytes FIFO */
+#define UART_FCR_HSU_16B_FIFO	0x00	/* chose 16 bytes FIFO */
+
+#define UART_FCR_HALF_EMPT_TXI	0x00	/* trigger TX_EMPT IRQ for half empty */
+#define UART_FCR_FULL_EMPT_TXI	0x08	/* trigger TX_EMPT IRQ for full empty */
+
+/*
+ * These register definitions are for the 16C950
+ */
+#define UART_ASR	0x01	/* Additional Status Register */
+#define UART_RFL	0x03	/* Receiver FIFO level */
+#define UART_TFL 	0x04	/* Transmitter FIFO level */
+#define UART_ICR	0x05	/* Index Control Register */
+
+/* The 16950 ICR registers */
+#define UART_ACR	0x00	/* Additional Control Register */
+#define UART_CPR	0x01	/* Clock Prescalar Register */
+#define UART_TCR	0x02	/* Times Clock Register */
+#define UART_CKS	0x03	/* Clock Select Register */
+#define UART_TTL	0x04	/* Transmitter Interrupt Trigger Level */
+#define UART_RTL	0x05	/* Receiver Interrupt Trigger Level */
+#define UART_FCL	0x06	/* Flow Control Level Lower */
+#define UART_FCH	0x07	/* Flow Control Level Higher */
+#define UART_ID1	0x08	/* ID #1 */
+#define UART_ID2	0x09	/* ID #2 */
+#define UART_ID3	0x0A	/* ID #3 */
+#define UART_REV	0x0B	/* Revision */
+#define UART_CSR	0x0C	/* Channel Software Reset */
+#define UART_NMR	0x0D	/* Nine-bit Mode Register */
+#define UART_CTR	0xFF
+
+/*
+ * The 16C950 Additional Control Register
+ */
+#define UART_ACR_RXDIS	0x01	/* Receiver disable */
+#define UART_ACR_TXDIS	0x02	/* Transmitter disable */
+#define UART_ACR_DSRFC	0x04	/* DSR Flow Control */
+#define UART_ACR_TLENB	0x20	/* 950 trigger levels enable */
+#define UART_ACR_ICRRD	0x40	/* ICR Read enable */
+#define UART_ACR_ASREN	0x80	/* Additional status enable */
+
+
+
+/*
+ * These definitions are for the RSA-DV II/S card, from
+ *
+ * Kiyokazu SUTO <suto@ks-and-ks.ne.jp>
+ */
+
+#define UART_RSA_BASE (-8)
+
+#define UART_RSA_MSR ((UART_RSA_BASE) + 0) /* I/O: Mode Select Register */
+
+#define UART_RSA_MSR_SWAP (1 << 0) /* Swap low/high 8 bytes in I/O port addr */
+#define UART_RSA_MSR_FIFO (1 << 2) /* Enable the external FIFO */
+#define UART_RSA_MSR_FLOW (1 << 3) /* Enable the auto RTS/CTS flow control */
+#define UART_RSA_MSR_ITYP (1 << 4) /* Level (1) / Edge triger (0) */
+
+#define UART_RSA_IER ((UART_RSA_BASE) + 1) /* I/O: Interrupt Enable Register */
+
+#define UART_RSA_IER_Rx_FIFO_H (1 << 0) /* Enable Rx FIFO half full int. */
+#define UART_RSA_IER_Tx_FIFO_H (1 << 1) /* Enable Tx FIFO half full int. */
+#define UART_RSA_IER_Tx_FIFO_E (1 << 2) /* Enable Tx FIFO empty int. */
+#define UART_RSA_IER_Rx_TOUT (1 << 3) /* Enable char receive timeout int */
+#define UART_RSA_IER_TIMER (1 << 4) /* Enable timer interrupt */
+
+#define UART_RSA_SRR ((UART_RSA_BASE) + 2) /* IN: Status Read Register */
+
+#define UART_RSA_SRR_Tx_FIFO_NEMP (1 << 0) /* Tx FIFO is not empty (1) */
+#define UART_RSA_SRR_Tx_FIFO_NHFL (1 << 1) /* Tx FIFO is not half full (1) */
+#define UART_RSA_SRR_Tx_FIFO_NFUL (1 << 2) /* Tx FIFO is not full (1) */
+#define UART_RSA_SRR_Rx_FIFO_NEMP (1 << 3) /* Rx FIFO is not empty (1) */
+#define UART_RSA_SRR_Rx_FIFO_NHFL (1 << 4) /* Rx FIFO is not half full (1) */
+#define UART_RSA_SRR_Rx_FIFO_NFUL (1 << 5) /* Rx FIFO is not full (1) */
+#define UART_RSA_SRR_Rx_TOUT (1 << 6) /* Character reception timeout occurred (1) */
+#define UART_RSA_SRR_TIMER (1 << 7) /* Timer interrupt occurred */
+
+#define UART_RSA_FRR ((UART_RSA_BASE) + 2) /* OUT: FIFO Reset Register */
+
+#define UART_RSA_TIVSR ((UART_RSA_BASE) + 3) /* I/O: Timer Interval Value Set Register */
+
+#define UART_RSA_TCR ((UART_RSA_BASE) + 4) /* OUT: Timer Control Register */
+
+#define UART_RSA_TCR_SWITCH (1 << 0) /* Timer on */
+
+/*
+ * The RSA DSV/II board has two fixed clock frequencies.  One is the
+ * standard rate, and the other is 8 times faster.
+ */
+#define SERIAL_RSA_BAUD_BASE (921600)
+#define SERIAL_RSA_BAUD_BASE_LO (SERIAL_RSA_BAUD_BASE / 8)
+
+/*
+ * Extra serial register definitions for the internal UARTs
+ * in TI OMAP processors.
+ */
+#define UART_OMAP_MDR1		0x08	/* Mode definition register */
+#define UART_OMAP_MDR2		0x09	/* Mode definition register 2 */
+#define UART_OMAP_SCR		0x10	/* Supplementary control register */
+#define UART_OMAP_SSR		0x11	/* Supplementary status register */
+#define UART_OMAP_EBLR		0x12	/* BOF length register */
+#define UART_OMAP_OSC_12M_SEL	0x13	/* OMAP1510 12MHz osc select */
+#define UART_OMAP_MVER		0x14	/* Module version register */
+#define UART_OMAP_SYSC		0x15	/* System configuration register */
+#define UART_OMAP_SYSS		0x16	/* System status register */
+#define UART_OMAP_WER		0x17	/* Wake-up enable register */
+
+/*
+ * These are the definitions for the MDR1 register
+ */
+#define UART_OMAP_MDR1_16X_MODE		0x00	/* UART 16x mode */
+#define UART_OMAP_MDR1_SIR_MODE		0x01	/* SIR mode */
+#define UART_OMAP_MDR1_16X_ABAUD_MODE	0x02	/* UART 16x auto-baud */
+#define UART_OMAP_MDR1_13X_MODE		0x03	/* UART 13x mode */
+#define UART_OMAP_MDR1_MIR_MODE		0x04	/* MIR mode */
+#define UART_OMAP_MDR1_FIR_MODE		0x05	/* FIR mode */
+#define UART_OMAP_MDR1_CIR_MODE		0x06	/* CIR mode */
+#define UART_OMAP_MDR1_DISABLE		0x07	/* Disable (default state) */
+
+/*
+ * These are definitions for the Exar XR17V35X and XR17(C|D)15X
+ */
+#define UART_EXAR_8XMODE	0x88	/* 8X sampling rate select */
+#define UART_EXAR_SLEEP		0x8b	/* Sleep mode */
+#define UART_EXAR_DVID		0x8d	/* Device identification */
+
+#define UART_EXAR_FCTR		0x08	/* Feature Control Register */
+#define UART_FCTR_EXAR_IRDA	0x08	/* IrDa data encode select */
+#define UART_FCTR_EXAR_485	0x10	/* Auto 485 half duplex dir ctl */
+#define UART_FCTR_EXAR_TRGA	0x00	/* FIFO trigger table A */
+#define UART_FCTR_EXAR_TRGB	0x60	/* FIFO trigger table B */
+#define UART_FCTR_EXAR_TRGC	0x80	/* FIFO trigger table C */
+#define UART_FCTR_EXAR_TRGD	0xc0	/* FIFO trigger table D programmable */
+
+#define UART_EXAR_TXTRG		0x0a	/* Tx FIFO trigger level write-only */
+#define UART_EXAR_RXTRG		0x0b	/* Rx FIFO trigger level write-only */
+
+#define UART_OMAP_SCR_TX_EMPTY_CTL_IT 0x08 /* TX Empty IRQ mode */
+
+#endif /* _LINUX_SERIAL_REG_H */
+
diff --git a/drivers/tty/tty_ioctl.c b/drivers/tty/tty_ioctl.c
index e18f318586ab..89e61808fa89 100644
--- a/drivers/tty/tty_ioctl.c
+++ b/drivers/tty/tty_ioctl.c
@@ -406,7 +406,7 @@ static int set_termios(struct tty_struct *tty, void __user *arg, int opt)
 	}
 
 	if (opt & TERMIOS_WAIT) {
-		tty_wait_until_sent(tty, 0);
+		tty_wait_until_sent(tty, 5 * HZ); /* Wait 5 seconds in jiffies */
 		if (signal_pending(current))
 			return -ERESTARTSYS;
 	}
diff --git a/drivers/uio/uio.c b/drivers/uio/uio.c
index be06f1a961c2..221c16ac8354 100644
--- a/drivers/uio/uio.c
+++ b/drivers/uio/uio.c
@@ -976,7 +976,8 @@ int __uio_register_device(struct module *owner,
 		 * freed until they are released.
 		 */
 		ret = request_irq(info->irq, uio_interrupt,
-				  info->irq_flags, info->name, idev);
+				  info->irq_flags | IRQF_THREAD_TBL_LOOKUP,
+				  info->name, idev);
 		if (ret) {
 			info->uio_dev = NULL;
 			goto err_request_irq;
diff --git a/drivers/usb/gadget/function/f_rndis.c b/drivers/usb/gadget/function/f_rndis.c
index 0739b05a0ef7..3cbffe7d07da 100644
--- a/drivers/usb/gadget/function/f_rndis.c
+++ b/drivers/usb/gadget/function/f_rndis.c
@@ -16,6 +16,7 @@
 #include <linux/module.h>
 #include <linux/device.h>
 #include <linux/etherdevice.h>
+#include <linux/usb/misc.h>
 
 #include <linux/atomic.h>
 
@@ -694,6 +695,18 @@ rndis_bind(struct usb_configuration *c, struct usb_function *f)
 	rndis_iad_descriptor.bFunctionSubClass = rndis_opts->subclass;
 	rndis_iad_descriptor.bFunctionProtocol = rndis_opts->protocol;
 
+	/*
+	 * Starting with Vista, Windows will match this Class/SubClass/Protocol
+	 * with rndiscmp.inf and load the proper driver without the need for a
+	 * custom .inf.
+	 * Ref: https://msdn.microsoft.com/library/ff538820(v=vs.85).aspx
+	 */
+	if (rndis_opts->use_ms_rndiscmp) {
+		rndis_iad_descriptor.bFunctionClass = USB_CLASS_MISC;
+		rndis_iad_descriptor.bFunctionSubClass = USB_MISC_SUBCLASS_RNDIS;
+		rndis_iad_descriptor.bFunctionProtocol = USB_MISC_RNDIS_PROTO_ENET;
+	}
+
 	/*
 	 * in drivers/usb/gadget/configfs.c:configfs_composite_bind()
 	 * configurations are bound in sequence with list_for_each_entry,
@@ -878,6 +891,40 @@ USB_ETHER_CONFIGFS_ITEM_ATTR_U8_RW(rndis, subclass);
 /* f_rndis_opts_protocol */
 USB_ETHER_CONFIGFS_ITEM_ATTR_U8_RW(rndis, protocol);
 
+static ssize_t
+rndis_opts_use_ms_rndiscmp_show(struct config_item *item, char *page)
+{
+	struct f_rndis_opts *opts = to_f_rndis_opts(item);
+	int ret;
+
+	mutex_lock(&opts->lock);
+	ret = sprintf(page, "%d\n", opts->use_ms_rndiscmp);
+	mutex_unlock(&opts->lock);
+
+	return ret;
+}
+
+static ssize_t
+rndis_opts_use_ms_rndiscmp_store(struct config_item *item, const char *page,
+				 size_t len)
+{
+	struct f_rndis_opts *opts = to_f_rndis_opts(item);
+	int ret;
+	bool use;
+
+	mutex_lock(&opts->lock);
+	ret = strtobool(page, &use);
+	if (!ret) {
+		opts->use_ms_rndiscmp = use;
+		ret = len;
+	}
+	mutex_unlock(&opts->lock);
+
+	return ret;
+}
+
+CONFIGFS_ATTR(rndis_opts_, use_ms_rndiscmp);
+
 static struct configfs_attribute *rndis_attrs[] = {
 	&rndis_opts_attr_dev_addr,
 	&rndis_opts_attr_host_addr,
@@ -886,6 +933,7 @@ static struct configfs_attribute *rndis_attrs[] = {
 	&rndis_opts_attr_class,
 	&rndis_opts_attr_subclass,
 	&rndis_opts_attr_protocol,
+	&rndis_opts_attr_use_ms_rndiscmp,
 	NULL,
 };
 
diff --git a/drivers/usb/gadget/function/u_rndis.h b/drivers/usb/gadget/function/u_rndis.h
index a8c409b2f52f..5db269110504 100644
--- a/drivers/usb/gadget/function/u_rndis.h
+++ b/drivers/usb/gadget/function/u_rndis.h
@@ -22,6 +22,7 @@ struct f_rndis_opts {
 	struct net_device		*net;
 	bool				bound;
 	bool				borrowed_net;
+	bool				use_ms_rndiscmp;
 
 	struct config_group		*rndis_interf_group;
 	struct usb_os_desc		rndis_os_desc;
diff --git a/drivers/usb/musb/am35x.c b/drivers/usb/musb/am35x.c
index 660641ab1545..a9f12b3a0397 100644
--- a/drivers/usb/musb/am35x.c
+++ b/drivers/usb/musb/am35x.c
@@ -19,6 +19,8 @@
 #include <linux/dma-mapping.h>
 #include <linux/usb/usb_phy_generic.h>
 #include <linux/platform_data/usb-omap.h>
+#include <linux/of.h>
+#include <linux/of_platform.h>
 
 #include "musb_core.h"
 
@@ -440,6 +442,40 @@ static const struct platform_device_info am35x_dev_info = {
 	.dma_mask	= DMA_BIT_MASK(32),
 };
 
+static int of_am35x_probe(struct device_node *np,
+			  struct musb_hdrc_platform_data *pdata)
+{
+	struct musb_hdrc_config *config = (struct musb_hdrc_config *) pdata->config;
+	struct omap_musb_board_data *data = (struct omap_musb_board_data *)pdata->board_data;
+
+	/* we presume that pdata was already prepared in pdata-quirks.c */
+	if (!of_property_read_u32(np, "mode", (u32 *)&pdata->mode))
+		data->mode = pdata->mode;
+	else
+		goto out_inval;
+
+	if (of_property_read_u32(np, "interface-type",
+				(u32 *)&data->interface_type))
+		goto out_inval;
+
+	if (of_property_read_u32(np, "power", (u32 *)&pdata->power))
+		goto out_inval;
+
+	if (of_property_read_u32(np, "num-eps", (u32 *)&config->num_eps))
+		goto out_inval;
+
+	if (of_property_read_u32(np, "ram-bits", (u32 *)&config->ram_bits))
+		goto out_inval;
+
+	config->multipoint = of_property_read_bool(np, "multipoint");
+
+	return 0;
+
+out_inval:
+
+	return -EINVAL;
+}
+
 static int am35x_probe(struct platform_device *pdev)
 {
 	struct musb_hdrc_platform_data	*pdata = dev_get_platdata(&pdev->dev);
@@ -451,18 +487,21 @@ static int am35x_probe(struct platform_device *pdev)
 
 	int				ret = -ENOMEM;
 
+	if (pdev->dev.of_node)
+		of_am35x_probe(pdev->dev.of_node, pdata);
+
 	glue = kzalloc(sizeof(*glue), GFP_KERNEL);
 	if (!glue)
 		goto err0;
 
-	phy_clk = clk_get(&pdev->dev, "fck");
+	phy_clk = clk_get(&pdev->dev, "hsotgusb_fck");
 	if (IS_ERR(phy_clk)) {
 		dev_err(&pdev->dev, "failed to get PHY clock\n");
 		ret = PTR_ERR(phy_clk);
 		goto err3;
 	}
 
-	clk = clk_get(&pdev->dev, "ick");
+	clk = clk_get(&pdev->dev, "hsotgusb_ick");
 	if (IS_ERR(clk)) {
 		dev_err(&pdev->dev, "failed to get clock\n");
 		ret = PTR_ERR(clk);
@@ -593,12 +632,25 @@ static int am35x_resume(struct device *dev)
 
 static SIMPLE_DEV_PM_OPS(am35x_pm_ops, am35x_suspend, am35x_resume);
 
+#ifdef CONFIG_OF
+static const struct of_device_id am35x_id_table[] = {
+	{
+		.compatible = "ti,musb-am35x"
+	},
+	{},
+};
+MODULE_DEVICE_TABLE(of, am35x_id_table);
+#endif
+
 static struct platform_driver am35x_driver = {
 	.probe		= am35x_probe,
 	.remove		= am35x_remove,
 	.driver		= {
 		.name	= "musb-am35x",
 		.pm	= &am35x_pm_ops,
+#ifdef CONFIG_OF
+		.of_match_table = of_match_ptr(am35x_id_table),
+#endif
 	},
 };
 
diff --git a/drivers/usb/musb/musb_host.c b/drivers/usb/musb/musb_host.c
index 30c5e7de0761..2a07c16c3d07 100644
--- a/drivers/usb/musb/musb_host.c
+++ b/drivers/usb/musb/musb_host.c
@@ -114,7 +114,6 @@ static void musb_h_tx_flush_fifo(struct musb_hw_ep *ep)
 				"Could not flush host TX%d fifo: csr: %04x\n",
 				ep->epnum, csr))
 			return;
-		mdelay(1);
 	}
 }
 
diff --git a/drivers/usb/serial/option.c b/drivers/usb/serial/option.c
index c6969ca72839..5ad86ac5995c 100644
--- a/drivers/usb/serial/option.c
+++ b/drivers/usb/serial/option.c
@@ -240,6 +240,7 @@ static void option_instat_callback(struct urb *urb);
 #define UBLOX_PRODUCT_R410M			0x90b2
 /* These Yuga products use Qualcomm's vendor ID */
 #define YUGA_PRODUCT_CLM920_NC5			0x9625
+#define QUECTEL_PRODUCT_EC20			0x9215
 
 #define QUECTEL_VENDOR_ID			0x2c7c
 /* These Quectel products use Quectel's vendor ID */
@@ -582,6 +583,8 @@ static void option_instat_callback(struct urb *urb);
 
 
 static const struct usb_device_id option_ids[] = {
+	{ USB_DEVICE(0x2c7c, 0x0125) }, /* Quectel EC25, EC20 R2.0  Mini PCIe */
+	{ USB_DEVICE(0x2c7c, 0x0121) }, /* Quectel EC21 Mini PCIe */
 	{ USB_DEVICE(OPTION_VENDOR_ID, OPTION_PRODUCT_COLT) },
 	{ USB_DEVICE(OPTION_VENDOR_ID, OPTION_PRODUCT_RICOLA) },
 	{ USB_DEVICE(OPTION_VENDOR_ID, OPTION_PRODUCT_RICOLA_LIGHT) },
@@ -1093,6 +1096,7 @@ static const struct usb_device_id option_ids[] = {
 	  .driver_info = NCTRL(0) | NCTRL(1) | NCTRL(2) | NCTRL(3) | RSVD(4) },
 	/* Quectel products using Qualcomm vendor ID */
 	{ USB_DEVICE(QUALCOMM_VENDOR_ID, QUECTEL_PRODUCT_UC15)},
+	{ USB_DEVICE(QUALCOMM_VENDOR_ID, QUECTEL_PRODUCT_EC20)},
 	{ USB_DEVICE(QUALCOMM_VENDOR_ID, QUECTEL_PRODUCT_UC20),
 	  .driver_info = RSVD(4) },
 	/* Yuga products use Qualcomm vendor ID */
diff --git a/drivers/watchdog/gpio_wdt.c b/drivers/watchdog/gpio_wdt.c
index 0923201ce874..5ea38588b183 100644
--- a/drivers/watchdog/gpio_wdt.c
+++ b/drivers/watchdog/gpio_wdt.c
@@ -8,6 +8,7 @@
 #include <linux/err.h>
 #include <linux/delay.h>
 #include <linux/module.h>
+#include <linux/of_gpio.h>
 #include <linux/gpio/consumer.h>
 #include <linux/of.h>
 #include <linux/platform_device.h>
@@ -29,6 +30,7 @@ enum {
 
 struct gpio_wdt_priv {
 	struct gpio_desc	*gpiod;
+	struct gpio_desc	*gpio_en;
 	bool			state;
 	bool			always_running;
 	unsigned int		hw_algo;
@@ -43,6 +45,10 @@ static void gpio_wdt_disable(struct gpio_wdt_priv *priv)
 	/* Put GPIO back to tristate */
 	if (priv->hw_algo == HW_ALGO_TOGGLE)
 		gpiod_direction_input(priv->gpiod);
+
+	/* Turn the WDT off if supported by HW */
+	if (!IS_ERR_OR_NULL(priv->gpio_en))
+		gpiod_set_value_cansleep(priv->gpio_en, 0);
 }
 
 static int gpio_wdt_ping(struct watchdog_device *wdd)
@@ -69,6 +75,10 @@ static int gpio_wdt_start(struct watchdog_device *wdd)
 {
 	struct gpio_wdt_priv *priv = watchdog_get_drvdata(wdd);
 
+	/* Turn the WDT on if supported by HW */
+	if (!IS_ERR_OR_NULL(priv->gpio_en))
+		gpiod_set_value_cansleep(priv->gpio_en, 1);
+
 	priv->state = 0;
 	gpiod_direction_output(priv->gpiod, priv->state);
 
@@ -119,6 +129,10 @@ static int gpio_wdt_probe(struct platform_device *pdev)
 
 	platform_set_drvdata(pdev, priv);
 
+	priv->gpio_en = devm_gpiod_get_optional(dev, "en", GPIOD_OUT_LOW);
+	if (IS_ERR_OR_NULL(priv->gpio_en))
+		dev_warn(&pdev->dev, "en-gpios property not found. Assume WDT is already enabled.\n");
+
 	ret = of_property_read_string(np, "hw_algo", &algo);
 	if (ret)
 		return ret;
diff --git a/drivers/watchdog/omap_wdt.c b/drivers/watchdog/omap_wdt.c
index 1616f93dfad7..0ccc3799c6df 100644
--- a/drivers/watchdog/omap_wdt.c
+++ b/drivers/watchdog/omap_wdt.c
@@ -85,6 +85,13 @@ static void omap_wdt_reload(struct omap_wdt_dev *wdev)
 	/* reloaded WCRR from WLDR */
 }
 
+static void omap_wdt_init_trgr_pattern(struct omap_wdt_dev *wdev)
+{
+	void __iomem    *base = wdev->base;
+
+	wdev->wdt_trgr_pattern = readl_relaxed(base + OMAP_WATCHDOG_TGR);
+}
+
 static void omap_wdt_enable(struct omap_wdt_dev *wdev)
 {
 	void __iomem *base = wdev->base;
@@ -238,7 +245,6 @@ static int omap_wdt_probe(struct platform_device *pdev)
 
 	wdev->omap_wdt_users	= false;
 	wdev->dev		= &pdev->dev;
-	wdev->wdt_trgr_pattern	= 0x1234;
 	mutex_init(&wdev->lock);
 
 	/* reserve static register mappings */
@@ -253,6 +259,8 @@ static int omap_wdt_probe(struct platform_device *pdev)
 	wdev->wdog.timeout = TIMER_MARGIN_DEFAULT;
 	wdev->wdog.parent = &pdev->dev;
 
+	omap_wdt_init_trgr_pattern(wdev);
+
 	watchdog_init_timeout(&wdev->wdog, timer_margin, &pdev->dev);
 
 	watchdog_set_nowayout(&wdev->wdog, nowayout);
diff --git a/fs/afs/dir_silly.c b/fs/afs/dir_silly.c
index 04f75a44f243..60cbce1995a5 100644
--- a/fs/afs/dir_silly.c
+++ b/fs/afs/dir_silly.c
@@ -236,7 +236,7 @@ int afs_silly_iput(struct dentry *dentry, struct inode *inode)
 	struct dentry *alias;
 	int ret;
 
-	DECLARE_WAIT_QUEUE_HEAD_ONSTACK(wq);
+	DECLARE_SWAIT_QUEUE_HEAD_ONSTACK(wq);
 
 	_enter("%p{%pd},%llx", dentry, dentry, vnode->fid.vnode);
 
diff --git a/fs/aio.c b/fs/aio.c
index 6a21d8919409..76ce0cc3ee4e 100644
--- a/fs/aio.c
+++ b/fs/aio.c
@@ -43,7 +43,6 @@
 #include <linux/mount.h>
 #include <linux/pseudo_fs.h>
 
-#include <asm/kmap_types.h>
 #include <linux/uaccess.h>
 #include <linux/nospec.h>
 
diff --git a/fs/btrfs/ctree.h b/fs/btrfs/ctree.h
index b6884eda9ff6..3b211040de94 100644
--- a/fs/btrfs/ctree.h
+++ b/fs/btrfs/ctree.h
@@ -17,7 +17,6 @@
 #include <linux/wait.h>
 #include <linux/slab.h>
 #include <trace/events/btrfs.h>
-#include <asm/kmap_types.h>
 #include <asm/unaligned.h>
 #include <linux/pagemap.h>
 #include <linux/btrfs.h>
diff --git a/fs/cifs/readdir.c b/fs/cifs/readdir.c
index 799be3a5d25e..d5165a7da071 100644
--- a/fs/cifs/readdir.c
+++ b/fs/cifs/readdir.c
@@ -81,7 +81,7 @@ cifs_prime_dcache(struct dentry *parent, struct qstr *name,
 	struct inode *inode;
 	struct super_block *sb = parent->d_sb;
 	struct cifs_sb_info *cifs_sb = CIFS_SB(sb);
-	DECLARE_WAIT_QUEUE_HEAD_ONSTACK(wq);
+	DECLARE_SWAIT_QUEUE_HEAD_ONSTACK(wq);
 
 	cifs_dbg(FYI, "%s: for %s\n", __func__, name->name);
 
diff --git a/fs/dcache.c b/fs/dcache.c
index ea0485861d93..26a187abf13a 100644
--- a/fs/dcache.c
+++ b/fs/dcache.c
@@ -2503,9 +2503,10 @@ EXPORT_SYMBOL(d_rehash);
 static inline unsigned start_dir_add(struct inode *dir)
 {
 
+	preempt_disable_rt();
 	for (;;) {
-		unsigned n = dir->i_dir_seq;
-		if (!(n & 1) && cmpxchg(&dir->i_dir_seq, n, n + 1) == n)
+		unsigned n = dir->__i_dir_seq;
+		if (!(n & 1) && cmpxchg(&dir->__i_dir_seq, n, n + 1) == n)
 			return n;
 		cpu_relax();
 	}
@@ -2513,26 +2514,30 @@ static inline unsigned start_dir_add(struct inode *dir)
 
 static inline void end_dir_add(struct inode *dir, unsigned n)
 {
-	smp_store_release(&dir->i_dir_seq, n + 2);
+	smp_store_release(&dir->__i_dir_seq, n + 2);
+	preempt_enable_rt();
 }
 
 static void d_wait_lookup(struct dentry *dentry)
 {
-	if (d_in_lookup(dentry)) {
-		DECLARE_WAITQUEUE(wait, current);
-		add_wait_queue(dentry->d_wait, &wait);
-		do {
-			set_current_state(TASK_UNINTERRUPTIBLE);
-			spin_unlock(&dentry->d_lock);
-			schedule();
-			spin_lock(&dentry->d_lock);
-		} while (d_in_lookup(dentry));
-	}
+	struct swait_queue __wait;
+
+	if (!d_in_lookup(dentry))
+		return;
+
+	INIT_LIST_HEAD(&__wait.task_list);
+	do {
+		prepare_to_swait_exclusive(dentry->d_wait, &__wait, TASK_UNINTERRUPTIBLE);
+		spin_unlock(&dentry->d_lock);
+		schedule();
+		spin_lock(&dentry->d_lock);
+	} while (d_in_lookup(dentry));
+	finish_swait(dentry->d_wait, &__wait);
 }
 
 struct dentry *d_alloc_parallel(struct dentry *parent,
 				const struct qstr *name,
-				wait_queue_head_t *wq)
+				struct swait_queue_head *wq)
 {
 	unsigned int hash = name->hash;
 	struct hlist_bl_head *b = in_lookup_hash(parent, hash);
@@ -2546,7 +2551,7 @@ struct dentry *d_alloc_parallel(struct dentry *parent,
 
 retry:
 	rcu_read_lock();
-	seq = smp_load_acquire(&parent->d_inode->i_dir_seq);
+	seq = smp_load_acquire(&parent->d_inode->__i_dir_seq);
 	r_seq = read_seqbegin(&rename_lock);
 	dentry = __d_lookup_rcu(parent, name, &d_seq);
 	if (unlikely(dentry)) {
@@ -2574,7 +2579,7 @@ struct dentry *d_alloc_parallel(struct dentry *parent,
 	}
 
 	hlist_bl_lock(b);
-	if (unlikely(READ_ONCE(parent->d_inode->i_dir_seq) != seq)) {
+	if (unlikely(READ_ONCE(parent->d_inode->__i_dir_seq) != seq)) {
 		hlist_bl_unlock(b);
 		rcu_read_unlock();
 		goto retry;
@@ -2647,7 +2652,7 @@ void __d_lookup_done(struct dentry *dentry)
 	hlist_bl_lock(b);
 	dentry->d_flags &= ~DCACHE_PAR_LOOKUP;
 	__hlist_bl_del(&dentry->d_u.d_in_lookup_hash);
-	wake_up_all(dentry->d_wait);
+	swake_up_all(dentry->d_wait);
 	dentry->d_wait = NULL;
 	hlist_bl_unlock(b);
 	INIT_HLIST_NODE(&dentry->d_u.d_alias);
diff --git a/fs/fuse/readdir.c b/fs/fuse/readdir.c
index 3441ffa740f3..2fcae5cfd272 100644
--- a/fs/fuse/readdir.c
+++ b/fs/fuse/readdir.c
@@ -158,7 +158,7 @@ static int fuse_direntplus_link(struct file *file,
 	struct inode *dir = d_inode(parent);
 	struct fuse_conn *fc;
 	struct inode *inode;
-	DECLARE_WAIT_QUEUE_HEAD_ONSTACK(wq);
+	DECLARE_SWAIT_QUEUE_HEAD_ONSTACK(wq);
 
 	if (!o->nodeid) {
 		/*
diff --git a/fs/inode.c b/fs/inode.c
index 5eea9912a0b9..85da949d9083 100644
--- a/fs/inode.c
+++ b/fs/inode.c
@@ -158,7 +158,7 @@ int inode_init_always(struct super_block *sb, struct inode *inode)
 	inode->i_bdev = NULL;
 	inode->i_cdev = NULL;
 	inode->i_link = NULL;
-	inode->i_dir_seq = 0;
+	inode->__i_dir_seq = 0;
 	inode->i_rdev = 0;
 	inode->dirtied_when = 0;
 
diff --git a/fs/namei.c b/fs/namei.c
index 7af66d5a0c1b..b3f51efa5066 100644
--- a/fs/namei.c
+++ b/fs/namei.c
@@ -1520,7 +1520,7 @@ static struct dentry *__lookup_slow(const struct qstr *name,
 {
 	struct dentry *dentry, *old;
 	struct inode *inode = dir->d_inode;
-	DECLARE_WAIT_QUEUE_HEAD_ONSTACK(wq);
+	DECLARE_SWAIT_QUEUE_HEAD_ONSTACK(wq);
 
 	/* Don't go there if it's already dead */
 	if (unlikely(IS_DEADDIR(inode)))
@@ -3014,7 +3014,7 @@ static struct dentry *lookup_open(struct nameidata *nd, struct file *file,
 	struct dentry *dentry;
 	int error, create_error = 0;
 	umode_t mode = op->mode;
-	DECLARE_WAIT_QUEUE_HEAD_ONSTACK(wq);
+	DECLARE_SWAIT_QUEUE_HEAD_ONSTACK(wq);
 
 	if (unlikely(IS_DEADDIR(dir_inode)))
 		return ERR_PTR(-ENOENT);
diff --git a/fs/namespace.c b/fs/namespace.c
index c7fbb50a5aaa..5e261e50625a 100644
--- a/fs/namespace.c
+++ b/fs/namespace.c
@@ -14,6 +14,7 @@
 #include <linux/mnt_namespace.h>
 #include <linux/user_namespace.h>
 #include <linux/namei.h>
+#include <linux/delay.h>
 #include <linux/security.h>
 #include <linux/cred.h>
 #include <linux/idr.h>
@@ -321,8 +322,11 @@ int __mnt_want_write(struct vfsmount *m)
 	 * incremented count after it has set MNT_WRITE_HOLD.
 	 */
 	smp_mb();
-	while (READ_ONCE(mnt->mnt.mnt_flags) & MNT_WRITE_HOLD)
-		cpu_relax();
+	while (READ_ONCE(mnt->mnt.mnt_flags) & MNT_WRITE_HOLD) {
+		preempt_enable();
+		cpu_chill();
+		preempt_disable();
+	}
 	/*
 	 * After the slowpath clears MNT_WRITE_HOLD, mnt_is_readonly will
 	 * be set to match its requirements. So we must not load that until
diff --git a/fs/nfs/dir.c b/fs/nfs/dir.c
index 4e011adaf967..7ac33755bad3 100644
--- a/fs/nfs/dir.c
+++ b/fs/nfs/dir.c
@@ -484,7 +484,7 @@ void nfs_prime_dcache(struct dentry *parent, struct nfs_entry *entry,
 		unsigned long dir_verifier)
 {
 	struct qstr filename = QSTR_INIT(entry->name, entry->len);
-	DECLARE_WAIT_QUEUE_HEAD_ONSTACK(wq);
+	DECLARE_SWAIT_QUEUE_HEAD_ONSTACK(wq);
 	struct dentry *dentry;
 	struct dentry *alias;
 	struct inode *inode;
@@ -1660,7 +1660,7 @@ int nfs_atomic_open(struct inode *dir, struct dentry *dentry,
 		    struct file *file, unsigned open_flags,
 		    umode_t mode)
 {
-	DECLARE_WAIT_QUEUE_HEAD_ONSTACK(wq);
+	DECLARE_SWAIT_QUEUE_HEAD_ONSTACK(wq);
 	struct nfs_open_context *ctx;
 	struct dentry *res;
 	struct iattr attr = { .ia_valid = ATTR_OPEN };
diff --git a/fs/nfs/unlink.c b/fs/nfs/unlink.c
index b27ebdccef70..f86c98a7ed04 100644
--- a/fs/nfs/unlink.c
+++ b/fs/nfs/unlink.c
@@ -13,7 +13,7 @@
 #include <linux/sunrpc/clnt.h>
 #include <linux/nfs_fs.h>
 #include <linux/sched.h>
-#include <linux/wait.h>
+#include <linux/swait.h>
 #include <linux/namei.h>
 #include <linux/fsnotify.h>
 
@@ -180,7 +180,7 @@ nfs_async_unlink(struct dentry *dentry, const struct qstr *name)
 
 	data->cred = get_current_cred();
 	data->res.dir_attr = &data->dir_attr;
-	init_waitqueue_head(&data->wq);
+	init_swait_queue_head(&data->wq);
 
 	status = -EBUSY;
 	spin_lock(&dentry->d_lock);
diff --git a/fs/proc/array.c b/fs/proc/array.c
index 65ec2029fa80..7052441be967 100644
--- a/fs/proc/array.c
+++ b/fs/proc/array.c
@@ -382,9 +382,9 @@ static inline void task_context_switch_counts(struct seq_file *m,
 static void task_cpus_allowed(struct seq_file *m, struct task_struct *task)
 {
 	seq_printf(m, "Cpus_allowed:\t%*pb\n",
-		   cpumask_pr_args(task->cpus_ptr));
+		   cpumask_pr_args(&task->cpus_mask));
 	seq_printf(m, "Cpus_allowed_list:\t%*pbl\n",
-		   cpumask_pr_args(task->cpus_ptr));
+		   cpumask_pr_args(&task->cpus_mask));
 }
 
 static inline void task_core_dumping(struct seq_file *m, struct mm_struct *mm)
diff --git a/fs/proc/base.c b/fs/proc/base.c
index 55ce0ee9c5c7..a66f399476fc 100644
--- a/fs/proc/base.c
+++ b/fs/proc/base.c
@@ -96,6 +96,7 @@
 #include <linux/posix-timers.h>
 #include <linux/time_namespace.h>
 #include <linux/resctrl.h>
+#include <linux/swait.h>
 #include <trace/events/oom.h>
 #include "internal.h"
 #include "fd.h"
@@ -2038,7 +2039,7 @@ bool proc_fill_cache(struct file *file, struct dir_context *ctx,
 
 	child = d_hash_and_lookup(dir, &qname);
 	if (!child) {
-		DECLARE_WAIT_QUEUE_HEAD_ONSTACK(wq);
+		DECLARE_SWAIT_QUEUE_HEAD_ONSTACK(wq);
 		child = d_alloc_parallel(dir, &qname, &wq);
 		if (IS_ERR(child))
 			goto end_instantiate;
diff --git a/fs/proc/proc_sysctl.c b/fs/proc/proc_sysctl.c
index 070d2df8ab9c..a1a964b631d7 100644
--- a/fs/proc/proc_sysctl.c
+++ b/fs/proc/proc_sysctl.c
@@ -683,7 +683,7 @@ static bool proc_sys_fill_cache(struct file *file,
 
 	child = d_lookup(dir, &qname);
 	if (!child) {
-		DECLARE_WAIT_QUEUE_HEAD_ONSTACK(wq);
+		DECLARE_SWAIT_QUEUE_HEAD_ONSTACK(wq);
 		child = d_alloc_parallel(dir, &qname, &wq);
 		if (IS_ERR(child))
 			return false;
diff --git a/fs/pstore/platform.c b/fs/pstore/platform.c
index b1ebf7b61732..b7e3a6bacbb0 100644
--- a/fs/pstore/platform.c
+++ b/fs/pstore/platform.c
@@ -383,7 +383,8 @@ void pstore_record_init(struct pstore_record *record,
  * end of the buffer.
  */
 static void pstore_dump(struct kmsg_dumper *dumper,
-			enum kmsg_dump_reason reason)
+			enum kmsg_dump_reason reason,
+			struct kmsg_dumper_iter *iter)
 {
 	unsigned long	total = 0;
 	const char	*why;
@@ -435,7 +436,7 @@ static void pstore_dump(struct kmsg_dumper *dumper,
 		dst_size -= header_size;
 
 		/* Write dump contents. */
-		if (!kmsg_dump_get_buffer(dumper, true, dst + header_size,
+		if (!kmsg_dump_get_buffer(iter, true, dst + header_size,
 					  dst_size, &dump_size))
 			break;
 
diff --git a/include/asm-generic/Kbuild b/include/asm-generic/Kbuild
index d1300c6e0a47..267f6dfb8960 100644
--- a/include/asm-generic/Kbuild
+++ b/include/asm-generic/Kbuild
@@ -30,7 +30,7 @@ mandatory-y += irq.h
 mandatory-y += irq_regs.h
 mandatory-y += irq_work.h
 mandatory-y += kdebug.h
-mandatory-y += kmap_types.h
+mandatory-y += kmap_size.h
 mandatory-y += kprobes.h
 mandatory-y += linkage.h
 mandatory-y += local.h
diff --git a/include/asm-generic/hardirq.h b/include/asm-generic/hardirq.h
index d14214dfc10b..7317e8258b48 100644
--- a/include/asm-generic/hardirq.h
+++ b/include/asm-generic/hardirq.h
@@ -7,9 +7,13 @@
 
 typedef struct {
 	unsigned int __softirq_pending;
+#ifdef ARCH_WANTS_NMI_IRQSTAT
+	unsigned int __nmi_count;
+#endif
 } ____cacheline_aligned irq_cpustat_t;
 
-#include <linux/irq_cpustat.h>	/* Standard mappings for irq_cpustat_t above */
+DECLARE_PER_CPU_ALIGNED(irq_cpustat_t, irq_stat);
+
 #include <linux/irq.h>
 
 #ifndef ack_bad_irq
diff --git a/include/asm-generic/kmap_size.h b/include/asm-generic/kmap_size.h
new file mode 100644
index 000000000000..9d6c7786a645
--- /dev/null
+++ b/include/asm-generic/kmap_size.h
@@ -0,0 +1,12 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _ASM_GENERIC_KMAP_SIZE_H
+#define _ASM_GENERIC_KMAP_SIZE_H
+
+/* For debug this provides guard pages between the maps */
+#ifdef CONFIG_DEBUG_HIGHMEM
+# define KM_MAX_IDX	33
+#else
+# define KM_MAX_IDX	16
+#endif
+
+#endif
diff --git a/include/asm-generic/kmap_types.h b/include/asm-generic/kmap_types.h
deleted file mode 100644
index 9f95b7b63d19..000000000000
--- a/include/asm-generic/kmap_types.h
+++ /dev/null
@@ -1,11 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0 */
-#ifndef _ASM_GENERIC_KMAP_TYPES_H
-#define _ASM_GENERIC_KMAP_TYPES_H
-
-#ifdef __WITH_KM_FENCE
-# define KM_TYPE_NR 41
-#else
-# define KM_TYPE_NR 20
-#endif
-
-#endif
diff --git a/include/asm-generic/preempt.h b/include/asm-generic/preempt.h
index d683f5e6d791..71c1535db56a 100644
--- a/include/asm-generic/preempt.h
+++ b/include/asm-generic/preempt.h
@@ -79,6 +79,9 @@ static __always_inline bool should_resched(int preempt_offset)
 }
 
 #ifdef CONFIG_PREEMPTION
+#ifdef CONFIG_PREEMPT_RT
+extern void preempt_schedule_lock(void);
+#endif
 extern asmlinkage void preempt_schedule(void);
 #define __preempt_schedule() preempt_schedule()
 extern asmlinkage void preempt_schedule_notrace(void);
diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 542471b76f41..c53febd7d169 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -153,7 +153,7 @@ struct request {
 	 */
 	union {
 		struct hlist_node hash;	/* merge hash */
-		struct list_head ipi_list;
+		struct llist_node ipi_list;
 	};
 
 	/*
diff --git a/include/linux/bottom_half.h b/include/linux/bottom_half.h
index a19519f4241d..eed86eb0a1de 100644
--- a/include/linux/bottom_half.h
+++ b/include/linux/bottom_half.h
@@ -4,7 +4,7 @@
 
 #include <linux/preempt.h>
 
-#ifdef CONFIG_TRACE_IRQFLAGS
+#if defined(CONFIG_PREEMPT_RT) || defined(CONFIG_TRACE_IRQFLAGS)
 extern void __local_bh_disable_ip(unsigned long ip, unsigned int cnt);
 #else
 static __always_inline void __local_bh_disable_ip(unsigned long ip, unsigned int cnt)
@@ -32,4 +32,10 @@ static inline void local_bh_enable(void)
 	__local_bh_enable_ip(_THIS_IP_, SOFTIRQ_DISABLE_OFFSET);
 }
 
+#ifdef CONFIG_PREEMPT_RT
+extern bool local_bh_blocked(void);
+#else
+static inline bool local_bh_blocked(void) { return false; }
+#endif
+
 #endif /* _LINUX_BH_H */
diff --git a/include/linux/console.h b/include/linux/console.h
index 4b1e26c4cb42..e6ff51883dee 100644
--- a/include/linux/console.h
+++ b/include/linux/console.h
@@ -16,6 +16,7 @@
 
 #include <linux/atomic.h>
 #include <linux/types.h>
+#include <linux/printk.h>
 
 struct vc_data;
 struct console_font_op;
@@ -137,10 +138,12 @@ static inline int con_debug_leave(void)
 #define CON_ANYTIME	(16) /* Safe to call when cpu is offline */
 #define CON_BRL		(32) /* Used for a braille device */
 #define CON_EXTENDED	(64) /* Use the extended output format a la /dev/kmsg */
+#define CON_HANDOVER	(128) /* Device was previously a boot console. */
 
 struct console {
 	char	name[16];
 	void	(*write)(struct console *, const char *, unsigned);
+	void	(*write_atomic)(struct console *co, const char *s, unsigned int count);
 	int	(*read)(struct console *, char *, unsigned);
 	struct tty_driver *(*device)(struct console *, int *);
 	void	(*unblank)(void);
@@ -150,6 +153,11 @@ struct console {
 	short	flags;
 	short	index;
 	int	cflag;
+#ifdef CONFIG_PRINTK
+	char	sync_buf[CONSOLE_LOG_MAX];
+#endif
+	atomic64_t printk_seq;
+	struct task_struct *thread;
 	void	*data;
 	struct	 console *next;
 };
@@ -230,4 +238,7 @@ extern void console_init(void);
 void dummycon_register_output_notifier(struct notifier_block *nb);
 void dummycon_unregister_output_notifier(struct notifier_block *nb);
 
+extern void console_atomic_lock(unsigned int *flags);
+extern void console_atomic_unlock(unsigned int flags);
+
 #endif /* _LINUX_CONSOLE_H */
diff --git a/include/linux/cpuhotplug.h b/include/linux/cpuhotplug.h
index bc56287a1ed1..0042ef362511 100644
--- a/include/linux/cpuhotplug.h
+++ b/include/linux/cpuhotplug.h
@@ -152,6 +152,7 @@ enum cpuhp_state {
 	CPUHP_AP_ONLINE,
 	CPUHP_TEARDOWN_CPU,
 	CPUHP_AP_ONLINE_IDLE,
+	CPUHP_AP_SCHED_WAIT_EMPTY,
 	CPUHP_AP_SMPBOOT_THREADS,
 	CPUHP_AP_X86_VDSO_VMA_ONLINE,
 	CPUHP_AP_IRQ_AFFINITY_ONLINE,
diff --git a/include/linux/cpumask.h b/include/linux/cpumask.h
index f0d895d6ac39..383684e30f12 100644
--- a/include/linux/cpumask.h
+++ b/include/linux/cpumask.h
@@ -199,6 +199,11 @@ static inline int cpumask_any_and_distribute(const struct cpumask *src1p,
 	return cpumask_next_and(-1, src1p, src2p);
 }
 
+static inline int cpumask_any_distribute(const struct cpumask *srcp)
+{
+	return cpumask_first(srcp);
+}
+
 #define for_each_cpu(cpu, mask)			\
 	for ((cpu) = 0; (cpu) < 1; (cpu)++, (void)mask)
 #define for_each_cpu_not(cpu, mask)		\
@@ -252,6 +257,7 @@ int cpumask_any_but(const struct cpumask *mask, unsigned int cpu);
 unsigned int cpumask_local_spread(unsigned int i, int node);
 int cpumask_any_and_distribute(const struct cpumask *src1p,
 			       const struct cpumask *src2p);
+int cpumask_any_distribute(const struct cpumask *srcp);
 
 /**
  * for_each_cpu - iterate over every cpu in a mask
diff --git a/include/linux/dcache.h b/include/linux/dcache.h
index 6f95c3300cbb..c1290db778bd 100644
--- a/include/linux/dcache.h
+++ b/include/linux/dcache.h
@@ -106,7 +106,7 @@ struct dentry {
 
 	union {
 		struct list_head d_lru;		/* LRU list */
-		wait_queue_head_t *d_wait;	/* in-lookup ones only */
+		struct swait_queue_head *d_wait;	/* in-lookup ones only */
 	};
 	struct list_head d_child;	/* child of parent list */
 	struct list_head d_subdirs;	/* our children */
@@ -238,7 +238,7 @@ extern void d_set_d_op(struct dentry *dentry, const struct dentry_operations *op
 extern struct dentry * d_alloc(struct dentry *, const struct qstr *);
 extern struct dentry * d_alloc_anon(struct super_block *);
 extern struct dentry * d_alloc_parallel(struct dentry *, const struct qstr *,
-					wait_queue_head_t *);
+					struct swait_queue_head *);
 extern struct dentry * d_splice_alias(struct inode *, struct dentry *);
 extern struct dentry * d_add_ci(struct dentry *, struct inode *, struct qstr *);
 extern struct dentry * d_exact_alias(struct dentry *, struct inode *);
diff --git a/include/linux/debug_locks.h b/include/linux/debug_locks.h
index 2915f56ad421..5a9e3e3769ce 100644
--- a/include/linux/debug_locks.h
+++ b/include/linux/debug_locks.h
@@ -3,8 +3,7 @@
 #define __LINUX_DEBUG_LOCKING_H
 
 #include <linux/atomic.h>
-#include <linux/bug.h>
-#include <linux/printk.h>
+#include <linux/cache.h>
 
 struct task_struct;
 
diff --git a/include/linux/delay.h b/include/linux/delay.h
index 1d0e2ce6b6d9..02b37178b54f 100644
--- a/include/linux/delay.h
+++ b/include/linux/delay.h
@@ -76,4 +76,10 @@ static inline void fsleep(unsigned long usecs)
 		msleep(DIV_ROUND_UP(usecs, 1000));
 }
 
+#ifdef CONFIG_PREEMPT_RT
+extern void cpu_chill(void);
+#else
+# define cpu_chill()	cpu_relax()
+#endif
+
 #endif /* defined(_LINUX_DELAY_H) */
diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index dd357a747780..aa8a52326aff 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -14,6 +14,8 @@
 #include <linux/types.h>
 #include <asm/page.h>
 
+struct rmd_config;
+
 /**
  * typedef dma_cookie_t - an opaque DMA cookie
  *
@@ -65,6 +67,7 @@ enum dma_transaction_type {
 	DMA_COMPLETION_NO_ORDER,
 	DMA_REPEAT,
 	DMA_LOAD_EOT,
+	DMA_RMD,
 /* last transaction type for creation of the capabilities mask */
 	DMA_TX_TYPE_END,
 };
@@ -916,6 +919,11 @@ struct dma_device {
 	struct dma_async_tx_descriptor *(*device_prep_dma_imm_data)(
 		struct dma_chan *chan, dma_addr_t dst, u64 data,
 		unsigned long flags);
+	struct dma_async_tx_descriptor *(*device_prep_dma_rmd)(
+		struct dma_chan *chan, dma_addr_t fpga_base,
+		struct rmd_config *rmd,	enum dma_transfer_direction direction,
+		dma_addr_t mem, dma_addr_t buf_ctrl, size_t chunk_size,
+		unsigned long tx_flags);
 
 	void (*device_caps)(struct dma_chan *chan,
 			    struct dma_slave_caps *caps);
@@ -1006,6 +1014,19 @@ static inline struct dma_async_tx_descriptor *dmaengine_prep_dma_cyclic(
 						period_len, dir, flags);
 }
 
+static inline struct dma_async_tx_descriptor *dmaengine_prep_rmd(
+		struct dma_chan *chan, dma_addr_t fpga_base,
+		struct rmd_config *rmd, enum dma_transfer_direction direction,
+		dma_addr_t mem, dma_addr_t buf_ctrl, size_t chunk_size,
+		unsigned long tx_flags)
+{
+	if (!chan || !chan->device || !chan->device->device_prep_dma_rmd)
+		return NULL;
+
+	return chan->device->device_prep_dma_rmd(chan, fpga_base, rmd,
+				direction, mem, buf_ctrl, chunk_size, tx_flags);
+}
+
 static inline struct dma_async_tx_descriptor *dmaengine_prep_interleaved_dma(
 		struct dma_chan *chan, struct dma_interleaved_template *xt,
 		unsigned long flags)
diff --git a/include/linux/entry-common.h b/include/linux/entry-common.h
index 474f29638d2c..733cc8ddd3ee 100644
--- a/include/linux/entry-common.h
+++ b/include/linux/entry-common.h
@@ -69,7 +69,7 @@
 
 #define EXIT_TO_USER_MODE_WORK						\
 	(_TIF_SIGPENDING | _TIF_NOTIFY_RESUME | _TIF_UPROBE |		\
-	 _TIF_NEED_RESCHED | _TIF_PATCH_PENDING |			\
+	 _TIF_NEED_RESCHED_MASK | _TIF_PATCH_PENDING |			\
 	 ARCH_EXIT_TO_USER_MODE_WORK)
 
 /**
diff --git a/include/linux/fs.h b/include/linux/fs.h
index 8bde32cf9711..61ad6d7bfc95 100644
--- a/include/linux/fs.h
+++ b/include/linux/fs.h
@@ -699,7 +699,7 @@ struct inode {
 		struct block_device	*i_bdev;
 		struct cdev		*i_cdev;
 		char			*i_link;
-		unsigned		i_dir_seq;
+		unsigned		__i_dir_seq;
 	};
 
 	__u32			i_generation;
diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index 754f67ac4326..76878b357ffa 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -6,6 +6,7 @@
 #include <linux/preempt.h>
 #include <linux/lockdep.h>
 #include <linux/ftrace_irq.h>
+#include <linux/sched.h>
 #include <linux/vtime.h>
 #include <asm/hardirq.h>
 
@@ -32,9 +33,9 @@ static __always_inline void rcu_irq_enter_check_tick(void)
  */
 #define __irq_enter()					\
 	do {						\
-		account_irq_enter_time(current);	\
 		preempt_count_add(HARDIRQ_OFFSET);	\
 		lockdep_hardirq_enter();		\
+		account_hardirq_enter(current);		\
 	} while (0)
 
 /*
@@ -62,8 +63,8 @@ void irq_enter_rcu(void);
  */
 #define __irq_exit()					\
 	do {						\
+		account_hardirq_exit(current);		\
 		lockdep_hardirq_exit();			\
-		account_irq_exit_time(current);		\
 		preempt_count_sub(HARDIRQ_OFFSET);	\
 	} while (0)
 
@@ -115,7 +116,6 @@ extern void rcu_nmi_exit(void);
 	do {							\
 		lockdep_off();					\
 		arch_nmi_enter();				\
-		printk_nmi_enter();				\
 		BUG_ON(in_nmi() == NMI_MASK);			\
 		__preempt_count_add(NMI_OFFSET + HARDIRQ_OFFSET);	\
 	} while (0)
@@ -134,7 +134,6 @@ extern void rcu_nmi_exit(void);
 	do {							\
 		BUG_ON(!in_nmi());				\
 		__preempt_count_sub(NMI_OFFSET + HARDIRQ_OFFSET);	\
-		printk_nmi_exit();				\
 		arch_nmi_exit();				\
 		lockdep_on();					\
 	} while (0)
diff --git a/include/linux/highmem-internal.h b/include/linux/highmem-internal.h
new file mode 100644
index 000000000000..f9bc6acd3679
--- /dev/null
+++ b/include/linux/highmem-internal.h
@@ -0,0 +1,222 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _LINUX_HIGHMEM_INTERNAL_H
+#define _LINUX_HIGHMEM_INTERNAL_H
+
+/*
+ * Outside of CONFIG_HIGHMEM to support X86 32bit iomap_atomic() cruft.
+ */
+#ifdef CONFIG_KMAP_LOCAL
+void *__kmap_local_pfn_prot(unsigned long pfn, pgprot_t prot);
+void *__kmap_local_page_prot(struct page *page, pgprot_t prot);
+void kunmap_local_indexed(void *vaddr);
+void kmap_local_fork(struct task_struct *tsk);
+void __kmap_local_sched_out(void);
+void __kmap_local_sched_in(void);
+static inline void kmap_assert_nomap(void)
+{
+	DEBUG_LOCKS_WARN_ON(current->kmap_ctrl.idx);
+}
+#else
+static inline void kmap_local_fork(struct task_struct *tsk) { }
+static inline void kmap_assert_nomap(void) { }
+#endif
+
+#ifdef CONFIG_HIGHMEM
+#include <asm/highmem.h>
+
+#ifndef ARCH_HAS_KMAP_FLUSH_TLB
+static inline void kmap_flush_tlb(unsigned long addr) { }
+#endif
+
+#ifndef kmap_prot
+#define kmap_prot PAGE_KERNEL
+#endif
+
+void *kmap_high(struct page *page);
+void kunmap_high(struct page *page);
+void __kmap_flush_unused(void);
+struct page *__kmap_to_page(void *addr);
+
+static inline void *kmap(struct page *page)
+{
+	void *addr;
+
+	might_sleep();
+	if (!PageHighMem(page))
+		addr = page_address(page);
+	else
+		addr = kmap_high(page);
+	kmap_flush_tlb((unsigned long)addr);
+	return addr;
+}
+
+static inline void kunmap(struct page *page)
+{
+	might_sleep();
+	if (!PageHighMem(page))
+		return;
+	kunmap_high(page);
+}
+
+static inline struct page *kmap_to_page(void *addr)
+{
+	return __kmap_to_page(addr);
+}
+
+static inline void kmap_flush_unused(void)
+{
+	__kmap_flush_unused();
+}
+
+static inline void *kmap_local_page(struct page *page)
+{
+	return __kmap_local_page_prot(page, kmap_prot);
+}
+
+static inline void *kmap_local_page_prot(struct page *page, pgprot_t prot)
+{
+	return __kmap_local_page_prot(page, prot);
+}
+
+static inline void *kmap_local_pfn(unsigned long pfn)
+{
+	return __kmap_local_pfn_prot(pfn, kmap_prot);
+}
+
+static inline void __kunmap_local(void *vaddr)
+{
+	kunmap_local_indexed(vaddr);
+}
+
+static inline void *kmap_atomic(struct page *page)
+{
+	if (IS_ENABLED(CONFIG_PREEMPT_RT))
+		migrate_disable();
+	else
+		preempt_disable();
+	pagefault_disable();
+	return __kmap_local_page_prot(page, kmap_prot);
+}
+
+static inline void __kunmap_atomic(void *addr)
+{
+	kunmap_local_indexed(addr);
+	pagefault_enable();
+	if (IS_ENABLED(CONFIG_PREEMPT_RT))
+		migrate_enable();
+	else
+		preempt_enable();
+}
+
+unsigned int __nr_free_highpages(void);
+extern atomic_long_t _totalhigh_pages;
+
+static inline unsigned int nr_free_highpages(void)
+{
+	return __nr_free_highpages();
+}
+
+static inline unsigned long totalhigh_pages(void)
+{
+	return (unsigned long)atomic_long_read(&_totalhigh_pages);
+}
+
+static inline void totalhigh_pages_inc(void)
+{
+	atomic_long_inc(&_totalhigh_pages);
+}
+
+static inline void totalhigh_pages_add(long count)
+{
+	atomic_long_add(count, &_totalhigh_pages);
+}
+
+#else /* CONFIG_HIGHMEM */
+
+static inline struct page *kmap_to_page(void *addr)
+{
+	return virt_to_page(addr);
+}
+
+static inline void *kmap(struct page *page)
+{
+	might_sleep();
+	return page_address(page);
+}
+
+static inline void kunmap_high(struct page *page) { }
+static inline void kmap_flush_unused(void) { }
+
+static inline void kunmap(struct page *page)
+{
+#ifdef ARCH_HAS_FLUSH_ON_KUNMAP
+	kunmap_flush_on_unmap(page_address(page));
+#endif
+}
+
+static inline void *kmap_local_page(struct page *page)
+{
+	return page_address(page);
+}
+
+static inline void *kmap_local_page_prot(struct page *page, pgprot_t prot)
+{
+	return kmap_local_page(page);
+}
+
+static inline void *kmap_local_pfn(unsigned long pfn)
+{
+	return kmap_local_page(pfn_to_page(pfn));
+}
+
+static inline void __kunmap_local(void *addr)
+{
+#ifdef ARCH_HAS_FLUSH_ON_KUNMAP
+	kunmap_flush_on_unmap(addr);
+#endif
+}
+
+static inline void *kmap_atomic(struct page *page)
+{
+	if (IS_ENABLED(CONFIG_PREEMPT_RT))
+		migrate_disable();
+	else
+		preempt_disable();
+	pagefault_disable();
+	return page_address(page);
+}
+
+static inline void __kunmap_atomic(void *addr)
+{
+#ifdef ARCH_HAS_FLUSH_ON_KUNMAP
+	kunmap_flush_on_unmap(addr);
+#endif
+	pagefault_enable();
+	if (IS_ENABLED(CONFIG_PREEMPT_RT))
+		migrate_enable();
+	else
+		preempt_enable();
+}
+
+static inline unsigned int nr_free_highpages(void) { return 0; }
+static inline unsigned long totalhigh_pages(void) { return 0UL; }
+
+#endif /* CONFIG_HIGHMEM */
+
+/*
+ * Prevent people trying to call kunmap_atomic() as if it were kunmap()
+ * kunmap_atomic() should get the return value of kmap_atomic, not the page.
+ */
+#define kunmap_atomic(__addr)					\
+do {								\
+	BUILD_BUG_ON(__same_type((__addr), struct page *));	\
+	__kunmap_atomic(__addr);				\
+} while (0)
+
+#define kunmap_local(__addr)					\
+do {								\
+	BUILD_BUG_ON(__same_type((__addr), struct page *));	\
+	__kunmap_local(__addr);					\
+} while (0)
+
+#endif
diff --git a/include/linux/highmem.h b/include/linux/highmem.h
index 14e6202ce47f..f597830f26b4 100644
--- a/include/linux/highmem.h
+++ b/include/linux/highmem.h
@@ -11,217 +11,137 @@
 
 #include <asm/cacheflush.h>
 
-#ifndef ARCH_HAS_FLUSH_ANON_PAGE
-static inline void flush_anon_page(struct vm_area_struct *vma, struct page *page, unsigned long vmaddr)
-{
-}
-#endif
+#include "highmem-internal.h"
 
-#ifndef ARCH_HAS_FLUSH_KERNEL_DCACHE_PAGE
-static inline void flush_kernel_dcache_page(struct page *page)
-{
-}
-static inline void flush_kernel_vmap_range(void *vaddr, int size)
-{
-}
-static inline void invalidate_kernel_vmap_range(void *vaddr, int size)
-{
-}
-#endif
-
-#include <asm/kmap_types.h>
-
-#ifdef CONFIG_HIGHMEM
-extern void *kmap_atomic_high_prot(struct page *page, pgprot_t prot);
-extern void kunmap_atomic_high(void *kvaddr);
-#include <asm/highmem.h>
-
-#ifndef ARCH_HAS_KMAP_FLUSH_TLB
-static inline void kmap_flush_tlb(unsigned long addr) { }
-#endif
-
-#ifndef kmap_prot
-#define kmap_prot PAGE_KERNEL
-#endif
-
-void *kmap_high(struct page *page);
-static inline void *kmap(struct page *page)
-{
-	void *addr;
-
-	might_sleep();
-	if (!PageHighMem(page))
-		addr = page_address(page);
-	else
-		addr = kmap_high(page);
-	kmap_flush_tlb((unsigned long)addr);
-	return addr;
-}
-
-void kunmap_high(struct page *page);
-
-static inline void kunmap(struct page *page)
-{
-	might_sleep();
-	if (!PageHighMem(page))
-		return;
-	kunmap_high(page);
-}
-
-/*
- * kmap_atomic/kunmap_atomic is significantly faster than kmap/kunmap because
- * no global lock is needed and because the kmap code must perform a global TLB
- * invalidation when the kmap pool wraps.
+/**
+ * kmap - Map a page for long term usage
+ * @page:	Pointer to the page to be mapped
+ *
+ * Returns: The virtual address of the mapping
+ *
+ * Can only be invoked from preemptible task context because on 32bit
+ * systems with CONFIG_HIGHMEM enabled this function might sleep.
  *
- * However when holding an atomic kmap it is not legal to sleep, so atomic
- * kmaps are appropriate for short, tight code paths only.
+ * For systems with CONFIG_HIGHMEM=n and for pages in the low memory area
+ * this returns the virtual address of the direct kernel mapping.
  *
- * The use of kmap_atomic/kunmap_atomic is discouraged - kmap/kunmap
- * gives a more generic (and caching) interface. But kmap_atomic can
- * be used in IRQ contexts, so in some (very limited) cases we need
- * it.
+ * The returned virtual address is globally visible and valid up to the
+ * point where it is unmapped via kunmap(). The pointer can be handed to
+ * other contexts.
+ *
+ * For highmem pages on 32bit systems this can be slow as the mapping space
+ * is limited and protected by a global lock. In case that there is no
+ * mapping slot available the function blocks until a slot is released via
+ * kunmap().
  */
-static inline void *kmap_atomic_prot(struct page *page, pgprot_t prot)
-{
-	preempt_disable();
-	pagefault_disable();
-	if (!PageHighMem(page))
-		return page_address(page);
-	return kmap_atomic_high_prot(page, prot);
-}
-#define kmap_atomic(page)	kmap_atomic_prot(page, kmap_prot)
-
-/* declarations for linux/mm/highmem.c */
-unsigned int nr_free_highpages(void);
-extern atomic_long_t _totalhigh_pages;
-static inline unsigned long totalhigh_pages(void)
-{
-	return (unsigned long)atomic_long_read(&_totalhigh_pages);
-}
-
-static inline void totalhigh_pages_inc(void)
-{
-	atomic_long_inc(&_totalhigh_pages);
-}
-
-static inline void totalhigh_pages_dec(void)
-{
-	atomic_long_dec(&_totalhigh_pages);
-}
-
-static inline void totalhigh_pages_add(long count)
-{
-	atomic_long_add(count, &_totalhigh_pages);
-}
-
-static inline void totalhigh_pages_set(long val)
-{
-	atomic_long_set(&_totalhigh_pages, val);
-}
-
-void kmap_flush_unused(void);
+static inline void *kmap(struct page *page);
 
-struct page *kmap_to_page(void *addr);
-
-#else /* CONFIG_HIGHMEM */
+/**
+ * kunmap - Unmap the virtual address mapped by kmap()
+ * @addr:	Virtual address to be unmapped
+ *
+ * Counterpart to kmap(). A NOOP for CONFIG_HIGHMEM=n and for mappings of
+ * pages in the low memory area.
+ */
+static inline void kunmap(struct page *page);
 
-static inline unsigned int nr_free_highpages(void) { return 0; }
+/**
+ * kmap_to_page - Get the page for a kmap'ed address
+ * @addr:	The address to look up
+ *
+ * Returns: The page which is mapped to @addr.
+ */
+static inline struct page *kmap_to_page(void *addr);
 
-static inline struct page *kmap_to_page(void *addr)
-{
-	return virt_to_page(addr);
-}
+/**
+ * kmap_flush_unused - Flush all unused kmap mappings in order to
+ *		       remove stray mappings
+ */
+static inline void kmap_flush_unused(void);
 
-static inline unsigned long totalhigh_pages(void) { return 0UL; }
+/**
+ * kmap_local_page - Map a page for temporary usage
+ * @page:	Pointer to the page to be mapped
+ *
+ * Returns: The virtual address of the mapping
+ *
+ * Can be invoked from any context.
+ *
+ * Requires careful handling when nesting multiple mappings because the map
+ * management is stack based. The unmap has to be in the reverse order of
+ * the map operation:
+ *
+ * addr1 = kmap_local_page(page1);
+ * addr2 = kmap_local_page(page2);
+ * ...
+ * kunmap_local(addr2);
+ * kunmap_local(addr1);
+ *
+ * Unmapping addr1 before addr2 is invalid and causes malfunction.
+ *
+ * Contrary to kmap() mappings the mapping is only valid in the context of
+ * the caller and cannot be handed to other contexts.
+ *
+ * On CONFIG_HIGHMEM=n kernels and for low memory pages this returns the
+ * virtual address of the direct mapping. Only real highmem pages are
+ * temporarily mapped.
+ *
+ * While it is significantly faster than kmap() for the higmem case it
+ * comes with restrictions about the pointer validity. Only use when really
+ * necessary.
+ *
+ * On HIGHMEM enabled systems mapping a highmem page has the side effect of
+ * disabling migration in order to keep the virtual address stable across
+ * preemption. No caller of kmap_local_page() can rely on this side effect.
+ */
+static inline void *kmap_local_page(struct page *page);
 
-static inline void *kmap(struct page *page)
-{
-	might_sleep();
-	return page_address(page);
-}
+/**
+ * kmap_atomic - Atomically map a page for temporary usage - Deprecated!
+ * @page:	Pointer to the page to be mapped
+ *
+ * Returns: The virtual address of the mapping
+ *
+ * Effectively a wrapper around kmap_local_page() which disables pagefaults
+ * and preemption.
+ *
+ * Do not use in new code. Use kmap_local_page() instead.
+ */
+static inline void *kmap_atomic(struct page *page);
 
-static inline void kunmap_high(struct page *page)
-{
-}
+/**
+ * kunmap_atomic - Unmap the virtual address mapped by kmap_atomic()
+ * @addr:	Virtual address to be unmapped
+ *
+ * Counterpart to kmap_atomic().
+ *
+ * Effectively a wrapper around kunmap_local() which additionally undoes
+ * the side effects of kmap_atomic(), i.e. reenabling pagefaults and
+ * preemption.
+ */
 
-static inline void kunmap(struct page *page)
-{
-#ifdef ARCH_HAS_FLUSH_ON_KUNMAP
-	kunmap_flush_on_unmap(page_address(page));
-#endif
-}
+/* Highmem related interfaces for management code */
+static inline unsigned int nr_free_highpages(void);
+static inline unsigned long totalhigh_pages(void);
 
-static inline void *kmap_atomic(struct page *page)
+#ifndef ARCH_HAS_FLUSH_ANON_PAGE
+static inline void flush_anon_page(struct vm_area_struct *vma, struct page *page, unsigned long vmaddr)
 {
-	preempt_disable();
-	pagefault_disable();
-	return page_address(page);
 }
-#define kmap_atomic_prot(page, prot)	kmap_atomic(page)
-
-static inline void kunmap_atomic_high(void *addr)
-{
-	/*
-	 * Mostly nothing to do in the CONFIG_HIGHMEM=n case as kunmap_atomic()
-	 * handles re-enabling faults + preemption
-	 */
-#ifdef ARCH_HAS_FLUSH_ON_KUNMAP
-	kunmap_flush_on_unmap(addr);
 #endif
-}
-
-#define kmap_atomic_pfn(pfn)	kmap_atomic(pfn_to_page(pfn))
-
-#define kmap_flush_unused()	do {} while(0)
-
-#endif /* CONFIG_HIGHMEM */
-
-#if defined(CONFIG_HIGHMEM) || defined(CONFIG_X86_32)
-
-DECLARE_PER_CPU(int, __kmap_atomic_idx);
 
-static inline int kmap_atomic_idx_push(void)
+#ifndef ARCH_HAS_FLUSH_KERNEL_DCACHE_PAGE
+static inline void flush_kernel_dcache_page(struct page *page)
 {
-	int idx = __this_cpu_inc_return(__kmap_atomic_idx) - 1;
-
-#ifdef CONFIG_DEBUG_HIGHMEM
-	WARN_ON_ONCE(in_irq() && !irqs_disabled());
-	BUG_ON(idx >= KM_TYPE_NR);
-#endif
-	return idx;
 }
-
-static inline int kmap_atomic_idx(void)
+static inline void flush_kernel_vmap_range(void *vaddr, int size)
 {
-	return __this_cpu_read(__kmap_atomic_idx) - 1;
 }
-
-static inline void kmap_atomic_idx_pop(void)
+static inline void invalidate_kernel_vmap_range(void *vaddr, int size)
 {
-#ifdef CONFIG_DEBUG_HIGHMEM
-	int idx = __this_cpu_dec_return(__kmap_atomic_idx);
-
-	BUG_ON(idx < 0);
-#else
-	__this_cpu_dec(__kmap_atomic_idx);
-#endif
 }
-
 #endif
 
-/*
- * Prevent people trying to call kunmap_atomic() as if it were kunmap()
- * kunmap_atomic() should get the return value of kmap_atomic, not the page.
- */
-#define kunmap_atomic(addr)                                     \
-do {                                                            \
-	BUILD_BUG_ON(__same_type((addr), struct page *));       \
-	kunmap_atomic_high(addr);                                  \
-	pagefault_enable();                                     \
-	preempt_enable();                                       \
-} while (0)
-
-
 /* when CONFIG_HIGHMEM is not set these will be plain clear/copy_page */
 #ifndef clear_user_highpage
 static inline void clear_user_highpage(struct page *page, unsigned long vaddr)
diff --git a/include/linux/interrupt.h b/include/linux/interrupt.h
index ee8299eb1f52..3a6e2dae74f5 100644
--- a/include/linux/interrupt.h
+++ b/include/linux/interrupt.h
@@ -75,6 +75,11 @@
 #define IRQF_EARLY_RESUME	0x00020000
 #define IRQF_COND_SUSPEND	0x00040000
 
+/* PFCxxx Flag to lookup IRQ Thread priority in table
+ * defined in kernel/irq/wsysinit-prio-table.c
+ */
+#define IRQF_THREAD_TBL_LOOKUP 0x00080000
+
 #define IRQF_TIMER		(__IRQF_TIMER | IRQF_NO_SUSPEND | IRQF_NO_THREAD)
 
 /*
@@ -560,7 +565,7 @@ struct softirq_action
 asmlinkage void do_softirq(void);
 asmlinkage void __do_softirq(void);
 
-#ifdef __ARCH_HAS_DO_SOFTIRQ
+#if defined(__ARCH_HAS_DO_SOFTIRQ) && !defined(CONFIG_PREEMPT_RT)
 void do_softirq_own_stack(void);
 #else
 static inline void do_softirq_own_stack(void)
@@ -654,26 +659,21 @@ enum
 	TASKLET_STATE_RUN	/* Tasklet is running (SMP only) */
 };
 
-#ifdef CONFIG_SMP
+#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT_RT)
 static inline int tasklet_trylock(struct tasklet_struct *t)
 {
 	return !test_and_set_bit(TASKLET_STATE_RUN, &(t)->state);
 }
 
-static inline void tasklet_unlock(struct tasklet_struct *t)
-{
-	smp_mb__before_atomic();
-	clear_bit(TASKLET_STATE_RUN, &(t)->state);
-}
+void tasklet_unlock(struct tasklet_struct *t);
+void tasklet_unlock_wait(struct tasklet_struct *t);
+void tasklet_unlock_spin_wait(struct tasklet_struct *t);
 
-static inline void tasklet_unlock_wait(struct tasklet_struct *t)
-{
-	while (test_bit(TASKLET_STATE_RUN, &(t)->state)) { barrier(); }
-}
 #else
-#define tasklet_trylock(t) 1
-#define tasklet_unlock_wait(t) do { } while (0)
-#define tasklet_unlock(t) do { } while (0)
+static inline int tasklet_trylock(struct tasklet_struct *t) { return 1; }
+static inline void tasklet_unlock(struct tasklet_struct *t) { }
+static inline void tasklet_unlock_wait(struct tasklet_struct *t) { }
+static inline void tasklet_unlock_spin_wait(struct tasklet_struct *t) { }
 #endif
 
 extern void __tasklet_schedule(struct tasklet_struct *t);
@@ -698,6 +698,17 @@ static inline void tasklet_disable_nosync(struct tasklet_struct *t)
 	smp_mb__after_atomic();
 }
 
+/*
+ * Do not use in new code. Disabling tasklets from atomic contexts is
+ * error prone and should be avoided.
+ */
+static inline void tasklet_disable_in_atomic(struct tasklet_struct *t)
+{
+	tasklet_disable_nosync(t);
+	tasklet_unlock_spin_wait(t);
+	smp_mb();
+}
+
 static inline void tasklet_disable(struct tasklet_struct *t)
 {
 	tasklet_disable_nosync(t);
diff --git a/include/linux/io-mapping.h b/include/linux/io-mapping.h
index c75e4d3d8833..4bb8223f2f82 100644
--- a/include/linux/io-mapping.h
+++ b/include/linux/io-mapping.h
@@ -60,22 +60,20 @@ io_mapping_fini(struct io_mapping *mapping)
 	iomap_free(mapping->base, mapping->size);
 }
 
-/* Atomic map/unmap */
+/* Temporary mappings which are only valid in the current context */
 static inline void __iomem *
-io_mapping_map_atomic_wc(struct io_mapping *mapping,
-			 unsigned long offset)
+io_mapping_map_local_wc(struct io_mapping *mapping, unsigned long offset)
 {
 	resource_size_t phys_addr;
 
 	BUG_ON(offset >= mapping->size);
 	phys_addr = mapping->base + offset;
-	return iomap_atomic_prot_pfn(PHYS_PFN(phys_addr), mapping->prot);
+	return __iomap_local_pfn_prot(PHYS_PFN(phys_addr), mapping->prot);
 }
 
-static inline void
-io_mapping_unmap_atomic(void __iomem *vaddr)
+static inline void io_mapping_unmap_local(void __iomem *vaddr)
 {
-	iounmap_atomic(vaddr);
+	kunmap_local_indexed((void __force *)vaddr);
 }
 
 static inline void __iomem *
@@ -97,7 +95,7 @@ io_mapping_unmap(void __iomem *vaddr)
 	iounmap(vaddr);
 }
 
-#else
+#else  /* HAVE_ATOMIC_IOMAP */
 
 #include <linux/uaccess.h>
 
@@ -144,25 +142,19 @@ io_mapping_unmap(void __iomem *vaddr)
 {
 }
 
-/* Atomic map/unmap */
+/* Temporary mappings which are only valid in the current context */
 static inline void __iomem *
-io_mapping_map_atomic_wc(struct io_mapping *mapping,
-			 unsigned long offset)
+io_mapping_map_local_wc(struct io_mapping *mapping, unsigned long offset)
 {
-	preempt_disable();
-	pagefault_disable();
 	return io_mapping_map_wc(mapping, offset, PAGE_SIZE);
 }
 
-static inline void
-io_mapping_unmap_atomic(void __iomem *vaddr)
+static inline void io_mapping_unmap_local(void __iomem *vaddr)
 {
 	io_mapping_unmap(vaddr);
-	pagefault_enable();
-	preempt_enable();
 }
 
-#endif /* HAVE_ATOMIC_IOMAP */
+#endif /* !HAVE_ATOMIC_IOMAP */
 
 static inline struct io_mapping *
 io_mapping_create_wc(resource_size_t base,
diff --git a/include/linux/irq_cpustat.h b/include/linux/irq_cpustat.h
deleted file mode 100644
index 6e8895cd4d92..000000000000
--- a/include/linux/irq_cpustat.h
+++ /dev/null
@@ -1,28 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0 */
-#ifndef __irq_cpustat_h
-#define __irq_cpustat_h
-
-/*
- * Contains default mappings for irq_cpustat_t, used by almost every
- * architecture.  Some arch (like s390) have per cpu hardware pages and
- * they define their own mappings for irq_stat.
- *
- * Keith Owens <kaos@ocs.com.au> July 2000.
- */
-
-
-/*
- * Simple wrappers reducing source bloat.  Define all irq_stat fields
- * here, even ones that are arch dependent.  That way we get common
- * definitions instead of differing sets for each arch.
- */
-
-#ifndef __ARCH_IRQ_STAT
-DECLARE_PER_CPU_ALIGNED(irq_cpustat_t, irq_stat);	/* defined in asm/hardirq.h */
-#define __IRQ_STAT(cpu, member)	(per_cpu(irq_stat.member, cpu))
-#endif
-
-/* arch dependent irq_stat fields */
-#define nmi_count(cpu)		__IRQ_STAT((cpu), __nmi_count)	/* i386 */
-
-#endif	/* __irq_cpustat_h */
diff --git a/include/linux/irq_work.h b/include/linux/irq_work.h
index 30823780c192..f941f2d7d71c 100644
--- a/include/linux/irq_work.h
+++ b/include/linux/irq_work.h
@@ -55,4 +55,10 @@ static inline void irq_work_run(void) { }
 static inline void irq_work_single(void *arg) { }
 #endif
 
+#if defined(CONFIG_IRQ_WORK) && defined(CONFIG_PREEMPT_RT)
+void irq_work_tick_soft(void);
+#else
+static inline void irq_work_tick_soft(void) { }
+#endif
+
 #endif /* _LINUX_IRQ_WORK_H */
diff --git a/include/linux/irqdesc.h b/include/linux/irqdesc.h
index 5745491303e0..2b9caf39fb07 100644
--- a/include/linux/irqdesc.h
+++ b/include/linux/irqdesc.h
@@ -68,6 +68,7 @@ struct irq_desc {
 	unsigned int		irqs_unhandled;
 	atomic_t		threads_handled;
 	int			threads_handled_last;
+	u64			random_ip;
 	raw_spinlock_t		lock;
 	struct cpumask		*percpu_enabled;
 	const struct cpumask	*percpu_affinity;
diff --git a/include/linux/irqflags.h b/include/linux/irqflags.h
index 3ed4e8771b64..a437b2e70d37 100644
--- a/include/linux/irqflags.h
+++ b/include/linux/irqflags.h
@@ -71,14 +71,6 @@ do {						\
 do {						\
 	__this_cpu_dec(hardirq_context);	\
 } while (0)
-# define lockdep_softirq_enter()		\
-do {						\
-	current->softirq_context++;		\
-} while (0)
-# define lockdep_softirq_exit()			\
-do {						\
-	current->softirq_context--;		\
-} while (0)
 
 # define lockdep_hrtimer_enter(__hrtimer)		\
 ({							\
@@ -140,6 +132,21 @@ do {						\
 # define lockdep_irq_work_exit(__work)		do { } while (0)
 #endif
 
+#if defined(CONFIG_TRACE_IRQFLAGS) && !defined(CONFIG_PREEMPT_RT)
+# define lockdep_softirq_enter()		\
+do {						\
+	current->softirq_context++;		\
+} while (0)
+# define lockdep_softirq_exit()			\
+do {						\
+	current->softirq_context--;		\
+} while (0)
+
+#else
+# define lockdep_softirq_enter()		do { } while (0)
+# define lockdep_softirq_exit()			do { } while (0)
+#endif
+
 #if defined(CONFIG_IRQSOFF_TRACER) || \
 	defined(CONFIG_PREEMPT_TRACER)
  extern void stop_critical_timings(void);
diff --git a/include/linux/kernel.h b/include/linux/kernel.h
index 2f05e9128201..2cff7554395d 100644
--- a/include/linux/kernel.h
+++ b/include/linux/kernel.h
@@ -204,6 +204,7 @@ extern int _cond_resched(void);
 extern void ___might_sleep(const char *file, int line, int preempt_offset);
 extern void __might_sleep(const char *file, int line, int preempt_offset);
 extern void __cant_sleep(const char *file, int line, int preempt_offset);
+extern void __cant_migrate(const char *file, int line);
 
 /**
  * might_sleep - annotation for functions that can sleep
@@ -219,6 +220,10 @@ extern void __cant_sleep(const char *file, int line, int preempt_offset);
  */
 # define might_sleep() \
 	do { __might_sleep(__FILE__, __LINE__, 0); might_resched(); } while (0)
+
+# define might_sleep_no_state_check() \
+	do { ___might_sleep(__FILE__, __LINE__, 0); might_resched(); } while (0)
+
 /**
  * cant_sleep - annotation for functions that cannot sleep
  *
@@ -227,6 +232,18 @@ extern void __cant_sleep(const char *file, int line, int preempt_offset);
 # define cant_sleep() \
 	do { __cant_sleep(__FILE__, __LINE__, 0); } while (0)
 # define sched_annotate_sleep()	(current->task_state_change = 0)
+
+/**
+ * cant_migrate - annotation for functions that cannot migrate
+ *
+ * Will print a stack trace if executed in code which is migratable
+ */
+# define cant_migrate()							\
+	do {								\
+		if (IS_ENABLED(CONFIG_SMP))				\
+			__cant_migrate(__FILE__, __LINE__);		\
+	} while (0)
+
 /**
  * non_block_start - annotate the start of section where sleeping is prohibited
  *
@@ -250,7 +267,9 @@ extern void __cant_sleep(const char *file, int line, int preempt_offset);
   static inline void __might_sleep(const char *file, int line,
 				   int preempt_offset) { }
 # define might_sleep() do { might_resched(); } while (0)
+# define might_sleep_no_state_check() do { might_resched(); } while (0)
 # define cant_sleep() do { } while (0)
+# define cant_migrate()		do { } while (0)
 # define sched_annotate_sleep() do { } while (0)
 # define non_block_start() do { } while (0)
 # define non_block_end() do { } while (0)
@@ -258,13 +277,6 @@ extern void __cant_sleep(const char *file, int line, int preempt_offset);
 
 #define might_sleep_if(cond) do { if (cond) might_sleep(); } while (0)
 
-#ifndef CONFIG_PREEMPT_RT
-# define cant_migrate()		cant_sleep()
-#else
-  /* Placeholder for now */
-# define cant_migrate()		do { } while (0)
-#endif
-
 /**
  * abs - return absolute value of an argument
  * @x: the value.  If it is unsigned type, it is converted to signed type first.
diff --git a/include/linux/kmsg_dump.h b/include/linux/kmsg_dump.h
index 3378bcbe585e..86673930c8ea 100644
--- a/include/linux/kmsg_dump.h
+++ b/include/linux/kmsg_dump.h
@@ -29,6 +29,18 @@ enum kmsg_dump_reason {
 	KMSG_DUMP_MAX
 };
 
+/**
+ * struct kmsg_dumper_iter - iterator for kernel crash message dumper
+ * @active:	Flag that specifies if this is currently dumping
+ * @cur_seq:	Points to the oldest message to dump (private)
+ * @next_seq:	Points after the newest message to dump (private)
+ */
+struct kmsg_dumper_iter {
+	bool	active;
+	u64	cur_seq;
+	u64	next_seq;
+};
+
 /**
  * struct kmsg_dumper - kernel crash message dumper structure
  * @list:	Entry in the dumper list (private)
@@ -39,33 +51,22 @@ enum kmsg_dump_reason {
  */
 struct kmsg_dumper {
 	struct list_head list;
-	void (*dump)(struct kmsg_dumper *dumper, enum kmsg_dump_reason reason);
+	void (*dump)(struct kmsg_dumper *dumper, enum kmsg_dump_reason reason,
+		     struct kmsg_dumper_iter *iter);
 	enum kmsg_dump_reason max_reason;
-	bool active;
 	bool registered;
-
-	/* private state of the kmsg iterator */
-	u32 cur_idx;
-	u32 next_idx;
-	u64 cur_seq;
-	u64 next_seq;
 };
 
 #ifdef CONFIG_PRINTK
 void kmsg_dump(enum kmsg_dump_reason reason);
 
-bool kmsg_dump_get_line_nolock(struct kmsg_dumper *dumper, bool syslog,
-			       char *line, size_t size, size_t *len);
-
-bool kmsg_dump_get_line(struct kmsg_dumper *dumper, bool syslog,
+bool kmsg_dump_get_line(struct kmsg_dumper_iter *iter, bool syslog,
 			char *line, size_t size, size_t *len);
 
-bool kmsg_dump_get_buffer(struct kmsg_dumper *dumper, bool syslog,
-			  char *buf, size_t size, size_t *len);
-
-void kmsg_dump_rewind_nolock(struct kmsg_dumper *dumper);
+bool kmsg_dump_get_buffer(struct kmsg_dumper_iter *iter, bool syslog,
+			  char *buf, size_t size, size_t *len_out);
 
-void kmsg_dump_rewind(struct kmsg_dumper *dumper);
+void kmsg_dump_rewind(struct kmsg_dumper_iter *iter);
 
 int kmsg_dump_register(struct kmsg_dumper *dumper);
 
@@ -77,30 +78,19 @@ static inline void kmsg_dump(enum kmsg_dump_reason reason)
 {
 }
 
-static inline bool kmsg_dump_get_line_nolock(struct kmsg_dumper *dumper,
-					     bool syslog, const char *line,
-					     size_t size, size_t *len)
-{
-	return false;
-}
-
-static inline bool kmsg_dump_get_line(struct kmsg_dumper *dumper, bool syslog,
+static inline bool kmsg_dump_get_line(struct kmsg_dumper_iter *iter, bool syslog,
 				const char *line, size_t size, size_t *len)
 {
 	return false;
 }
 
-static inline bool kmsg_dump_get_buffer(struct kmsg_dumper *dumper, bool syslog,
+static inline bool kmsg_dump_get_buffer(struct kmsg_dumper_iter *iter, bool syslog,
 					char *buf, size_t size, size_t *len)
 {
 	return false;
 }
 
-static inline void kmsg_dump_rewind_nolock(struct kmsg_dumper *dumper)
-{
-}
-
-static inline void kmsg_dump_rewind(struct kmsg_dumper *dumper)
+static inline void kmsg_dump_rewind(struct kmsg_dumper_iter *iter)
 {
 }
 
diff --git a/include/linux/ksz8863.h b/include/linux/ksz8863.h
new file mode 100644
index 000000000000..478ad2af6a89
--- /dev/null
+++ b/include/linux/ksz8863.h
@@ -0,0 +1,347 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/* Micrel ksz8863 common definitions
+ *
+ * Copyright (c) 2019 WAGO Kontakttechnik GmbH & Co. KG
+ */
+
+#ifndef __KSZ8863_H__
+#define __KSZ8863_H__
+
+#include <linux/device.h>
+#include <net/dsa.h>
+#include <linux/phy.h>
+#include <linux/gpio/consumer.h>
+#include <linux/mutex.h>
+
+#define KSZ8863_NUM_PORTS 3
+#define KSZ8863_PORT_OFFSET KSZ8863_EXT_PORT1_ID
+
+#define KSZ8863_EXT_PORTS 2
+
+#define KSZ8863_EXT_PORT1_ID 1
+#define KSZ8863_EXT_PORT2_ID 2
+
+// clang-format off
+#define KSZ8863_REG_CHIP_ID0			0x00
+#define KSZ8863_REG_CHIP_ID0_FAMILY_ID			0x88
+
+#define KSZ8863_REG_CHIP_ID1			0x01
+
+#define KSZ8863_REG_CHIP_ID1_ID_MASK			0xF0
+#define KSZ8863_REG_CHIP_ID1_ID_SHIFT			4
+#define KSZ8863_REG_CHIP_ID1_ID				0x03
+#define KSZ8863_REG_CHIP_ID1_REV_ID_MASK		0x0E
+#define KSZ8863_REG_CHIP_ID1_REV_ID_SHIFT		1
+#define KSZ8863_REG_CHIP_ID1_START			0x01
+
+#define KSZ8863_REG_GL_CTRL0			0x02
+
+#define KSZ8863_REG_GL_CTRL0_NEW_BACKOFF		BIT(7)
+#define KSZ8863_REG_GL_CTRL0_FLUSH_DYN_MAC_TABLE	BIT(5)
+#define KSZ8863_REG_GL_CTRL0_FLUSH_STA_MAC_TABLE	BIT(4)
+#define KSZ8863_REG_GL_CTRL0_PASS_PAUSE			BIT(3)
+#define KSZ8863_REG_GL_CTRL0_LINK_AUTO_AGING		BIT(0)
+
+#define KSZ8863_REG_GL_CTRL1			0x03
+
+#define KSZ8863_REG_GL_CTRL1_PASS_ALL			BIT(7)
+#define KSZ8863_REG_GL_CTRL1_TAIL_TAG_ENABLE		BIT(6)
+#define KSZ8863_REG_GL_CTRL1_TX_FLOW_CTRL		BIT(5)
+#define KSZ8863_REG_GL_CTRL1_RX_FLOW_CTRL		BIT(4)
+#define KSZ8863_REG_GL_CTRL1_CHECK_LENGTH		BIT(3)
+#define KSZ8863_REG_GL_CTRL1_AGING_ENABLE		BIT(2)
+#define KSZ8863_REG_GL_CTRL1_FAST_AGING			BIT(1)
+#define KSZ8863_REG_GL_CTRL1_AGGR_BACKOFF		BIT(0)
+
+#define KSZ8863_REG_GL_CTRL2			0x04
+
+#define KSZ8863_REG_GL_CTRL2_UNICAST_VLAN_BOUNDARY	BIT(7)
+#define KSZ8863_REG_GL_CTRL2_MULTICAST_STORM_DISABLE	BIT(6)
+#define KSZ8863_REG_GL_CTRL2_BACK_PRESSURE		BIT(5)
+#define KSZ8863_REG_GL_CTRL2_FAIR_FLOW_CTRL		BIT(4)
+#define KSZ8863_REG_GL_CTRL2_NO_EXC_COLLISION_DROP	BIT(3)
+#define KSZ8863_REG_GL_CTRL2_HUGE_PACKET		BIT(2)
+#define KSZ8863_REG_GL_CTRL2_LEGAL_PACKET		BIT(1)
+
+#define KSZ8863_REG_GL_CTRL3			0x05
+
+#define KSZ8863_REG_GL_CTRL3_VLAN_ENABLE		BIT(7)
+#define KSZ8863_REG_GL_CTRL3_IGMP_SNOOP			BIT(6)
+#define KSZ8863_REG_GL_CTRL3_WEIGHTED_FAIR_QUEUE_ENABLE	BIT(3)
+#define KSZ8863_REG_GL_CTRL3_MIRROR_RX_TX		BIT(0)
+
+#define KSZ8863_REG_GL_CTRL4			0x06
+
+#define KSZ8863_REG_GL_CTRL4_HALF_DUPLEX		BIT(6)
+#define KSZ8863_REG_GL_CTRL4_FLOW_CTRL			BIT(5)
+#define KSZ8863_REG_GL_CTRL4_10_MBIT			BIT(4)
+#define KSZ8863_REG_GL_CTRL4_REPLACE_VID		BIT(3)
+#define KSZ8863_REG_GL_CTRL4_BCAST_STORM_RATE_HI_MASK	0x07
+
+#define KSZ8863_REG_GL_CTRL5			0x07
+
+#define KSZ8863_REG_GL_CTRL5_BCAST_STORM_RATE_LO_MASK	0xFF
+
+#define KSZ8863_REG_GL_CTRL9			0x0B
+
+#define KSZ8863_REG_GL_CTRL9_SPI_CLK_125_MHZ		0x80
+#define KSZ8863_REG_GL_CTRL9_SPI_CLK_62_5_MHZ		0x40
+#define KSZ8863_REG_GL_CTRL9_SPI_CLK_31_25_MHZ		0x00
+
+#define KSZ8863_REG_GL_CTRL10			0x0C
+#define KSZ8863_REG_GL_CTRL11			0x0D
+
+#define KSZ8863_REG_GL_CTRL12			0x0E
+
+#define KSZ8863_REG_GL_CTRL12_UNKNOWN_DA_ENABLE		BIT(7)
+#define KSZ8863_REG_GL_CTRL12_DRIVER_16MA		BIT(6)
+#define KSZ8863_REG_GL_CTRL12_UNKNOWN_DA_2_PORT3	BIT(2)
+#define KSZ8863_REG_GL_CTRL12_UNKNOWN_DA_2_PORT2	BIT(1)
+#define KSZ8863_REG_GL_CTRL12_UNKNOWN_DA_2_PORT1	BIT(0)
+#define KSZ8863_REG_GL_CTRL12_PORT_MASK			0x7
+
+#define KSZ8863_REG_GL_CTRL13			0x0F
+
+#define KSZ8863_REG_GL_CTRL13_PORT_PHY_ADDR_MASK	0xF8
+#define KSZ8863_REG_GL_CTRL13_PORT_PHY_ADDR_SHIFT	3
+
+#define KSZ8863_REG_PORTS_BASE			0x10
+
+#define KSZ8863_REG_PORT1_CTRL0			0x10
+#define KSZ8863_REG_PORT2_CTRL0			0x20
+#define KSZ8863_REG_PORT3_CTRL0			0x30
+
+#define KSZ8863_REG_PORT_CTRL0_BROADCAST_STORM		BIT(7)
+#define KSZ8863_REG_PORT_CTRL0_DIFFSERV_ENABLE		BIT(6)
+#define KSZ8863_REG_PORT_CTRL0_802_1P_ENABLE		BIT(5)
+#define KSZ8863_REG_PORT_CTRL0_PRIO_MASK		0x18
+#define KSZ8863_REG_PORT_CTRL0_PRIO_SHIFT		3
+#define KSZ8863_REG_PORT_CTRL0_PRIO_0			(0 << KSZ8863_REG_PORT_CTRL_PRIO_SHIFT)
+#define KSZ8863_REG_PORT_CTRL0_PRIO_1			(1 << KSZ8863_REG_PORT_CTRL_PRIO_SHIFT)
+#define KSZ8863_REG_PORT_CTRL0_PRIO_2			(2 << KSZ8863_REG_PORT_CTRL_PRIO_SHIFT)
+#define KSZ8863_REG_PORT_CTRL0_PRIO_3			(3 << KSZ8863_REG_PORT_CTRL_PRIO_SHIFT)
+#define KSZ8863_REG_PORT_CTRL0_INSERT_TAG		BIT(2)
+#define KSZ8863_REG_PORT_CTRL0_REMOVE_TAG		BIT(1)
+#define KSZ8863_REG_PORT_CTRL0_4_PRIOS_ENABLE		BIT(0)
+
+#define KSZ8863_REG_PORT1_CTRL1			0x11
+#define KSZ8863_REG_PORT2_CTRL1			0x21
+#define KSZ8863_REG_PORT3_CTRL1			0x31
+
+#define KSZ8863_REG_PORT_CTRL1_MIRROR_SNIFFER		BIT(7)
+#define KSZ8863_REG_PORT_CTRL1_MIRROR_RX		BIT(6)
+#define KSZ8863_REG_PORT_CTRL1_MIRROR_TX		BIT(5)
+#define KSZ8863_REG_PORT_CTRL1_MIRROR_MASK		0xE0
+#define KSZ8863_REG_PORT_CTRL1_DOUBLE_TAG		BIT(4)
+#define KSZ8863_REG_PORT_CTRL1_802_1P_REMAPPING		BIT(3)
+#define KSZ8863_REG_PORT_CTRL1_VLAN_MEMBERSHIP		0x07
+
+#define KSZ8863_PORT_CTL1_VLAN_MEMBERSHIP_PORT1		BIT(0)
+#define KSZ8863_PORT_CTL1_VLAN_MEMBERSHIP_PORT2		BIT(1)
+#define KSZ8863_PORT_CTL1_VLAN_MEMBERSHIP_PORT3		BIT(2)
+
+#define KSZ8863_REG_PORT1_CTRL2			0x12
+#define KSZ8863_REG_PORT2_CTRL2			0x22
+#define KSZ8863_REG_PORT3_CTRL2			0x32
+
+#define KSZ8863_REG_PORT_CTRL2_2_PRIORITIES_ENABLE	BIT(7)
+#define KSZ8863_REG_PORT_CTRL2_INGRESS_FILTER		BIT(6)
+#define KSZ8863_REG_PORT_CTRL2_DISCARD_NON_VID		BIT(5)
+#define KSZ8863_REG_PORT_CTRL2_FORCE_FLOW_CTRL		BIT(4)
+#define KSZ8863_REG_PORT_CTRL2_BACK_PRESSURE		BIT(3)
+#define KSZ8863_REG_PORT_CTRL2_TX_ENABLE		BIT(2)
+#define KSZ8863_REG_PORT_CTRL2_RX_ENABLE		BIT(1)
+#define KSZ8863_REG_PORT_CTRL2_LEARN_DISABLE		BIT(0)
+
+#define KSZ8863_REG_PORT1_CTRL5			0x15
+#define KSZ8863_REG_PORT2_CTRL5			0x25
+#define KSZ8863_REG_PORT3_CTRL5			0x35
+
+#define KSZ8863_REG_PORT_CTRL5_3_MII_MAC_MODE		BIT(7)
+#define KSZ8863_REG_PORT_CTRL5_SA_MAC2			BIT(6)
+#define KSZ8863_REG_PORT_CTRL5_SA_MAC1			BIT(5)
+#define KSZ8863_REG_PORT_CTRL5_DROP_TAG			BIT(4)
+#define KSZ8863_REG_PORT_CTRL5_INGRESS_LIMIT_MODE_MAS	0x0C
+#define KSZ8863_REG_PORT_CTRL5_INGRESS_ALL		0x00
+#define KSZ8863_REG_PORT_CTRL5_INGRESS_UNICAST		0x04
+#define KSZ8863_REG_PORT_CTRL5_INGRESS_MULTICAST	0x08
+#define KSZ8863_REG_PORT_CTRL5_INGRESS_BROADCAST	0x0C
+#define KSZ8863_REG_PORT_CTRL5_COUNT_IFG		BIT(1)
+#define KSZ8863_REG_PORT_CTRL5_COUNT_PREAMBLE		BIT(0)
+
+#define KSZ8863_REG_PORT1_CTRL13		0x1D
+#define KSZ8863_REG_PORT2_CTRL13		0x2D
+
+#define KSZ8863_REG_PORT_CTRL13_LED_OFF			BIT(7)
+#define KSZ8863_REG_PORT_CTRL13_TX_DISABLE		BIT(6)
+#define KSZ8863_REG_PORT_CTRL13_AUTO_NEG_RESTART	BIT(5)
+#define KSZ8863_REG_PORT_CTRL13_REMOTE_FAULT_DISABLE	BIT(4)
+#define KSZ8863_REG_PORT_CTRL13_POWER_DOWN		BIT(3)
+#define KSZ8863_REG_PORT_CTRL13_AUTO_MDIX_DISABLE	BIT(2)
+#define KSZ8863_REG_PORT_CTRL13_FORCE_MDIX		BIT(1)
+#define KSZ8863_REG_PORT_CTRL13_LOOPBACK		BIT(0)
+
+#define KSZ8863_REG_INDIRECT_ACCESS_CTRL0	0x79
+
+#define KSZ8863_REG_INDIRECT_ACCESS_CTRL0_READ		BIT(4)
+#define KSZ8863_REG_INDIRECT_ACCESS_CTRL0_WRITE		0
+#define KSZ8863_REG_INDIRECT_ACCESS_CTRL0_TBL_SHIFT	2
+/* table select see ksz8863_table enum */
+#define KSZ8863_REG_INDIRECT_ACCESS_CTRL0_ADDR_MASK	0x3
+
+#define KSZ8863_REG_INDIRECT_ACCESS_CTRL1	0x7A
+
+#define KSZ8863_REG_INDIRECT_DATA_REG8		0x7B
+
+#define KSZ8863_REG_INDIRECT_DATA_REG8_CPU_READ_WIP	BIT(7)
+#define KSZ8863_REG_INDIRECT_DATA_REG8_DATA_MASK	0x7
+
+#define KSZ8863_REG_INDIRECT_DATA_REG7		0x7C
+#define KSZ8863_REG_INDIRECT_DATA_REG6		0x7D
+#define KSZ8863_REG_INDIRECT_DATA_REG5		0x7E
+#define KSZ8863_REG_INDIRECT_DATA_REG4		0x7F
+#define KSZ8863_REG_INDIRECT_DATA_REG3		0x80
+#define KSZ8863_REG_INDIRECT_DATA_REG2		0x81
+#define KSZ8863_REG_INDIRECT_DATA_REG1		0x82
+#define KSZ8863_REG_INDIRECT_DATA_REG0		0x83
+
+#define KSZ8863_REG_INDIRECT_DATA_MAX_SIZE	9
+
+#define KSZ8863_REG_PORT1_EGRESS_RL_Q0		0x9A
+#define KSZ8863_REG_PORT2_EGRESS_RL_Q0		0x9E
+#define KSZ8863_REG_PORT3_EGRESS_RL_Q0		0xA2
+
+#define KSZ8863_REG_PORT1_EGRESS_RL_Q1		0x9B
+#define KSZ8863_REG_PORT2_EGRESS_RL_Q1		0x9F
+#define KSZ8863_REG_PORT3_EGRESS_RL_Q1		0xA3
+
+#define KSZ8863_REG_PORT1_EGRESS_RL_Q2		0x9C
+#define KSZ8863_REG_PORT2_EGRESS_RL_Q2		0xA0
+#define KSZ8863_REG_PORT3_EGRESS_RL_Q2		0xA4
+
+#define KSZ8863_REG_PORT1_EGRESS_RL_Q3		0x9D
+#define KSZ8863_REG_PORT2_EGRESS_RL_Q3		0xA1
+#define KSZ8863_REG_PORT3_EGRESS_RL_Q3		0xA5
+
+#define KSZ8863_REG_PORT_EGRESS_RL_Q0_ENABLE		BIT(7)
+#define KSZ8863_REG_PORT_EGRESS_RL_MASK			0x7F
+
+#define KSZ8863_REG_INTERNAL_1V8_LDO_CTRL	0xC1
+#define KSZ8863_REG_INTERNAL_1V8_LDO_CTRL_DISABLE	BIT(6)
+
+#define KSZ8863_REG_PWR_LED			0xC3
+#define KSZ8863_REG_PWR_LED_CPU_IF_PD			BIT(7)
+#define KSZ8863_REG_PWR_LED_SW_PD			BIT(6)
+#define KSZ8863_REG_PWR_LED_LED_MODE_SEL_MASK		0x30
+#define KSZ8863_REG_PWR_LED_LED_MODE_SEL_SHIFT		4
+#define KSZ8863_REG_PWR_LED_LED_MODE_0			0
+#define KSZ8863_REG_PWR_LED_LED_MODE_1			1
+#define KSZ8863_REG_PWR_LED_LED_MODE_2			2
+#define KSZ8863_REG_PWR_LED_LED_MODE_3			3
+#define KSZ8863_REG_PWR_LED_LED_OUT			BIT(3)
+#define KSZ8863_REG_PWR_LED_PLL_PD			BIT(2)
+#define KSZ8863_REG_PWR_LED_PWR_MODE_MASK		0x03
+#define KSZ8863_REG_PWR_LED_PWR_MODE_SEL_SHIFT		0
+
+// clang-format on
+
+struct ksz8863_port {
+	struct net_device *bridge;
+};
+
+struct ksz8863_chip {
+	/* The dsa_switch this structure is related to */
+	struct dsa_switch *ds;
+
+	/* The device this structure is associated to */
+	struct device *dev;
+
+	/* protection for switch register access */
+	struct mutex reg_lock;
+
+	/* The MII bus and the address on the bus that is used to
+	 * communication with the switch
+	 */
+	struct mii_bus *sw_bus;
+	u8 sw_addr;
+	const struct ksz8863_bus_ops *mii_ops;
+
+	/* A switch may have a GPIO line tied to its reset pin. Parse
+	 * this from the device tree, and use it before performing
+	 * switch soft reset.
+	 */
+	struct gpio_desc *reset_gpio;
+	bool reset;
+
+	/* If true driver will disable internal 1.8V LDO */
+	bool disable_internal_ldo;
+
+	/* Port information */
+	struct ksz8863_port ports[KSZ8863_NUM_PORTS];
+
+	/* switched / separated mode */
+	bool switched;
+
+	/* device mac address */
+	u8 eth_addr[ETH_ALEN];
+
+	/* directory within debugfs */
+	struct dentry *debugfs_root_entry;
+
+	/* switch ops */
+	struct ks8863_ops {
+		int (*flush_dyn_mac_table)(struct ksz8863_chip *chip);
+	} ops;
+};
+
+enum ksz8863_tables {
+	KSZ8863_TBL_STATIC_MAC,
+	KSZ8863_TBL_VLAN,
+	KSZ8863_TBL_DYN_MAC,
+	KSZ8863_TBL_MIB_CNT,
+};
+
+/* this struct is designed for easy coping to
+ * ksz8863 inderect data register. Do not alter
+ * the fields order!
+ */
+struct ksz8863_static_mac_tbl_entry {
+	u8 mac[ETH_ALEN];
+	u16 forward_ports:3;
+	u16 valid:1;
+	u16 override:1;
+	u16 use_fid:1;
+	u16 fid:4;
+	u16:6;
+} __packed;
+
+/* this struct is designed for easy coping to
+ * ksz8863 inderect data register. Do not alter
+ * the fields order!
+ */
+struct ksz8863_dyn_mac_tbl_entry {
+	u8 mac[ETH_ALEN];
+	u8 fid:4;
+	u8 port:2;
+	u8 time:2;
+	u16 valid_nb:10;
+	u16 empty:1;
+	u16:4;
+	u16 nrdy:1;
+} __packed;
+
+#define KSZ8863_SMAC_ENTRY_FWD_PORTS_PORT1	0x1
+#define KSZ8863_SMAC_ENTRY_FWD_PORTS_PORT2	0x2
+#define KSZ8863_SMAC_ENTRY_FWD_PORTS_PORT3	0x4
+#define KSZ8863_SMAC_MAX_ENTRIES		8
+
+struct ksz8863_bus_ops {
+	int (*read)(struct ksz8863_chip *chip, int reg, u8 *val);
+	int (*write)(struct ksz8863_chip *chip, int reg, u8 val);
+	int (*read_table)(struct ksz8863_chip *chip, enum ksz8863_tables table,
+			  u16 address, u8 *buffer, size_t size);
+	int (*write_table)(struct ksz8863_chip *chip, enum ksz8863_tables table,
+			   u16 address, u8 *buffer, size_t size);
+};
+
+#endif /* __KSZ8863_H__ */
diff --git a/include/linux/local_lock_internal.h b/include/linux/local_lock_internal.h
index 4a8795b21d77..271f911f2803 100644
--- a/include/linux/local_lock_internal.h
+++ b/include/linux/local_lock_internal.h
@@ -7,33 +7,90 @@
 #include <linux/lockdep.h>
 
 typedef struct {
-#ifdef CONFIG_DEBUG_LOCK_ALLOC
+#ifdef CONFIG_PREEMPT_RT
+	spinlock_t              lock;
+	struct task_struct      *owner;
+	int                     nestcnt;
+
+#elif defined(CONFIG_DEBUG_LOCK_ALLOC)
 	struct lockdep_map	dep_map;
 	struct task_struct	*owner;
 #endif
 } local_lock_t;
 
-#ifdef CONFIG_DEBUG_LOCK_ALLOC
-# define LL_DEP_MAP_INIT(lockname)			\
+#ifdef CONFIG_PREEMPT_RT
+
+#define INIT_LOCAL_LOCK(lockname)	{	\
+	__SPIN_LOCK_UNLOCKED((lockname).lock),	\
+	.owner		= NULL,			\
+	.nestcnt	= 0,			\
+	}
+#else
+
+# ifdef CONFIG_DEBUG_LOCK_ALLOC
+#  define LL_DEP_MAP_INIT(lockname)			\
 	.dep_map = {					\
 		.name = #lockname,			\
 		.wait_type_inner = LD_WAIT_CONFIG,	\
 	}
-#else
-# define LL_DEP_MAP_INIT(lockname)
-#endif
+# else
+#  define LL_DEP_MAP_INIT(lockname)
+# endif
 
 #define INIT_LOCAL_LOCK(lockname)	{ LL_DEP_MAP_INIT(lockname) }
 
-#define __local_lock_init(lock)					\
+#endif
+
+#ifdef CONFIG_PREEMPT_RT
+
+static inline void ___local_lock_init(local_lock_t *l)
+{
+	l->owner = NULL;
+	l->nestcnt = 0;
+}
+
+#define __local_lock_init(l)					\
+do {								\
+	spin_lock_init(&(l)->lock);				\
+	___local_lock_init(l);					\
+} while (0)
+
+#else
+
+#define __local_lock_init(l)					\
 do {								\
 	static struct lock_class_key __key;			\
 								\
-	debug_check_no_locks_freed((void *)lock, sizeof(*lock));\
-	lockdep_init_map_wait(&(lock)->dep_map, #lock, &__key, 0, LD_WAIT_CONFIG);\
+	debug_check_no_locks_freed((void *)l, sizeof(*l));	\
+	lockdep_init_map_wait(&(l)->dep_map, #l, &__key, 0, LD_WAIT_CONFIG);\
 } while (0)
+#endif
+
+#ifdef CONFIG_PREEMPT_RT
+
+static inline void local_lock_acquire(local_lock_t *l)
+{
+	if (l->owner != current) {
+		spin_lock(&l->lock);
+		DEBUG_LOCKS_WARN_ON(l->owner);
+		DEBUG_LOCKS_WARN_ON(l->nestcnt);
+		l->owner = current;
+	}
+	l->nestcnt++;
+}
+
+static inline void local_lock_release(local_lock_t *l)
+{
+	DEBUG_LOCKS_WARN_ON(l->nestcnt == 0);
+	DEBUG_LOCKS_WARN_ON(l->owner != current);
+	if (--l->nestcnt)
+		return;
+
+	l->owner = NULL;
+	spin_unlock(&l->lock);
+}
 
-#ifdef CONFIG_DEBUG_LOCK_ALLOC
+#elif defined(CONFIG_DEBUG_LOCK_ALLOC)
 static inline void local_lock_acquire(local_lock_t *l)
 {
 	lock_map_acquire(&l->dep_map);
@@ -53,21 +110,50 @@ static inline void local_lock_acquire(local_lock_t *l) { }
 static inline void local_lock_release(local_lock_t *l) { }
 #endif /* !CONFIG_DEBUG_LOCK_ALLOC */
 
+#ifdef CONFIG_PREEMPT_RT
+
 #define __local_lock(lock)					\
 	do {							\
-		preempt_disable();				\
+		migrate_disable();				\
 		local_lock_acquire(this_cpu_ptr(lock));		\
 	} while (0)
 
+#define __local_unlock(lock)					\
+	do {							\
+		local_lock_release(this_cpu_ptr(lock));		\
+		migrate_enable();				\
+	} while (0)
+
 #define __local_lock_irq(lock)					\
 	do {							\
-		local_irq_disable();				\
+		migrate_disable();				\
 		local_lock_acquire(this_cpu_ptr(lock));		\
 	} while (0)
 
 #define __local_lock_irqsave(lock, flags)			\
 	do {							\
-		local_irq_save(flags);				\
+		migrate_disable();				\
+		flags = 0;					\
+		local_lock_acquire(this_cpu_ptr(lock));		\
+	} while (0)
+
+#define __local_unlock_irq(lock)				\
+	do {							\
+		local_lock_release(this_cpu_ptr(lock));		\
+		migrate_enable();				\
+	} while (0)
+
+#define __local_unlock_irqrestore(lock, flags)			\
+	do {							\
+		local_lock_release(this_cpu_ptr(lock));		\
+		migrate_enable();				\
+	} while (0)
+
+#else
+
+#define __local_lock(lock)					\
+	do {							\
+		preempt_disable();				\
 		local_lock_acquire(this_cpu_ptr(lock));		\
 	} while (0)
 
@@ -77,6 +163,18 @@ static inline void local_lock_release(local_lock_t *l) { }
 		preempt_enable();				\
 	} while (0)
 
+#define __local_lock_irq(lock)					\
+	do {							\
+		local_irq_disable();				\
+		local_lock_acquire(this_cpu_ptr(lock));		\
+	} while (0)
+
+#define __local_lock_irqsave(lock, flags)			\
+	do {							\
+		local_irq_save(flags);				\
+		local_lock_acquire(this_cpu_ptr(lock));		\
+	} while (0)
+
 #define __local_unlock_irq(lock)				\
 	do {							\
 		local_lock_release(this_cpu_ptr(lock));		\
@@ -88,3 +186,5 @@ static inline void local_lock_release(local_lock_t *l) { }
 		local_lock_release(this_cpu_ptr(lock));		\
 		local_irq_restore(flags);			\
 	} while (0)
+
+#endif
diff --git a/include/linux/marvell_phy.h b/include/linux/marvell_phy.h
index ff7b7607c8cf..7c1e13d0f0d9 100644
--- a/include/linux/marvell_phy.h
+++ b/include/linux/marvell_phy.h
@@ -24,6 +24,7 @@
 #define MARVELL_PHY_ID_88E3016		0x01410e60
 #define MARVELL_PHY_ID_88X3310		0x002b09a0
 #define MARVELL_PHY_ID_88E2110		0x002b09b0
+#define MARVELL_PHY_ID_88E6321		0x01410c00
 
 /* The MV88e6390 Ethernet switch contains embedded PHYs. These PHYs do
  * not have a model ID. So the switch driver traps reads to the ID2
diff --git a/include/linux/mdio.h b/include/linux/mdio.h
index dbd69b3d170b..328a850adf73 100644
--- a/include/linux/mdio.h
+++ b/include/linux/mdio.h
@@ -16,6 +16,8 @@
 #define MII_DEVADDR_C45_SHIFT	16
 #define MII_REGADDR_C45_MASK	GENMASK(15, 0)
 
+#define MII_ADDR_KSZ (1<<15)        /* Special Treatment for KSZ8863 3-Port Switch */
+
 struct gpio_desc;
 struct mii_bus;
 struct reset_control;
diff --git a/include/linux/mfd/tps65218.h b/include/linux/mfd/tps65218.h
index f4ca367e3473..7f33a4042369 100644
--- a/include/linux/mfd/tps65218.h
+++ b/include/linux/mfd/tps65218.h
@@ -214,8 +214,14 @@ enum tps65218_regulator_id {
 	/* LS's */
 	TPS65218_LS_2,
 	TPS65218_LS_3,
+
+#define TPS65218_REGULATOR_FIRST	TPS65218_DCDC_1
+#define TPS65218_REGULATOR_LAST		TPS65218_LS_3
 };
 
+#define TPS65218_REG_ID_VALID(rid)						\
+	((rid) >= TPS65218_REGULATOR_FIRST && (rid) <= TPS65218_REGULATOR_LAST)
+
 #define TPS65218_MAX_REG_ID		TPS65218_LDO_1
 
 /* Number of step-down converters available */
diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h
index 915f4f100383..f27c4b98c7f2 100644
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@ -12,6 +12,7 @@
 #include <linux/completion.h>
 #include <linux/cpumask.h>
 #include <linux/uprobes.h>
+#include <linux/rcupdate.h>
 #include <linux/page-flags-layout.h>
 #include <linux/workqueue.h>
 #include <linux/seqlock.h>
@@ -556,6 +557,9 @@ struct mm_struct {
 		bool tlb_flush_batched;
 #endif
 		struct uprobes_state uprobes_state;
+#ifdef CONFIG_PREEMPT_RT
+		struct rcu_head delayed_drop;
+#endif
 #ifdef CONFIG_HUGETLB_PAGE
 		atomic_long_t hugetlb_usage;
 #endif
diff --git a/include/linux/mutex.h b/include/linux/mutex.h
index dcd185cbfe79..90f090efcb58 100644
--- a/include/linux/mutex.h
+++ b/include/linux/mutex.h
@@ -22,6 +22,20 @@
 
 struct ww_acquire_ctx;
 
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+# define __DEP_MAP_MUTEX_INITIALIZER(lockname)			\
+		, .dep_map = {					\
+			.name = #lockname,			\
+			.wait_type_inner = LD_WAIT_SLEEP,	\
+		}
+#else
+# define __DEP_MAP_MUTEX_INITIALIZER(lockname)
+#endif
+
+#ifdef CONFIG_PREEMPT_RT
+# include <linux/mutex_rt.h>
+#else
+
 /*
  * Simple, straightforward mutexes with strict semantics:
  *
@@ -68,14 +82,6 @@ struct mutex {
 struct ww_class;
 struct ww_acquire_ctx;
 
-struct ww_mutex {
-	struct mutex base;
-	struct ww_acquire_ctx *ctx;
-#ifdef CONFIG_DEBUG_MUTEXES
-	struct ww_class *ww_class;
-#endif
-};
-
 /*
  * This is the control structure for tasks blocked on mutex,
  * which resides on the blocked task's kernel stack:
@@ -119,16 +125,6 @@ do {									\
 	__mutex_init((mutex), #mutex, &__key);				\
 } while (0)
 
-#ifdef CONFIG_DEBUG_LOCK_ALLOC
-# define __DEP_MAP_MUTEX_INITIALIZER(lockname)			\
-		, .dep_map = {					\
-			.name = #lockname,			\
-			.wait_type_inner = LD_WAIT_SLEEP,	\
-		}
-#else
-# define __DEP_MAP_MUTEX_INITIALIZER(lockname)
-#endif
-
 #define __MUTEX_INITIALIZER(lockname) \
 		{ .owner = ATOMIC_LONG_INIT(0) \
 		, .wait_lock = __SPIN_LOCK_UNLOCKED(lockname.wait_lock) \
@@ -224,4 +220,6 @@ enum mutex_trylock_recursive_enum {
 extern /* __deprecated */ __must_check enum mutex_trylock_recursive_enum
 mutex_trylock_recursive(struct mutex *lock);
 
+#endif /* !PREEMPT_RT */
+
 #endif /* __LINUX_MUTEX_H */
diff --git a/include/linux/mutex_rt.h b/include/linux/mutex_rt.h
new file mode 100644
index 000000000000..f0b2e07cd5c5
--- /dev/null
+++ b/include/linux/mutex_rt.h
@@ -0,0 +1,130 @@
+// SPDX-License-Identifier: GPL-2.0-only
+#ifndef __LINUX_MUTEX_RT_H
+#define __LINUX_MUTEX_RT_H
+
+#ifndef __LINUX_MUTEX_H
+#error "Please include mutex.h"
+#endif
+
+#include <linux/rtmutex.h>
+
+/* FIXME: Just for __lockfunc */
+#include <linux/spinlock.h>
+
+struct mutex {
+	struct rt_mutex		lock;
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	struct lockdep_map	dep_map;
+#endif
+};
+
+#define __MUTEX_INITIALIZER(mutexname)					\
+	{								\
+		.lock = __RT_MUTEX_INITIALIZER(mutexname.lock)		\
+		__DEP_MAP_MUTEX_INITIALIZER(mutexname)			\
+	}
+
+#define DEFINE_MUTEX(mutexname)						\
+	struct mutex mutexname = __MUTEX_INITIALIZER(mutexname)
+
+extern void __mutex_do_init(struct mutex *lock, const char *name, struct lock_class_key *key);
+extern void __lockfunc _mutex_lock(struct mutex *lock);
+extern void __lockfunc _mutex_lock_io_nested(struct mutex *lock, int subclass);
+extern int __lockfunc _mutex_lock_interruptible(struct mutex *lock);
+extern int __lockfunc _mutex_lock_killable(struct mutex *lock);
+extern void __lockfunc _mutex_lock_nested(struct mutex *lock, int subclass);
+extern void __lockfunc _mutex_lock_nest_lock(struct mutex *lock, struct lockdep_map *nest_lock);
+extern int __lockfunc _mutex_lock_interruptible_nested(struct mutex *lock, int subclass);
+extern int __lockfunc _mutex_lock_killable_nested(struct mutex *lock, int subclass);
+extern int __lockfunc _mutex_trylock(struct mutex *lock);
+extern void __lockfunc _mutex_unlock(struct mutex *lock);
+
+#define mutex_is_locked(l)		rt_mutex_is_locked(&(l)->lock)
+#define mutex_lock(l)			_mutex_lock(l)
+#define mutex_lock_interruptible(l)	_mutex_lock_interruptible(l)
+#define mutex_lock_killable(l)		_mutex_lock_killable(l)
+#define mutex_trylock(l)		_mutex_trylock(l)
+#define mutex_unlock(l)			_mutex_unlock(l)
+#define mutex_lock_io(l)		_mutex_lock_io_nested(l, 0);
+
+#define __mutex_owner(l)		((l)->lock.owner)
+
+#ifdef CONFIG_DEBUG_MUTEXES
+#define mutex_destroy(l)		rt_mutex_destroy(&(l)->lock)
+#else
+static inline void mutex_destroy(struct mutex *lock) {}
+#endif
+
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+# define mutex_lock_nested(l, s)	_mutex_lock_nested(l, s)
+# define mutex_lock_interruptible_nested(l, s) \
+					_mutex_lock_interruptible_nested(l, s)
+# define mutex_lock_killable_nested(l, s) \
+					_mutex_lock_killable_nested(l, s)
+# define mutex_lock_io_nested(l, s)	_mutex_lock_io_nested(l, s)
+
+# define mutex_lock_nest_lock(lock, nest_lock)				\
+do {									\
+	typecheck(struct lockdep_map *, &(nest_lock)->dep_map);		\
+	_mutex_lock_nest_lock(lock, &(nest_lock)->dep_map);		\
+} while (0)
+
+#else
+# define mutex_lock_nested(l, s)	_mutex_lock(l)
+# define mutex_lock_interruptible_nested(l, s) \
+					_mutex_lock_interruptible(l)
+# define mutex_lock_killable_nested(l, s) \
+					_mutex_lock_killable(l)
+# define mutex_lock_nest_lock(lock, nest_lock) mutex_lock(lock)
+# define mutex_lock_io_nested(l, s)	_mutex_lock_io_nested(l, s)
+#endif
+
+# define mutex_init(mutex)				\
+do {							\
+	static struct lock_class_key __key;		\
+							\
+	rt_mutex_init(&(mutex)->lock);			\
+	__mutex_do_init((mutex), #mutex, &__key);	\
+} while (0)
+
+# define __mutex_init(mutex, name, key)			\
+do {							\
+	rt_mutex_init(&(mutex)->lock);			\
+	__mutex_do_init((mutex), name, key);		\
+} while (0)
+
+/**
+ * These values are chosen such that FAIL and SUCCESS match the
+ * values of the regular mutex_trylock().
+ */
+enum mutex_trylock_recursive_enum {
+	MUTEX_TRYLOCK_FAILED    = 0,
+	MUTEX_TRYLOCK_SUCCESS   = 1,
+	MUTEX_TRYLOCK_RECURSIVE,
+};
+/**
+ * mutex_trylock_recursive - trylock variant that allows recursive locking
+ * @lock: mutex to be locked
+ *
+ * This function should not be used, _ever_. It is purely for hysterical GEM
+ * raisins, and once those are gone this will be removed.
+ *
+ * Returns:
+ *  MUTEX_TRYLOCK_FAILED    - trylock failed,
+ *  MUTEX_TRYLOCK_SUCCESS   - lock acquired,
+ *  MUTEX_TRYLOCK_RECURSIVE - we already owned the lock.
+ */
+int __rt_mutex_owner_current(struct rt_mutex *lock);
+
+static inline /* __deprecated */ __must_check enum mutex_trylock_recursive_enum
+mutex_trylock_recursive(struct mutex *lock)
+{
+	if (unlikely(__rt_mutex_owner_current(&lock->lock)))
+		return MUTEX_TRYLOCK_RECURSIVE;
+
+	return mutex_trylock(lock);
+}
+
+extern int atomic_dec_and_mutex_lock(atomic_t *cnt, struct mutex *lock);
+
+#endif
diff --git a/include/linux/netdevice.h b/include/linux/netdevice.h
index 8753e98a8d58..b97a61a80914 100644
--- a/include/linux/netdevice.h
+++ b/include/linux/netdevice.h
@@ -3240,24 +3240,41 @@ DECLARE_PER_CPU_ALIGNED(struct softnet_data, softnet_data);
 
 static inline int dev_recursion_level(void)
 {
+#ifdef CONFIG_PREEMPT_RT
+	return atomic_read(&current->xmit_recursion);
+#else
 	return this_cpu_read(softnet_data.xmit.recursion);
+#endif
 }
 
 #define XMIT_RECURSION_LIMIT	8
 static inline bool dev_xmit_recursion(void)
 {
+#ifdef CONFIG_PREEMPT_RT
+	return unlikely(atomic_read(&current->xmit_recursion) >
+			XMIT_RECURSION_LIMIT);
+#else
 	return unlikely(__this_cpu_read(softnet_data.xmit.recursion) >
 			XMIT_RECURSION_LIMIT);
+#endif
 }
 
 static inline void dev_xmit_recursion_inc(void)
 {
+#ifdef CONFIG_PREEMPT_RT
+	atomic_inc(&current->xmit_recursion);
+#else
 	__this_cpu_inc(softnet_data.xmit.recursion);
+#endif
 }
 
 static inline void dev_xmit_recursion_dec(void)
 {
+#ifdef CONFIG_PREEMPT_RT
+	atomic_dec(&current->xmit_recursion);
+#else
 	__this_cpu_dec(softnet_data.xmit.recursion);
+#endif
 }
 
 void __netif_schedule(struct Qdisc *q);
diff --git a/include/linux/nfs_xdr.h b/include/linux/nfs_xdr.h
index d63cb862d58e..1630690ba709 100644
--- a/include/linux/nfs_xdr.h
+++ b/include/linux/nfs_xdr.h
@@ -1670,7 +1670,7 @@ struct nfs_unlinkdata {
 	struct nfs_removeargs args;
 	struct nfs_removeres res;
 	struct dentry *dentry;
-	wait_queue_head_t wq;
+	struct swait_queue_head wq;
 	const struct cred *cred;
 	struct nfs_fattr dir_attr;
 	long timeout;
diff --git a/include/linux/notifier.h b/include/linux/notifier.h
index 2fb373a5c1ed..723bc2df6388 100644
--- a/include/linux/notifier.h
+++ b/include/linux/notifier.h
@@ -58,7 +58,7 @@ struct notifier_block {
 };
 
 struct atomic_notifier_head {
-	spinlock_t lock;
+	raw_spinlock_t lock;
 	struct notifier_block __rcu *head;
 };
 
@@ -78,7 +78,7 @@ struct srcu_notifier_head {
 };
 
 #define ATOMIC_INIT_NOTIFIER_HEAD(name) do {	\
-		spin_lock_init(&(name)->lock);	\
+		raw_spin_lock_init(&(name)->lock);	\
 		(name)->head = NULL;		\
 	} while (0)
 #define BLOCKING_INIT_NOTIFIER_HEAD(name) do {	\
@@ -95,7 +95,7 @@ extern void srcu_init_notifier_head(struct srcu_notifier_head *nh);
 		cleanup_srcu_struct(&(name)->srcu);
 
 #define ATOMIC_NOTIFIER_INIT(name) {				\
-		.lock = __SPIN_LOCK_UNLOCKED(name.lock),	\
+		.lock = __RAW_SPIN_LOCK_UNLOCKED(name.lock),	\
 		.head = NULL }
 #define BLOCKING_NOTIFIER_INIT(name) {				\
 		.rwsem = __RWSEM_INITIALIZER((name).rwsem),	\
diff --git a/include/linux/pid.h b/include/linux/pid.h
index fa10acb8d6a4..2f86f84e9fc1 100644
--- a/include/linux/pid.h
+++ b/include/linux/pid.h
@@ -3,6 +3,7 @@
 #define _LINUX_PID_H
 
 #include <linux/rculist.h>
+#include <linux/atomic.h>
 #include <linux/wait.h>
 #include <linux/refcount.h>
 
diff --git a/include/linux/platform_data/pfc-modbus-rtu.h b/include/linux/platform_data/pfc-modbus-rtu.h
new file mode 100644
index 000000000000..e4f7c4b5afc0
--- /dev/null
+++ b/include/linux/platform_data/pfc-modbus-rtu.h
@@ -0,0 +1,66 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+
+/*
+ * WAGO Serial Modbus RTU Driver
+ *
+ * Copyright (C) 2013 Wago Kontakttechnik GmbH
+ *
+ * Author: Timur Celik <timur.celik@wago.com>
+ *
+ */
+
+#ifndef PFC_MODBUS_RTU_H_
+#define PFC_MODBUS_RTU_H_
+#ifdef CONFIG_SERIAL_OMAP_MODBUS
+
+
+#include <asm/atomic.h>
+#include <linux/kfifo.h>
+#include <linux/hrtimer.h>
+#include <linux/interrupt.h>
+
+#define MB_TIMESTAMP_LENGTH	8
+#define MB_FRAME_MAX		256
+#define MB_FRAME_MIN		4
+
+/* State machine definition */
+typedef enum { ERR, IF, IC } MB_STATE;
+
+/* Modbus specific data of the uart port */
+struct modbus_port
+{
+	MB_STATE		state;
+	struct hrtimer		timer;
+	struct kfifo		frame;
+	struct kfifo		length;	  /* contains frame length */
+	atomic_t		ch_cnt;
+	atomic_t		frame_cnt;
+	u8			slave_id; /* if zero all frames are received
+					     (master mode) */
+	u64			to_15;
+	u64			to_35;
+	u64			to_15_override;
+	u64			to_35_override;
+	bool			activated;
+	unsigned int		modem_status;
+	struct tasklet_struct	rx_tsklt;
+	struct tasklet_struct	tx_tsklt;
+	struct tasklet_struct	mdr_tsklt;
+	bool			timestamp_enabled;
+};
+
+struct uart_omap_port;
+
+enum hrtimer_restart modbus_omap_to_handler(struct hrtimer* hrt);
+int modbus_omap_rhr_handler(struct uart_omap_port *up);
+int modbus_omap_rlsi_handler(struct uart_omap_port *up, unsigned int lsr);
+void modbus_omap_calc_to(struct uart_omap_port *up, unsigned int baud);
+int modbus_omap_enable(struct uart_omap_port *up);
+int modbus_omap_disable(struct uart_omap_port *up);
+int modbus_omap_startup(struct uart_omap_port *up);
+void modbus_omap_shutdown(struct uart_omap_port *up);
+void modbus_omap_timestamp_enable(struct uart_omap_port *up);
+void modbus_omap_timestamp_disable(struct uart_omap_port *up);
+
+#endif
+#endif /* PFC_MODBUS_RTU_H_ */
diff --git a/include/linux/preempt.h b/include/linux/preempt.h
index 7d9c1c0e149c..af39859f02ee 100644
--- a/include/linux/preempt.h
+++ b/include/linux/preempt.h
@@ -77,31 +77,37 @@
 /* preempt_count() and related functions, depends on PREEMPT_NEED_RESCHED */
 #include <asm/preempt.h>
 
+#define nmi_count()	(preempt_count() & NMI_MASK)
 #define hardirq_count()	(preempt_count() & HARDIRQ_MASK)
-#define softirq_count()	(preempt_count() & SOFTIRQ_MASK)
-#define irq_count()	(preempt_count() & (HARDIRQ_MASK | SOFTIRQ_MASK \
-				 | NMI_MASK))
+#ifdef CONFIG_PREEMPT_RT
+# define softirq_count()	(current->softirq_disable_cnt & SOFTIRQ_MASK)
+#else
+# define softirq_count()	(preempt_count() & SOFTIRQ_MASK)
+#endif
+#define irq_count()	(nmi_count() | hardirq_count() | softirq_count())
 
 /*
- * Are we doing bottom half or hardware interrupt processing?
+ * Macros to retrieve the current execution context:
  *
- * in_irq()       - We're in (hard) IRQ context
+ * in_nmi()		- We're in NMI context
+ * in_hardirq()		- We're in hard IRQ context
+ * in_serving_softirq()	- We're in softirq context
+ * in_task()		- We're in task context
+ */
+#define in_nmi()		(nmi_count())
+#define in_hardirq()		(hardirq_count())
+#define in_serving_softirq()	(softirq_count() & SOFTIRQ_OFFSET)
+#define in_task()		(!(in_nmi() | in_hardirq() | in_serving_softirq()))
+
+/*
+ * The following macros are deprecated and should not be used in new code:
+ * in_irq()       - Obsolete version of in_hardirq()
  * in_softirq()   - We have BH disabled, or are processing softirqs
  * in_interrupt() - We're in NMI,IRQ,SoftIRQ context or have BH disabled
- * in_serving_softirq() - We're in softirq context
- * in_nmi()       - We're in NMI context
- * in_task()	  - We're in task context
- *
- * Note: due to the BH disabled confusion: in_softirq(),in_interrupt() really
- *       should not be used in new code.
  */
 #define in_irq()		(hardirq_count())
 #define in_softirq()		(softirq_count())
 #define in_interrupt()		(irq_count())
-#define in_serving_softirq()	(softirq_count() & SOFTIRQ_OFFSET)
-#define in_nmi()		(preempt_count() & NMI_MASK)
-#define in_task()		(!(preempt_count() & \
-				   (NMI_MASK | HARDIRQ_MASK | SOFTIRQ_OFFSET)))
 
 /*
  * The preempt_count offset after preempt_disable();
@@ -115,7 +121,11 @@
 /*
  * The preempt_count offset after spin_lock()
  */
+#if !defined(CONFIG_PREEMPT_RT)
 #define PREEMPT_LOCK_OFFSET	PREEMPT_DISABLE_OFFSET
+#else
+#define PREEMPT_LOCK_OFFSET	0
+#endif
 
 /*
  * The preempt_count offset needed for things like:
@@ -164,6 +174,20 @@ extern void preempt_count_sub(int val);
 #define preempt_count_inc() preempt_count_add(1)
 #define preempt_count_dec() preempt_count_sub(1)
 
+#ifdef CONFIG_PREEMPT_LAZY
+#define add_preempt_lazy_count(val)	do { preempt_lazy_count() += (val); } while (0)
+#define sub_preempt_lazy_count(val)	do { preempt_lazy_count() -= (val); } while (0)
+#define inc_preempt_lazy_count()	add_preempt_lazy_count(1)
+#define dec_preempt_lazy_count()	sub_preempt_lazy_count(1)
+#define preempt_lazy_count()		(current_thread_info()->preempt_lazy_count)
+#else
+#define add_preempt_lazy_count(val)	do { } while (0)
+#define sub_preempt_lazy_count(val)	do { } while (0)
+#define inc_preempt_lazy_count()	do { } while (0)
+#define dec_preempt_lazy_count()	do { } while (0)
+#define preempt_lazy_count()		(0)
+#endif
+
 #ifdef CONFIG_PREEMPT_COUNT
 
 #define preempt_disable() \
@@ -172,13 +196,25 @@ do { \
 	barrier(); \
 } while (0)
 
+#define preempt_lazy_disable() \
+do { \
+	inc_preempt_lazy_count(); \
+	barrier(); \
+} while (0)
+
 #define sched_preempt_enable_no_resched() \
 do { \
 	barrier(); \
 	preempt_count_dec(); \
 } while (0)
 
-#define preempt_enable_no_resched() sched_preempt_enable_no_resched()
+#ifdef CONFIG_PREEMPT_RT
+# define preempt_enable_no_resched() sched_preempt_enable_no_resched()
+# define preempt_check_resched_rt() preempt_check_resched()
+#else
+# define preempt_enable_no_resched() preempt_enable()
+# define preempt_check_resched_rt() barrier();
+#endif
 
 #define preemptible()	(preempt_count() == 0 && !irqs_disabled())
 
@@ -203,6 +239,18 @@ do { \
 		__preempt_schedule(); \
 } while (0)
 
+/*
+ * open code preempt_check_resched() because it is not exported to modules and
+ * used by local_unlock() or bpf_enable_instrumentation().
+ */
+#define preempt_lazy_enable() \
+do { \
+	dec_preempt_lazy_count(); \
+	barrier(); \
+	if (should_resched(0)) \
+		__preempt_schedule(); \
+} while (0)
+
 #else /* !CONFIG_PREEMPTION */
 #define preempt_enable() \
 do { \
@@ -210,6 +258,12 @@ do { \
 	preempt_count_dec(); \
 } while (0)
 
+#define preempt_lazy_enable() \
+do { \
+	dec_preempt_lazy_count(); \
+	barrier(); \
+} while (0)
+
 #define preempt_enable_notrace() \
 do { \
 	barrier(); \
@@ -248,8 +302,12 @@ do { \
 #define preempt_disable_notrace()		barrier()
 #define preempt_enable_no_resched_notrace()	barrier()
 #define preempt_enable_notrace()		barrier()
+#define preempt_check_resched_rt()		barrier()
 #define preemptible()				0
 
+#define preempt_lazy_disable()			barrier()
+#define preempt_lazy_enable()			barrier()
+
 #endif /* CONFIG_PREEMPT_COUNT */
 
 #ifdef MODULE
@@ -268,10 +326,22 @@ do { \
 } while (0)
 #define preempt_fold_need_resched() \
 do { \
-	if (tif_need_resched()) \
+	if (tif_need_resched_now()) \
 		set_preempt_need_resched(); \
 } while (0)
 
+#ifdef CONFIG_PREEMPT_RT
+# define preempt_disable_rt()		preempt_disable()
+# define preempt_enable_rt()		preempt_enable()
+# define preempt_disable_nort()		barrier()
+# define preempt_enable_nort()		barrier()
+#else
+# define preempt_disable_rt()		barrier()
+# define preempt_enable_rt()		barrier()
+# define preempt_disable_nort()		preempt_disable()
+# define preempt_enable_nort()		preempt_enable()
+#endif
+
 #ifdef CONFIG_PREEMPT_NOTIFIERS
 
 struct preempt_notifier;
@@ -322,34 +392,78 @@ static inline void preempt_notifier_init(struct preempt_notifier *notifier,
 
 #endif
 
-/**
- * migrate_disable - Prevent migration of the current task
+#ifdef CONFIG_SMP
+
+/*
+ * Migrate-Disable and why it is undesired.
  *
- * Maps to preempt_disable() which also disables preemption. Use
- * migrate_disable() to annotate that the intent is to prevent migration,
- * but not necessarily preemption.
+ * When a preempted task becomes elegible to run under the ideal model (IOW it
+ * becomes one of the M highest priority tasks), it might still have to wait
+ * for the preemptee's migrate_disable() section to complete. Thereby suffering
+ * a reduction in bandwidth in the exact duration of the migrate_disable()
+ * section.
  *
- * Can be invoked nested like preempt_disable() and needs the corresponding
- * number of migrate_enable() invocations.
- */
-static __always_inline void migrate_disable(void)
-{
-	preempt_disable();
-}
-
-/**
- * migrate_enable - Allow migration of the current task
+ * Per this argument, the change from preempt_disable() to migrate_disable()
+ * gets us:
+ *
+ * - a higher priority tasks gains reduced wake-up latency; with preempt_disable()
+ *   it would have had to wait for the lower priority task.
+ *
+ * - a lower priority tasks; which under preempt_disable() could've instantly
+ *   migrated away when another CPU becomes available, is now constrained
+ *   by the ability to push the higher priority task away, which might itself be
+ *   in a migrate_disable() section, reducing it's available bandwidth.
+ *
+ * IOW it trades latency / moves the interference term, but it stays in the
+ * system, and as long as it remains unbounded, the system is not fully
+ * deterministic.
  *
- * Counterpart to migrate_disable().
  *
- * As migrate_disable() can be invoked nested, only the outermost invocation
- * reenables migration.
+ * The reason we have it anyway.
+ *
+ * PREEMPT_RT breaks a number of assumptions traditionally held. By forcing a
+ * number of primitives into becoming preemptible, they would also allow
+ * migration. This turns out to break a bunch of per-cpu usage. To this end,
+ * all these primitives employ migirate_disable() to restore this implicit
+ * assumption.
+ *
+ * This is a 'temporary' work-around at best. The correct solution is getting
+ * rid of the above assumptions and reworking the code to employ explicit
+ * per-cpu locking or short preempt-disable regions.
+ *
+ * The end goal must be to get rid of migrate_disable(), alternatively we need
+ * a schedulability theory that does not depend on abritrary migration.
+ *
+ *
+ * Notes on the implementation.
+ *
+ * The implementation is particularly tricky since existing code patterns
+ * dictate neither migrate_disable() nor migrate_enable() is allowed to block.
+ * This means that it cannot use cpus_read_lock() to serialize against hotplug,
+ * nor can it easily migrate itself into a pending affinity mask change on
+ * migrate_enable().
+ *
+ *
+ * Note: even non-work-conserving schedulers like semi-partitioned depends on
+ *       migration, so migrate_disable() is not only a problem for
+ *       work-conserving schedulers.
  *
- * Currently mapped to preempt_enable().
  */
-static __always_inline void migrate_enable(void)
+extern void migrate_disable(void);
+extern void migrate_enable(void);
+
+#else
+
+static inline void migrate_disable(void)
 {
-	preempt_enable();
+	preempt_lazy_disable();
 }
 
+static inline void migrate_enable(void)
+{
+	preempt_lazy_enable();
+}
+
+#endif /* CONFIG_SMP */
+
 #endif /* __LINUX_PREEMPT_H */
diff --git a/include/linux/printk.h b/include/linux/printk.h
index fe7eb2351610..7e4352467d83 100644
--- a/include/linux/printk.h
+++ b/include/linux/printk.h
@@ -46,6 +46,12 @@ static inline const char *printk_skip_headers(const char *buffer)
 
 #define CONSOLE_EXT_LOG_MAX	8192
 
+/*
+ * The maximum size of a record formatted for console printing
+ * (i.e. with the prefix prepended to every line).
+ */
+#define CONSOLE_LOG_MAX		4096
+
 /* printk's without a loglevel use this.. */
 #define MESSAGE_LOGLEVEL_DEFAULT CONFIG_MESSAGE_LOGLEVEL_DEFAULT
 
@@ -149,18 +155,6 @@ static inline __printf(1, 2) __cold
 void early_printk(const char *s, ...) { }
 #endif
 
-#ifdef CONFIG_PRINTK_NMI
-extern void printk_nmi_enter(void);
-extern void printk_nmi_exit(void);
-extern void printk_nmi_direct_enter(void);
-extern void printk_nmi_direct_exit(void);
-#else
-static inline void printk_nmi_enter(void) { }
-static inline void printk_nmi_exit(void) { }
-static inline void printk_nmi_direct_enter(void) { }
-static inline void printk_nmi_direct_exit(void) { }
-#endif /* PRINTK_NMI */
-
 struct dev_printk_info;
 
 #ifdef CONFIG_PRINTK
@@ -207,8 +201,6 @@ __printf(1, 2) void dump_stack_set_arch_desc(const char *fmt, ...);
 void dump_stack_print_info(const char *log_lvl);
 void show_regs_print_info(const char *log_lvl);
 extern asmlinkage void dump_stack(void) __cold;
-extern void printk_safe_flush(void);
-extern void printk_safe_flush_on_panic(void);
 #else
 static inline __printf(1, 0)
 int vprintk(const char *s, va_list args)
@@ -272,14 +264,6 @@ static inline void show_regs_print_info(const char *log_lvl)
 static inline void dump_stack(void)
 {
 }
-
-static inline void printk_safe_flush(void)
-{
-}
-
-static inline void printk_safe_flush_on_panic(void)
-{
-}
 #endif
 
 extern int kptr_restrict;
@@ -497,6 +481,8 @@ extern int kptr_restrict;
 	no_printk(KERN_DEBUG pr_fmt(fmt), ##__VA_ARGS__)
 #endif
 
+bool pr_flush(int timeout_ms, bool reset_on_progress);
+
 /*
  * ratelimited messages with local ratelimit_state,
  * no local ratelimit_state used in the !PRINTK case
diff --git a/include/linux/random.h b/include/linux/random.h
index f45b8be3e3c4..0e41d0527809 100644
--- a/include/linux/random.h
+++ b/include/linux/random.h
@@ -35,7 +35,7 @@ static inline void add_latent_entropy(void) {}
 
 extern void add_input_randomness(unsigned int type, unsigned int code,
 				 unsigned int value) __latent_entropy;
-extern void add_interrupt_randomness(int irq, int irq_flags) __latent_entropy;
+extern void add_interrupt_randomness(int irq, int irq_flags, __u64 ip) __latent_entropy;
 
 extern void get_random_bytes(void *buf, int nbytes);
 extern int wait_for_random_bytes(void);
diff --git a/include/linux/rbtree.h b/include/linux/rbtree.h
index d7db17996322..c33b0e16d04b 100644
--- a/include/linux/rbtree.h
+++ b/include/linux/rbtree.h
@@ -19,19 +19,9 @@
 
 #include <linux/kernel.h>
 #include <linux/stddef.h>
+#include <linux/rbtree_type.h>
 #include <linux/rcupdate.h>
 
-struct rb_node {
-	unsigned long  __rb_parent_color;
-	struct rb_node *rb_right;
-	struct rb_node *rb_left;
-} __attribute__((aligned(sizeof(long))));
-    /* The alignment might seem pointless, but allegedly CRIS needs it */
-
-struct rb_root {
-	struct rb_node *rb_node;
-};
-
 #define rb_parent(r)   ((struct rb_node *)((r)->__rb_parent_color & ~3))
 
 #define RB_ROOT	(struct rb_root) { NULL, }
@@ -112,21 +102,6 @@ static inline void rb_link_node_rcu(struct rb_node *node, struct rb_node *parent
 			typeof(*pos), field); 1; }); \
 	     pos = n)
 
-/*
- * Leftmost-cached rbtrees.
- *
- * We do not cache the rightmost node based on footprint
- * size vs number of potential users that could benefit
- * from O(1) rb_last(). Just not worth it, users that want
- * this feature can always implement the logic explicitly.
- * Furthermore, users that want to cache both pointers may
- * find it a bit asymmetric, but that's ok.
- */
-struct rb_root_cached {
-	struct rb_root rb_root;
-	struct rb_node *rb_leftmost;
-};
-
 #define RB_ROOT_CACHED (struct rb_root_cached) { {NULL, }, NULL }
 
 /* Same as rb_first(), but O(1) */
diff --git a/include/linux/rbtree_type.h b/include/linux/rbtree_type.h
new file mode 100644
index 000000000000..77a89dd2c7c6
--- /dev/null
+++ b/include/linux/rbtree_type.h
@@ -0,0 +1,31 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+#ifndef _LINUX_RBTREE_TYPE_H
+#define _LINUX_RBTREE_TYPE_H
+
+struct rb_node {
+	unsigned long  __rb_parent_color;
+	struct rb_node *rb_right;
+	struct rb_node *rb_left;
+} __attribute__((aligned(sizeof(long))));
+/* The alignment might seem pointless, but allegedly CRIS needs it */
+
+struct rb_root {
+	struct rb_node *rb_node;
+};
+
+/*
+ * Leftmost-cached rbtrees.
+ *
+ * We do not cache the rightmost node based on footprint
+ * size vs number of potential users that could benefit
+ * from O(1) rb_last(). Just not worth it, users that want
+ * this feature can always implement the logic explicitly.
+ * Furthermore, users that want to cache both pointers may
+ * find it a bit asymmetric, but that's ok.
+ */
+struct rb_root_cached {
+	struct rb_root rb_root;
+	struct rb_node *rb_leftmost;
+};
+
+#endif
diff --git a/include/linux/rcupdate.h b/include/linux/rcupdate.h
index c5adba5e79e7..f251ba473f77 100644
--- a/include/linux/rcupdate.h
+++ b/include/linux/rcupdate.h
@@ -52,6 +52,11 @@ void __rcu_read_unlock(void);
  * types of kernel builds, the rcu_read_lock() nesting depth is unknowable.
  */
 #define rcu_preempt_depth() (current->rcu_read_lock_nesting)
+#ifndef CONFIG_PREEMPT_RT
+#define sched_rcu_preempt_depth()	rcu_preempt_depth()
+#else
+static inline int sched_rcu_preempt_depth(void) { return 0; }
+#endif
 
 #else /* #ifdef CONFIG_PREEMPT_RCU */
 
@@ -77,6 +82,8 @@ static inline int rcu_preempt_depth(void)
 	return 0;
 }
 
+#define sched_rcu_preempt_depth()	rcu_preempt_depth()
+
 #endif /* #else #ifdef CONFIG_PREEMPT_RCU */
 
 /* Internal to kernel */
@@ -327,7 +334,8 @@ static inline void rcu_preempt_sleep_check(void) { }
 #define rcu_sleep_check()						\
 	do {								\
 		rcu_preempt_sleep_check();				\
-		RCU_LOCKDEP_WARN(lock_is_held(&rcu_bh_lock_map),	\
+		if (!IS_ENABLED(CONFIG_PREEMPT_RT))			\
+		    RCU_LOCKDEP_WARN(lock_is_held(&rcu_bh_lock_map),	\
 				 "Illegal context switch in RCU-bh read-side critical section"); \
 		RCU_LOCKDEP_WARN(lock_is_held(&rcu_sched_lock_map),	\
 				 "Illegal context switch in RCU-sched read-side critical section"); \
diff --git a/include/linux/rtmutex.h b/include/linux/rtmutex.h
index 6fd615a0eea9..b02009f53026 100644
--- a/include/linux/rtmutex.h
+++ b/include/linux/rtmutex.h
@@ -14,11 +14,15 @@
 #define __LINUX_RT_MUTEX_H
 
 #include <linux/linkage.h>
-#include <linux/rbtree.h>
-#include <linux/spinlock_types.h>
+#include <linux/rbtree_type.h>
+#include <linux/spinlock_types_raw.h>
 
 extern int max_lock_depth; /* for sysctl */
 
+#ifdef CONFIG_DEBUG_MUTEXES
+#include <linux/debug_locks.h>
+#endif
+
 /**
  * The rt_mutex structure
  *
@@ -31,12 +35,7 @@ struct rt_mutex {
 	raw_spinlock_t		wait_lock;
 	struct rb_root_cached   waiters;
 	struct task_struct	*owner;
-#ifdef CONFIG_DEBUG_RT_MUTEXES
 	int			save_state;
-	const char		*name, *file;
-	int			line;
-	void			*magic;
-#endif
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 	struct lockdep_map	dep_map;
 #endif
@@ -49,6 +48,7 @@ struct hrtimer_sleeper;
  extern int rt_mutex_debug_check_no_locks_freed(const void *from,
 						unsigned long len);
  extern void rt_mutex_debug_check_no_locks_held(struct task_struct *task);
+ extern void rt_mutex_debug_task_free(struct task_struct *tsk);
 #else
  static inline int rt_mutex_debug_check_no_locks_freed(const void *from,
 						       unsigned long len)
@@ -56,25 +56,15 @@ struct hrtimer_sleeper;
 	return 0;
  }
 # define rt_mutex_debug_check_no_locks_held(task)	do { } while (0)
+# define rt_mutex_debug_task_free(t)			do { } while (0)
 #endif
 
-#ifdef CONFIG_DEBUG_RT_MUTEXES
-# define __DEBUG_RT_MUTEX_INITIALIZER(mutexname) \
-	, .name = #mutexname, .file = __FILE__, .line = __LINE__
-
-# define rt_mutex_init(mutex) \
+#define rt_mutex_init(mutex) \
 do { \
 	static struct lock_class_key __key; \
 	__rt_mutex_init(mutex, __func__, &__key); \
 } while (0)
 
- extern void rt_mutex_debug_task_free(struct task_struct *tsk);
-#else
-# define __DEBUG_RT_MUTEX_INITIALIZER(mutexname)
-# define rt_mutex_init(mutex)			__rt_mutex_init(mutex, NULL, NULL)
-# define rt_mutex_debug_task_free(t)			do { } while (0)
-#endif
-
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 #define __DEP_MAP_RT_MUTEX_INITIALIZER(mutexname) \
 	, .dep_map = { .name = #mutexname }
@@ -82,12 +72,19 @@ do { \
 #define __DEP_MAP_RT_MUTEX_INITIALIZER(mutexname)
 #endif
 
-#define __RT_MUTEX_INITIALIZER(mutexname) \
-	{ .wait_lock = __RAW_SPIN_LOCK_UNLOCKED(mutexname.wait_lock) \
+#define __RT_MUTEX_INITIALIZER_PLAIN(mutexname) \
+	  .wait_lock = __RAW_SPIN_LOCK_UNLOCKED(mutexname.wait_lock) \
 	, .waiters = RB_ROOT_CACHED \
 	, .owner = NULL \
-	__DEBUG_RT_MUTEX_INITIALIZER(mutexname) \
-	__DEP_MAP_RT_MUTEX_INITIALIZER(mutexname)}
+	__DEP_MAP_RT_MUTEX_INITIALIZER(mutexname)
+
+#define __RT_MUTEX_INITIALIZER(mutexname) \
+	{ __RT_MUTEX_INITIALIZER_PLAIN(mutexname) \
+	, .save_state = 0 }
+
+#define __RT_MUTEX_INITIALIZER_SAVE_STATE(mutexname) \
+	{ __RT_MUTEX_INITIALIZER_PLAIN(mutexname)    \
+	, .save_state = 1 }
 
 #define DEFINE_RT_MUTEX(mutexname) \
 	struct rt_mutex mutexname = __RT_MUTEX_INITIALIZER(mutexname)
@@ -115,9 +112,6 @@ extern void rt_mutex_lock(struct rt_mutex *lock);
 #endif
 
 extern int rt_mutex_lock_interruptible(struct rt_mutex *lock);
-extern int rt_mutex_timed_lock(struct rt_mutex *lock,
-			       struct hrtimer_sleeper *timeout);
-
 extern int rt_mutex_trylock(struct rt_mutex *lock);
 
 extern void rt_mutex_unlock(struct rt_mutex *lock);
diff --git a/include/linux/rwlock_rt.h b/include/linux/rwlock_rt.h
new file mode 100644
index 000000000000..aafdb0a685d5
--- /dev/null
+++ b/include/linux/rwlock_rt.h
@@ -0,0 +1,109 @@
+// SPDX-License-Identifier: GPL-2.0-only
+#ifndef __LINUX_RWLOCK_RT_H
+#define __LINUX_RWLOCK_RT_H
+
+#ifndef __LINUX_SPINLOCK_H
+#error Do not include directly. Use spinlock.h
+#endif
+
+extern void __lockfunc rt_write_lock(rwlock_t *rwlock);
+extern void __lockfunc rt_read_lock(rwlock_t *rwlock);
+extern int __lockfunc rt_write_trylock(rwlock_t *rwlock);
+extern int __lockfunc rt_read_trylock(rwlock_t *rwlock);
+extern void __lockfunc rt_write_unlock(rwlock_t *rwlock);
+extern void __lockfunc rt_read_unlock(rwlock_t *rwlock);
+extern int __lockfunc rt_read_can_lock(rwlock_t *rwlock);
+extern int __lockfunc rt_write_can_lock(rwlock_t *rwlock);
+extern void __rt_rwlock_init(rwlock_t *rwlock, char *name, struct lock_class_key *key);
+
+#define read_can_lock(rwlock)		rt_read_can_lock(rwlock)
+#define write_can_lock(rwlock)		rt_write_can_lock(rwlock)
+
+#define read_trylock(lock)	__cond_lock(lock, rt_read_trylock(lock))
+#define write_trylock(lock)	__cond_lock(lock, rt_write_trylock(lock))
+
+static inline int __write_trylock_rt_irqsave(rwlock_t *lock, unsigned long *flags)
+{
+	*flags = 0;
+	return rt_write_trylock(lock);
+}
+
+#define write_trylock_irqsave(lock, flags)		\
+	__cond_lock(lock, __write_trylock_rt_irqsave(lock, &(flags)))
+
+#define read_lock_irqsave(lock, flags)			\
+	do {						\
+		typecheck(unsigned long, flags);	\
+		rt_read_lock(lock);			\
+		flags = 0;				\
+	} while (0)
+
+#define write_lock_irqsave(lock, flags)			\
+	do {						\
+		typecheck(unsigned long, flags);	\
+		rt_write_lock(lock);			\
+		flags = 0;				\
+	} while (0)
+
+#define read_lock(lock)		rt_read_lock(lock)
+
+#define read_lock_bh(lock)				\
+	do {						\
+		local_bh_disable();			\
+		rt_read_lock(lock);			\
+	} while (0)
+
+#define read_lock_irq(lock)	read_lock(lock)
+
+#define write_lock(lock)	rt_write_lock(lock)
+
+#define write_lock_bh(lock)				\
+	do {						\
+		local_bh_disable();			\
+		rt_write_lock(lock);			\
+	} while (0)
+
+#define write_lock_irq(lock)	write_lock(lock)
+
+#define read_unlock(lock)	rt_read_unlock(lock)
+
+#define read_unlock_bh(lock)				\
+	do {						\
+		rt_read_unlock(lock);			\
+		local_bh_enable();			\
+	} while (0)
+
+#define read_unlock_irq(lock)	read_unlock(lock)
+
+#define write_unlock(lock)	rt_write_unlock(lock)
+
+#define write_unlock_bh(lock)				\
+	do {						\
+		rt_write_unlock(lock);			\
+		local_bh_enable();			\
+	} while (0)
+
+#define write_unlock_irq(lock)	write_unlock(lock)
+
+#define read_unlock_irqrestore(lock, flags)		\
+	do {						\
+		typecheck(unsigned long, flags);	\
+		(void) flags;				\
+		rt_read_unlock(lock);			\
+	} while (0)
+
+#define write_unlock_irqrestore(lock, flags) \
+	do {						\
+		typecheck(unsigned long, flags);	\
+		(void) flags;				\
+		rt_write_unlock(lock);			\
+	} while (0)
+
+#define rwlock_init(rwl)				\
+do {							\
+	static struct lock_class_key __key;		\
+							\
+	__rt_rwlock_init(rwl, #rwl, &__key);		\
+} while (0)
+
+#endif
diff --git a/include/linux/rwlock_types.h b/include/linux/rwlock_types.h
index 3bd03e18061c..0ad226b5d8fd 100644
--- a/include/linux/rwlock_types.h
+++ b/include/linux/rwlock_types.h
@@ -1,6 +1,10 @@
 #ifndef __LINUX_RWLOCK_TYPES_H
 #define __LINUX_RWLOCK_TYPES_H
 
+#if !defined(__LINUX_SPINLOCK_TYPES_H)
+# error "Do not include directly, include spinlock_types.h"
+#endif
+
 /*
  * include/linux/rwlock_types.h - generic rwlock type definitions
  *				  and initializers
diff --git a/include/linux/rwlock_types_rt.h b/include/linux/rwlock_types_rt.h
new file mode 100644
index 000000000000..4762391d659b
--- /dev/null
+++ b/include/linux/rwlock_types_rt.h
@@ -0,0 +1,56 @@
+// SPDX-License-Identifier: GPL-2.0-only
+#ifndef __LINUX_RWLOCK_TYPES_RT_H
+#define __LINUX_RWLOCK_TYPES_RT_H
+
+#ifndef __LINUX_SPINLOCK_TYPES_H
+#error "Do not include directly. Include spinlock_types.h instead"
+#endif
+
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+# define RW_DEP_MAP_INIT(lockname)	.dep_map = { .name = #lockname }
+#else
+# define RW_DEP_MAP_INIT(lockname)
+#endif
+
+typedef struct rt_rw_lock rwlock_t;
+
+#define __RW_LOCK_UNLOCKED(name) __RWLOCK_RT_INITIALIZER(name)
+
+#define DEFINE_RWLOCK(name) \
+	rwlock_t name = __RW_LOCK_UNLOCKED(name)
+
+/*
+ * A reader biased implementation primarily for CPU pinning.
+ *
+ * Can be selected as general replacement for the single reader RT rwlock
+ * variant
+ */
+struct rt_rw_lock {
+	struct rt_mutex		rtmutex;
+	atomic_t		readers;
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	struct lockdep_map	dep_map;
+#endif
+};
+
+#define READER_BIAS	(1U << 31)
+#define WRITER_BIAS	(1U << 30)
+
+#define __RWLOCK_RT_INITIALIZER(name)					\
+{									\
+	.readers = ATOMIC_INIT(READER_BIAS),				\
+	.rtmutex = __RT_MUTEX_INITIALIZER_SAVE_STATE(name.rtmutex),	\
+	RW_DEP_MAP_INIT(name)						\
+}
+
+void __rwlock_biased_rt_init(struct rt_rw_lock *lock, const char *name,
+			     struct lock_class_key *key);
+
+#define rwlock_biased_rt_init(rwlock)					\
+	do {								\
+		static struct lock_class_key __key;			\
+									\
+		__rwlock_biased_rt_init((rwlock), #rwlock, &__key);	\
+	} while (0)
+
+#endif
diff --git a/include/linux/rwsem-rt.h b/include/linux/rwsem-rt.h
new file mode 100644
index 000000000000..0ba8aae9a198
--- /dev/null
+++ b/include/linux/rwsem-rt.h
@@ -0,0 +1,70 @@
+// SPDX-License-Identifier: GPL-2.0-only
+#ifndef _LINUX_RWSEM_RT_H
+#define _LINUX_RWSEM_RT_H
+
+#ifndef _LINUX_RWSEM_H
+#error "Include rwsem.h"
+#endif
+
+#include <linux/rtmutex.h>
+#include <linux/swait.h>
+
+#define READER_BIAS		(1U << 31)
+#define WRITER_BIAS		(1U << 30)
+
+struct rw_semaphore {
+	atomic_t		readers;
+	struct rt_mutex		rtmutex;
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	struct lockdep_map	dep_map;
+#endif
+};
+
+#define __RWSEM_INITIALIZER(name)				\
+{								\
+	.readers = ATOMIC_INIT(READER_BIAS),			\
+	.rtmutex = __RT_MUTEX_INITIALIZER(name.rtmutex),	\
+	RW_DEP_MAP_INIT(name)					\
+}
+
+#define DECLARE_RWSEM(lockname) \
+	struct rw_semaphore lockname = __RWSEM_INITIALIZER(lockname)
+
+extern void  __rwsem_init(struct rw_semaphore *rwsem, const char *name,
+			  struct lock_class_key *key);
+
+#define __init_rwsem(sem, name, key)			\
+do {							\
+		rt_mutex_init(&(sem)->rtmutex);		\
+		__rwsem_init((sem), (name), (key));	\
+} while (0)
+
+#define init_rwsem(sem)					\
+do {							\
+	static struct lock_class_key __key;		\
+							\
+	__init_rwsem((sem), #sem, &__key);		\
+} while (0)
+
+static inline int rwsem_is_locked(struct rw_semaphore *sem)
+{
+	return atomic_read(&sem->readers) != READER_BIAS;
+}
+
+static inline int rwsem_is_contended(struct rw_semaphore *sem)
+{
+	return atomic_read(&sem->readers) > 0;
+}
+
+extern void __down_read(struct rw_semaphore *sem);
+extern int __down_read_interruptible(struct rw_semaphore *sem);
+extern int __down_read_killable(struct rw_semaphore *sem);
+extern int __down_read_trylock(struct rw_semaphore *sem);
+extern void __down_write(struct rw_semaphore *sem);
+extern int __must_check __down_write_killable(struct rw_semaphore *sem);
+extern int __down_write_trylock(struct rw_semaphore *sem);
+extern void __up_read(struct rw_semaphore *sem);
+extern void __up_write(struct rw_semaphore *sem);
+extern void __downgrade_write(struct rw_semaphore *sem);
+
+#endif
diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index 4c715be48717..9323af8a9244 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -16,6 +16,11 @@
 #include <linux/spinlock.h>
 #include <linux/atomic.h>
 #include <linux/err.h>
+
+#ifdef CONFIG_PREEMPT_RT
+#include <linux/rwsem-rt.h>
+#else /* PREEMPT_RT */
+
 #ifdef CONFIG_RWSEM_SPIN_ON_OWNER
 #include <linux/osq_lock.h>
 #endif
@@ -119,6 +124,13 @@ static inline int rwsem_is_contended(struct rw_semaphore *sem)
 	return !list_empty(&sem->wait_list);
 }
 
+#endif /* !PREEMPT_RT */
+
+/*
+ * The functions below are the same for all rwsem implementations including
+ * the RT specific variant.
+ */
+
 /*
  * lock for reading
  */
diff --git a/include/linux/sched.h b/include/linux/sched.h
index 76cd21fa5501..8caa85cefc20 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -34,6 +34,7 @@
 #include <linux/rseq.h>
 #include <linux/seqlock.h>
 #include <linux/kcsan.h>
+#include <asm/kmap_size.h>
 
 /* task_struct member predeclarations (sorted alphabetically): */
 struct audit_context;
@@ -111,12 +112,8 @@ struct io_uring_task;
 					 __TASK_TRACED | EXIT_DEAD | EXIT_ZOMBIE | \
 					 TASK_PARKED)
 
-#define task_is_traced(task)		((task->state & __TASK_TRACED) != 0)
-
 #define task_is_stopped(task)		((task->state & __TASK_STOPPED) != 0)
 
-#define task_is_stopped_or_traced(task)	((task->state & (__TASK_STOPPED | __TASK_TRACED)) != 0)
-
 #ifdef CONFIG_DEBUG_ATOMIC_SLEEP
 
 /*
@@ -140,6 +137,9 @@ struct io_uring_task;
 		smp_store_mb(current->state, (state_value));	\
 	} while (0)
 
+#define __set_current_state_no_track(state_value)		\
+	current->state = (state_value);
+
 #define set_special_state(state_value)					\
 	do {								\
 		unsigned long flags; /* may shadow */			\
@@ -193,6 +193,9 @@ struct io_uring_task;
 #define set_current_state(state_value)					\
 	smp_store_mb(current->state, (state_value))
 
+#define __set_current_state_no_track(state_value)	\
+	__set_current_state(state_value)
+
 /*
  * set_special_state() should be used for those states when the blocking task
  * can not use the regular condition based wait-loop. In that case we must
@@ -637,6 +640,13 @@ struct wake_q_node {
 	struct wake_q_node *next;
 };
 
+struct kmap_ctrl {
+#ifdef CONFIG_KMAP_LOCAL
+	int				idx;
+	pte_t				pteval[KM_MAX_IDX];
+#endif
+};
+
 struct task_struct {
 #ifdef CONFIG_THREAD_INFO_IN_TASK
 	/*
@@ -647,6 +657,8 @@ struct task_struct {
 #endif
 	/* -1 unrunnable, 0 runnable, >0 stopped: */
 	volatile long			state;
+	/* saved state for "spinlock sleepers" */
+	volatile long			saved_state;
 
 	/*
 	 * This begins the randomizable portion of task_struct. Only
@@ -722,6 +734,11 @@ struct task_struct {
 	int				nr_cpus_allowed;
 	const cpumask_t			*cpus_ptr;
 	cpumask_t			cpus_mask;
+	void				*migration_pending;
+#ifdef CONFIG_SMP
+	unsigned short			migration_disabled;
+#endif
+	unsigned short			migration_flags;
 
 #ifdef CONFIG_PREEMPT_RCU
 	int				rcu_read_lock_nesting;
@@ -968,11 +985,16 @@ struct task_struct {
 	/* Signal handlers: */
 	struct signal_struct		*signal;
 	struct sighand_struct __rcu		*sighand;
+	struct sigqueue			*sigqueue_cache;
 	sigset_t			blocked;
 	sigset_t			real_blocked;
 	/* Restored if set_restore_sigmask() was used: */
 	sigset_t			saved_sigmask;
 	struct sigpending		pending;
+#ifdef CONFIG_PREEMPT_RT
+	/* TODO: move me into ->restart_block ? */
+	struct				kernel_siginfo forced_info;
+#endif
 	unsigned long			sas_ss_sp;
 	size_t				sas_ss_size;
 	unsigned int			sas_ss_flags;
@@ -999,6 +1021,7 @@ struct task_struct {
 	raw_spinlock_t			pi_lock;
 
 	struct wake_q_node		wake_q;
+	struct wake_q_node		wake_q_sleeper;
 
 #ifdef CONFIG_RT_MUTEXES
 	/* PI waiters blocked on a rt_mutex held by this task: */
@@ -1026,6 +1049,9 @@ struct task_struct {
 	int				softirq_context;
 	int				irq_config;
 #endif
+#ifdef CONFIG_PREEMPT_RT
+	int				softirq_disable_cnt;
+#endif
 
 #ifdef CONFIG_LOCKDEP
 # define MAX_LOCK_DEPTH			48UL
@@ -1311,6 +1337,7 @@ struct task_struct {
 	unsigned int			sequential_io;
 	unsigned int			sequential_io_avg;
 #endif
+	struct kmap_ctrl		kmap_ctrl;
 #ifdef CONFIG_DEBUG_ATOMIC_SLEEP
 	unsigned long			task_state_change;
 #endif
@@ -1348,6 +1375,10 @@ struct task_struct {
 	struct callback_head		mce_kill_me;
 #endif
 
+#ifdef CONFIG_PREEMPT_RT
+	atomic_t xmit_recursion;
+#endif
+
 	/*
 	 * New fields for task_struct should be added above here, so that
 	 * they are included in the randomized portion of task_struct.
@@ -1755,6 +1786,7 @@ extern struct task_struct *find_get_task_by_vpid(pid_t nr);
 
 extern int wake_up_state(struct task_struct *tsk, unsigned int state);
 extern int wake_up_process(struct task_struct *tsk);
+extern int wake_up_lock_sleeper(struct task_struct *tsk);
 extern void wake_up_new_task(struct task_struct *tsk);
 
 #ifdef CONFIG_SMP
@@ -1845,6 +1877,89 @@ static inline int test_tsk_need_resched(struct task_struct *tsk)
 	return unlikely(test_tsk_thread_flag(tsk,TIF_NEED_RESCHED));
 }
 
+#ifdef CONFIG_PREEMPT_LAZY
+static inline void set_tsk_need_resched_lazy(struct task_struct *tsk)
+{
+	set_tsk_thread_flag(tsk,TIF_NEED_RESCHED_LAZY);
+}
+
+static inline void clear_tsk_need_resched_lazy(struct task_struct *tsk)
+{
+	clear_tsk_thread_flag(tsk,TIF_NEED_RESCHED_LAZY);
+}
+
+static inline int test_tsk_need_resched_lazy(struct task_struct *tsk)
+{
+	return unlikely(test_tsk_thread_flag(tsk,TIF_NEED_RESCHED_LAZY));
+}
+
+static inline int need_resched_lazy(void)
+{
+	return test_thread_flag(TIF_NEED_RESCHED_LAZY);
+}
+
+static inline int need_resched_now(void)
+{
+	return test_thread_flag(TIF_NEED_RESCHED);
+}
+
+#else
+static inline void clear_tsk_need_resched_lazy(struct task_struct *tsk) { }
+static inline int need_resched_lazy(void) { return 0; }
+
+static inline int need_resched_now(void)
+{
+	return test_thread_flag(TIF_NEED_RESCHED);
+}
+
+#endif
+
+
+static inline bool __task_is_stopped_or_traced(struct task_struct *task)
+{
+	if (task->state & (__TASK_STOPPED | __TASK_TRACED))
+		return true;
+#ifdef CONFIG_PREEMPT_RT
+	if (task->saved_state & (__TASK_STOPPED | __TASK_TRACED))
+		return true;
+#endif
+	return false;
+}
+
+static inline bool task_is_stopped_or_traced(struct task_struct *task)
+{
+	bool traced_stopped;
+
+#ifdef CONFIG_PREEMPT_RT
+	unsigned long flags;
+
+	raw_spin_lock_irqsave(&task->pi_lock, flags);
+	traced_stopped = __task_is_stopped_or_traced(task);
+	raw_spin_unlock_irqrestore(&task->pi_lock, flags);
+#else
+	traced_stopped = __task_is_stopped_or_traced(task);
+#endif
+	return traced_stopped;
+}
+
+static inline bool task_is_traced(struct task_struct *task)
+{
+	bool traced = false;
+
+	if (task->state & __TASK_TRACED)
+		return true;
+#ifdef CONFIG_PREEMPT_RT
+	/* in case the task is sleeping on tasklist_lock */
+	raw_spin_lock_irq(&task->pi_lock);
+	if (task->state & __TASK_TRACED)
+		traced = true;
+	else if (task->saved_state & __TASK_TRACED)
+		traced = true;
+	raw_spin_unlock_irq(&task->pi_lock);
+#endif
+	return traced;
+}
+
 /*
  * cond_resched() and cond_resched_lock(): latency reduction via
  * explicit rescheduling in places that are safe. The return
diff --git a/include/linux/sched/hotplug.h b/include/linux/sched/hotplug.h
index 9a62ffdd296f..412cdaba33eb 100644
--- a/include/linux/sched/hotplug.h
+++ b/include/linux/sched/hotplug.h
@@ -11,8 +11,10 @@ extern int sched_cpu_activate(unsigned int cpu);
 extern int sched_cpu_deactivate(unsigned int cpu);
 
 #ifdef CONFIG_HOTPLUG_CPU
+extern int sched_cpu_wait_empty(unsigned int cpu);
 extern int sched_cpu_dying(unsigned int cpu);
 #else
+# define sched_cpu_wait_empty	NULL
 # define sched_cpu_dying	NULL
 #endif
 
diff --git a/include/linux/sched/mm.h b/include/linux/sched/mm.h
index d5ece7a9a403..aa8fcf6ed0be 100644
--- a/include/linux/sched/mm.h
+++ b/include/linux/sched/mm.h
@@ -49,6 +49,17 @@ static inline void mmdrop(struct mm_struct *mm)
 		__mmdrop(mm);
 }
 
+#ifdef CONFIG_PREEMPT_RT
+extern void __mmdrop_delayed(struct rcu_head *rhp);
+static inline void mmdrop_delayed(struct mm_struct *mm)
+{
+	if (atomic_dec_and_test(&mm->mm_count))
+		call_rcu(&mm->delayed_drop, __mmdrop_delayed);
+}
+#else
+# define mmdrop_delayed(mm)    mmdrop(mm)
+#endif
+
 /**
  * mmget() - Pin the address space associated with a &struct mm_struct.
  * @mm: The address space to pin.
diff --git a/include/linux/sched/rt.h b/include/linux/sched/rt.h
index e5af028c08b4..994c25640e15 100644
--- a/include/linux/sched/rt.h
+++ b/include/linux/sched/rt.h
@@ -39,20 +39,12 @@ static inline struct task_struct *rt_mutex_get_top_task(struct task_struct *p)
 }
 extern void rt_mutex_setprio(struct task_struct *p, struct task_struct *pi_task);
 extern void rt_mutex_adjust_pi(struct task_struct *p);
-static inline bool tsk_is_pi_blocked(struct task_struct *tsk)
-{
-	return tsk->pi_blocked_on != NULL;
-}
 #else
 static inline struct task_struct *rt_mutex_get_top_task(struct task_struct *task)
 {
 	return NULL;
 }
 # define rt_mutex_adjust_pi(p)		do { } while (0)
-static inline bool tsk_is_pi_blocked(struct task_struct *tsk)
-{
-	return false;
-}
 #endif
 
 extern void normalize_rt_tasks(void);
diff --git a/include/linux/sched/wake_q.h b/include/linux/sched/wake_q.h
index 26a2013ac39c..6e2dff721547 100644
--- a/include/linux/sched/wake_q.h
+++ b/include/linux/sched/wake_q.h
@@ -58,6 +58,17 @@ static inline bool wake_q_empty(struct wake_q_head *head)
 
 extern void wake_q_add(struct wake_q_head *head, struct task_struct *task);
 extern void wake_q_add_safe(struct wake_q_head *head, struct task_struct *task);
-extern void wake_up_q(struct wake_q_head *head);
+extern void wake_q_add_sleeper(struct wake_q_head *head, struct task_struct *task);
+extern void __wake_up_q(struct wake_q_head *head, bool sleeper);
+
+static inline void wake_up_q(struct wake_q_head *head)
+{
+	__wake_up_q(head, false);
+}
+
+static inline void wake_up_q_sleeper(struct wake_q_head *head)
+{
+	__wake_up_q(head, true);
+}
 
 #endif /* _LINUX_SCHED_WAKE_Q_H */
diff --git a/include/linux/serial_8250.h b/include/linux/serial_8250.h
index 2b70f736b091..68d756373b53 100644
--- a/include/linux/serial_8250.h
+++ b/include/linux/serial_8250.h
@@ -7,6 +7,7 @@
 #ifndef _LINUX_SERIAL_8250_H
 #define _LINUX_SERIAL_8250_H
 
+#include <linux/atomic.h>
 #include <linux/serial_core.h>
 #include <linux/serial_reg.h>
 #include <linux/platform_device.h>
@@ -125,6 +126,8 @@ struct uart_8250_port {
 #define MSR_SAVE_FLAGS UART_MSR_ANY_DELTA
 	unsigned char		msr_saved_flags;
 
+	atomic_t		console_printing;
+
 	struct uart_8250_dma	*dma;
 	const struct uart_8250_ops *ops;
 
@@ -180,6 +183,8 @@ void serial8250_init_port(struct uart_8250_port *up);
 void serial8250_set_defaults(struct uart_8250_port *up);
 void serial8250_console_write(struct uart_8250_port *up, const char *s,
 			      unsigned int count);
+void serial8250_console_write_atomic(struct uart_8250_port *up, const char *s,
+				     unsigned int count);
 int serial8250_console_setup(struct uart_port *port, char *options, bool probe);
 int serial8250_console_exit(struct uart_port *port);
 
diff --git a/include/linux/shmem_fs.h b/include/linux/shmem_fs.h
index a5a5d1d4d7b1..0470d1582b09 100644
--- a/include/linux/shmem_fs.h
+++ b/include/linux/shmem_fs.h
@@ -31,7 +31,7 @@ struct shmem_sb_info {
 	struct percpu_counter used_blocks;  /* How many are allocated */
 	unsigned long max_inodes;   /* How many inodes are allowed */
 	unsigned long free_inodes;  /* How many are left for allocation */
-	spinlock_t stat_lock;	    /* Serialize shmem_sb_info changes */
+	raw_spinlock_t stat_lock;   /* Serialize shmem_sb_info changes */
 	umode_t mode;		    /* Mount mode for root directory */
 	unsigned char huge;	    /* Whether to try for hugepages */
 	kuid_t uid;		    /* Mount uid for root directory */
diff --git a/include/linux/signal.h b/include/linux/signal.h
index b256f9c65661..ebf6c515a7b2 100644
--- a/include/linux/signal.h
+++ b/include/linux/signal.h
@@ -265,6 +265,7 @@ static inline void init_sigpending(struct sigpending *sig)
 }
 
 extern void flush_sigqueue(struct sigpending *queue);
+extern void flush_task_sigqueue(struct task_struct *tsk);
 
 /* Test if 'sig' is valid signal. Use this instead of testing _NSIG directly */
 static inline int valid_signal(unsigned long sig)
diff --git a/include/linux/skbuff.h b/include/linux/skbuff.h
index a828cf99c521..2e4f80cd41df 100644
--- a/include/linux/skbuff.h
+++ b/include/linux/skbuff.h
@@ -295,6 +295,7 @@ struct sk_buff_head {
 
 	__u32		qlen;
 	spinlock_t	lock;
+	raw_spinlock_t	raw_lock;
 };
 
 struct sk_buff;
@@ -1884,6 +1885,12 @@ static inline void skb_queue_head_init(struct sk_buff_head *list)
 	__skb_queue_head_init(list);
 }
 
+static inline void skb_queue_head_init_raw(struct sk_buff_head *list)
+{
+	raw_spin_lock_init(&list->raw_lock);
+	__skb_queue_head_init(list);
+}
+
 static inline void skb_queue_head_init_class(struct sk_buff_head *list,
 		struct lock_class_key *class)
 {
diff --git a/include/linux/smp.h b/include/linux/smp.h
index 9f13966d3d92..c1f6aaade44a 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -239,6 +239,9 @@ static inline int get_boot_cpu_id(void)
 #define get_cpu()		({ preempt_disable(); __smp_processor_id(); })
 #define put_cpu()		preempt_enable()
 
+#define get_cpu_light()		({ migrate_disable(); __smp_processor_id(); })
+#define put_cpu_light()		migrate_enable()
+
 /*
  * Callback to arch code if there's nosmp or maxcpus=0 on the
  * boot command line:
diff --git a/include/linux/spi/kbus.h b/include/linux/spi/kbus.h
new file mode 100644
index 000000000000..07e27b4cbd55
--- /dev/null
+++ b/include/linux/spi/kbus.h
@@ -0,0 +1,142 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * include/linux/spi/kbus.h
+ *
+ * Copyright (C) 2020 WAGO
+ *	Heinrich Toews <heinrich.toews@wago.com>
+ */
+
+#ifndef KBUS_H
+#define KBUS_H
+
+#include <linux/types.h>
+
+#define KBUS_DRIVER_MAJOR 240
+#define KBUS_IRQ_TIMEOUT 10 /* wait for a max. of 10 ms */
+
+#define KBUS__DEFAULT_SPEED 12000000
+#define KBUS_READYN_IRQ
+#define KBUS_DATA_FULLDPX
+
+#undef KBUS__WAIT_ACTIVE
+#undef KBUS_IRQN_IRQ
+
+/*---------------------------------------------------------------------------*/
+
+#define PAC_KBUS_SYNC_CYCLES 3
+#define SPI_MODE_MASK                                                          \
+	(SPI_CPHA | SPI_CPOL | SPI_CS_HIGH | SPI_LSB_FIRST | SPI_3WIRE |       \
+	 SPI_LOOP | SPI_NO_CS | SPI_READY)
+
+#define KBUS__MAX_BUF_LEN PAGE_SIZE /* FIXME */
+
+#define KBUS_TESTING 0 /* enable when gpio testing wanted */
+
+#define KBUS__USE_DMA_ONLY 0
+
+#if KBUS_TESTING
+#define kbus_wago_mpoint(k) wago_mpoint()
+#define kbus_dbg(format, arg...) pr_debug("kbus-dbg:" format, ##arg)
+#else
+#define kbus_wago_mpoint(k)                                                    \
+	({                                                                     \
+		if (0)                                                         \
+			wago_mpoint();                                         \
+		0;                                                             \
+	})
+#define kbus_dbg(format, arg...)                                               \
+	({                                                                     \
+		if (0)                                                         \
+			pr_debug("kbus-dbg:" format, ##arg);                   \
+		0;                                                             \
+	})
+#endif
+
+#define KBUS_ENABLE_IRQ(irq)                                                   \
+	do {                                                                   \
+		if (irq != -1)                                                 \
+			enable_irq(irq);                                       \
+	} while (0)
+
+#define KBUS_DISABLE_IRQ(irq)                                                  \
+	do {                                                                   \
+		if (irq != -1)                                                 \
+			disable_irq(irq);                                      \
+	} while (0)
+
+struct kbus_drv_data {
+	u8 cmdsel; /* 0: data, 1: cmd mode */
+	int kbus_err;
+	int kbus_err_state;
+	u8 *tx_buf;
+	u8 *rx_buf;
+	bool use_dma;
+	u32 timeout_ms;
+	dma_addr_t tx_buf_dma;
+	dma_addr_t rx_buf_dma;
+	int kbus_irq;
+	int kbus_irq_enabled;
+	int kbus_irq_state;
+	struct task_struct
+		*dma_task; /* task pointer to boost the dma task if necessary */
+	bool kbus_dma_boost_en;
+	u8 kbus_dma_boost_prio;
+	const char *kbus_dma_boost_irq_thread;
+	u8 kbus_dma_normal_prio;
+
+	/* gpios */
+	struct gpio_desc *gpio_nrdy;
+	struct gpio_desc *gpio_nrst;
+	struct gpio_desc *gpio_nsync;
+	struct gpio_desc *gpio_cmdsel;
+	struct gpio_desc *gpio_nirq;
+	struct gpio_desc *gpio_nerr;
+
+	wait_queue_head_t kbus_irq_wq;
+	struct spi_device *spi;
+	const char *kbus_tty_device_name;
+};
+
+/* For userspace ioctl communication */
+struct kbus_data {
+	__u8 __user *tx_buf;
+	__u8 __user *rx_buf;
+	__u32 byte_len;
+	__u8 __user *err; /* will only be set when err occurs! */
+	__u8 __user *err_state;
+	__u32 timeout_ms;
+};
+
+struct kbus_cmd {
+	__u8 __user *tx_buf;
+	__u8 __user *rx_buf;
+	__u32 byte_len_tx;
+	__u32 byte_len_rx;
+	__u8 __user *err; /* will only be set when err occurs! */
+	__u8 __user *err_state;
+	__u32 timeout_ms;
+};
+
+struct kbus_spi_config {
+	__u8 bits_per_word; /* bits_per_word */
+	__u8 mode; /* see SPI_ mode bits in spi.h */
+	__u32 max_speed_hz;
+};
+
+extern int kbus_wait_for_irq(void);
+extern int kbus_wait_for_event(int *event);
+extern int kbus_error(void);
+extern int kbus_wait_for_gpio(int gpio);
+extern void kbus_boost_dma_task(u8 enable);
+
+/* IOCTL commands */
+#define KBUS_IOC_MAGIC 'K'
+#define KBUS_IOC_DATA _IOW(KBUS_IOC_MAGIC, 1, struct kbus_data)
+#define KBUS_IOC_CMD _IOW(KBUS_IOC_MAGIC, 2, struct kbus_cmd)
+#define KBUS_IOC_CONFIG _IOW(KBUS_IOC_MAGIC, 3, struct kbus_spi_config)
+#define KBUS_IOC_BINARY _IOW(KBUS_IOC_MAGIC, 4, struct kbus_data)
+
+extern struct spi_driver
+	kbus_driver; /* used by spi-omap2-mcspi to recognize the kbus device */
+
+#endif /* KBUS_H */
diff --git a/include/linux/spinlock.h b/include/linux/spinlock.h
index 79897841a2cc..c3c70291b46c 100644
--- a/include/linux/spinlock.h
+++ b/include/linux/spinlock.h
@@ -309,7 +309,11 @@ static inline void do_raw_spin_unlock(raw_spinlock_t *lock) __releases(lock)
 })
 
 /* Include rwlock functions */
-#include <linux/rwlock.h>
+#ifdef CONFIG_PREEMPT_RT
+# include <linux/rwlock_rt.h>
+#else
+# include <linux/rwlock.h>
+#endif
 
 /*
  * Pull the _spin_*()/_read_*()/_write_*() functions/declarations:
@@ -320,6 +324,10 @@ static inline void do_raw_spin_unlock(raw_spinlock_t *lock) __releases(lock)
 # include <linux/spinlock_api_up.h>
 #endif
 
+#ifdef CONFIG_PREEMPT_RT
+# include <linux/spinlock_rt.h>
+#else /* PREEMPT_RT */
+
 /*
  * Map the spin_lock functions to the raw variants for PREEMPT_RT=n
  */
@@ -454,6 +462,8 @@ static __always_inline int spin_is_contended(spinlock_t *lock)
 
 #define assert_spin_locked(lock)	assert_raw_spin_locked(&(lock)->rlock)
 
+#endif /* !PREEMPT_RT */
+
 /*
  * Pull the atomic_t declaration:
  * (asm-mips/atomic.h needs above definitions)
diff --git a/include/linux/spinlock_api_smp.h b/include/linux/spinlock_api_smp.h
index 19a9be9d97ee..da38149f2843 100644
--- a/include/linux/spinlock_api_smp.h
+++ b/include/linux/spinlock_api_smp.h
@@ -187,6 +187,8 @@ static inline int __raw_spin_trylock_bh(raw_spinlock_t *lock)
 	return 0;
 }
 
-#include <linux/rwlock_api_smp.h>
+#ifndef CONFIG_PREEMPT_RT
+# include <linux/rwlock_api_smp.h>
+#endif
 
 #endif /* __LINUX_SPINLOCK_API_SMP_H */
diff --git a/include/linux/spinlock_rt.h b/include/linux/spinlock_rt.h
new file mode 100644
index 000000000000..3085132eae38
--- /dev/null
+++ b/include/linux/spinlock_rt.h
@@ -0,0 +1,155 @@
+// SPDX-License-Identifier: GPL-2.0-only
+#ifndef __LINUX_SPINLOCK_RT_H
+#define __LINUX_SPINLOCK_RT_H
+
+#ifndef __LINUX_SPINLOCK_H
+#error Do not include directly. Use spinlock.h
+#endif
+
+#include <linux/bug.h>
+
+extern void
+__rt_spin_lock_init(spinlock_t *lock, const char *name, struct lock_class_key *key);
+
+#define spin_lock_init(slock)				\
+do {							\
+	static struct lock_class_key __key;		\
+							\
+	rt_mutex_init(&(slock)->lock);			\
+	__rt_spin_lock_init(slock, #slock, &__key);	\
+} while (0)
+
+extern void __lockfunc rt_spin_lock(spinlock_t *lock);
+extern void __lockfunc rt_spin_lock_nested(spinlock_t *lock, int subclass);
+extern void __lockfunc rt_spin_lock_nest_lock(spinlock_t *lock, struct lockdep_map *nest_lock);
+extern void __lockfunc rt_spin_unlock(spinlock_t *lock);
+extern void __lockfunc rt_spin_lock_unlock(spinlock_t *lock);
+extern int __lockfunc rt_spin_trylock_irqsave(spinlock_t *lock, unsigned long *flags);
+extern int __lockfunc rt_spin_trylock_bh(spinlock_t *lock);
+extern int __lockfunc rt_spin_trylock(spinlock_t *lock);
+extern int atomic_dec_and_spin_lock(atomic_t *atomic, spinlock_t *lock);
+
+/*
+ * lockdep-less calls, for derived types like rwlock:
+ * (for trylock they can use rt_mutex_trylock() directly.
+ * Migrate disable handling must be done at the call site.
+ */
+extern void __lockfunc __rt_spin_lock(struct rt_mutex *lock);
+extern void __lockfunc __rt_spin_trylock(struct rt_mutex *lock);
+extern void __lockfunc __rt_spin_unlock(struct rt_mutex *lock);
+
+#define spin_lock(lock)			rt_spin_lock(lock)
+
+#define spin_lock_bh(lock)			\
+	do {					\
+		local_bh_disable();		\
+		rt_spin_lock(lock);		\
+	} while (0)
+
+#define spin_lock_irq(lock)		spin_lock(lock)
+
+#define spin_do_trylock(lock)		__cond_lock(lock, rt_spin_trylock(lock))
+
+#define spin_trylock(lock)			\
+({						\
+	int __locked;				\
+	__locked = spin_do_trylock(lock);	\
+	__locked;				\
+})
+
+#ifdef CONFIG_LOCKDEP
+# define spin_lock_nested(lock, subclass)		\
+	do {						\
+		rt_spin_lock_nested(lock, subclass);	\
+	} while (0)
+
+#define spin_lock_bh_nested(lock, subclass)		\
+	do {						\
+		local_bh_disable();			\
+		rt_spin_lock_nested(lock, subclass);	\
+	} while (0)
+
+# define spin_lock_nest_lock(lock, subclass)		\
+	do {                                                           \
+		typecheck(struct lockdep_map *, &(subclass)->dep_map);	\
+		rt_spin_lock_nest_lock(lock, &(subclass)->dep_map);	\
+	} while (0)
+
+# define spin_lock_irqsave_nested(lock, flags, subclass) \
+	do {						 \
+		typecheck(unsigned long, flags);	 \
+		flags = 0;				 \
+		rt_spin_lock_nested(lock, subclass);	 \
+	} while (0)
+#else
+# define spin_lock_nested(lock, subclass)	spin_lock(((void)(subclass), (lock)))
+# define spin_lock_nest_lock(lock, subclass)	spin_lock(((void)(subclass), (lock)))
+# define spin_lock_bh_nested(lock, subclass)	spin_lock_bh(((void)(subclass), (lock)))
+
+# define spin_lock_irqsave_nested(lock, flags, subclass) \
+	do {						 \
+		typecheck(unsigned long, flags);	 \
+		flags = 0;				 \
+		spin_lock(((void)(subclass), (lock)));	 \
+	} while (0)
+#endif
+
+#define spin_lock_irqsave(lock, flags)			 \
+	do {						 \
+		typecheck(unsigned long, flags);	 \
+		flags = 0;				 \
+		spin_lock(lock);			 \
+	} while (0)
+
+#define spin_unlock(lock)			rt_spin_unlock(lock)
+
+#define spin_unlock_bh(lock)				\
+	do {						\
+		rt_spin_unlock(lock);			\
+		local_bh_enable();			\
+	} while (0)
+
+#define spin_unlock_irq(lock)		spin_unlock(lock)
+
+#define spin_unlock_irqrestore(lock, flags)		\
+	do {						\
+		typecheck(unsigned long, flags);	\
+		(void) flags;				\
+		spin_unlock(lock);			\
+	} while (0)
+
+#define spin_trylock_bh(lock)	__cond_lock(lock, rt_spin_trylock_bh(lock))
+#define spin_trylock_irq(lock)	spin_trylock(lock)
+
+#define spin_trylock_irqsave(lock, flags)		\
+({							\
+	int __locked;					\
+							\
+	typecheck(unsigned long, flags);		\
+	flags = 0;					\
+	__locked = spin_trylock(lock);			\
+	__locked;					\
+})
+
+#ifdef CONFIG_GENERIC_LOCKBREAK
+# define spin_is_contended(lock)	((lock)->break_lock)
+#else
+# define spin_is_contended(lock)	(((void)(lock), 0))
+#endif
+
+static inline int spin_can_lock(spinlock_t *lock)
+{
+	return !rt_mutex_is_locked(&lock->lock);
+}
+
+static inline int spin_is_locked(spinlock_t *lock)
+{
+	return rt_mutex_is_locked(&lock->lock);
+}
+
+static inline void assert_spin_locked(spinlock_t *lock)
+{
+	BUG_ON(!spin_is_locked(lock));
+}
+
+#endif
diff --git a/include/linux/spinlock_types.h b/include/linux/spinlock_types.h
index b981caafe8bf..8d896d3e1a01 100644
--- a/include/linux/spinlock_types.h
+++ b/include/linux/spinlock_types.h
@@ -9,93 +9,15 @@
  * Released under the General Public License (GPL).
  */
 
-#if defined(CONFIG_SMP)
-# include <asm/spinlock_types.h>
-#else
-# include <linux/spinlock_types_up.h>
-#endif
-
-#include <linux/lockdep_types.h>
-
-typedef struct raw_spinlock {
-	arch_spinlock_t raw_lock;
-#ifdef CONFIG_DEBUG_SPINLOCK
-	unsigned int magic, owner_cpu;
-	void *owner;
-#endif
-#ifdef CONFIG_DEBUG_LOCK_ALLOC
-	struct lockdep_map dep_map;
-#endif
-} raw_spinlock_t;
-
-#define SPINLOCK_MAGIC		0xdead4ead
-
-#define SPINLOCK_OWNER_INIT	((void *)-1L)
-
-#ifdef CONFIG_DEBUG_LOCK_ALLOC
-# define RAW_SPIN_DEP_MAP_INIT(lockname)		\
-	.dep_map = {					\
-		.name = #lockname,			\
-		.wait_type_inner = LD_WAIT_SPIN,	\
-	}
-# define SPIN_DEP_MAP_INIT(lockname)			\
-	.dep_map = {					\
-		.name = #lockname,			\
-		.wait_type_inner = LD_WAIT_CONFIG,	\
-	}
-#else
-# define RAW_SPIN_DEP_MAP_INIT(lockname)
-# define SPIN_DEP_MAP_INIT(lockname)
-#endif
+#include <linux/spinlock_types_raw.h>
 
-#ifdef CONFIG_DEBUG_SPINLOCK
-# define SPIN_DEBUG_INIT(lockname)		\
-	.magic = SPINLOCK_MAGIC,		\
-	.owner_cpu = -1,			\
-	.owner = SPINLOCK_OWNER_INIT,
+#ifndef CONFIG_PREEMPT_RT
+# include <linux/spinlock_types_nort.h>
+# include <linux/rwlock_types.h>
 #else
-# define SPIN_DEBUG_INIT(lockname)
+# include <linux/rtmutex.h>
+# include <linux/spinlock_types_rt.h>
+# include <linux/rwlock_types_rt.h>
 #endif
 
-#define __RAW_SPIN_LOCK_INITIALIZER(lockname)	\
-	{					\
-	.raw_lock = __ARCH_SPIN_LOCK_UNLOCKED,	\
-	SPIN_DEBUG_INIT(lockname)		\
-	RAW_SPIN_DEP_MAP_INIT(lockname) }
-
-#define __RAW_SPIN_LOCK_UNLOCKED(lockname)	\
-	(raw_spinlock_t) __RAW_SPIN_LOCK_INITIALIZER(lockname)
-
-#define DEFINE_RAW_SPINLOCK(x)	raw_spinlock_t x = __RAW_SPIN_LOCK_UNLOCKED(x)
-
-typedef struct spinlock {
-	union {
-		struct raw_spinlock rlock;
-
-#ifdef CONFIG_DEBUG_LOCK_ALLOC
-# define LOCK_PADSIZE (offsetof(struct raw_spinlock, dep_map))
-		struct {
-			u8 __padding[LOCK_PADSIZE];
-			struct lockdep_map dep_map;
-		};
-#endif
-	};
-} spinlock_t;
-
-#define ___SPIN_LOCK_INITIALIZER(lockname)	\
-	{					\
-	.raw_lock = __ARCH_SPIN_LOCK_UNLOCKED,	\
-	SPIN_DEBUG_INIT(lockname)		\
-	SPIN_DEP_MAP_INIT(lockname) }
-
-#define __SPIN_LOCK_INITIALIZER(lockname) \
-	{ { .rlock = ___SPIN_LOCK_INITIALIZER(lockname) } }
-
-#define __SPIN_LOCK_UNLOCKED(lockname) \
-	(spinlock_t) __SPIN_LOCK_INITIALIZER(lockname)
-
-#define DEFINE_SPINLOCK(x)	spinlock_t x = __SPIN_LOCK_UNLOCKED(x)
-
-#include <linux/rwlock_types.h>
-
 #endif /* __LINUX_SPINLOCK_TYPES_H */
diff --git a/include/linux/spinlock_types_nort.h b/include/linux/spinlock_types_nort.h
new file mode 100644
index 000000000000..e4549f0dd197
--- /dev/null
+++ b/include/linux/spinlock_types_nort.h
@@ -0,0 +1,39 @@
+#ifndef __LINUX_SPINLOCK_TYPES_NORT_H
+#define __LINUX_SPINLOCK_TYPES_NORT_H
+
+#ifndef __LINUX_SPINLOCK_TYPES_H
+#error "Do not include directly. Include spinlock_types.h instead"
+#endif
+
+/*
+ * The non RT version maps spinlocks to raw_spinlocks
+ */
+typedef struct spinlock {
+	union {
+		struct raw_spinlock rlock;
+
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+# define LOCK_PADSIZE (offsetof(struct raw_spinlock, dep_map))
+		struct {
+			u8 __padding[LOCK_PADSIZE];
+			struct lockdep_map dep_map;
+		};
+#endif
+	};
+} spinlock_t;
+
+#define ___SPIN_LOCK_INITIALIZER(lockname)	\
+{						\
+	.raw_lock = __ARCH_SPIN_LOCK_UNLOCKED,	\
+	SPIN_DEBUG_INIT(lockname)		\
+	SPIN_DEP_MAP_INIT(lockname) }
+
+#define __SPIN_LOCK_INITIALIZER(lockname) \
+	{ { .rlock = ___SPIN_LOCK_INITIALIZER(lockname) } }
+
+#define __SPIN_LOCK_UNLOCKED(lockname) \
+	(spinlock_t) __SPIN_LOCK_INITIALIZER(lockname)
+
+#define DEFINE_SPINLOCK(x)	spinlock_t x = __SPIN_LOCK_UNLOCKED(x)
+
+#endif
diff --git a/include/linux/spinlock_types_raw.h b/include/linux/spinlock_types_raw.h
new file mode 100644
index 000000000000..1d4a180e983d
--- /dev/null
+++ b/include/linux/spinlock_types_raw.h
@@ -0,0 +1,65 @@
+#ifndef __LINUX_SPINLOCK_TYPES_RAW_H
+#define __LINUX_SPINLOCK_TYPES_RAW_H
+
+#include <linux/types.h>
+
+#if defined(CONFIG_SMP)
+# include <asm/spinlock_types.h>
+#else
+# include <linux/spinlock_types_up.h>
+#endif
+
+#include <linux/lockdep_types.h>
+
+typedef struct raw_spinlock {
+	arch_spinlock_t raw_lock;
+#ifdef CONFIG_DEBUG_SPINLOCK
+	unsigned int magic, owner_cpu;
+	void *owner;
+#endif
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	struct lockdep_map dep_map;
+#endif
+} raw_spinlock_t;
+
+#define SPINLOCK_MAGIC		0xdead4ead
+
+#define SPINLOCK_OWNER_INIT	((void *)-1L)
+
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+# define RAW_SPIN_DEP_MAP_INIT(lockname)		\
+	.dep_map = {					\
+		.name = #lockname,			\
+		.wait_type_inner = LD_WAIT_SPIN,	\
+	}
+# define SPIN_DEP_MAP_INIT(lockname)			\
+	.dep_map = {					\
+		.name = #lockname,			\
+		.wait_type_inner = LD_WAIT_CONFIG,	\
+	}
+#else
+# define RAW_SPIN_DEP_MAP_INIT(lockname)
+# define SPIN_DEP_MAP_INIT(lockname)
+#endif
+
+#ifdef CONFIG_DEBUG_SPINLOCK
+# define SPIN_DEBUG_INIT(lockname)		\
+	.magic = SPINLOCK_MAGIC,		\
+	.owner_cpu = -1,			\
+	.owner = SPINLOCK_OWNER_INIT,
+#else
+# define SPIN_DEBUG_INIT(lockname)
+#endif
+
+#define __RAW_SPIN_LOCK_INITIALIZER(lockname)	\
+{						\
+	.raw_lock = __ARCH_SPIN_LOCK_UNLOCKED,	\
+	SPIN_DEBUG_INIT(lockname)		\
+	RAW_SPIN_DEP_MAP_INIT(lockname) }
+
+#define __RAW_SPIN_LOCK_UNLOCKED(lockname)	\
+	(raw_spinlock_t) __RAW_SPIN_LOCK_INITIALIZER(lockname)
+
+#define DEFINE_RAW_SPINLOCK(x)  raw_spinlock_t x = __RAW_SPIN_LOCK_UNLOCKED(x)
+
+#endif
diff --git a/include/linux/spinlock_types_rt.h b/include/linux/spinlock_types_rt.h
new file mode 100644
index 000000000000..446da786e5d5
--- /dev/null
+++ b/include/linux/spinlock_types_rt.h
@@ -0,0 +1,38 @@
+// SPDX-License-Identifier: GPL-2.0-only
+#ifndef __LINUX_SPINLOCK_TYPES_RT_H
+#define __LINUX_SPINLOCK_TYPES_RT_H
+
+#ifndef __LINUX_SPINLOCK_TYPES_H
+#error "Do not include directly. Include spinlock_types.h instead"
+#endif
+
+#include <linux/cache.h>
+
+/*
+ * PREEMPT_RT: spinlocks - an RT mutex plus lock-break field:
+ */
+typedef struct spinlock {
+	struct rt_mutex		lock;
+	unsigned int		break_lock;
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	struct lockdep_map	dep_map;
+#endif
+} spinlock_t;
+
+#define __RT_SPIN_INITIALIZER(name) \
+	{ \
+	.wait_lock = __RAW_SPIN_LOCK_UNLOCKED(name.wait_lock), \
+	.save_state = 1, \
+	}
+/*
+.wait_list = PLIST_HEAD_INIT_RAW((name).lock.wait_list, (name).lock.wait_lock)
+*/
+
+#define __SPIN_LOCK_UNLOCKED(name)			\
+	{ .lock = __RT_SPIN_INITIALIZER(name.lock),		\
+	  SPIN_DEP_MAP_INIT(name) }
+
+#define DEFINE_SPINLOCK(name) \
+	spinlock_t name = __SPIN_LOCK_UNLOCKED(name)
+
+#endif
diff --git a/include/linux/spinlock_types_up.h b/include/linux/spinlock_types_up.h
index c09b6407ae1b..d9b371fa13e0 100644
--- a/include/linux/spinlock_types_up.h
+++ b/include/linux/spinlock_types_up.h
@@ -1,7 +1,7 @@
 #ifndef __LINUX_SPINLOCK_TYPES_UP_H
 #define __LINUX_SPINLOCK_TYPES_UP_H
 
-#ifndef __LINUX_SPINLOCK_TYPES_H
+#if !defined(__LINUX_SPINLOCK_TYPES_H) && !defined(__LINUX_RT_MUTEX_H)
 # error "please don't include this file directly"
 #endif
 
diff --git a/include/linux/stop_machine.h b/include/linux/stop_machine.h
index 76d8b09384a7..30577c3aecf8 100644
--- a/include/linux/stop_machine.h
+++ b/include/linux/stop_machine.h
@@ -24,6 +24,7 @@ typedef int (*cpu_stop_fn_t)(void *arg);
 struct cpu_stop_work {
 	struct list_head	list;		/* cpu_stopper->works */
 	cpu_stop_fn_t		fn;
+	unsigned long		caller;
 	void			*arg;
 	struct cpu_stop_done	*done;
 };
@@ -36,6 +37,8 @@ void stop_machine_park(int cpu);
 void stop_machine_unpark(int cpu);
 void stop_machine_yield(const struct cpumask *cpumask);
 
+extern void print_stop_info(const char *log_lvl, struct task_struct *task);
+
 #else	/* CONFIG_SMP */
 
 #include <linux/workqueue.h>
@@ -80,6 +83,8 @@ static inline bool stop_one_cpu_nowait(unsigned int cpu,
 	return false;
 }
 
+static inline void print_stop_info(const char *log_lvl, struct task_struct *task) { }
+
 #endif	/* CONFIG_SMP */
 
 /*
diff --git a/include/linux/switch.h b/include/linux/switch.h
new file mode 100644
index 000000000000..cc866785adbe
--- /dev/null
+++ b/include/linux/switch.h
@@ -0,0 +1,180 @@
+/*
+ * switch.h: Switch configuration API
+ *
+ * Copyright (C) 2008 Felix Fietkau <nbd@nbd.name>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version 2
+ * of the License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+#ifndef _LINUX_SWITCH_H
+#define _LINUX_SWITCH_H
+
+#include <net/genetlink.h>
+#include <uapi/linux/switch.h>
+
+struct switch_dev;
+struct switch_op;
+struct switch_val;
+struct switch_attr;
+struct switch_attrlist;
+struct switch_led_trigger;
+
+int register_switch(struct switch_dev *dev, struct net_device *netdev);
+void unregister_switch(struct switch_dev *dev);
+
+/**
+ * struct switch_attrlist - attribute list
+ *
+ * @n_attr: number of attributes
+ * @attr: pointer to the attributes array
+ */
+struct switch_attrlist {
+	int n_attr;
+	const struct switch_attr *attr;
+};
+
+enum switch_port_speed {
+	SWITCH_PORT_SPEED_UNKNOWN = 0,
+	SWITCH_PORT_SPEED_10 = 10,
+	SWITCH_PORT_SPEED_100 = 100,
+	SWITCH_PORT_SPEED_1000 = 1000,
+};
+
+struct switch_port_link {
+	bool link;
+	bool duplex;
+	bool aneg;
+	bool tx_flow;
+	bool rx_flow;
+	enum switch_port_speed speed;
+	/* in ethtool adv_t format */
+	u32 eee;
+};
+
+struct switch_port_stats {
+	unsigned long long tx_bytes;
+	unsigned long long rx_bytes;
+};
+
+/**
+ * struct switch_dev_ops - switch driver operations
+ *
+ * @attr_global: global switch attribute list
+ * @attr_port: port attribute list
+ * @attr_vlan: vlan attribute list
+ *
+ * Callbacks:
+ *
+ * @get_vlan_ports: read the port list of a VLAN
+ * @set_vlan_ports: set the port list of a VLAN
+ *
+ * @get_port_pvid: get the primary VLAN ID of a port
+ * @set_port_pvid: set the primary VLAN ID of a port
+ *
+ * @apply_config: apply all changed settings to the switch
+ * @reset_switch: resetting the switch
+ */
+struct switch_dev_ops {
+	struct switch_attrlist attr_global, attr_port, attr_vlan;
+
+	int (*get_vlan_ports)(struct switch_dev *dev, struct switch_val *val);
+	int (*set_vlan_ports)(struct switch_dev *dev, struct switch_val *val);
+
+	int (*get_port_pvid)(struct switch_dev *dev, int port, int *val);
+	int (*set_port_pvid)(struct switch_dev *dev, int port, int val);
+
+	int (*apply_config)(struct switch_dev *dev);
+	int (*reset_switch)(struct switch_dev *dev);
+
+	int (*get_port_link)(struct switch_dev *dev, int port,
+			     struct switch_port_link *link);
+	int (*set_port_link)(struct switch_dev *dev, int port,
+			     struct switch_port_link *link);
+	int (*get_port_stats)(struct switch_dev *dev, int port,
+			      struct switch_port_stats *stats);
+
+	int (*phy_read16)(struct switch_dev *dev, int addr, u8 reg, u16 *value);
+	int (*phy_write16)(struct switch_dev *dev, int addr, u8 reg, u16 value);
+};
+
+struct switch_dev {
+	struct device_node *of_node;
+	const struct switch_dev_ops *ops;
+	/* will be automatically filled */
+	char devname[IFNAMSIZ];
+
+	const char *name;
+	/* NB: either alias or netdev must be set */
+	const char *alias;
+	struct net_device *netdev;
+	struct mii_bus *mii_bus;
+
+	unsigned int ports;
+	unsigned int vlans;
+	unsigned int cpu_port;
+
+	/* the following fields are internal for swconfig */
+	unsigned int id;
+	struct list_head dev_list;
+	unsigned long def_global, def_port, def_vlan;
+
+	struct mutex sw_mutex;
+	struct switch_port *portbuf;
+	struct switch_portmap *portmap;
+	struct switch_port_link linkbuf;
+
+	char buf[128];
+
+#ifdef CONFIG_SWCONFIG_LEDS
+	struct switch_led_trigger *led_trigger;
+#endif
+};
+
+struct switch_port {
+	u32 id;
+	u32 flags;
+};
+
+struct switch_portmap {
+	u32 virt;
+	const char *s;
+};
+
+struct switch_val {
+	const struct switch_attr *attr;
+	unsigned int port_vlan;
+	unsigned int len;
+	union {
+		const char *s;
+		u32 i;
+		struct switch_port *ports;
+		struct switch_port_link *link;
+	} value;
+};
+
+struct switch_attr {
+	int disabled;
+	int type;
+	const char *name;
+	const char *description;
+
+	int (*set)(struct switch_dev *dev, const struct switch_attr *attr, struct switch_val *val);
+	int (*get)(struct switch_dev *dev, const struct switch_attr *attr, struct switch_val *val);
+
+	/* for driver internal use */
+	int id;
+	int ofs;
+	int max;
+};
+
+int switch_generic_set_link(struct switch_dev *dev, int port,
+			    struct switch_port_link *link);
+
+#endif /* _LINUX_SWITCH_H */
diff --git a/include/linux/thread_info.h b/include/linux/thread_info.h
index e93e249a4e9b..c88b9cecc78a 100644
--- a/include/linux/thread_info.h
+++ b/include/linux/thread_info.h
@@ -97,7 +97,17 @@ static inline int test_ti_thread_flag(struct thread_info *ti, int flag)
 #define test_thread_flag(flag) \
 	test_ti_thread_flag(current_thread_info(), flag)
 
-#define tif_need_resched() test_thread_flag(TIF_NEED_RESCHED)
+#ifdef CONFIG_PREEMPT_LAZY
+#define tif_need_resched()	(test_thread_flag(TIF_NEED_RESCHED) || \
+				 test_thread_flag(TIF_NEED_RESCHED_LAZY))
+#define tif_need_resched_now()	(test_thread_flag(TIF_NEED_RESCHED))
+#define tif_need_resched_lazy()	test_thread_flag(TIF_NEED_RESCHED_LAZY))
+
+#else
+#define tif_need_resched()	test_thread_flag(TIF_NEED_RESCHED)
+#define tif_need_resched_now()	test_thread_flag(TIF_NEED_RESCHED)
+#define tif_need_resched_lazy()	0
+#endif
 
 #ifndef CONFIG_HAVE_ARCH_WITHIN_STACK_FRAMES
 static inline int arch_within_stack_frames(const void * const stack,
diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index d321fe5ad1a1..89c3f7162267 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -67,6 +67,8 @@ struct trace_entry {
 	unsigned char		flags;
 	unsigned char		preempt_count;
 	int			pid;
+	unsigned char		migrate_disable;
+	unsigned char		preempt_lazy_count;
 };
 
 #define TRACE_EVENT_TYPE_MAX						\
@@ -148,17 +150,78 @@ enum print_line_t {
 
 enum print_line_t trace_handle_return(struct trace_seq *s);
 
-void tracing_generic_entry_update(struct trace_entry *entry,
-				  unsigned short type,
-				  unsigned long flags,
-				  int pc);
+static inline void tracing_generic_entry_update(struct trace_entry *entry,
+						unsigned short type,
+						unsigned int trace_ctx)
+{
+	entry->preempt_count		= trace_ctx & 0xff;
+	entry->migrate_disable		= (trace_ctx >> 8) & 0xff;
+	entry->preempt_lazy_count	= (trace_ctx >> 16) & 0xff;
+	entry->pid			= current->pid;
+	entry->type			= type;
+	entry->flags			= trace_ctx >> 24;
+}
+
+unsigned int tracing_gen_ctx_irq_test(unsigned int irqs_status);
+
+enum trace_flag_type {
+	TRACE_FLAG_IRQS_OFF		= 0x01,
+	TRACE_FLAG_IRQS_NOSUPPORT	= 0x02,
+	TRACE_FLAG_NEED_RESCHED		= 0x04,
+	TRACE_FLAG_HARDIRQ		= 0x08,
+	TRACE_FLAG_SOFTIRQ		= 0x10,
+	TRACE_FLAG_PREEMPT_RESCHED	= 0x20,
+	TRACE_FLAG_NMI			= 0x40,
+	TRACE_FLAG_NEED_RESCHED_LAZY	= 0x80,
+};
+
+#ifdef CONFIG_TRACE_IRQFLAGS_SUPPORT
+static inline unsigned int tracing_gen_ctx_flags(unsigned long irqflags)
+{
+	unsigned int irq_status = irqs_disabled_flags(irqflags) ?
+		TRACE_FLAG_IRQS_OFF : 0;
+	return tracing_gen_ctx_irq_test(irq_status);
+}
+static inline unsigned int tracing_gen_ctx(void)
+{
+	unsigned long irqflags;
+
+	local_save_flags(irqflags);
+	return tracing_gen_ctx_flags(irqflags);
+}
+#else
+
+static inline unsigned int tracing_gen_ctx_flags(unsigned long irqflags)
+{
+	return tracing_gen_ctx_irq_test(TRACE_FLAG_IRQS_NOSUPPORT);
+}
+static inline unsigned int tracing_gen_ctx(void)
+{
+	return tracing_gen_ctx_irq_test(TRACE_FLAG_IRQS_NOSUPPORT);
+}
+#endif
+
+static inline unsigned int tracing_gen_ctx_dec(void)
+{
+	unsigned int trace_ctx;
+
+	trace_ctx = tracing_gen_ctx();
+	/*
+	 * Subtract one from the preeption counter if preemption is enabled,
+	 * see trace_event_buffer_reserve()for details.
+	 */
+	if (IS_ENABLED(CONFIG_PREEMPTION))
+		trace_ctx--;
+	return trace_ctx;
+}
+
 struct trace_event_file;
 
 struct ring_buffer_event *
 trace_event_buffer_lock_reserve(struct trace_buffer **current_buffer,
 				struct trace_event_file *trace_file,
 				int type, unsigned long len,
-				unsigned long flags, int pc);
+				unsigned int trace_ctx);
 
 #define TRACE_RECORD_CMDLINE	BIT(0)
 #define TRACE_RECORD_TGID	BIT(1)
@@ -232,8 +295,7 @@ struct trace_event_buffer {
 	struct ring_buffer_event	*event;
 	struct trace_event_file		*trace_file;
 	void				*entry;
-	unsigned long			flags;
-	int				pc;
+	unsigned int			trace_ctx;
 	struct pt_regs			*regs;
 };
 
diff --git a/include/linux/tty_driver.h b/include/linux/tty_driver.h
index 358446247ccd..a6437cba80ae 100644
--- a/include/linux/tty_driver.h
+++ b/include/linux/tty_driver.h
@@ -421,6 +421,7 @@ static inline struct tty_driver *tty_driver_kref_get(struct tty_driver *d)
 #define TTY_DRIVER_HARDWARE_BREAK	0x0020
 #define TTY_DRIVER_DYNAMIC_ALLOC	0x0040
 #define TTY_DRIVER_UNNUMBERED_NODE	0x0080
+#define TTY_DRIVER_IGNORE_FLUSH         0x0100
 
 /* tty driver types */
 #define TTY_DRIVER_TYPE_SYSTEM		0x0001
diff --git a/include/linux/u64_stats_sync.h b/include/linux/u64_stats_sync.h
index c6abb79501b3..72bf38b97df8 100644
--- a/include/linux/u64_stats_sync.h
+++ b/include/linux/u64_stats_sync.h
@@ -66,7 +66,7 @@
 #include <linux/seqlock.h>
 
 struct u64_stats_sync {
-#if BITS_PER_LONG==32 && defined(CONFIG_SMP)
+#if BITS_PER_LONG==32 && (defined(CONFIG_SMP) || defined(CONFIG_PREEMPT_RT))
 	seqcount_t	seq;
 #endif
 };
@@ -117,22 +117,26 @@ static inline void u64_stats_inc(u64_stats_t *p)
 
 static inline void u64_stats_init(struct u64_stats_sync *syncp)
 {
-#if BITS_PER_LONG == 32 && defined(CONFIG_SMP)
+#if BITS_PER_LONG == 32 && (defined(CONFIG_SMP) || defined(CONFIG_PREEMPT_RT))
 	seqcount_init(&syncp->seq);
 #endif
 }
 
 static inline void u64_stats_update_begin(struct u64_stats_sync *syncp)
 {
-#if BITS_PER_LONG==32 && defined(CONFIG_SMP)
+#if BITS_PER_LONG == 32 && (defined(CONFIG_SMP) || defined(CONFIG_PREEMPT_RT))
+	if (IS_ENABLED(CONFIG_PREEMPT_RT))
+		preempt_disable();
 	write_seqcount_begin(&syncp->seq);
 #endif
 }
 
 static inline void u64_stats_update_end(struct u64_stats_sync *syncp)
 {
-#if BITS_PER_LONG==32 && defined(CONFIG_SMP)
+#if BITS_PER_LONG == 32 && (defined(CONFIG_SMP) || defined(CONFIG_PREEMPT_RT))
 	write_seqcount_end(&syncp->seq);
+	if (IS_ENABLED(CONFIG_PREEMPT_RT))
+		preempt_enable();
 #endif
 }
 
@@ -141,8 +145,11 @@ u64_stats_update_begin_irqsave(struct u64_stats_sync *syncp)
 {
 	unsigned long flags = 0;
 
-#if BITS_PER_LONG==32 && defined(CONFIG_SMP)
-	local_irq_save(flags);
+#if BITS_PER_LONG == 32 && (defined(CONFIG_SMP) || defined(CONFIG_PREEMPT_RT))
+	if (IS_ENABLED(CONFIG_PREEMPT_RT))
+		preempt_disable();
+	else
+		local_irq_save(flags);
 	write_seqcount_begin(&syncp->seq);
 #endif
 	return flags;
@@ -152,15 +159,18 @@ static inline void
 u64_stats_update_end_irqrestore(struct u64_stats_sync *syncp,
 				unsigned long flags)
 {
-#if BITS_PER_LONG==32 && defined(CONFIG_SMP)
+#if BITS_PER_LONG == 32 && (defined(CONFIG_SMP) || defined(CONFIG_PREEMPT_RT))
 	write_seqcount_end(&syncp->seq);
-	local_irq_restore(flags);
+	if (IS_ENABLED(CONFIG_PREEMPT_RT))
+		preempt_enable();
+	else
+		local_irq_restore(flags);
 #endif
 }
 
 static inline unsigned int __u64_stats_fetch_begin(const struct u64_stats_sync *syncp)
 {
-#if BITS_PER_LONG==32 && defined(CONFIG_SMP)
+#if BITS_PER_LONG == 32 && (defined(CONFIG_SMP) || defined(CONFIG_PREEMPT_RT))
 	return read_seqcount_begin(&syncp->seq);
 #else
 	return 0;
@@ -169,7 +179,7 @@ static inline unsigned int __u64_stats_fetch_begin(const struct u64_stats_sync *
 
 static inline unsigned int u64_stats_fetch_begin(const struct u64_stats_sync *syncp)
 {
-#if BITS_PER_LONG==32 && !defined(CONFIG_SMP)
+#if BITS_PER_LONG == 32 && (!defined(CONFIG_SMP) && !defined(CONFIG_PREEMPT_RT))
 	preempt_disable();
 #endif
 	return __u64_stats_fetch_begin(syncp);
@@ -178,7 +188,7 @@ static inline unsigned int u64_stats_fetch_begin(const struct u64_stats_sync *sy
 static inline bool __u64_stats_fetch_retry(const struct u64_stats_sync *syncp,
 					 unsigned int start)
 {
-#if BITS_PER_LONG==32 && defined(CONFIG_SMP)
+#if BITS_PER_LONG == 32 && (defined(CONFIG_SMP) || defined(CONFIG_PREEMPT_RT))
 	return read_seqcount_retry(&syncp->seq, start);
 #else
 	return false;
@@ -188,7 +198,7 @@ static inline bool __u64_stats_fetch_retry(const struct u64_stats_sync *syncp,
 static inline bool u64_stats_fetch_retry(const struct u64_stats_sync *syncp,
 					 unsigned int start)
 {
-#if BITS_PER_LONG==32 && !defined(CONFIG_SMP)
+#if BITS_PER_LONG == 32 && (!defined(CONFIG_SMP) && !defined(CONFIG_PREEMPT_RT))
 	preempt_enable();
 #endif
 	return __u64_stats_fetch_retry(syncp, start);
@@ -202,7 +212,9 @@ static inline bool u64_stats_fetch_retry(const struct u64_stats_sync *syncp,
  */
 static inline unsigned int u64_stats_fetch_begin_irq(const struct u64_stats_sync *syncp)
 {
-#if BITS_PER_LONG==32 && !defined(CONFIG_SMP)
+#if BITS_PER_LONG == 32 && defined(CONFIG_PREEMPT_RT)
+	preempt_disable();
+#elif BITS_PER_LONG == 32 && !defined(CONFIG_SMP)
 	local_irq_disable();
 #endif
 	return __u64_stats_fetch_begin(syncp);
@@ -211,7 +223,9 @@ static inline unsigned int u64_stats_fetch_begin_irq(const struct u64_stats_sync
 static inline bool u64_stats_fetch_retry_irq(const struct u64_stats_sync *syncp,
 					     unsigned int start)
 {
-#if BITS_PER_LONG==32 && !defined(CONFIG_SMP)
+#if BITS_PER_LONG == 32 && defined(CONFIG_PREEMPT_RT)
+	preempt_enable();
+#elif BITS_PER_LONG == 32 && !defined(CONFIG_SMP)
 	local_irq_enable();
 #endif
 	return __u64_stats_fetch_retry(syncp, start);
diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 322dcbfcc933..9a3a10ea3e3c 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -63,7 +63,9 @@ DECLARE_PER_CPU(struct vm_event_state, vm_event_states);
  */
 static inline void __count_vm_event(enum vm_event_item item)
 {
+	preempt_disable_rt();
 	raw_cpu_inc(vm_event_states.event[item]);
+	preempt_enable_rt();
 }
 
 static inline void count_vm_event(enum vm_event_item item)
@@ -73,7 +75,9 @@ static inline void count_vm_event(enum vm_event_item item)
 
 static inline void __count_vm_events(enum vm_event_item item, long delta)
 {
+	preempt_disable_rt();
 	raw_cpu_add(vm_event_states.event[item], delta);
+	preempt_enable_rt();
 }
 
 static inline void count_vm_events(enum vm_event_item item, long delta)
diff --git a/include/linux/vtime.h b/include/linux/vtime.h
index 2cdeca062db3..041d6524d144 100644
--- a/include/linux/vtime.h
+++ b/include/linux/vtime.h
@@ -83,36 +83,46 @@ static inline void vtime_init_idle(struct task_struct *tsk, int cpu) { }
 #endif
 
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE
-extern void vtime_account_irq_enter(struct task_struct *tsk);
-static inline void vtime_account_irq_exit(struct task_struct *tsk)
-{
-	/* On hard|softirq exit we always account to hard|softirq cputime */
-	vtime_account_kernel(tsk);
-}
+extern void vtime_account_irq(struct task_struct *tsk, unsigned int offset);
+extern void vtime_account_softirq(struct task_struct *tsk);
+extern void vtime_account_hardirq(struct task_struct *tsk);
 extern void vtime_flush(struct task_struct *tsk);
 #else /* !CONFIG_VIRT_CPU_ACCOUNTING_NATIVE */
-static inline void vtime_account_irq_enter(struct task_struct *tsk) { }
-static inline void vtime_account_irq_exit(struct task_struct *tsk) { }
+static inline void vtime_account_irq(struct task_struct *tsk, unsigned int offset) { }
+static inline void vtime_account_softirq(struct task_struct *tsk) { }
+static inline void vtime_account_hardirq(struct task_struct *tsk) { }
 static inline void vtime_flush(struct task_struct *tsk) { }
 #endif
 
 
 #ifdef CONFIG_IRQ_TIME_ACCOUNTING
-extern void irqtime_account_irq(struct task_struct *tsk);
+extern void irqtime_account_irq(struct task_struct *tsk, unsigned int offset);
 #else
-static inline void irqtime_account_irq(struct task_struct *tsk) { }
+static inline void irqtime_account_irq(struct task_struct *tsk, unsigned int offset) { }
 #endif
 
-static inline void account_irq_enter_time(struct task_struct *tsk)
+static inline void account_softirq_enter(struct task_struct *tsk)
+{
+	vtime_account_irq(tsk, SOFTIRQ_OFFSET);
+	irqtime_account_irq(tsk, SOFTIRQ_OFFSET);
+}
+
+static inline void account_softirq_exit(struct task_struct *tsk)
+{
+	vtime_account_softirq(tsk);
+	irqtime_account_irq(tsk, 0);
+}
+
+static inline void account_hardirq_enter(struct task_struct *tsk)
 {
-	vtime_account_irq_enter(tsk);
-	irqtime_account_irq(tsk);
+	vtime_account_irq(tsk, HARDIRQ_OFFSET);
+	irqtime_account_irq(tsk, HARDIRQ_OFFSET);
 }
 
-static inline void account_irq_exit_time(struct task_struct *tsk)
+static inline void account_hardirq_exit(struct task_struct *tsk)
 {
-	vtime_account_irq_exit(tsk);
-	irqtime_account_irq(tsk);
+	vtime_account_hardirq(tsk);
+	irqtime_account_irq(tsk, 0);
 }
 
 #endif /* _LINUX_KERNEL_VTIME_H */
diff --git a/include/linux/wait.h b/include/linux/wait.h
index 27fb99cfeb02..93b42387b4c6 100644
--- a/include/linux/wait.h
+++ b/include/linux/wait.h
@@ -10,6 +10,7 @@
 
 #include <asm/current.h>
 #include <uapi/linux/wait.h>
+#include <linux/atomic.h>
 
 typedef struct wait_queue_entry wait_queue_entry_t;
 
diff --git a/include/linux/wsysinit-prio.h b/include/linux/wsysinit-prio.h
new file mode 100644
index 000000000000..b5cc5b6faf5c
--- /dev/null
+++ b/include/linux/wsysinit-prio.h
@@ -0,0 +1,27 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+
+/*
+ * Copyright (C) 2014 Wago Kontakttechnik GmbH
+ *
+ * Author: Heinrich Toews <heinrich.toews@wago.com>
+ *
+ */
+
+#ifndef __WSYSINIT_PRIO_H
+#define __WSYSINIT_PRIO_H
+
+struct wsysinit_irq_prio_tbl_entry {
+	const char *name;
+	int prio;
+};
+
+extern struct wsysinit_irq_prio_tbl_entry *
+wsysinit_tbl_get_entry_by_name(const char *name);
+extern int wsysinit_tbl_get_prio_by_name(const char *name);
+extern void wsysinit_tbl_dump(void);
+extern void wsysinit_tbl_sysfs_init(void);
+
+struct task_struct;
+extern void wsysinit_set_fifo_nocheck(struct task_struct *task);
+
+#endif /* __WSYSINIT_PRIO_H */
diff --git a/include/linux/ww_mutex.h b/include/linux/ww_mutex.h
index 850424e5d030..8ef2feb0d8fe 100644
--- a/include/linux/ww_mutex.h
+++ b/include/linux/ww_mutex.h
@@ -28,6 +28,14 @@ struct ww_class {
 	unsigned int is_wait_die;
 };
 
+struct ww_mutex {
+	struct mutex base;
+	struct ww_acquire_ctx *ctx;
+#ifdef CONFIG_DEBUG_MUTEXES
+	struct ww_class *ww_class;
+#endif
+};
+
 struct ww_acquire_ctx {
 	struct task_struct *task;
 	unsigned long stamp;
diff --git a/include/misc/wago-tests.h b/include/misc/wago-tests.h
new file mode 100644
index 000000000000..b92721349eeb
--- /dev/null
+++ b/include/misc/wago-tests.h
@@ -0,0 +1,40 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+
+/*
+ *
+ * Copyright (C) 2014 Wago Kontakttechnik GmbH
+ *
+ * Author: Heinrich Toews <heinrich.toews@wago.com>
+ *
+ */
+
+#ifndef _WAGO_TESTS_H_
+#define _WAGO_TESTS_H_
+
+#define WAGO_TEST_DEBUG
+
+#ifdef WAGO_TEST_DEBUG
+#define pac_kdebug(format, arg...)	\
+	printk(KERN_INFO "pac-kdebug: " format , ## arg)
+#else
+#define pac_kdebug(format, arg...)       \
+	({ if (0) printk(KERN_INFO "pac-kdebug: " format , ## arg); 0; })
+#endif
+
+#include <linux/gpio.h>
+#include <linux/time.h>
+
+#define WAGO_TEST__MAX_MEASUREMENTS       20
+#define WAGO_TEST__GPIO                  175  	/* FB-nINT_GPIO175 */
+
+struct wago_trace_data {
+	struct timespec64 mpoints[WAGO_TEST__MAX_MEASUREMENTS];
+	int mpoint_index;
+};
+
+extern void wago_tests_init(struct wago_trace_data *tdata, u8 gpios_enable);
+extern void wago_tests_deinit(void);
+extern void wago_measure_reset(void);
+extern void wago_mpoint(void);
+
+#endif /* _WAGO_TESTS_H_ */
diff --git a/include/net/dsa.h b/include/net/dsa.h
index 35429a140dfa..2286451ef503 100644
--- a/include/net/dsa.h
+++ b/include/net/dsa.h
@@ -45,6 +45,7 @@ struct phylink_link_state;
 #define DSA_TAG_PROTO_OCELOT_VALUE		15
 #define DSA_TAG_PROTO_AR9331_VALUE		16
 #define DSA_TAG_PROTO_RTL4_A_VALUE		17
+#define DSA_TAG_PROTO_KSZ8863_VALUE		18
 
 enum dsa_tag_protocol {
 	DSA_TAG_PROTO_NONE		= DSA_TAG_PROTO_NONE_VALUE,
@@ -65,6 +66,7 @@ enum dsa_tag_protocol {
 	DSA_TAG_PROTO_OCELOT		= DSA_TAG_PROTO_OCELOT_VALUE,
 	DSA_TAG_PROTO_AR9331		= DSA_TAG_PROTO_AR9331_VALUE,
 	DSA_TAG_PROTO_RTL4_A		= DSA_TAG_PROTO_RTL4_A_VALUE,
+	DSA_TAG_PROTO_KSZ8863		= DSA_TAG_PROTO_KSZ8863_VALUE,
 };
 
 struct packet_type;
@@ -943,4 +945,3 @@ static struct dsa_tag_driver *dsa_tag_driver_array[] =	{		\
 };									\
 module_dsa_tag_drivers(dsa_tag_driver_array)
 #endif
-
diff --git a/include/net/gen_stats.h b/include/net/gen_stats.h
index 1424e02cef90..163f8415e5db 100644
--- a/include/net/gen_stats.h
+++ b/include/net/gen_stats.h
@@ -6,6 +6,7 @@
 #include <linux/socket.h>
 #include <linux/rtnetlink.h>
 #include <linux/pkt_sched.h>
+#include <net/net_seq_lock.h>
 
 /* Note: this used to be in include/uapi/linux/gen_stats.h */
 struct gnet_stats_basic_packed {
@@ -42,15 +43,15 @@ int gnet_stats_start_copy_compat(struct sk_buff *skb, int type,
 				 spinlock_t *lock, struct gnet_dump *d,
 				 int padattr);
 
-int gnet_stats_copy_basic(const seqcount_t *running,
+int gnet_stats_copy_basic(net_seqlock_t *running,
 			  struct gnet_dump *d,
 			  struct gnet_stats_basic_cpu __percpu *cpu,
 			  struct gnet_stats_basic_packed *b);
-void __gnet_stats_copy_basic(const seqcount_t *running,
+void __gnet_stats_copy_basic(net_seqlock_t *running,
 			     struct gnet_stats_basic_packed *bstats,
 			     struct gnet_stats_basic_cpu __percpu *cpu,
 			     struct gnet_stats_basic_packed *b);
-int gnet_stats_copy_basic_hw(const seqcount_t *running,
+int gnet_stats_copy_basic_hw(net_seqlock_t *running,
 			     struct gnet_dump *d,
 			     struct gnet_stats_basic_cpu __percpu *cpu,
 			     struct gnet_stats_basic_packed *b);
@@ -70,13 +71,13 @@ int gen_new_estimator(struct gnet_stats_basic_packed *bstats,
 		      struct gnet_stats_basic_cpu __percpu *cpu_bstats,
 		      struct net_rate_estimator __rcu **rate_est,
 		      spinlock_t *lock,
-		      seqcount_t *running, struct nlattr *opt);
+		      net_seqlock_t *running, struct nlattr *opt);
 void gen_kill_estimator(struct net_rate_estimator __rcu **ptr);
 int gen_replace_estimator(struct gnet_stats_basic_packed *bstats,
 			  struct gnet_stats_basic_cpu __percpu *cpu_bstats,
 			  struct net_rate_estimator __rcu **ptr,
 			  spinlock_t *lock,
-			  seqcount_t *running, struct nlattr *opt);
+			  net_seqlock_t *running, struct nlattr *opt);
 bool gen_estimator_active(struct net_rate_estimator __rcu **ptr);
 bool gen_estimator_read(struct net_rate_estimator __rcu **ptr,
 			struct gnet_stats_rate_est64 *sample);
diff --git a/include/net/net_seq_lock.h b/include/net/net_seq_lock.h
new file mode 100644
index 000000000000..67710bace741
--- /dev/null
+++ b/include/net/net_seq_lock.h
@@ -0,0 +1,15 @@
+#ifndef __NET_NET_SEQ_LOCK_H__
+#define __NET_NET_SEQ_LOCK_H__
+
+#ifdef CONFIG_PREEMPT_RT
+# define net_seqlock_t			seqlock_t
+# define net_seq_begin(__r)		read_seqbegin(__r)
+# define net_seq_retry(__r, __s)	read_seqretry(__r, __s)
+
+#else
+# define net_seqlock_t			seqcount_t
+# define net_seq_begin(__r)		read_seqcount_begin(__r)
+# define net_seq_retry(__r, __s)	read_seqcount_retry(__r, __s)
+#endif
+
+#endif
diff --git a/include/net/sch_generic.h b/include/net/sch_generic.h
index 3648164faa06..6a0434d2c279 100644
--- a/include/net/sch_generic.h
+++ b/include/net/sch_generic.h
@@ -10,6 +10,7 @@
 #include <linux/percpu.h>
 #include <linux/dynamic_queue_limits.h>
 #include <linux/list.h>
+#include <net/net_seq_lock.h>
 #include <linux/refcount.h>
 #include <linux/workqueue.h>
 #include <linux/mutex.h>
@@ -100,7 +101,7 @@ struct Qdisc {
 	struct sk_buff_head	gso_skb ____cacheline_aligned_in_smp;
 	struct qdisc_skb_head	q;
 	struct gnet_stats_basic_packed bstats;
-	seqcount_t		running;
+	net_seqlock_t		running;
 	struct gnet_stats_queue	qstats;
 	unsigned long		state;
 	struct Qdisc            *next_sched;
@@ -141,7 +142,11 @@ static inline bool qdisc_is_running(struct Qdisc *qdisc)
 {
 	if (qdisc->flags & TCQ_F_NOLOCK)
 		return spin_is_locked(&qdisc->seqlock);
+#ifdef CONFIG_PREEMPT_RT
+	return spin_is_locked(&qdisc->running.lock) ? true : false;
+#else
 	return (raw_read_seqcount(&qdisc->running) & 1) ? true : false;
+#endif
 }
 
 static inline bool qdisc_is_percpu_stats(const struct Qdisc *q)
@@ -165,17 +170,35 @@ static inline bool qdisc_run_begin(struct Qdisc *qdisc)
 	} else if (qdisc_is_running(qdisc)) {
 		return false;
 	}
+#ifdef CONFIG_PREEMPT_RT
+	if (spin_trylock(&qdisc->running.lock)) {
+		seqcount_t *s = &qdisc->running.seqcount.seqcount;
+		/*
+		 * Variant of write_seqcount_t_begin() telling lockdep that a
+		 * trylock was attempted.
+		 */
+		raw_write_seqcount_t_begin(s);
+		seqcount_acquire(&s->dep_map, 0, 1, _RET_IP_);
+		return true;
+	}
+	return false;
+#else
 	/* Variant of write_seqcount_begin() telling lockdep a trylock
 	 * was attempted.
 	 */
 	raw_write_seqcount_begin(&qdisc->running);
 	seqcount_acquire(&qdisc->running.dep_map, 0, 1, _RET_IP_);
 	return true;
+#endif
 }
 
 static inline void qdisc_run_end(struct Qdisc *qdisc)
 {
+#ifdef CONFIG_PREEMPT_RT
+	write_sequnlock(&qdisc->running);
+#else
 	write_seqcount_end(&qdisc->running);
+#endif
 	if (qdisc->flags & TCQ_F_NOLOCK)
 		spin_unlock(&qdisc->seqlock);
 }
@@ -550,7 +573,7 @@ static inline spinlock_t *qdisc_root_sleeping_lock(const struct Qdisc *qdisc)
 	return qdisc_lock(root);
 }
 
-static inline seqcount_t *qdisc_root_sleeping_running(const struct Qdisc *qdisc)
+static inline net_seqlock_t *qdisc_root_sleeping_running(const struct Qdisc *qdisc)
 {
 	struct Qdisc *root = qdisc_root_sleeping(qdisc);
 
diff --git a/include/trace/events/pxc.h b/include/trace/events/pxc.h
new file mode 100644
index 000000000000..77d265ae0d69
--- /dev/null
+++ b/include/trace/events/pxc.h
@@ -0,0 +1,377 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+
+/*
+ *
+ * Copyright (C) 2015 Wago Kontakttechnik GmbH
+ *
+ * Author: Heinrich Toews <heinrich.toews@wago.com>
+ *
+ */
+
+#undef TRACE_SYSTEM
+#define TRACE_SYSTEM pxc
+
+#if !defined(_TRACE_PXC_H) || defined(TRACE_HEADER_MULTI_READ)
+#define _TRACE_PXC_H
+
+#include <linux/tracepoint.h>
+
+#ifdef PFC_CLOCK_TRACER
+TRACE_EVENT(pfc_clock, 	/* trace pfc clock events */
+
+	    TP_PROTO(cycle_t cycle_now, cycle_t cycle_delta, struct timekeeper *tk, s64 nsec, char *msg),
+
+	    TP_ARGS(cycle_now, cycle_delta, tk, nsec, msg),
+
+	TP_STRUCT__entry(
+		__field(cycle_t, cycle_now)
+		__field(cycle_t, cycle_delta)
+		__field(s64, nsec)
+		__field(u32, mult)
+		__field(u32, shift)
+		__field(u64, xtime_nsec)
+		__array(char, msg, 128)
+	),
+
+	TP_fast_assign(
+	        __entry->cycle_now = cycle_now;
+		__entry->cycle_delta = cycle_delta;
+		__entry->nsec = nsec;
+		__entry->mult = tk->mult;
+		__entry->shift = tk->shift;
+		__entry->xtime_nsec = tk->xtime_nsec;
+		strncpy(         __entry->msg, msg,                128);
+	),
+
+	TP_printk("%llu d:%llu->%lli|%u|%u|%llu|%s",
+		  __entry->cycle_now, __entry->cycle_delta, __entry->nsec,
+		  __entry->mult, __entry->shift, __entry->xtime_nsec,
+		  __entry->msg)
+);
+#endif
+
+#if defined(PXC_ETH_EMAC)
+TRACE_EVENT(pxc_eth_emac, 	/* trace davinci_emac events */
+
+	TP_PROTO(struct emac_priv *priv, char *msg),
+
+	TP_ARGS(priv, msg),
+
+	TP_STRUCT__entry(
+		__field(unsigned, link)
+		__field(unsigned, speed)
+		__field(unsigned, duplex)
+		__array(char, phy_id, 32)
+		__array(char, other_phy_id, 32)
+		__array(char, msg, 128)
+		__array(char, devname, 64)
+	),
+
+	TP_fast_assign(
+		__entry->link   = priv->link;
+		__entry->speed  = priv->speed;
+		__entry->duplex = priv->duplex;
+		strncpy(      __entry->phy_id, priv->phy_id,        32);
+		strncpy(__entry->other_phy_id, priv->other_phy_id,  32);
+		strncpy(         __entry->msg, msg,                128);
+		if (priv->ndev)
+			strncpy(__entry->devname, priv->ndev->name, 64);
+	),
+
+	TP_printk("%6s 1-%s 2-%s: %4s|%d/%4s %s", __entry->devname, __entry->phy_id,  __entry->other_phy_id,
+		  __entry->link ? "UP" : "DOWN", __entry->speed, __entry->duplex == 1 ? "Full" : "Half",
+		  __entry->msg)
+);
+
+TRACE_EVENT(pxc_eth_emac_phy,	/* trace phy events */
+
+	TP_PROTO(struct phy_device *phydev, char *msg),
+
+        TP_ARGS(phydev, msg),
+
+	TP_STRUCT__entry(
+		__array(char, msg, 128)
+		__array(char, phyname, 16)
+		__field(int, link)
+		__field(int, speed)
+		__field(int, duplex) /* DUPLEX_HALF=0, DUPLEX_FULL=1 */
+		__field(int, state)
+		__field(int, irq)
+		__array(char, devname, 64)
+		__field(u32, dev_flags)
+	),
+
+	TP_fast_assign(
+		if (phydev->attached_dev)
+			strncpy(__entry->devname, phydev->attached_dev->name, 64);
+		strncpy(__entry->msg, msg, 128);
+		strncpy(__entry->phyname, dev_name(&phydev->dev), 16);
+	        __entry->link = phydev->link;
+		__entry->speed = phydev->speed;
+		__entry->duplex = phydev->duplex;
+		__entry->state = phydev->state;
+		__entry->irq = phydev->irq;
+		__entry->dev_flags = phydev->dev_flags;
+	),
+
+	TP_printk("%6s(%s) %4s|%d/%4s|%d|%d|%u %s", __entry->devname, __entry->phyname,
+		  __entry->link ? "UP" : "DOWN", __entry->speed, __entry->duplex == 1 ? "Full" : "Half",
+		  __entry->state, __entry->irq, __entry->dev_flags, __entry->msg)
+);
+#endif
+
+#if defined(PXC_SPI_TRACER)
+TRACE_EVENT(pxc_spi, 	/* trace mcspi events */
+
+        TP_PROTO(char *msg, const char *func, int data),
+
+	TP_ARGS(msg, func, data),
+
+	TP_STRUCT__entry(
+		__array(char, msg, 128)
+		__array(char, func, 32)
+	        __field(int, data)
+	),
+
+	TP_fast_assign(
+		strncpy(         __entry->msg, msg,                128);
+		strncpy(         __entry->func, func,                32);
+	        __entry->data = data;
+	),
+
+	TP_printk("%s[%d]: %s", __entry->func, __entry->data, __entry->msg)
+);
+
+TRACE_EVENT(pxc_spi_measure_a, 	/* trace mcspi events */
+
+	    TP_PROTO(char *msg, struct wago_trace_data *tdata),
+
+	    TP_ARGS(msg, tdata),
+
+	TP_STRUCT__entry(
+		__array(char, msg, 128)
+	        __field(unsigned int, completion_delay)
+	        __field(unsigned int, completion_delay_work)
+	        __field(unsigned int, async_delay)
+	        __field(unsigned int, enqueue_delay)
+	        __field(unsigned int, work_delay)
+	),
+
+	TP_fast_assign(
+		strncpy(         __entry->msg, msg,                128);
+		if (tdata->mpoint_index > 5) {
+			__entry->completion_delay = tdata->mpoints[5].tv_nsec - tdata->mpoints[1].tv_nsec;
+			__entry->completion_delay_work = tdata->mpoints[4].tv_nsec - tdata->mpoints[2].tv_nsec;
+			__entry->async_delay = tdata->mpoints[1].tv_nsec - tdata->mpoints[0].tv_nsec;
+			__entry->enqueue_delay = tdata->mpoints[2].tv_nsec - tdata->mpoints[1].tv_nsec;
+			__entry->work_delay = tdata->mpoints[3].tv_nsec - tdata->mpoints[2].tv_nsec;
+		}
+	),
+
+	TP_printk("%s: completion_delay=%u, completion_delay_work=%u, async_delay=%u, enqueue_delay=%u, work_delay=%u",
+		  __entry->msg, __entry->completion_delay, __entry->completion_delay_work,
+		  __entry->async_delay, __entry->enqueue_delay, __entry->work_delay)
+);
+
+TRACE_EVENT(pxc_spi_measure_b, 	/* trace mcspi events */
+
+	    TP_PROTO(char *msg, struct wago_trace_data *tdata),
+
+	    TP_ARGS(msg, tdata),
+
+	TP_STRUCT__entry(
+		__array(char, msg, 128)
+	        __field(unsigned int, completion_delay)
+	        __field(unsigned int, completion_delay_work)
+	        __field(unsigned int, async_delay)
+	        __field(unsigned int, enqueue_delay)
+	        __field(unsigned int, work_delay)
+	        __field(unsigned int, delay1)
+	        __field(unsigned int, delay2)
+	),
+
+	TP_fast_assign(
+		strncpy(         __entry->msg, msg,                128);
+		if (tdata->mpoint_index > 8) {
+			__entry->completion_delay = tdata->mpoints[7].tv_nsec - tdata->mpoints[2].tv_nsec;
+			__entry->completion_delay_work = tdata->mpoints[6].tv_nsec - tdata->mpoints[4].tv_nsec;
+			__entry->async_delay = tdata->mpoints[2].tv_nsec - tdata->mpoints[1].tv_nsec;
+			__entry->enqueue_delay = tdata->mpoints[4].tv_nsec - tdata->mpoints[2].tv_nsec;
+			__entry->work_delay = tdata->mpoints[5].tv_nsec - tdata->mpoints[4].tv_nsec;
+			__entry->delay1 = tdata->mpoints[7].tv_nsec - tdata->mpoints[1].tv_nsec;
+			__entry->delay2 = tdata->mpoints[8].tv_nsec - tdata->mpoints[0].tv_nsec;
+		}
+	),
+
+	TP_printk("%s: completion_delay=%u, completion_delay_work=%u, async_delay=%u, enqueue_delay=%u, work_delay=%u, delay1=%u, delay2=%u",
+		  __entry->msg, __entry->completion_delay, __entry->completion_delay_work,
+		  __entry->async_delay, __entry->enqueue_delay, __entry->work_delay, __entry->delay1, __entry->delay2)
+);
+#endif	/* PXC_SPI_TRACER */
+
+#if defined(PXC_SPI_KBUS_TRACER)
+TRACE_EVENT(pxc_kbus, 	/* trace kbus events */
+
+        TP_PROTO(const char *func, char *msg),
+
+	TP_ARGS(func, msg),
+
+	TP_STRUCT__entry(
+		__array(char, msg, 128)
+		__array(char, func, 32)
+	),
+
+	TP_fast_assign(
+		strncpy(         __entry->msg, msg,                128);
+		strncpy(         __entry->func, func,               32);
+	),
+
+        TP_printk("%s:%s", __entry->func, __entry->msg)
+);
+
+TRACE_EVENT(pxc_kbusmsg, 	/* trace kbus events */
+
+	    TP_PROTO(const char *func, struct spi_message *m, char *msg),
+
+	    TP_ARGS(func, m, msg),
+
+	TP_STRUCT__entry(
+		__array(char, msg, 128)
+		__array(char, func, 32)
+	        __field(int, status)
+	),
+
+	TP_fast_assign(
+		strncpy(         __entry->msg, msg,                128);
+		strncpy(         __entry->func, func,               32);
+	        __entry->status = m->status;
+	),
+
+	    TP_printk("%s:%s m->status=%d", __entry->func, __entry->msg, __entry->status)
+);
+
+TRACE_EVENT(pxc_kbusdump, 	/* trace kbus events */
+
+	    TP_PROTO(const char *func, char *msg, int word_len, u16 word),
+
+	    TP_ARGS(func, msg, word_len, word),
+
+	TP_STRUCT__entry(
+		__array(char, func, 32)
+		__array(char, msg, 128)
+	        __field( int, word_len)
+	        __field( u16, word)
+	),
+
+	TP_fast_assign(
+		strncpy(         __entry->func, func,               32);
+		strncpy(         __entry->msg, msg,                128);
+	        __entry->word_len = word_len;
+	        __entry->word = word;
+	),
+
+	    TP_printk("%s:%s-%d 0x%x", __entry->func, __entry->msg, __entry->word_len, __entry->word) /* 0x%04x */
+);
+
+TRACE_EVENT(pxc_buf32,
+
+	TP_PROTO(const char *prefix, char *in_buf, int len, int offs),
+
+        TP_ARGS(prefix, in_buf, len, offs),
+
+	TP_STRUCT__entry(
+		__array(char, prefix, 32)
+	        __field(int, copy_len)
+		__array(char, buf, 32)
+		__field(char *, in_buf_p)
+	),
+
+	TP_fast_assign(
+		strncpy(__entry->prefix, prefix, 32);
+		if (len < 32)
+			memset(__entry->buf, 0, 32);
+		strncpy(__entry->buf, in_buf + offs, len > 32 ? 32 : len);
+		__entry->copy_len = len;
+		__entry->in_buf_p = in_buf;
+	),
+
+	TP_printk("DATADUMP(%s) copylen %4d (in_buf_p %p) "
+		  "[%.2x-%.2x-%.2x-%.2x-%.2x-%.2x-%.2x-%.2x"
+		  "-%.2x-%.2x-%.2x-%.2x-%.2x-%.2x-%.2x-%.2x"
+		  "-%.2x-%.2x-%.2x-%.2x-%.2x-%.2x-%.2x-%.2x"
+		  "-%.2x-%.2x-%.2x-%.2x-%.2x-%.2x-%.2x-%.2x]",
+		  __entry->prefix,
+		  __entry->copy_len,
+		  __entry->in_buf_p,
+		  __entry->buf[0],  __entry->buf[1],  __entry->buf[2],  __entry->buf[3],
+		  __entry->buf[4],  __entry->buf[5],  __entry->buf[6],  __entry->buf[7],
+		  __entry->buf[8],  __entry->buf[9],  __entry->buf[10], __entry->buf[11],
+		  __entry->buf[12], __entry->buf[13], __entry->buf[14], __entry->buf[15],
+		  __entry->buf[16], __entry->buf[17], __entry->buf[18], __entry->buf[19],
+		  __entry->buf[20], __entry->buf[21], __entry->buf[22], __entry->buf[23],
+		  __entry->buf[24], __entry->buf[25], __entry->buf[26], __entry->buf[27],
+		  __entry->buf[28], __entry->buf[29], __entry->buf[30], __entry->buf[31])
+);
+
+TRACE_EVENT(pxc_kbus_mdata, 	/* trace kbus measurement events */
+
+        TP_PROTO(const char *func, char *msg, struct wago_trace_data *tdata),
+
+	TP_ARGS(func, msg, tdata),
+
+	TP_STRUCT__entry(
+		__array(char, msg, 128)
+		__array(char, func, 32)
+	        __field(unsigned int, delay1)
+	        __field(unsigned int, delay2)
+	),
+
+	TP_fast_assign(
+		strncpy(         __entry->msg, msg,                128);
+		strncpy(         __entry->func, func,               32);
+		if (tdata->mpoint_index > 3) {
+			__entry->delay1 = tdata->mpoints[3].tv_nsec - tdata->mpoints[0].tv_nsec;
+			__entry->delay2 = tdata->mpoints[2].tv_nsec - tdata->mpoints[1].tv_nsec;
+		}
+	),
+
+	TP_printk("%s:%s delay1=%u delay2=%u", __entry->func, __entry->msg,
+		  __entry->delay1, __entry->delay2)
+);
+#endif	/* PXC_SPI_KBUS_TRACER */
+
+#if defined(PXC_CAN_TRACER)
+TRACE_EVENT(pxc_canpkt,
+
+        TP_PROTO(struct can_frame *canframe),
+
+	TP_ARGS(canframe),
+
+	TP_STRUCT__entry(
+	        __field(struct can_frame *, canframe)
+	),
+
+	TP_fast_assign(
+		__entry->canframe = canframe;
+	),
+
+	TP_printk("%s-0x%x: can_id=0x%x, len=%d, data:%.2x.%.2x.%.2x.%.2x.%.2x.%.2x.%.2x.%.2x",
+		  (__entry->canframe->can_id & CAN_ERR_FLAG) ? " err" : "data",
+		  __entry->canframe->can_id >> CAN_EFF_ID_BITS,
+		  __entry->canframe->can_id & CAN_ERR_MASK,
+		  __entry->canframe->can_dlc,
+		  __entry->canframe->data[0],
+		  __entry->canframe->data[1],
+		  __entry->canframe->data[2],
+		  __entry->canframe->data[3],
+		  __entry->canframe->data[4],
+		  __entry->canframe->data[5],
+		  __entry->canframe->data[6],
+		  __entry->canframe->data[7])
+);
+#endif	/* PXC_CAN_TRACER */
+
+#endif /* if !defined(_TRACE_PXC_H) || defined(TRACE_HEADER_MULTI_READ) */
+
+/* This part must be outside protection */
+#include <trace/define_trace.h>
diff --git a/include/trace/events/sched.h b/include/trace/events/sched.h
index c96a4337afe6..e48f584abf5f 100644
--- a/include/trace/events/sched.h
+++ b/include/trace/events/sched.h
@@ -650,6 +650,18 @@ DECLARE_TRACE(sched_update_nr_running_tp,
 	TP_PROTO(struct rq *rq, int change),
 	TP_ARGS(rq, change));
 
+DECLARE_TRACE(sched_migrate_disable_tp,
+	      TP_PROTO(struct task_struct *p),
+	      TP_ARGS(p));
+
+DECLARE_TRACE(sched_migrate_enable_tp,
+	      TP_PROTO(struct task_struct *p),
+	      TP_ARGS(p));
+
+DECLARE_TRACE(sched_migrate_pull_tp,
+	      TP_PROTO(struct task_struct *p),
+	      TP_ARGS(p));
+
 #endif /* _TRACE_SCHED_H */
 
 /* This part must be outside protection */
diff --git a/include/uapi/asm-generic/ioctls.h b/include/uapi/asm-generic/ioctls.h
index cdc9f4ca8c27..a8ee8f366e2b 100644
--- a/include/uapi/asm-generic/ioctls.h
+++ b/include/uapi/asm-generic/ioctls.h
@@ -75,6 +75,13 @@
 #define TCSETXW		0x5435
 #define TIOCSIG		_IOW('T', 0x36, int)  /* pty: generate signal */
 #define TIOCVHANGUP	0x5437
+
+#define TIOCSMBRTU     _IOW('T', 0x38, unsigned char) /* Enable/Disable Modbus RTU */
+#define TIOCSMBRTUADDR _IOW('T', 0x39, unsigned char) /* Set Modbus RTU Address */
+#define TIOCSMBRTUIFT  _IOW('T', 0x40, unsigned int) /* Set Modbus RTU interframe timeout */
+#define TIOCSMBRTUICT  _IOW('T', 0x41, unsigned int) /* Set Modbus RTU intercharacter timeout */
+#define TIOCSMBRTUTS   _IOW('T', 0x42, unsigned char) /* Enable/Disable Timestamp */
+
 #define TIOCGPKT	_IOR('T', 0x38, int) /* Get packet mode state */
 #define TIOCGPTLCK	_IOR('T', 0x39, int) /* Get Pty lock state */
 #define TIOCGEXCL	_IOR('T', 0x40, int) /* Get exclusive mode state */
diff --git a/include/uapi/linux/if_ether.h b/include/uapi/linux/if_ether.h
index d6de2b167448..6b67f808386e 100644
--- a/include/uapi/linux/if_ether.h
+++ b/include/uapi/linux/if_ether.h
@@ -150,6 +150,7 @@
 #define ETH_P_MAP	0x00F9		/* Qualcomm multiplexing and
 					 * aggregation protocol
 					 */
+#define ETH_P_TAIL	0x001D		/* Tail Tagging (micrel)        */
 
 /*
  *	This is an Ethernet frame header.
diff --git a/include/uapi/linux/rmd.h b/include/uapi/linux/rmd.h
new file mode 100644
index 000000000000..4818aa46acd9
--- /dev/null
+++ b/include/uapi/linux/rmd.h
@@ -0,0 +1,125 @@
+// SPDX-License-Identifier: GPL-2.0+ WITH Linux-syscall-note
+
+#ifndef _UAPI_LINUX_RMD_H
+#define _UAPI_LINUX_RMD_H
+
+#include <linux/ioctl.h>
+#include <linux/types.h>
+
+/* driver version */
+#define RMD_DRV_VERSION 0x00000001
+
+#define RMD_FLAG_SOFTCOPY	1
+#define RMD_FLAG_NEW_DATA_BYTE	2
+#define RMD_FLAG_NEW_DATA	(1 << (RMD_FLAG_NEW_DATA_BYTE * 8))
+#define RMD_FLAG_LOCK_BYTE	3
+#define RMD_FLAG_LOCK		(1 << (RMD_FLAG_LOCK_BYTE * 8))
+
+#define RMD_BUFFER_INDEX_SIZE	3
+
+static inline __u32 rmd_atomic_load(__u32 *ptr)
+{
+	return __atomic_load_n(ptr, __ATOMIC_SEQ_CST);
+}
+
+static inline __u32 rmd_atomic_xchg(__u32 *ptr, __u32 val)
+{
+	return __atomic_exchange_n(ptr, val, __ATOMIC_SEQ_CST);
+}
+
+static inline __u32 rmd_atomic_cmpxchg(__u32 *ptr, __u32 expected,
+								__u32 desired)
+{
+	__atomic_compare_exchange_n(ptr, &expected, desired, 0,
+					__ATOMIC_SEQ_CST, __ATOMIC_SEQ_CST);
+	return expected;
+}
+
+struct rmd_config {
+	int flags;
+	__u32 idet_fifo_pdi_offset;	/* offset of pdi fifo		*/
+	__u32 idet_fifo_pdi_size;	/* size of pdi fifo in bytes	*/
+	__u32 idet_fifo_pdo_offset;	/* offset of pdo fifo		*/
+	__u32 idet_fifo_pdo_size;	/* size of pdo fifo in bytes	*/
+	__u32 idet_pdi_cs_offset;	/* offset of rx fifo ctrl/stat SFR */
+	__u32 idet_pdo_cs_offset;	/* offset of tx fifo ctrl/stat SFR */
+	__u8 tx_commit_bit_offset;	/* bit offset in control/status SFR to
+					 * commit data when written to tx buffer
+					 */
+
+	__u8 tx_discard_bit_offset;	/* bit offset in control/status SFR to
+					 * discard data when written to tx
+					 * buffer
+					 */
+
+	__u8 rx_read_finish_bit_offset;	/* bit offset in control/status SFR to
+					 * indicate end-of-reading from the rx
+					 * buffer
+					 */
+
+	__u8 rx_data_valid_bit_offset;	/* bit offset in control/status SFR to
+					 * get the valid status of read data
+					 * from the rx buffer
+					 */
+};
+
+struct rmd_cycle {
+	__u32 timeout_us;		/* max time in ns to wait for the cycle
+					 * event
+					 */
+	__u32 cycle_time_us;		/* calculated time in us from the
+					 * previous cycle
+					 */
+};
+
+#define RMD_DRV_MAGIC				'R'
+
+/* Get the driver version. The user should check this version during
+ * initialization
+ */
+#define RMD_GET_DRV_VERSION			_IO(RMD_DRV_MAGIC, 1)
+
+#define RMD_SET_CONFIG				_IOW(RMD_DRV_MAGIC, 2, \
+							struct rmd_config)
+
+#define RMD_GET_CONFIG				_IOR(RMD_DRV_MAGIC, 3, \
+							struct rmd_config)
+
+/* Get the size of read/write buffer. This size needs to be used when the
+ * user mmaps the specific buffer.
+ */
+#define RMD_GET_BUFFER_SIZE			_IO(RMD_DRV_MAGIC, 4)
+
+/* Get the chunk size of read/write buffer elements */
+#define RMD_GET_BUFFER_CHUNK_SIZE		_IO(RMD_DRV_MAGIC, 5)
+
+/* Get the current write buffer index. This is the buffer index where the
+ * user can write new data to.
+ */
+#define RMD_GET_WRITE_BUFFER_IDX		_IO(RMD_DRV_MAGIC, 8)
+
+/* Get the current read buffer index that holds the last valid data. */
+ #define RMD_GET_READ_BUFFER_IDX		_IO(RMD_DRV_MAGIC, 9)
+
+/* Wait for the next cycle event from the rmd. */
+#define RMD_WAIT_FOR_NEXT_CYCLE			_IOWR(RMD_DRV_MAGIC, 10, \
+							struct rmd_cycle)
+
+/* Enable/disable the tx data flow */
+#define RMD_ACTIVATE_TX_PATH			_IOW(RMD_DRV_MAGIC, 11, __u8)
+
+/* Get the offset to the tripple buffer control field of the read buffer */
+#define RMD_GET_READ_TRIPPLE_BUF_CTRL_OFFSET	_IO(RMD_DRV_MAGIC, 12)
+
+/* Get the offset to the tripple buffer control field of the write buffer */
+#define RMD_GET_WRITE_TRIPPLE_BUF_CTRL_OFFSET	_IO(RMD_DRV_MAGIC, 13)
+
+/* Get the size of tripple buffer read and write control field. This size needs
+ * to be used when mapping the tripple buffer control field.
+ */
+#define RMD_GET_TRIPPLE_BUF_CTRL_SIZE		_IO(RMD_DRV_MAGIC, 14)
+
+/* Restarts the FPGA image using the nRST pin */
+#define RMD_RESTART_FPGA_IMAGE			_IO(RMD_DRV_MAGIC, 15)
+
+#endif /* _UAPI_LINUX_RMD_H */
diff --git a/include/uapi/linux/switch.h b/include/uapi/linux/switch.h
new file mode 100644
index 000000000000..5f089bc8516e
--- /dev/null
+++ b/include/uapi/linux/switch.h
@@ -0,0 +1,120 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+/*
+ * switch.h: Switch configuration API
+ *
+ * Copyright (C) 2008 Felix Fietkau <nbd@nbd.name>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version 2
+ * of the License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef _UAPI_LINUX_SWITCH_H
+#define _UAPI_LINUX_SWITCH_H
+
+#include <linux/types.h>
+#include <linux/netdevice.h>
+#include <linux/netlink.h>
+#include <linux/genetlink.h>
+#ifndef __KERNEL__
+#include <netlink/netlink.h>
+#include <netlink/genl/genl.h>
+#include <netlink/genl/ctrl.h>
+#endif
+
+/* main attributes */
+enum {
+	SWITCH_ATTR_UNSPEC,
+	/* global */
+	SWITCH_ATTR_TYPE,
+	/* device */
+	SWITCH_ATTR_ID,
+	SWITCH_ATTR_DEV_NAME,
+	SWITCH_ATTR_ALIAS,
+	SWITCH_ATTR_NAME,
+	SWITCH_ATTR_VLANS,
+	SWITCH_ATTR_PORTS,
+	SWITCH_ATTR_PORTMAP,
+	SWITCH_ATTR_CPU_PORT,
+	/* attributes */
+	SWITCH_ATTR_OP_ID,
+	SWITCH_ATTR_OP_TYPE,
+	SWITCH_ATTR_OP_NAME,
+	SWITCH_ATTR_OP_PORT,
+	SWITCH_ATTR_OP_VLAN,
+	SWITCH_ATTR_OP_VALUE_INT,
+	SWITCH_ATTR_OP_VALUE_STR,
+	SWITCH_ATTR_OP_VALUE_PORTS,
+	SWITCH_ATTR_OP_VALUE_LINK,
+	SWITCH_ATTR_OP_DESCRIPTION,
+	/* port lists */
+	SWITCH_ATTR_PORT,
+	SWITCH_ATTR_MAX
+};
+
+enum {
+	/* port map */
+	SWITCH_PORTMAP_PORTS,
+	SWITCH_PORTMAP_SEGMENT,
+	SWITCH_PORTMAP_VIRT,
+	SWITCH_PORTMAP_MAX
+};
+
+/* commands */
+enum {
+	SWITCH_CMD_UNSPEC,
+	SWITCH_CMD_GET_SWITCH,
+	SWITCH_CMD_NEW_ATTR,
+	SWITCH_CMD_LIST_GLOBAL,
+	SWITCH_CMD_GET_GLOBAL,
+	SWITCH_CMD_SET_GLOBAL,
+	SWITCH_CMD_LIST_PORT,
+	SWITCH_CMD_GET_PORT,
+	SWITCH_CMD_SET_PORT,
+	SWITCH_CMD_LIST_VLAN,
+	SWITCH_CMD_GET_VLAN,
+	SWITCH_CMD_SET_VLAN
+};
+
+/* data types */
+enum switch_val_type {
+	SWITCH_TYPE_UNSPEC,
+	SWITCH_TYPE_INT,
+	SWITCH_TYPE_STRING,
+	SWITCH_TYPE_PORTS,
+	SWITCH_TYPE_LINK,
+	SWITCH_TYPE_NOVAL,
+};
+
+/* port nested attributes */
+enum {
+	SWITCH_PORT_UNSPEC,
+	SWITCH_PORT_ID,
+	SWITCH_PORT_FLAG_TAGGED,
+	SWITCH_PORT_ATTR_MAX
+};
+
+/* link nested attributes */
+enum {
+	SWITCH_LINK_UNSPEC,
+	SWITCH_LINK_FLAG_LINK,
+	SWITCH_LINK_FLAG_DUPLEX,
+	SWITCH_LINK_FLAG_ANEG,
+	SWITCH_LINK_FLAG_TX_FLOW,
+	SWITCH_LINK_FLAG_RX_FLOW,
+	SWITCH_LINK_SPEED,
+	SWITCH_LINK_FLAG_EEE_100BASET,
+	SWITCH_LINK_FLAG_EEE_1000BASET,
+	SWITCH_LINK_ATTR_MAX,
+};
+
+#define SWITCH_ATTR_DEFAULTS_OFFSET	0x1000
+
+
+#endif /* _UAPI_LINUX_SWITCH_H */
diff --git a/include/uapi/linux/usb/misc.h b/include/uapi/linux/usb/misc.h
new file mode 100644
index 000000000000..a6661f607fb2
--- /dev/null
+++ b/include/uapi/linux/usb/misc.h
@@ -0,0 +1,14 @@
+/*
+ * USB Miscellaneous Device Class definitions
+ *
+ * Ref: http://www.usb.org/developers/defined_class/#BaseClassEFh
+ */
+
+#ifndef __UAPI_LINUX_USB_MISC_H
+#define __UAPI_LINUX_USB_MISC_H
+
+#define USB_MISC_SUBCLASS_RNDIS			0x04
+
+#define USB_MISC_RNDIS_PROTO_ENET		0x01
+
+#endif /* __UAPI_LINUX_USB_MISC_H */
diff --git a/init/Kconfig b/init/Kconfig
index d559abf38c90..5b0b58f49e11 100644
--- a/init/Kconfig
+++ b/init/Kconfig
@@ -969,6 +969,7 @@ config CFS_BANDWIDTH
 config RT_GROUP_SCHED
 	bool "Group scheduling for SCHED_RR/FIFO"
 	depends on CGROUP_SCHED
+	depends on !PREEMPT_RT
 	default n
 	help
 	  This feature lets you explicitly allocate real CPU bandwidth
@@ -1896,6 +1897,7 @@ choice
 
 config SLAB
 	bool "SLAB"
+	depends on !PREEMPT_RT
 	select HAVE_HARDENED_USERCOPY_ALLOCATOR
 	help
 	  The regular slab allocator that is established and known to work
@@ -1916,6 +1918,7 @@ config SLUB
 config SLOB
 	depends on EXPERT
 	bool "SLOB (Simple Allocator)"
+	depends on !PREEMPT_RT
 	help
 	   SLOB replaces the stock allocator with a drastically simpler
 	   allocator. SLOB is generally more space efficient but
@@ -1982,7 +1985,7 @@ config SHUFFLE_PAGE_ALLOCATOR
 
 config SLUB_CPU_PARTIAL
 	default y
-	depends on SLUB && SMP
+	depends on SLUB && SMP && !PREEMPT_RT
 	bool "SLUB per cpu partial cache"
 	help
 	  Per cpu partial caches accelerate objects allocation and freeing
diff --git a/jenkins/Makefile b/jenkins/Makefile
new file mode 100644
index 000000000000..089090e164c6
--- /dev/null
+++ b/jenkins/Makefile
@@ -0,0 +1,201 @@
+# Usage:
+#
+# make -f jenkins/Makefile uImage/<defconfig> LOADADDR=<loadaddr>
+# make -f jenkins/Makefile dtbs/<defconfig>
+#
+# or
+#
+# make -f jenkins/Makefile [all|clean|...] DEFCONFIG=<defconfig> LOADADDR=<loadaddr>
+#
+# CROSS_COMPILE = default empty
+# ARCH = default auto detect using device config
+# VERBOSE = default off
+
+SHELL := /bin/sh
+
+.PHONY: default
+default: all
+
+space := 
+space += 
+
+TOOLCHAIN_PREFIX = 
+CROSS_COMPILE ?= $(TOOLCHAIN_PREFIX)
+ARCH ?= $(firstword $(subst /,$(space),$(DEFCONFIG)))
+
+VERBOSE =
+BUILDDIR = .build
+SRCDIR = .
+PROJECTNAME = $(notdir $(realpath $(SRCDIR)))
+SNAPSHOT_SUFFIX ?= $(addprefix -next.,$(SNAPSHOT))
+
+CONFIGURE_TARGETS  =
+CONFIGURE_TARGETS += $(addprefix $(BUILDDIR),$(defconfigsuffix))
+
+BUILD_TARGETS  =
+BUILD_TARGETS += $(kbuild_buildtargets)
+
+CHECK_TARGETS  =
+
+DIST_TARGETS  = 
+DIST_TARGETS += package_source
+DIST_TARGETS += package_binaries
+
+CLEAN_TARGETS  =
+CLEAN_TARGETS += $(addprefix clean,$(defconfigsuffix))
+
+#INSTALL_TARGETS  =
+#INSTALL_TARGETS += $(addprefix install,$(defconfigsuffix))
+
+########################################################################################################################
+# Internal macros and definitions
+packagesource_excludes  = 
+packagesource_excludes += '.git'
+packagesource_excludes += '.git/*'
+packagesource_excludes += 'jenkins'
+packagesource_excludes += 'jenkins/*'
+packagesource_excludes += '$(BUILDDIR)'
+packagesource_excludes += '$(BUILDDIR)/*'
+
+defconfigdir = $(SRCDIR)/arch
+defconfigfiles = $(shell find $(defconfigdir) -type f -name '*_defconfig')
+defconfigs = $(patsubst $(defconfigdir)/%,%,$(defconfigfiles))
+defconfigsuffix = $(addprefix /,$(filter $(DEFCONFIG),$(defconfigs)))
+
+kbuild_configuretargets  =
+kbuild_configuretargets += $(addprefix $(BUILDDIR)/,$(defconfigs))
+
+kbuild_buildtargets  =
+kbuild_buildtargets += uImage
+kbuild_buildtargets += dtbs
+kbuild_buildtargets += modules
+kbuild_buildtargets += modules_install
+
+kbuild_targets  = $(kbuild_buildtargets)
+kbuild_targets += vmlinux
+kbuild_targets += zImage
+kbuild_targets += clean
+kbuild_targets += image_name
+kbuild_targets += modules_install
+#kbuild_targets += headers_install
+#kbuild_targets += install
+
+# $(call makecmd,outputdir,targets[,makeflags])
+makecmd = INSTALL_MOD_PATH=. $(MAKE) $(MFLAGS) $3 '-C$(SRCDIR)' 'O=$1' $2
+# $(call tgzcmd,outputfile,inputdir[,archive-root][,excludes...][,inputs])
+tgzcmd = tar \
+  -P \
+  -I 'gzip -n' \
+  --sort name \
+  $(addprefix --mtime @,$(SOURCE_DATE_EPOCH)) \
+  --owner 0 \
+  --group 0 \
+  $(addprefix --exclude ,$4) \
+  --transform 's/$(subst /,\/,$(realpath $2))/$(subst /,\/,$(or $3,$(notdir $2)))/' \
+  -cvf \
+  $1 \
+  $(realpath $(or $(wildcard $(addprefix $2/,$5)),$2))
+
+# Force eager evaluation
+defconfigs := $(defconfigs)
+VERSIONSUFFIX := $(VERSIONSUFFIX)
+CROSS_COMPILE := $(CROSS_COMPILE)
+
+ifeq ($(VERBOSE),1)
+  Q =
+else
+  Q = @
+endif
+
+ifeq ($(strip $(MAKE_RESTARTS)),)
+  ifeq ($(strip $(ARCH)),)
+    $(error ARCH='$(ARCH)' invalid)
+  endif
+  ifeq ($(strip $(defconfigsuffix)),)
+    $(info DEFCONFIG='$(DEFCONFIG)' not found)
+  endif
+endif
+
+export ARCH
+export CROSS_COMPILE
+
+########################################################################################################################
+# Rules
+
+-include $(BUILDDIR)/rules.mk
+
+.PHONY: configure all check dist install clean distclean
+
+$(BUILDDIR)/rules.mk: $(defconfigfiles) $(filter-out $(BUILDDIR)/rules.mk,$(MAKEFILE_LIST)) | $(BUILDDIR)
+	@ \
+	{ \
+	  printf '.NOTPARALLEL:\n'; \
+	  $(foreach target,$(kbuild_targets), \
+	    printf '.PHONY: $(target)\n'; \
+	    printf   '$(target) : $$(addprefix $(target),$$(defconfigsuffix))\n'; \
+	    printf   '\n'; \
+	    $(foreach defconfig,$(defconfigs), \
+	      printf '.PHONY: $(target)/$(defconfig)\n'; \
+	      printf '$(target)/$(defconfig): $$(MAKEFILE_LIST) | $$(BUILDDIR)/$(defconfig)\n'; \
+	      printf   '\t+$$(Q)$$(call makecmd,$$(BUILDDIR)/$(defconfig),$(target) $$(MAKEOVERRIDES))\n'; \
+	      printf   '\n'; \
+	    ) \
+	  ) \
+	} > $@
+	@{ \
+	  printf '\n'; \
+	  $(foreach defconfig,$(defconfigs), \
+	    printf '.PHONY: $$(BUILDDIR)/$(defconfig)/linux-modules.tgz\n'; \
+	    printf '$$(BUILDDIR)/$(defconfig)/linux-modules.tgz: $$(BUILDDIR)/$(defconfig)/wago-kernelrelease.txt $(addsuffix /$(defconfig),uImage dtbs) $$(MAKEFILE_LIST) | $$(BUILDDIR)/$(defconfig)\n'; \
+	    printf   '\t$$(Q)$$(call tgzcmd,$$@,$$(BUILDDIR)/$(defconfig)/lib,,$$(packagebinary_excludes),modules) \\\n'; \
+	    printf   '\t  && cp -f $$(abspath $$@) $$(BUILDDIR)/$(defconfig)/linux-modules-"$$$$(cat $$<)"$$(SNAPSHOT_SUFFIX).tgz\n'; \
+	    printf   '\n'; \
+	    printf '.PHONY: $$(BUILDDIR)/$(defconfig)/linux-Source.tgz\n'; \
+	    printf '$$(BUILDDIR)/$(defconfig)/linux-Source.tgz: $$(BUILDDIR)/$(defconfig)/wago-kernelrelease.txt $$(BUILDDIR)/linux-Source.tgz $$(MAKEFILE_LIST) | $$(BUILDDIR)/$(defconfig)\n'; \
+	    printf   '\t$$(Q)cp -f $$(abspath $$(BUILDDIR)/linux-Source.tgz) $$(BUILDDIR)/$(defconfig)/linux-Source-"$$$$(cat $$<)"$$(SNAPSHOT_SUFFIX).tgz\n'; \
+	    printf   '\n'; \
+	    printf '.PHONY: $$(BUILDDIR)/$(defconfig)/wago-kernelrelease.txt\n'; \
+	    printf '$$(BUILDDIR)/$(defconfig)/wago-kernelrelease.txt: $$(MAKEFILE_LIST) | $$(BUILDDIR)/$(defconfig)\n'; \
+	    printf   '\t+$$(Q)$$(call makecmd,$$(BUILDDIR)/$(defconfig),kernelrelease $$(MAKEOVERRIDES),-s) 1>$$@\n'; \
+	    printf   '\n'; \
+	    ) \
+	} >> $@
+
+$(kbuild_configuretargets): $(MAKEFILE_LIST)
+	+$(Q)$(call makecmd,$@,$(notdir $@) $(MAKEOVERRIDES))
+
+.PHONY: $(BUILDDIR)/linux-Source.tgz
+$(BUILDDIR)/linux-Source.tgz: $(SRCDIR) $(MAKEFILE_LIST)
+	$(Q)$(call tgzcmd,$@,$<,$(PROJECTNAME),$(packagesource_excludes))
+
+.PHONY: package_binaries
+package_binaries : $(addsuffix /linux-modules.tgz,$(addprefix $(BUILDDIR),$(defconfigsuffix)))
+
+.PHONY: package_source
+package_source : $(addsuffix /linux-Source.tgz,$(addprefix $(BUILDDIR),$(defconfigsuffix)))
+
+.PHONY: kernelrelease
+kernelrelease : $(addsuffix /wago-kernelrelease.txt,$(addprefix $(BUILDDIR),$(defconfigsuffix)))
+	$(Q)$(addprefix cat ,$<)
+
+configure: $(CONFIGURE_TARGETS)
+
+all: $(BUILD_TARGETS)
+
+check: $(CHECK_TARGETS)
+
+dist: configure $(DIST_TARGETS)
+
+install: $(INSTALL_TARGETS)
+
+clean: $(CLEAN_TARGETS)
+
+distclean:
+	$(Q)-rm -rf $(BUILDDIR)
+
+$(BUILDDIR):
+	$(Q)mkdir -p $@
+
+debug-print-%:
+	@printf '%s:\n' $*; \
+	 printf '%s\n' $($*)
diff --git a/kernel/Kconfig.locks b/kernel/Kconfig.locks
index 3de8fd11873b..4198f0273ecd 100644
--- a/kernel/Kconfig.locks
+++ b/kernel/Kconfig.locks
@@ -251,7 +251,7 @@ config ARCH_USE_QUEUED_RWLOCKS
 
 config QUEUED_RWLOCKS
 	def_bool y if ARCH_USE_QUEUED_RWLOCKS
-	depends on SMP
+	depends on SMP && !PREEMPT_RT
 
 config ARCH_HAS_MMIOWB
 	bool
diff --git a/kernel/Kconfig.preempt b/kernel/Kconfig.preempt
index bf82259cff96..b5cd1e278eb5 100644
--- a/kernel/Kconfig.preempt
+++ b/kernel/Kconfig.preempt
@@ -1,5 +1,11 @@
 # SPDX-License-Identifier: GPL-2.0-only
 
+config HAVE_PREEMPT_LAZY
+	bool
+
+config PREEMPT_LAZY
+	def_bool y if HAVE_PREEMPT_LAZY && PREEMPT_RT
+
 choice
 	prompt "Preemption Model"
 	default PREEMPT_NONE
@@ -59,6 +65,7 @@ config PREEMPT_RT
 	bool "Fully Preemptible Kernel (Real-Time)"
 	depends on EXPERT && ARCH_SUPPORTS_RT
 	select PREEMPTION
+	select RT_MUTEXES
 	help
 	  This option turns the kernel into a real-time kernel by replacing
 	  various locking primitives (spinlocks, rwlocks, etc.) with
diff --git a/kernel/cgroup/cpuset.c b/kernel/cgroup/cpuset.c
index 53c70c470a38..8f4b2b9aa06c 100644
--- a/kernel/cgroup/cpuset.c
+++ b/kernel/cgroup/cpuset.c
@@ -345,7 +345,7 @@ void cpuset_read_unlock(void)
 	percpu_up_read(&cpuset_rwsem);
 }
 
-static DEFINE_SPINLOCK(callback_lock);
+static DEFINE_RAW_SPINLOCK(callback_lock);
 
 static struct workqueue_struct *cpuset_migrate_mm_wq;
 
@@ -1280,7 +1280,7 @@ static int update_parent_subparts_cpumask(struct cpuset *cpuset, int cmd,
 	 * Newly added CPUs will be removed from effective_cpus and
 	 * newly deleted ones will be added back to effective_cpus.
 	 */
-	spin_lock_irq(&callback_lock);
+	raw_spin_lock_irq(&callback_lock);
 	if (adding) {
 		cpumask_or(parent->subparts_cpus,
 			   parent->subparts_cpus, tmp->addmask);
@@ -1299,7 +1299,7 @@ static int update_parent_subparts_cpumask(struct cpuset *cpuset, int cmd,
 	}
 
 	parent->nr_subparts_cpus = cpumask_weight(parent->subparts_cpus);
-	spin_unlock_irq(&callback_lock);
+	raw_spin_unlock_irq(&callback_lock);
 
 	return cmd == partcmd_update;
 }
@@ -1404,7 +1404,7 @@ static void update_cpumasks_hier(struct cpuset *cs, struct tmpmasks *tmp)
 			continue;
 		rcu_read_unlock();
 
-		spin_lock_irq(&callback_lock);
+		raw_spin_lock_irq(&callback_lock);
 
 		cpumask_copy(cp->effective_cpus, tmp->new_cpus);
 		if (cp->nr_subparts_cpus &&
@@ -1435,7 +1435,7 @@ static void update_cpumasks_hier(struct cpuset *cs, struct tmpmasks *tmp)
 					= cpumask_weight(cp->subparts_cpus);
 			}
 		}
-		spin_unlock_irq(&callback_lock);
+		raw_spin_unlock_irq(&callback_lock);
 
 		WARN_ON(!is_in_v2_mode() &&
 			!cpumask_equal(cp->cpus_allowed, cp->effective_cpus));
@@ -1553,7 +1553,7 @@ static int update_cpumask(struct cpuset *cs, struct cpuset *trialcs,
 			return -EINVAL;
 	}
 
-	spin_lock_irq(&callback_lock);
+	raw_spin_lock_irq(&callback_lock);
 	cpumask_copy(cs->cpus_allowed, trialcs->cpus_allowed);
 
 	/*
@@ -1564,7 +1564,7 @@ static int update_cpumask(struct cpuset *cs, struct cpuset *trialcs,
 			       cs->cpus_allowed);
 		cs->nr_subparts_cpus = cpumask_weight(cs->subparts_cpus);
 	}
-	spin_unlock_irq(&callback_lock);
+	raw_spin_unlock_irq(&callback_lock);
 
 	update_cpumasks_hier(cs, &tmp);
 
@@ -1758,9 +1758,9 @@ static void update_nodemasks_hier(struct cpuset *cs, nodemask_t *new_mems)
 			continue;
 		rcu_read_unlock();
 
-		spin_lock_irq(&callback_lock);
+		raw_spin_lock_irq(&callback_lock);
 		cp->effective_mems = *new_mems;
-		spin_unlock_irq(&callback_lock);
+		raw_spin_unlock_irq(&callback_lock);
 
 		WARN_ON(!is_in_v2_mode() &&
 			!nodes_equal(cp->mems_allowed, cp->effective_mems));
@@ -1828,9 +1828,9 @@ static int update_nodemask(struct cpuset *cs, struct cpuset *trialcs,
 	if (retval < 0)
 		goto done;
 
-	spin_lock_irq(&callback_lock);
+	raw_spin_lock_irq(&callback_lock);
 	cs->mems_allowed = trialcs->mems_allowed;
-	spin_unlock_irq(&callback_lock);
+	raw_spin_unlock_irq(&callback_lock);
 
 	/* use trialcs->mems_allowed as a temp variable */
 	update_nodemasks_hier(cs, &trialcs->mems_allowed);
@@ -1921,9 +1921,9 @@ static int update_flag(cpuset_flagbits_t bit, struct cpuset *cs,
 	spread_flag_changed = ((is_spread_slab(cs) != is_spread_slab(trialcs))
 			|| (is_spread_page(cs) != is_spread_page(trialcs)));
 
-	spin_lock_irq(&callback_lock);
+	raw_spin_lock_irq(&callback_lock);
 	cs->flags = trialcs->flags;
-	spin_unlock_irq(&callback_lock);
+	raw_spin_unlock_irq(&callback_lock);
 
 	if (!cpumask_empty(trialcs->cpus_allowed) && balance_flag_changed)
 		rebuild_sched_domains_locked();
@@ -2432,7 +2432,7 @@ static int cpuset_common_seq_show(struct seq_file *sf, void *v)
 	cpuset_filetype_t type = seq_cft(sf)->private;
 	int ret = 0;
 
-	spin_lock_irq(&callback_lock);
+	raw_spin_lock_irq(&callback_lock);
 
 	switch (type) {
 	case FILE_CPULIST:
@@ -2454,7 +2454,7 @@ static int cpuset_common_seq_show(struct seq_file *sf, void *v)
 		ret = -EINVAL;
 	}
 
-	spin_unlock_irq(&callback_lock);
+	raw_spin_unlock_irq(&callback_lock);
 	return ret;
 }
 
@@ -2767,14 +2767,14 @@ static int cpuset_css_online(struct cgroup_subsys_state *css)
 
 	cpuset_inc();
 
-	spin_lock_irq(&callback_lock);
+	raw_spin_lock_irq(&callback_lock);
 	if (is_in_v2_mode()) {
 		cpumask_copy(cs->effective_cpus, parent->effective_cpus);
 		cs->effective_mems = parent->effective_mems;
 		cs->use_parent_ecpus = true;
 		parent->child_ecpus_count++;
 	}
-	spin_unlock_irq(&callback_lock);
+	raw_spin_unlock_irq(&callback_lock);
 
 	if (!test_bit(CGRP_CPUSET_CLONE_CHILDREN, &css->cgroup->flags))
 		goto out_unlock;
@@ -2801,12 +2801,12 @@ static int cpuset_css_online(struct cgroup_subsys_state *css)
 	}
 	rcu_read_unlock();
 
-	spin_lock_irq(&callback_lock);
+	raw_spin_lock_irq(&callback_lock);
 	cs->mems_allowed = parent->mems_allowed;
 	cs->effective_mems = parent->mems_allowed;
 	cpumask_copy(cs->cpus_allowed, parent->cpus_allowed);
 	cpumask_copy(cs->effective_cpus, parent->cpus_allowed);
-	spin_unlock_irq(&callback_lock);
+	raw_spin_unlock_irq(&callback_lock);
 out_unlock:
 	percpu_up_write(&cpuset_rwsem);
 	put_online_cpus();
@@ -2862,7 +2862,7 @@ static void cpuset_css_free(struct cgroup_subsys_state *css)
 static void cpuset_bind(struct cgroup_subsys_state *root_css)
 {
 	percpu_down_write(&cpuset_rwsem);
-	spin_lock_irq(&callback_lock);
+	raw_spin_lock_irq(&callback_lock);
 
 	if (is_in_v2_mode()) {
 		cpumask_copy(top_cpuset.cpus_allowed, cpu_possible_mask);
@@ -2873,7 +2873,7 @@ static void cpuset_bind(struct cgroup_subsys_state *root_css)
 		top_cpuset.mems_allowed = top_cpuset.effective_mems;
 	}
 
-	spin_unlock_irq(&callback_lock);
+	raw_spin_unlock_irq(&callback_lock);
 	percpu_up_write(&cpuset_rwsem);
 }
 
@@ -2970,12 +2970,12 @@ hotplug_update_tasks_legacy(struct cpuset *cs,
 {
 	bool is_empty;
 
-	spin_lock_irq(&callback_lock);
+	raw_spin_lock_irq(&callback_lock);
 	cpumask_copy(cs->cpus_allowed, new_cpus);
 	cpumask_copy(cs->effective_cpus, new_cpus);
 	cs->mems_allowed = *new_mems;
 	cs->effective_mems = *new_mems;
-	spin_unlock_irq(&callback_lock);
+	raw_spin_unlock_irq(&callback_lock);
 
 	/*
 	 * Don't call update_tasks_cpumask() if the cpuset becomes empty,
@@ -3012,10 +3012,10 @@ hotplug_update_tasks(struct cpuset *cs,
 	if (nodes_empty(*new_mems))
 		*new_mems = parent_cs(cs)->effective_mems;
 
-	spin_lock_irq(&callback_lock);
+	raw_spin_lock_irq(&callback_lock);
 	cpumask_copy(cs->effective_cpus, new_cpus);
 	cs->effective_mems = *new_mems;
-	spin_unlock_irq(&callback_lock);
+	raw_spin_unlock_irq(&callback_lock);
 
 	if (cpus_updated)
 		update_tasks_cpumask(cs);
@@ -3170,7 +3170,7 @@ static void cpuset_hotplug_workfn(struct work_struct *work)
 
 	/* synchronize cpus_allowed to cpu_active_mask */
 	if (cpus_updated) {
-		spin_lock_irq(&callback_lock);
+		raw_spin_lock_irq(&callback_lock);
 		if (!on_dfl)
 			cpumask_copy(top_cpuset.cpus_allowed, &new_cpus);
 		/*
@@ -3190,17 +3190,17 @@ static void cpuset_hotplug_workfn(struct work_struct *work)
 			}
 		}
 		cpumask_copy(top_cpuset.effective_cpus, &new_cpus);
-		spin_unlock_irq(&callback_lock);
+		raw_spin_unlock_irq(&callback_lock);
 		/* we don't mess with cpumasks of tasks in top_cpuset */
 	}
 
 	/* synchronize mems_allowed to N_MEMORY */
 	if (mems_updated) {
-		spin_lock_irq(&callback_lock);
+		raw_spin_lock_irq(&callback_lock);
 		if (!on_dfl)
 			top_cpuset.mems_allowed = new_mems;
 		top_cpuset.effective_mems = new_mems;
-		spin_unlock_irq(&callback_lock);
+		raw_spin_unlock_irq(&callback_lock);
 		update_tasks_nodemask(&top_cpuset);
 	}
 
@@ -3301,11 +3301,11 @@ void cpuset_cpus_allowed(struct task_struct *tsk, struct cpumask *pmask)
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&callback_lock, flags);
+	raw_spin_lock_irqsave(&callback_lock, flags);
 	rcu_read_lock();
 	guarantee_online_cpus(task_cs(tsk), pmask);
 	rcu_read_unlock();
-	spin_unlock_irqrestore(&callback_lock, flags);
+	raw_spin_unlock_irqrestore(&callback_lock, flags);
 }
 
 /**
@@ -3366,11 +3366,11 @@ nodemask_t cpuset_mems_allowed(struct task_struct *tsk)
 	nodemask_t mask;
 	unsigned long flags;
 
-	spin_lock_irqsave(&callback_lock, flags);
+	raw_spin_lock_irqsave(&callback_lock, flags);
 	rcu_read_lock();
 	guarantee_online_mems(task_cs(tsk), &mask);
 	rcu_read_unlock();
-	spin_unlock_irqrestore(&callback_lock, flags);
+	raw_spin_unlock_irqrestore(&callback_lock, flags);
 
 	return mask;
 }
@@ -3462,14 +3462,14 @@ bool __cpuset_node_allowed(int node, gfp_t gfp_mask)
 		return true;
 
 	/* Not hardwall and node outside mems_allowed: scan up cpusets */
-	spin_lock_irqsave(&callback_lock, flags);
+	raw_spin_lock_irqsave(&callback_lock, flags);
 
 	rcu_read_lock();
 	cs = nearest_hardwall_ancestor(task_cs(current));
 	allowed = node_isset(node, cs->mems_allowed);
 	rcu_read_unlock();
 
-	spin_unlock_irqrestore(&callback_lock, flags);
+	raw_spin_unlock_irqrestore(&callback_lock, flags);
 	return allowed;
 }
 
diff --git a/kernel/cgroup/rstat.c b/kernel/cgroup/rstat.c
index d51175cedfca..b424f3157b34 100644
--- a/kernel/cgroup/rstat.c
+++ b/kernel/cgroup/rstat.c
@@ -149,8 +149,9 @@ static void cgroup_rstat_flush_locked(struct cgroup *cgrp, bool may_sleep)
 		raw_spinlock_t *cpu_lock = per_cpu_ptr(&cgroup_rstat_cpu_lock,
 						       cpu);
 		struct cgroup *pos = NULL;
+		unsigned long flags;
 
-		raw_spin_lock(cpu_lock);
+		raw_spin_lock_irqsave(cpu_lock, flags);
 		while ((pos = cgroup_rstat_cpu_pop_updated(pos, cgrp, cpu))) {
 			struct cgroup_subsys_state *css;
 
@@ -162,7 +163,7 @@ static void cgroup_rstat_flush_locked(struct cgroup *cgrp, bool may_sleep)
 				css->ss->css_rstat_flush(css, cpu);
 			rcu_read_unlock();
 		}
-		raw_spin_unlock(cpu_lock);
+		raw_spin_unlock_irqrestore(cpu_lock, flags);
 
 		/* if @may_sleep, play nice and yield if necessary */
 		if (may_sleep && (need_resched() ||
diff --git a/kernel/cpu.c b/kernel/cpu.c
index 2b8d7a5db383..4e11e91010e1 100644
--- a/kernel/cpu.c
+++ b/kernel/cpu.c
@@ -1606,7 +1606,7 @@ static struct cpuhp_step cpuhp_hp_states[] = {
 		.name			= "ap:online",
 	},
 	/*
-	 * Handled on controll processor until the plugged processor manages
+	 * Handled on control processor until the plugged processor manages
 	 * this itself.
 	 */
 	[CPUHP_TEARDOWN_CPU] = {
@@ -1615,6 +1615,13 @@ static struct cpuhp_step cpuhp_hp_states[] = {
 		.teardown.single	= takedown_cpu,
 		.cant_stop		= true,
 	},
+
+	[CPUHP_AP_SCHED_WAIT_EMPTY] = {
+		.name			= "sched:waitempty",
+		.startup.single		= NULL,
+		.teardown.single	= sched_cpu_wait_empty,
+	},
+
 	/* Handle smpboot threads park/unpark */
 	[CPUHP_AP_SMPBOOT_THREADS] = {
 		.name			= "smpboot/threads:online",
diff --git a/kernel/debug/kdb/kdb_main.c b/kernel/debug/kdb/kdb_main.c
index 930ac1b25ec7..dbf1d126ac5e 100644
--- a/kernel/debug/kdb/kdb_main.c
+++ b/kernel/debug/kdb/kdb_main.c
@@ -2101,7 +2101,7 @@ static int kdb_dmesg(int argc, const char **argv)
 	int adjust = 0;
 	int n = 0;
 	int skip = 0;
-	struct kmsg_dumper dumper = { .active = 1 };
+	struct kmsg_dumper_iter iter = { .active = 1 };
 	size_t len;
 	char buf[201];
 
@@ -2126,8 +2126,8 @@ static int kdb_dmesg(int argc, const char **argv)
 		kdb_set(2, setargs);
 	}
 
-	kmsg_dump_rewind_nolock(&dumper);
-	while (kmsg_dump_get_line_nolock(&dumper, 1, NULL, 0, NULL))
+	kmsg_dump_rewind(&iter);
+	while (kmsg_dump_get_line(&iter, 1, NULL, 0, NULL))
 		n++;
 
 	if (lines < 0) {
@@ -2159,8 +2159,8 @@ static int kdb_dmesg(int argc, const char **argv)
 	if (skip >= n || skip < 0)
 		return 0;
 
-	kmsg_dump_rewind_nolock(&dumper);
-	while (kmsg_dump_get_line_nolock(&dumper, 1, buf, sizeof(buf), &len)) {
+	kmsg_dump_rewind(&iter);
+	while (kmsg_dump_get_line(&iter, 1, buf, sizeof(buf), &len)) {
 		if (skip) {
 			skip--;
 			continue;
diff --git a/kernel/entry/common.c b/kernel/entry/common.c
index e9e2df3f3f9e..c9e7db560974 100644
--- a/kernel/entry/common.c
+++ b/kernel/entry/common.c
@@ -2,6 +2,7 @@
 
 #include <linux/context_tracking.h>
 #include <linux/entry-common.h>
+#include <linux/highmem.h>
 #include <linux/livepatch.h>
 #include <linux/audit.h>
 
@@ -148,9 +149,17 @@ static unsigned long exit_to_user_mode_loop(struct pt_regs *regs,
 
 		local_irq_enable_exit_to_user(ti_work);
 
-		if (ti_work & _TIF_NEED_RESCHED)
+		if (ti_work & _TIF_NEED_RESCHED_MASK)
 			schedule();
 
+#ifdef ARCH_RT_DELAYS_SIGNAL_SEND
+		if (unlikely(current->forced_info.si_signo)) {
+			struct task_struct *t = current;
+			force_sig_info(&t->forced_info);
+			t->forced_info.si_signo = 0;
+		}
+#endif
+
 		if (ti_work & _TIF_UPROBE)
 			uprobe_notify_resume(regs);
 
@@ -194,6 +203,7 @@ static void exit_to_user_mode_prepare(struct pt_regs *regs)
 
 	/* Ensure that the address limit is intact and no locks are held */
 	addr_limit_user_check();
+	kmap_assert_nomap();
 	lockdep_assert_irqs_disabled();
 	lockdep_sys_exit();
 }
@@ -353,7 +363,7 @@ void irqentry_exit_cond_resched(void)
 		rcu_irq_exit_check_preempt();
 		if (IS_ENABLED(CONFIG_DEBUG_ENTRY))
 			WARN_ON_ONCE(!on_thread_stack());
-		if (need_resched())
+		if (should_resched(0))
 			preempt_schedule_irq();
 	}
 }
diff --git a/kernel/exit.c b/kernel/exit.c
index d13d67fc5f4e..f5933bd07932 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -152,7 +152,7 @@ static void __exit_signal(struct task_struct *tsk)
 	 * Do this under ->siglock, we can race with another thread
 	 * doing sigqueue_free() if we have SIGQUEUE_PREALLOC signals.
 	 */
-	flush_sigqueue(&tsk->pending);
+	flush_task_sigqueue(tsk);
 	tsk->sighand = NULL;
 	spin_unlock(&sighand->siglock);
 
diff --git a/kernel/fork.c b/kernel/fork.c
index c675fdbd3dce..ab260c64a0e0 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -42,6 +42,7 @@
 #include <linux/mmu_notifier.h>
 #include <linux/fs.h>
 #include <linux/mm.h>
+#include <linux/kprobes.h>
 #include <linux/vmacache.h>
 #include <linux/nsproxy.h>
 #include <linux/capability.h>
@@ -288,7 +289,7 @@ static inline void free_thread_stack(struct task_struct *tsk)
 			return;
 		}
 
-		vfree_atomic(tsk->stack);
+		vfree(tsk->stack);
 		return;
 	}
 #endif
@@ -688,6 +689,19 @@ void __mmdrop(struct mm_struct *mm)
 }
 EXPORT_SYMBOL_GPL(__mmdrop);
 
+#ifdef CONFIG_PREEMPT_RT
+/*
+ * RCU callback for delayed mm drop. Not strictly rcu, but we don't
+ * want another facility to make this work.
+ */
+void __mmdrop_delayed(struct rcu_head *rhp)
+{
+	struct mm_struct *mm = container_of(rhp, struct mm_struct, delayed_drop);
+
+	__mmdrop(mm);
+}
+#endif
+
 static void mmdrop_async_fn(struct work_struct *work)
 {
 	struct mm_struct *mm;
@@ -729,6 +743,15 @@ void __put_task_struct(struct task_struct *tsk)
 	WARN_ON(refcount_read(&tsk->usage));
 	WARN_ON(tsk == current);
 
+	/*
+	 * Remove function-return probe instances associated with this
+	 * task and put them back on the free list.
+	 */
+	kprobe_flush_task(tsk);
+
+	/* Task is done with its stack. */
+	put_task_stack(tsk);
+
 	io_uring_free(tsk);
 	cgroup_free(tsk);
 	task_numa_free(tsk, true);
@@ -926,10 +949,16 @@ static struct task_struct *dup_task_struct(struct task_struct *orig, int node)
 	tsk->splice_pipe = NULL;
 	tsk->task_frag.page = NULL;
 	tsk->wake_q.next = NULL;
+	tsk->wake_q_sleeper.next = NULL;
+
+#ifdef CONFIG_PREEMPT_RT
+	atomic_set(&tsk->xmit_recursion, 0);
+#endif
 
 	account_kernel_stack(tsk, 1);
 
 	kcov_task_init(tsk);
+	kmap_local_fork(tsk);
 
 #ifdef CONFIG_FAULT_INJECTION
 	tsk->fail_nth = 0;
@@ -1992,6 +2021,7 @@ static __latent_entropy struct task_struct *copy_process(
 	spin_lock_init(&p->alloc_lock);
 
 	init_sigpending(&p->pending);
+	p->sigqueue_cache = NULL;
 
 	p->utime = p->stime = p->gtime = 0;
 #ifdef CONFIG_ARCH_HAS_SCALED_CPUTIME
diff --git a/kernel/futex.c b/kernel/futex.c
index 0693b3ea0f9a..a23437a410e3 100644
--- a/kernel/futex.c
+++ b/kernel/futex.c
@@ -1499,6 +1499,7 @@ static int wake_futex_pi(u32 __user *uaddr, u32 uval, struct futex_pi_state *pi_
 	struct task_struct *new_owner;
 	bool postunlock = false;
 	DEFINE_WAKE_Q(wake_q);
+	DEFINE_WAKE_Q(wake_sleeper_q);
 	int ret = 0;
 
 	new_owner = rt_mutex_next_owner(&pi_state->pi_mutex);
@@ -1548,14 +1549,15 @@ static int wake_futex_pi(u32 __user *uaddr, u32 uval, struct futex_pi_state *pi_
 		 * not fail.
 		 */
 		pi_state_update_owner(pi_state, new_owner);
-		postunlock = __rt_mutex_futex_unlock(&pi_state->pi_mutex, &wake_q);
+		postunlock = __rt_mutex_futex_unlock(&pi_state->pi_mutex, &wake_q,
+						     &wake_sleeper_q);
 	}
 
 out_unlock:
 	raw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);
 
 	if (postunlock)
-		rt_mutex_postunlock(&wake_q);
+		rt_mutex_postunlock(&wake_q, &wake_sleeper_q);
 
 	return ret;
 }
@@ -2156,6 +2158,16 @@ static int futex_requeue(u32 __user *uaddr1, unsigned int flags,
 				 */
 				requeue_pi_wake_futex(this, &key2, hb2);
 				continue;
+			} else if (ret == -EAGAIN) {
+				/*
+				 * Waiter was woken by timeout or
+				 * signal and has set pi_blocked_on to
+				 * PI_WAKEUP_INPROGRESS before we
+				 * tried to enqueue it on the rtmutex.
+				 */
+				this->pi_state = NULL;
+				put_pi_state(pi_state);
+				continue;
 			} else if (ret) {
 				/*
 				 * rt_mutex_start_proxy_lock() detected a
@@ -2849,7 +2861,7 @@ static int futex_lock_pi(u32 __user *uaddr, unsigned int flags,
 		goto no_block;
 	}
 
-	rt_mutex_init_waiter(&rt_waiter);
+	rt_mutex_init_waiter(&rt_waiter, false);
 
 	/*
 	 * On PREEMPT_RT_FULL, when hb->lock becomes an rt_mutex, we must not
@@ -3174,7 +3186,7 @@ static int futex_wait_requeue_pi(u32 __user *uaddr, unsigned int flags,
 {
 	struct hrtimer_sleeper timeout, *to;
 	struct rt_mutex_waiter rt_waiter;
-	struct futex_hash_bucket *hb;
+	struct futex_hash_bucket *hb, *hb2;
 	union futex_key key2 = FUTEX_KEY_INIT;
 	struct futex_q q = futex_q_init;
 	int res, ret;
@@ -3195,7 +3207,7 @@ static int futex_wait_requeue_pi(u32 __user *uaddr, unsigned int flags,
 	 * The waiter is allocated on our stack, manipulated by the requeue
 	 * code while we sleep on uaddr.
 	 */
-	rt_mutex_init_waiter(&rt_waiter);
+	rt_mutex_init_waiter(&rt_waiter, false);
 
 	ret = get_futex_key(uaddr2, flags & FLAGS_SHARED, &key2, FUTEX_WRITE);
 	if (unlikely(ret != 0))
@@ -3226,20 +3238,55 @@ static int futex_wait_requeue_pi(u32 __user *uaddr, unsigned int flags,
 	/* Queue the futex_q, drop the hb lock, wait for wakeup. */
 	futex_wait_queue_me(hb, &q, to);
 
-	spin_lock(&hb->lock);
-	ret = handle_early_requeue_pi_wakeup(hb, &q, &key2, to);
-	spin_unlock(&hb->lock);
-	if (ret)
-		goto out;
+	/*
+	 * On RT we must avoid races with requeue and trying to block
+	 * on two mutexes (hb->lock and uaddr2's rtmutex) by
+	 * serializing access to pi_blocked_on with pi_lock.
+	 */
+	raw_spin_lock_irq(&current->pi_lock);
+	if (current->pi_blocked_on) {
+		/*
+		 * We have been requeued or are in the process of
+		 * being requeued.
+		 */
+		raw_spin_unlock_irq(&current->pi_lock);
+	} else {
+		/*
+		 * Setting pi_blocked_on to PI_WAKEUP_INPROGRESS
+		 * prevents a concurrent requeue from moving us to the
+		 * uaddr2 rtmutex. After that we can safely acquire
+		 * (and possibly block on) hb->lock.
+		 */
+		current->pi_blocked_on = PI_WAKEUP_INPROGRESS;
+		raw_spin_unlock_irq(&current->pi_lock);
+
+		spin_lock(&hb->lock);
+
+		/*
+		 * Clean up pi_blocked_on. We might leak it otherwise
+		 * when we succeeded with the hb->lock in the fast
+		 * path.
+		 */
+		raw_spin_lock_irq(&current->pi_lock);
+		current->pi_blocked_on = NULL;
+		raw_spin_unlock_irq(&current->pi_lock);
+
+		ret = handle_early_requeue_pi_wakeup(hb, &q, &key2, to);
+		spin_unlock(&hb->lock);
+		if (ret)
+			goto out;
+	}
 
 	/*
-	 * In order for us to be here, we know our q.key == key2, and since
-	 * we took the hb->lock above, we also know that futex_requeue() has
-	 * completed and we no longer have to concern ourselves with a wakeup
-	 * race with the atomic proxy lock acquisition by the requeue code. The
-	 * futex_requeue dropped our key1 reference and incremented our key2
-	 * reference count.
+	 * In order to be here, we have either been requeued, are in
+	 * the process of being requeued, or requeue successfully
+	 * acquired uaddr2 on our behalf.  If pi_blocked_on was
+	 * non-null above, we may be racing with a requeue.  Do not
+	 * rely on q->lock_ptr to be hb2->lock until after blocking on
+	 * hb->lock or hb2->lock. The futex_requeue dropped our key1
+	 * reference and incremented our key2 reference count.
 	 */
+	hb2 = hash_futex(&key2);
 
 	/* Check if the requeue code acquired the second futex for us. */
 	if (!q.rt_waiter) {
@@ -3248,14 +3295,15 @@ static int futex_wait_requeue_pi(u32 __user *uaddr, unsigned int flags,
 		 * did a lock-steal - fix up the PI-state in that case.
 		 */
 		if (q.pi_state && (q.pi_state->owner != current)) {
-			spin_lock(q.lock_ptr);
+			spin_lock(&hb2->lock);
+			BUG_ON(&hb2->lock != q.lock_ptr);
 			ret = fixup_pi_state_owner(uaddr2, &q, current);
 			/*
 			 * Drop the reference to the pi state which
 			 * the requeue_pi() code acquired for us.
 			 */
 			put_pi_state(q.pi_state);
-			spin_unlock(q.lock_ptr);
+			spin_unlock(&hb2->lock);
 			/*
 			 * Adjust the return value. It's either -EFAULT or
 			 * success (1) but the caller expects 0 for success.
@@ -3274,7 +3322,8 @@ static int futex_wait_requeue_pi(u32 __user *uaddr, unsigned int flags,
 		pi_mutex = &q.pi_state->pi_mutex;
 		ret = rt_mutex_wait_proxy_lock(pi_mutex, to, &rt_waiter);
 
-		spin_lock(q.lock_ptr);
+		spin_lock(&hb2->lock);
+		BUG_ON(&hb2->lock != q.lock_ptr);
 		if (ret && !rt_mutex_cleanup_proxy_lock(pi_mutex, &rt_waiter))
 			ret = 0;
 
diff --git a/kernel/irq/Makefile b/kernel/irq/Makefile
index b4f53717d143..3a37fae041ee 100644
--- a/kernel/irq/Makefile
+++ b/kernel/irq/Makefile
@@ -18,3 +18,4 @@ obj-$(CONFIG_GENERIC_IRQ_IPI) += ipi.o
 obj-$(CONFIG_SMP) += affinity.o
 obj-$(CONFIG_GENERIC_IRQ_DEBUGFS) += debugfs.o
 obj-$(CONFIG_GENERIC_IRQ_MATRIX_ALLOCATOR) += matrix.o
+obj-$(CONFIG_IRQ_PRIORITY_TABLE) += wsysinit-prio-table.o
diff --git a/kernel/irq/handle.c b/kernel/irq/handle.c
index 762a928e18f9..7929fcdb7817 100644
--- a/kernel/irq/handle.c
+++ b/kernel/irq/handle.c
@@ -192,10 +192,16 @@ irqreturn_t handle_irq_event_percpu(struct irq_desc *desc)
 {
 	irqreturn_t retval;
 	unsigned int flags = 0;
+	struct pt_regs *regs = get_irq_regs();
+	u64 ip = regs ? instruction_pointer(regs) : 0;
 
 	retval = __handle_irq_event_percpu(desc, &flags);
 
-	add_interrupt_randomness(desc->irq_data.irq, flags);
+#ifdef CONFIG_PREEMPT_RT
+	desc->random_ip = ip;
+#else
+	add_interrupt_randomness(desc->irq_data.irq, flags, ip);
+#endif
 
 	if (!noirqdebug)
 		note_interrupt(desc, retval);
diff --git a/kernel/irq/manage.c b/kernel/irq/manage.c
index c460e0496006..58e7fe2d72b3 100644
--- a/kernel/irq/manage.c
+++ b/kernel/irq/manage.c
@@ -22,6 +22,10 @@
 #include <uapi/linux/sched/types.h>
 #include <linux/task_work.h>
 
+#ifdef CONFIG_IRQ_PRIORITY_TABLE
+#include <linux/wsysinit-prio.h>
+#endif
+
 #include "internals.h"
 
 #if defined(CONFIG_IRQ_FORCED_THREADING) && !defined(CONFIG_PREEMPT_RT)
@@ -1155,6 +1159,23 @@ static int irq_thread(void *data)
 	irqreturn_t (*handler_fn)(struct irq_desc *desc,
 			struct irqaction *action);
 
+	sched_set_fifo(current);
+
+#ifdef CONFIG_IRQ_PRIORITY_TABLE
+	if (action->flags & IRQF_THREAD_TBL_LOOKUP) {
+		int priority = wsysinit_tbl_get_prio_by_name(action->name);
+
+		if (priority > 0) {
+			struct sched_param sp = { .sched_priority = priority };
+
+			if (sp.sched_priority > MAX_USER_RT_PRIO)
+				sp.sched_priority = MAX_USER_RT_PRIO - 1;
+
+			sched_setscheduler_nocheck(current, SCHED_FIFO, &sp);
+		}
+	}
+#endif
+
 	if (force_irqthreads && test_bit(IRQTF_FORCED_THREAD,
 					&action->thread_flags))
 		handler_fn = irq_forced_thread_fn;
@@ -1175,6 +1196,12 @@ static int irq_thread(void *data)
 		if (action_ret == IRQ_WAKE_THREAD)
 			irq_wake_secondary(desc, action);
 
+		if (IS_ENABLED(CONFIG_PREEMPT_RT)) {
+			migrate_disable();
+			add_interrupt_randomness(action->irq, 0,
+				 desc->random_ip ^ (unsigned long) action);
+			migrate_enable();
+		}
 		wake_threads_waitq(desc);
 	}
 
@@ -1320,8 +1347,6 @@ setup_irq_thread(struct irqaction *new, unsigned int irq, bool secondary)
 	if (IS_ERR(t))
 		return PTR_ERR(t);
 
-	sched_set_fifo(t);
-
 	/*
 	 * We keep the reference to the task struct even if
 	 * the thread dies to avoid that the interrupt code
@@ -2711,7 +2736,7 @@ EXPORT_SYMBOL_GPL(irq_get_irqchip_state);
  *	This call sets the internal irqchip state of an interrupt,
  *	depending on the value of @which.
  *
- *	This function should be called with preemption disabled if the
+ *	This function should be called with migration disabled if the
  *	interrupt controller has per-cpu registers.
  */
 int irq_set_irqchip_state(unsigned int irq, enum irqchip_irq_state which,
diff --git a/kernel/irq/spurious.c b/kernel/irq/spurious.c
index f865e5f4d382..dc7311dd74b1 100644
--- a/kernel/irq/spurious.c
+++ b/kernel/irq/spurious.c
@@ -443,6 +443,10 @@ MODULE_PARM_DESC(noirqdebug, "Disable irq lockup detection when true");
 
 static int __init irqfixup_setup(char *str)
 {
+#ifdef CONFIG_PREEMPT_RT
+	pr_warn("irqfixup boot option not supported w/ CONFIG_PREEMPT_RT\n");
+	return 1;
+#endif
 	irqfixup = 1;
 	printk(KERN_WARNING "Misrouted IRQ fixup support enabled.\n");
 	printk(KERN_WARNING "This may impact system performance.\n");
@@ -455,6 +459,10 @@ module_param(irqfixup, int, 0644);
 
 static int __init irqpoll_setup(char *str)
 {
+#ifdef CONFIG_PREEMPT_RT
+	pr_warn("irqpoll boot option not supported w/ CONFIG_PREEMPT_RT\n");
+	return 1;
+#endif
 	irqfixup = 2;
 	printk(KERN_WARNING "Misrouted IRQ fixup and polling support "
 				"enabled\n");
diff --git a/kernel/irq/wsysinit-prio-table.c b/kernel/irq/wsysinit-prio-table.c
new file mode 100644
index 000000000000..6b3bb9d6f506
--- /dev/null
+++ b/kernel/irq/wsysinit-prio-table.c
@@ -0,0 +1,210 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) 2014 Wago Kontakttechnik GmbH
+ *
+ * Author: Heinrich Toews <heinrich.toews@wago.com>
+ */
+
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/device.h>
+#include <linux/export.h>
+#include <linux/sched.h>
+#include <uapi/linux/sched/types.h>
+
+#include <linux/wsysinit-prio.h>
+
+extern struct device *wsysinit_sysfs_device;
+static struct wsysinit_irq_prio_tbl_entry *tbl_entry;
+
+/* Don't forget:
+ *    You must not change the order of can and kbus, the entries are referenced
+ *    by their index further down below in the module.
+ *    You have to set the IRQF_THREAD_TBL_LOOKUP irq flag during
+ *    a request_irq() registration to enable table lookup.
+ */
+struct wsysinit_irq_prio_tbl_entry wsysinit_irq_prio_tbl[] = {
+	{ "can", 84 },
+	{ "kbus", 86 },
+	{ "spi", 86 },
+	{ "rmd", 82 },
+	{ "UIO_DPC31_XINT", 70 },
+	{ "UIO_DPC31_SYNC", 70 },
+	{ "UIO_DPC31_DXOUT", 70 },
+	{ "RUN", 90 },
+	{ "STOP", 90 },
+	{ "RESET", 90 },
+	{ "RESET_ALL", 90 },
+	{ "uio_alarm_usv", 91 },
+	{ "44e07000", 86 },
+	{ "49054000", 86 },
+	{ NULL, 0 },
+};
+EXPORT_SYMBOL(wsysinit_irq_prio_tbl);
+
+struct wsysinit_irq_prio_tbl_entry *
+wsysinit_tbl_get_entry_by_name(const char *name)
+{
+	struct wsysinit_irq_prio_tbl_entry *entry = NULL;
+	struct wsysinit_irq_prio_tbl_entry *p;
+
+	for (p = &wsysinit_irq_prio_tbl[0]; p->name != NULL; p++) {
+		if (!strcmp(name, p->name)) {
+			entry = p;
+			break;
+		} else if (strstr(name, p->name)) {
+			entry = p;
+		}
+	}
+
+	if (entry)
+		pr_info("matched %s with %s using priority %d\n", entry->name,
+			name, entry->prio);
+	else
+		pr_info("no entry found for %s\n", name);
+
+	return entry;
+}
+EXPORT_SYMBOL(wsysinit_tbl_get_entry_by_name);
+
+int wsysinit_tbl_get_prio_by_name(const char *name)
+{
+	struct wsysinit_irq_prio_tbl_entry *entry;
+
+	entry = wsysinit_tbl_get_entry_by_name(name);
+	if (entry == NULL)
+		return -1;
+
+	return entry->prio;
+}
+EXPORT_SYMBOL(wsysinit_tbl_get_prio_by_name);
+
+void wsysinit_tbl_dump(void)
+{
+	int i;
+
+	pr_info(">>>> PFCxxx IRQ Thread Priority Table:\n");
+	for (i = 0; wsysinit_irq_prio_tbl[i].name != NULL; i++)
+		pr_info("\t%s\t%d\n", wsysinit_irq_prio_tbl[i].name,
+			wsysinit_irq_prio_tbl[i].prio);
+	pr_info("<<<<\n");
+}
+EXPORT_SYMBOL(wsysinit_tbl_dump);
+
+void wsysinit_set_fifo_nocheck(struct task_struct *task)
+{
+	int prio;
+	char comm[TASK_COMM_LEN];
+
+	get_task_comm(comm, task);
+
+	prio = wsysinit_tbl_get_prio_by_name(comm);
+	if (prio != -1) {
+		struct sched_param p = { .sched_priority = prio };
+
+		sched_setscheduler_nocheck(task, SCHED_FIFO, &p);
+	}
+}
+EXPORT_SYMBOL(wsysinit_set_fifo_nocheck);
+
+static ssize_t wsysinit_sysfs_irqprio_show(struct device *dev,
+					   struct device_attribute *attr,
+					   char *buf)
+{
+	if (tbl_entry == NULL)
+		return -1;
+
+	sprintf(buf, "%d\n", tbl_entry->prio);
+	return strlen(buf);
+}
+
+static ssize_t wsysinit_sysfs_irqprio_store(struct device *dev,
+					    struct device_attribute *attr,
+					    const char *buf, size_t count)
+{
+	u32 new_prio;
+
+	if (tbl_entry == NULL)
+		return -1;
+
+	if (kstrtou32(buf, 10, &new_prio))
+		return -EINVAL;
+
+	if (new_prio < 1 || new_prio > 99)
+		return -EINVAL;
+
+	tbl_entry->prio = (int)new_prio;
+
+	pr_info("%s: prio updated to %d\n", tbl_entry->name, tbl_entry->prio);
+
+	return count;
+}
+
+static ssize_t can_irqprio_show(struct device *dev,
+				struct device_attribute *attr, char *buf)
+{
+	tbl_entry = &wsysinit_irq_prio_tbl[0];
+	return wsysinit_sysfs_irqprio_show(dev, attr, buf);
+}
+
+static ssize_t can_irqprio_store(struct device *dev,
+				 struct device_attribute *attr, const char *buf,
+				 size_t count)
+{
+	tbl_entry = &wsysinit_irq_prio_tbl[0];
+	return wsysinit_sysfs_irqprio_store(dev, attr, buf, count);
+}
+
+static ssize_t kbus_irqprio_show(struct device *dev,
+				 struct device_attribute *attr, char *buf)
+{
+	tbl_entry = &wsysinit_irq_prio_tbl[1];
+	return wsysinit_sysfs_irqprio_show(dev, attr, buf);
+}
+
+static ssize_t kbus_irqprio_store(struct device *dev,
+				  struct device_attribute *attr,
+				  const char *buf, size_t count)
+{
+	tbl_entry = &wsysinit_irq_prio_tbl[1];
+	return wsysinit_sysfs_irqprio_store(dev, attr, buf, count);
+}
+
+static ssize_t dump_irqprio_store(struct device *dev,
+				  struct device_attribute *attr,
+				  const char *buf, size_t count)
+{
+	u32 val;
+
+	if (kstrtou32(buf, 10, &val))
+		return -EINVAL;
+
+	if (val == 1)
+		wsysinit_tbl_dump();
+	else
+		return -EINVAL;
+
+	return count;
+}
+
+DEVICE_ATTR_ADMIN_RW(can_irqprio);
+DEVICE_ATTR_ADMIN_RW(kbus_irqprio);
+DEVICE_ATTR_WO(dump_irqprio);
+
+void wsysinit_tbl_sysfs_init(void)
+{
+	pr_info("%s: create sysfs entries\n", __func__);
+
+	if (device_create_file(wsysinit_sysfs_device, &dev_attr_can_irqprio))
+		pr_err("%s: can0: failed to create sysfs entry!\n", __func__);
+
+	if (device_create_file(wsysinit_sysfs_device, &dev_attr_kbus_irqprio))
+		pr_err("%s: kbus: failed to create sysfs entry!\n", __func__);
+
+	if (device_create_file(wsysinit_sysfs_device, &dev_attr_dump_irqprio))
+		pr_err("%s: failed to create sysfs entry: 'dump_irqprio'!\n",
+		       __func__);
+}
+EXPORT_SYMBOL(wsysinit_tbl_sysfs_init);
diff --git a/kernel/irq_work.c b/kernel/irq_work.c
index eca83965b631..8183d30e1bb1 100644
--- a/kernel/irq_work.c
+++ b/kernel/irq_work.c
@@ -18,6 +18,7 @@
 #include <linux/cpu.h>
 #include <linux/notifier.h>
 #include <linux/smp.h>
+#include <linux/interrupt.h>
 #include <asm/processor.h>
 
 
@@ -52,13 +53,19 @@ void __weak arch_irq_work_raise(void)
 /* Enqueue on current CPU, work must already be claimed and preempt disabled */
 static void __irq_work_queue_local(struct irq_work *work)
 {
+	struct llist_head *list;
+	bool lazy_work, realtime = IS_ENABLED(CONFIG_PREEMPT_RT);
+
+	lazy_work = atomic_read(&work->flags) & IRQ_WORK_LAZY;
+
 	/* If the work is "lazy", handle it from next tick if any */
-	if (atomic_read(&work->flags) & IRQ_WORK_LAZY) {
-		if (llist_add(&work->llnode, this_cpu_ptr(&lazy_list)) &&
-		    tick_nohz_tick_stopped())
-			arch_irq_work_raise();
-	} else {
-		if (llist_add(&work->llnode, this_cpu_ptr(&raised_list)))
+	if (lazy_work || (realtime && !(atomic_read(&work->flags) & IRQ_WORK_HARD_IRQ)))
+		list = this_cpu_ptr(&lazy_list);
+	else
+		list = this_cpu_ptr(&raised_list);
+
+	if (llist_add(&work->llnode, list)) {
+		if (!lazy_work || tick_nohz_tick_stopped())
 			arch_irq_work_raise();
 	}
 }
@@ -102,7 +109,13 @@ bool irq_work_queue_on(struct irq_work *work, int cpu)
 	if (cpu != smp_processor_id()) {
 		/* Arch remote IPI send/receive backend aren't NMI safe */
 		WARN_ON_ONCE(in_nmi());
-		__smp_call_single_queue(cpu, &work->llnode);
+
+		if (IS_ENABLED(CONFIG_PREEMPT_RT) && !(atomic_read(&work->flags) & IRQ_WORK_HARD_IRQ)) {
+			if (llist_add(&work->llnode, &per_cpu(lazy_list, cpu)))
+				arch_send_call_function_single_ipi(cpu);
+		} else {
+			__smp_call_single_queue(cpu, &work->llnode);
+		}
 	} else {
 		__irq_work_queue_local(work);
 	}
@@ -120,9 +133,8 @@ bool irq_work_needs_cpu(void)
 	raised = this_cpu_ptr(&raised_list);
 	lazy = this_cpu_ptr(&lazy_list);
 
-	if (llist_empty(raised) || arch_irq_work_has_interrupt())
-		if (llist_empty(lazy))
-			return false;
+	if (llist_empty(raised) && llist_empty(lazy))
+		return false;
 
 	/* All work should have been flushed before going offline */
 	WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
@@ -160,8 +172,12 @@ static void irq_work_run_list(struct llist_head *list)
 	struct irq_work *work, *tmp;
 	struct llist_node *llnode;
 
+#ifndef CONFIG_PREEMPT_RT
+	/*
+	 * nort: On RT IRQ-work may run in SOFTIRQ context.
+	 */
 	BUG_ON(!irqs_disabled());
-
+#endif
 	if (llist_empty(list))
 		return;
 
@@ -177,7 +193,16 @@ static void irq_work_run_list(struct llist_head *list)
 void irq_work_run(void)
 {
 	irq_work_run_list(this_cpu_ptr(&raised_list));
-	irq_work_run_list(this_cpu_ptr(&lazy_list));
+	if (IS_ENABLED(CONFIG_PREEMPT_RT)) {
+		/*
+		 * NOTE: we raise softirq via IPI for safety,
+		 * and execute in irq_work_tick() to move the
+		 * overhead from hard to soft irq context.
+		 */
+		if (!llist_empty(this_cpu_ptr(&lazy_list)))
+			raise_softirq(TIMER_SOFTIRQ);
+	} else
+		irq_work_run_list(this_cpu_ptr(&lazy_list));
 }
 EXPORT_SYMBOL_GPL(irq_work_run);
 
@@ -187,8 +212,17 @@ void irq_work_tick(void)
 
 	if (!llist_empty(raised) && !arch_irq_work_has_interrupt())
 		irq_work_run_list(raised);
+
+	if (!IS_ENABLED(CONFIG_PREEMPT_RT))
+		irq_work_run_list(this_cpu_ptr(&lazy_list));
+}
+
+#if defined(CONFIG_IRQ_WORK) && defined(CONFIG_PREEMPT_RT)
+void irq_work_tick_soft(void)
+{
 	irq_work_run_list(this_cpu_ptr(&lazy_list));
 }
+#endif
 
 /*
  * Synchronize against the irq_work @entry, ensures the entry is not
diff --git a/kernel/kexec_core.c b/kernel/kexec_core.c
index c589c7a9562c..fb0ca1a805b5 100644
--- a/kernel/kexec_core.c
+++ b/kernel/kexec_core.c
@@ -978,7 +978,6 @@ void crash_kexec(struct pt_regs *regs)
 	old_cpu = atomic_cmpxchg(&panic_cpu, PANIC_CPU_INVALID, this_cpu);
 	if (old_cpu == PANIC_CPU_INVALID) {
 		/* This is the 1st CPU which comes here, so go ahead. */
-		printk_safe_flush_on_panic();
 		__crash_kexec(regs);
 
 		/*
diff --git a/kernel/ksysfs.c b/kernel/ksysfs.c
index 35859da8bd4f..dfff31ed644a 100644
--- a/kernel/ksysfs.c
+++ b/kernel/ksysfs.c
@@ -138,6 +138,15 @@ KERNEL_ATTR_RO(vmcoreinfo);
 
 #endif /* CONFIG_CRASH_CORE */
 
+#if defined(CONFIG_PREEMPT_RT)
+static ssize_t realtime_show(struct kobject *kobj,
+			     struct kobj_attribute *attr, char *buf)
+{
+	return sprintf(buf, "%d\n", 1);
+}
+KERNEL_ATTR_RO(realtime);
+#endif
+
 /* whether file capabilities are enabled */
 static ssize_t fscaps_show(struct kobject *kobj,
 				  struct kobj_attribute *attr, char *buf)
@@ -228,6 +237,9 @@ static struct attribute * kernel_attrs[] = {
 #ifndef CONFIG_TINY_RCU
 	&rcu_expedited_attr.attr,
 	&rcu_normal_attr.attr,
+#endif
+#ifdef CONFIG_PREEMPT_RT
+	&realtime_attr.attr,
 #endif
 	NULL
 };
diff --git a/kernel/kthread.c b/kernel/kthread.c
index 5edf7e19ab26..cdfaf64263b3 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -243,6 +243,7 @@ EXPORT_SYMBOL_GPL(kthread_parkme);
 
 static int kthread(void *_create)
 {
+	static const struct sched_param param = { .sched_priority = 0 };
 	/* Copy data: it's on kthread's stack */
 	struct kthread_create_info *create = _create;
 	int (*threadfn)(void *data) = create->threadfn;
@@ -273,6 +274,13 @@ static int kthread(void *_create)
 	init_completion(&self->parked);
 	current->vfork_done = &self->exited;
 
+	/*
+	 * The new thread inherited kthreadd's priority and CPU mask. Reset
+	 * back to default in case they have been changed.
+	 */
+	sched_setscheduler_nocheck(current, SCHED_NORMAL, &param);
+	set_cpus_allowed_ptr(current, housekeeping_cpumask(HK_FLAG_KTHREAD));
+
 	/* OK, tell user we're spawned, wait for stop or wakeup */
 	__set_current_state(TASK_UNINTERRUPTIBLE);
 	create->result = current;
@@ -370,7 +378,6 @@ struct task_struct *__kthread_create_on_node(int (*threadfn)(void *data),
 	}
 	task = create->result;
 	if (!IS_ERR(task)) {
-		static const struct sched_param param = { .sched_priority = 0 };
 		char name[TASK_COMM_LEN];
 
 		/*
@@ -379,13 +386,6 @@ struct task_struct *__kthread_create_on_node(int (*threadfn)(void *data),
 		 */
 		vsnprintf(name, sizeof(name), namefmt, args);
 		set_task_comm(task, name);
-		/*
-		 * root may have changed our (kthreadd's) priority or CPU mask.
-		 * The kernel thread should not inherit these properties.
-		 */
-		sched_setscheduler_nocheck(task, SCHED_NORMAL, &param);
-		set_cpus_allowed_ptr(task,
-				     housekeeping_cpumask(HK_FLAG_KTHREAD));
 	}
 	kfree(create);
 	return task;
diff --git a/kernel/locking/Makefile b/kernel/locking/Makefile
index 6d11cfb9b41f..c7fbf737e16e 100644
--- a/kernel/locking/Makefile
+++ b/kernel/locking/Makefile
@@ -3,7 +3,7 @@
 # and is generally not a function of system call inputs.
 KCOV_INSTRUMENT		:= n
 
-obj-y += mutex.o semaphore.o rwsem.o percpu-rwsem.o
+obj-y += semaphore.o rwsem.o percpu-rwsem.o
 
 # Avoid recursion lockdep -> KCSAN -> ... -> lockdep.
 KCSAN_SANITIZE_lockdep.o := n
@@ -15,19 +15,23 @@ CFLAGS_REMOVE_mutex-debug.o = $(CC_FLAGS_FTRACE)
 CFLAGS_REMOVE_rtmutex-debug.o = $(CC_FLAGS_FTRACE)
 endif
 
-obj-$(CONFIG_DEBUG_MUTEXES) += mutex-debug.o
 obj-$(CONFIG_LOCKDEP) += lockdep.o
 ifeq ($(CONFIG_PROC_FS),y)
 obj-$(CONFIG_LOCKDEP) += lockdep_proc.o
 endif
 obj-$(CONFIG_SMP) += spinlock.o
-obj-$(CONFIG_LOCK_SPIN_ON_OWNER) += osq_lock.o
 obj-$(CONFIG_PROVE_LOCKING) += spinlock.o
 obj-$(CONFIG_QUEUED_SPINLOCKS) += qspinlock.o
 obj-$(CONFIG_RT_MUTEXES) += rtmutex.o
 obj-$(CONFIG_DEBUG_RT_MUTEXES) += rtmutex-debug.o
 obj-$(CONFIG_DEBUG_SPINLOCK) += spinlock.o
 obj-$(CONFIG_DEBUG_SPINLOCK) += spinlock_debug.o
+ifneq ($(CONFIG_PREEMPT_RT),y)
+obj-y += mutex.o
+obj-$(CONFIG_LOCK_SPIN_ON_OWNER) += osq_lock.o
+obj-$(CONFIG_DEBUG_MUTEXES) += mutex-debug.o
+endif
+obj-$(CONFIG_PREEMPT_RT) += mutex-rt.o rwsem-rt.o rwlock-rt.o
 obj-$(CONFIG_QUEUED_RWLOCKS) += qrwlock.o
 obj-$(CONFIG_LOCK_TORTURE_TEST) += locktorture.o
 obj-$(CONFIG_WW_MUTEX_SELFTEST) += test-ww_mutex.o
diff --git a/kernel/locking/lockdep.c b/kernel/locking/lockdep.c
index 780012eb2f3f..886c5165fb69 100644
--- a/kernel/locking/lockdep.c
+++ b/kernel/locking/lockdep.c
@@ -5291,6 +5291,7 @@ static noinstr void check_flags(unsigned long flags)
 		}
 	}
 
+#ifndef CONFIG_PREEMPT_RT
 	/*
 	 * We dont accurately track softirq state in e.g.
 	 * hardirq contexts (such as on 4KSTACKS), so only
@@ -5305,6 +5306,7 @@ static noinstr void check_flags(unsigned long flags)
 			DEBUG_LOCKS_WARN_ON(!current->softirqs_enabled);
 		}
 	}
+#endif
 
 	if (!debug_locks)
 		print_irqtrace_events(current);
diff --git a/kernel/locking/mutex-rt.c b/kernel/locking/mutex-rt.c
new file mode 100644
index 000000000000..2b849e6b9b4a
--- /dev/null
+++ b/kernel/locking/mutex-rt.c
@@ -0,0 +1,224 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Real-Time Preemption Support
+ *
+ * started by Ingo Molnar:
+ *
+ *  Copyright (C) 2004-2006 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>
+ *  Copyright (C) 2006, Timesys Corp., Thomas Gleixner <tglx@timesys.com>
+ *
+ * historic credit for proving that Linux spinlocks can be implemented via
+ * RT-aware mutexes goes to many people: The Pmutex project (Dirk Grambow
+ * and others) who prototyped it on 2.4 and did lots of comparative
+ * research and analysis; TimeSys, for proving that you can implement a
+ * fully preemptible kernel via the use of IRQ threading and mutexes;
+ * Bill Huey for persuasively arguing on lkml that the mutex model is the
+ * right one; and to MontaVista, who ported pmutexes to 2.6.
+ *
+ * This code is a from-scratch implementation and is not based on pmutexes,
+ * but the idea of converting spinlocks to mutexes is used here too.
+ *
+ * lock debugging, locking tree, deadlock detection:
+ *
+ *  Copyright (C) 2004, LynuxWorks, Inc., Igor Manyilov, Bill Huey
+ *  Released under the General Public License (GPL).
+ *
+ * Includes portions of the generic R/W semaphore implementation from:
+ *
+ *  Copyright (c) 2001   David Howells (dhowells@redhat.com).
+ *  - Derived partially from idea by Andrea Arcangeli <andrea@suse.de>
+ *  - Derived also from comments by Linus
+ *
+ * Pending ownership of locks and ownership stealing:
+ *
+ *  Copyright (C) 2005, Kihon Technologies Inc., Steven Rostedt
+ *
+ *   (also by Steven Rostedt)
+ *    - Converted single pi_lock to individual task locks.
+ *
+ * By Esben Nielsen:
+ *    Doing priority inheritance with help of the scheduler.
+ *
+ *  Copyright (C) 2006, Timesys Corp., Thomas Gleixner <tglx@timesys.com>
+ *  - major rework based on Esben Nielsens initial patch
+ *  - replaced thread_info references by task_struct refs
+ *  - removed task->pending_owner dependency
+ *  - BKL drop/reacquire for semaphore style locks to avoid deadlocks
+ *    in the scheduler return path as discussed with Steven Rostedt
+ *
+ *  Copyright (C) 2006, Kihon Technologies Inc.
+ *    Steven Rostedt <rostedt@goodmis.org>
+ *  - debugged and patched Thomas Gleixner's rework.
+ *  - added back the cmpxchg to the rework.
+ *  - turned atomic require back on for SMP.
+ */
+
+#include <linux/spinlock.h>
+#include <linux/rtmutex.h>
+#include <linux/sched.h>
+#include <linux/delay.h>
+#include <linux/module.h>
+#include <linux/kallsyms.h>
+#include <linux/syscalls.h>
+#include <linux/interrupt.h>
+#include <linux/plist.h>
+#include <linux/fs.h>
+#include <linux/futex.h>
+#include <linux/hrtimer.h>
+#include <linux/blkdev.h>
+
+#include "rtmutex_common.h"
+
+/*
+ * struct mutex functions
+ */
+void __mutex_do_init(struct mutex *mutex, const char *name,
+		     struct lock_class_key *key)
+{
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	/*
+	 * Make sure we are not reinitializing a held lock:
+	 */
+	debug_check_no_locks_freed((void *)mutex, sizeof(*mutex));
+	lockdep_init_map(&mutex->dep_map, name, key, 0);
+#endif
+	mutex->lock.save_state = 0;
+}
+EXPORT_SYMBOL(__mutex_do_init);
+
+static int _mutex_lock_blk_flush(struct mutex *lock, int state)
+{
+	/*
+	 * Flush blk before ->pi_blocked_on is set. At schedule() time it is too
+	 * late if one of the callbacks needs to acquire a sleeping lock.
+	 */
+	if (blk_needs_flush_plug(current))
+		blk_schedule_flush_plug(current);
+	return __rt_mutex_lock_state(&lock->lock, state);
+}
+
+void __lockfunc _mutex_lock(struct mutex *lock)
+{
+	mutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);
+	_mutex_lock_blk_flush(lock, TASK_UNINTERRUPTIBLE);
+}
+EXPORT_SYMBOL(_mutex_lock);
+
+void __lockfunc _mutex_lock_io_nested(struct mutex *lock, int subclass)
+{
+	int token;
+
+	token = io_schedule_prepare();
+
+	mutex_acquire_nest(&lock->dep_map, subclass, 0, NULL, _RET_IP_);
+	__rt_mutex_lock_state(&lock->lock, TASK_UNINTERRUPTIBLE);
+
+	io_schedule_finish(token);
+}
+EXPORT_SYMBOL_GPL(_mutex_lock_io_nested);
+
+int __lockfunc _mutex_lock_interruptible(struct mutex *lock)
+{
+	int ret;
+
+	mutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);
+	ret = _mutex_lock_blk_flush(lock, TASK_INTERRUPTIBLE);
+	if (ret)
+		mutex_release(&lock->dep_map, _RET_IP_);
+	return ret;
+}
+EXPORT_SYMBOL(_mutex_lock_interruptible);
+
+int __lockfunc _mutex_lock_killable(struct mutex *lock)
+{
+	int ret;
+
+	mutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);
+	ret = _mutex_lock_blk_flush(lock, TASK_KILLABLE);
+	if (ret)
+		mutex_release(&lock->dep_map, _RET_IP_);
+	return ret;
+}
+EXPORT_SYMBOL(_mutex_lock_killable);
+
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+void __lockfunc _mutex_lock_nested(struct mutex *lock, int subclass)
+{
+	mutex_acquire_nest(&lock->dep_map, subclass, 0, NULL, _RET_IP_);
+	_mutex_lock_blk_flush(lock, TASK_UNINTERRUPTIBLE);
+}
+EXPORT_SYMBOL(_mutex_lock_nested);
+
+void __lockfunc _mutex_lock_nest_lock(struct mutex *lock, struct lockdep_map *nest)
+{
+	mutex_acquire_nest(&lock->dep_map, 0, 0, nest, _RET_IP_);
+	_mutex_lock_blk_flush(lock, TASK_UNINTERRUPTIBLE);
+}
+EXPORT_SYMBOL(_mutex_lock_nest_lock);
+
+int __lockfunc _mutex_lock_interruptible_nested(struct mutex *lock, int subclass)
+{
+	int ret;
+
+	mutex_acquire_nest(&lock->dep_map, subclass, 0, NULL, _RET_IP_);
+	ret = _mutex_lock_blk_flush(lock, TASK_INTERRUPTIBLE);
+	if (ret)
+		mutex_release(&lock->dep_map, _RET_IP_);
+	return ret;
+}
+EXPORT_SYMBOL(_mutex_lock_interruptible_nested);
+
+int __lockfunc _mutex_lock_killable_nested(struct mutex *lock, int subclass)
+{
+	int ret;
+
+	mutex_acquire(&lock->dep_map, subclass, 0, _RET_IP_);
+	ret = _mutex_lock_blk_flush(lock, TASK_KILLABLE);
+	if (ret)
+		mutex_release(&lock->dep_map, _RET_IP_);
+	return ret;
+}
+EXPORT_SYMBOL(_mutex_lock_killable_nested);
+#endif
+
+int __lockfunc _mutex_trylock(struct mutex *lock)
+{
+	int ret = __rt_mutex_trylock(&lock->lock);
+
+	if (ret)
+		mutex_acquire(&lock->dep_map, 0, 1, _RET_IP_);
+
+	return ret;
+}
+EXPORT_SYMBOL(_mutex_trylock);
+
+void __lockfunc _mutex_unlock(struct mutex *lock)
+{
+	mutex_release(&lock->dep_map, _RET_IP_);
+	__rt_mutex_unlock(&lock->lock);
+}
+EXPORT_SYMBOL(_mutex_unlock);
+
+/**
+ * atomic_dec_and_mutex_lock - return holding mutex if we dec to 0
+ * @cnt: the atomic which we are to dec
+ * @lock: the mutex to return holding if we dec to 0
+ *
+ * return true and hold lock if we dec to 0, return false otherwise
+ */
+int atomic_dec_and_mutex_lock(atomic_t *cnt, struct mutex *lock)
+{
+	/* dec if we can't possibly hit 0 */
+	if (atomic_add_unless(cnt, -1, 1))
+		return 0;
+	/* we might hit 0, so take the lock */
+	mutex_lock(lock);
+	if (!atomic_dec_and_test(cnt)) {
+		/* when we actually did the dec, we didn't hit 0 */
+		mutex_unlock(lock);
+		return 0;
+	}
+	/* we hit 0, and we hold the lock */
+	return 1;
+}
+EXPORT_SYMBOL(atomic_dec_and_mutex_lock);
diff --git a/kernel/locking/rtmutex-debug.c b/kernel/locking/rtmutex-debug.c
index 36e69100e8e0..fb150100335f 100644
--- a/kernel/locking/rtmutex-debug.c
+++ b/kernel/locking/rtmutex-debug.c
@@ -32,110 +32,12 @@
 
 #include "rtmutex_common.h"
 
-static void printk_task(struct task_struct *p)
-{
-	if (p)
-		printk("%16s:%5d [%p, %3d]", p->comm, task_pid_nr(p), p, p->prio);
-	else
-		printk("<none>");
-}
-
-static void printk_lock(struct rt_mutex *lock, int print_owner)
-{
-	if (lock->name)
-		printk(" [%p] {%s}\n",
-			lock, lock->name);
-	else
-		printk(" [%p] {%s:%d}\n",
-			lock, lock->file, lock->line);
-
-	if (print_owner && rt_mutex_owner(lock)) {
-		printk(".. ->owner: %p\n", lock->owner);
-		printk(".. held by:  ");
-		printk_task(rt_mutex_owner(lock));
-		printk("\n");
-	}
-}
-
 void rt_mutex_debug_task_free(struct task_struct *task)
 {
 	DEBUG_LOCKS_WARN_ON(!RB_EMPTY_ROOT(&task->pi_waiters.rb_root));
 	DEBUG_LOCKS_WARN_ON(task->pi_blocked_on);
 }
 
-/*
- * We fill out the fields in the waiter to store the information about
- * the deadlock. We print when we return. act_waiter can be NULL in
- * case of a remove waiter operation.
- */
-void debug_rt_mutex_deadlock(enum rtmutex_chainwalk chwalk,
-			     struct rt_mutex_waiter *act_waiter,
-			     struct rt_mutex *lock)
-{
-	struct task_struct *task;
-
-	if (!debug_locks || chwalk == RT_MUTEX_FULL_CHAINWALK || !act_waiter)
-		return;
-
-	task = rt_mutex_owner(act_waiter->lock);
-	if (task && task != current) {
-		act_waiter->deadlock_task_pid = get_pid(task_pid(task));
-		act_waiter->deadlock_lock = lock;
-	}
-}
-
-void debug_rt_mutex_print_deadlock(struct rt_mutex_waiter *waiter)
-{
-	struct task_struct *task;
-
-	if (!waiter->deadlock_lock || !debug_locks)
-		return;
-
-	rcu_read_lock();
-	task = pid_task(waiter->deadlock_task_pid, PIDTYPE_PID);
-	if (!task) {
-		rcu_read_unlock();
-		return;
-	}
-
-	if (!debug_locks_off()) {
-		rcu_read_unlock();
-		return;
-	}
-
-	pr_warn("\n");
-	pr_warn("============================================\n");
-	pr_warn("WARNING: circular locking deadlock detected!\n");
-	pr_warn("%s\n", print_tainted());
-	pr_warn("--------------------------------------------\n");
-	printk("%s/%d is deadlocking current task %s/%d\n\n",
-	       task->comm, task_pid_nr(task),
-	       current->comm, task_pid_nr(current));
-
-	printk("\n1) %s/%d is trying to acquire this lock:\n",
-	       current->comm, task_pid_nr(current));
-	printk_lock(waiter->lock, 1);
-
-	printk("\n2) %s/%d is blocked on this lock:\n",
-		task->comm, task_pid_nr(task));
-	printk_lock(waiter->deadlock_lock, 1);
-
-	debug_show_held_locks(current);
-	debug_show_held_locks(task);
-
-	printk("\n%s/%d's [blocked] stackdump:\n\n",
-		task->comm, task_pid_nr(task));
-	show_stack(task, NULL, KERN_DEFAULT);
-	printk("\n%s/%d's [current] stackdump:\n\n",
-		current->comm, task_pid_nr(current));
-	dump_stack();
-	debug_show_all_locks();
-	rcu_read_unlock();
-
-	printk("[ turning off deadlock detection."
-	       "Please report this trace. ]\n\n");
-}
-
 void debug_rt_mutex_lock(struct rt_mutex *lock)
 {
 }
@@ -158,12 +60,10 @@ void debug_rt_mutex_proxy_unlock(struct rt_mutex *lock)
 void debug_rt_mutex_init_waiter(struct rt_mutex_waiter *waiter)
 {
 	memset(waiter, 0x11, sizeof(*waiter));
-	waiter->deadlock_task_pid = NULL;
 }
 
 void debug_rt_mutex_free_waiter(struct rt_mutex_waiter *waiter)
 {
-	put_pid(waiter->deadlock_task_pid);
 	memset(waiter, 0x22, sizeof(*waiter));
 }
 
@@ -173,10 +73,8 @@ void debug_rt_mutex_init(struct rt_mutex *lock, const char *name, struct lock_cl
 	 * Make sure we are not reinitializing a held lock:
 	 */
 	debug_check_no_locks_freed((void *)lock, sizeof(*lock));
-	lock->name = name;
 
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 	lockdep_init_map(&lock->dep_map, name, key, 0);
 #endif
 }
-
diff --git a/kernel/locking/rtmutex-debug.h b/kernel/locking/rtmutex-debug.h
index fc549713bba3..659e93e256c6 100644
--- a/kernel/locking/rtmutex-debug.h
+++ b/kernel/locking/rtmutex-debug.h
@@ -18,20 +18,9 @@ extern void debug_rt_mutex_unlock(struct rt_mutex *lock);
 extern void debug_rt_mutex_proxy_lock(struct rt_mutex *lock,
 				      struct task_struct *powner);
 extern void debug_rt_mutex_proxy_unlock(struct rt_mutex *lock);
-extern void debug_rt_mutex_deadlock(enum rtmutex_chainwalk chwalk,
-				    struct rt_mutex_waiter *waiter,
-				    struct rt_mutex *lock);
-extern void debug_rt_mutex_print_deadlock(struct rt_mutex_waiter *waiter);
-# define debug_rt_mutex_reset_waiter(w)			\
-	do { (w)->deadlock_lock = NULL; } while (0)
 
 static inline bool debug_rt_mutex_detect_deadlock(struct rt_mutex_waiter *waiter,
 						  enum rtmutex_chainwalk walk)
 {
 	return (waiter != NULL);
 }
-
-static inline void rt_mutex_print_deadlock(struct rt_mutex_waiter *w)
-{
-	debug_rt_mutex_print_deadlock(w);
-}
diff --git a/kernel/locking/rtmutex.c b/kernel/locking/rtmutex.c
index 2f8cd616d3b2..4ea87d6c9ab7 100644
--- a/kernel/locking/rtmutex.c
+++ b/kernel/locking/rtmutex.c
@@ -8,6 +8,11 @@
  *  Copyright (C) 2005-2006 Timesys Corp., Thomas Gleixner <tglx@timesys.com>
  *  Copyright (C) 2005 Kihon Technologies Inc., Steven Rostedt
  *  Copyright (C) 2006 Esben Nielsen
+ * Adaptive Spinlocks:
+ *  Copyright (C) 2008 Novell, Inc., Gregory Haskins, Sven Dietrich,
+ *				     and Peter Morreale,
+ * Adaptive Spinlocks simplification:
+ *  Copyright (C) 2008 Red Hat, Inc., Steven Rostedt <srostedt@redhat.com>
  *
  *  See Documentation/locking/rt-mutex-design.rst for details.
  */
@@ -19,6 +24,7 @@
 #include <linux/sched/wake_q.h>
 #include <linux/sched/debug.h>
 #include <linux/timer.h>
+#include <linux/ww_mutex.h>
 
 #include "rtmutex_common.h"
 
@@ -136,6 +142,12 @@ static void fixup_rt_mutex_waiters(struct rt_mutex *lock)
 		WRITE_ONCE(*p, owner & ~RT_MUTEX_HAS_WAITERS);
 }
 
+static int rt_mutex_real_waiter(struct rt_mutex_waiter *waiter)
+{
+	return waiter && waiter != PI_WAKEUP_INPROGRESS &&
+		waiter != PI_REQUEUE_INPROGRESS;
+}
+
 /*
  * We can speed up the acquire/release, if there's no debugging state to be
  * set up.
@@ -227,7 +239,7 @@ static inline bool unlock_rt_mutex_safe(struct rt_mutex *lock,
  * Only use with rt_mutex_waiter_{less,equal}()
  */
 #define task_to_waiter(p)	\
-	&(struct rt_mutex_waiter){ .prio = (p)->prio, .deadline = (p)->dl.deadline }
+	&(struct rt_mutex_waiter){ .prio = (p)->prio, .deadline = (p)->dl.deadline, .task = (p) }
 
 static inline int
 rt_mutex_waiter_less(struct rt_mutex_waiter *left,
@@ -267,6 +279,27 @@ rt_mutex_waiter_equal(struct rt_mutex_waiter *left,
 	return 1;
 }
 
+#define STEAL_NORMAL  0
+#define STEAL_LATERAL 1
+
+static inline int
+rt_mutex_steal(struct rt_mutex *lock, struct rt_mutex_waiter *waiter, int mode)
+{
+	struct rt_mutex_waiter *top_waiter = rt_mutex_top_waiter(lock);
+
+	if (waiter == top_waiter || rt_mutex_waiter_less(waiter, top_waiter))
+		return 1;
+
+	/*
+	 * Note that RT tasks are excluded from lateral-steals
+	 * to prevent the introduction of an unbounded latency.
+	 */
+	if (mode == STEAL_NORMAL || rt_task(waiter->task))
+		return 0;
+
+	return rt_mutex_waiter_equal(waiter, top_waiter);
+}
+
 static void
 rt_mutex_enqueue(struct rt_mutex *lock, struct rt_mutex_waiter *waiter)
 {
@@ -371,6 +404,14 @@ static bool rt_mutex_cond_detect_deadlock(struct rt_mutex_waiter *waiter,
 	return debug_rt_mutex_detect_deadlock(waiter, chwalk);
 }
 
+static void rt_mutex_wake_waiter(struct rt_mutex_waiter *waiter)
+{
+	if (waiter->savestate)
+		wake_up_lock_sleeper(waiter->task);
+	else
+		wake_up_process(waiter->task);
+}
+
 /*
  * Max number of times we'll walk the boosting chain:
  */
@@ -378,7 +419,8 @@ int max_lock_depth = 1024;
 
 static inline struct rt_mutex *task_blocked_on_lock(struct task_struct *p)
 {
-	return p->pi_blocked_on ? p->pi_blocked_on->lock : NULL;
+	return rt_mutex_real_waiter(p->pi_blocked_on) ?
+		p->pi_blocked_on->lock : NULL;
 }
 
 /*
@@ -514,7 +556,7 @@ static int rt_mutex_adjust_prio_chain(struct task_struct *task,
 	 * reached or the state of the chain has changed while we
 	 * dropped the locks.
 	 */
-	if (!waiter)
+	if (!rt_mutex_real_waiter(waiter))
 		goto out_unlock_pi;
 
 	/*
@@ -597,7 +639,6 @@ static int rt_mutex_adjust_prio_chain(struct task_struct *task,
 	 * walk, we detected a deadlock.
 	 */
 	if (lock == orig_lock || rt_mutex_owner(lock) == top_task) {
-		debug_rt_mutex_deadlock(chwalk, orig_waiter, lock);
 		raw_spin_unlock(&lock->wait_lock);
 		ret = -EDEADLK;
 		goto out_unlock_pi;
@@ -694,13 +735,16 @@ static int rt_mutex_adjust_prio_chain(struct task_struct *task,
 	 * follow here. This is the end of the chain we are walking.
 	 */
 	if (!rt_mutex_owner(lock)) {
+		struct rt_mutex_waiter *lock_top_waiter;
+
 		/*
 		 * If the requeue [7] above changed the top waiter,
 		 * then we need to wake the new top waiter up to try
 		 * to get the lock.
 		 */
-		if (prerequeue_top_waiter != rt_mutex_top_waiter(lock))
-			wake_up_process(rt_mutex_top_waiter(lock)->task);
+		lock_top_waiter = rt_mutex_top_waiter(lock);
+		if (prerequeue_top_waiter != lock_top_waiter)
+			rt_mutex_wake_waiter(lock_top_waiter);
 		raw_spin_unlock_irq(&lock->wait_lock);
 		return 0;
 	}
@@ -801,9 +845,11 @@ static int rt_mutex_adjust_prio_chain(struct task_struct *task,
  * @task:   The task which wants to acquire the lock
  * @waiter: The waiter that is queued to the lock's wait tree if the
  *	    callsite called task_blocked_on_lock(), otherwise NULL
+ * @mode:   Lock steal mode (STEAL_NORMAL, STEAL_LATERAL)
  */
-static int try_to_take_rt_mutex(struct rt_mutex *lock, struct task_struct *task,
-				struct rt_mutex_waiter *waiter)
+static int __try_to_take_rt_mutex(struct rt_mutex *lock,
+				  struct task_struct *task,
+				  struct rt_mutex_waiter *waiter, int mode)
 {
 	lockdep_assert_held(&lock->wait_lock);
 
@@ -839,12 +885,11 @@ static int try_to_take_rt_mutex(struct rt_mutex *lock, struct task_struct *task,
 	 */
 	if (waiter) {
 		/*
-		 * If waiter is not the highest priority waiter of
-		 * @lock, give up.
+		 * If waiter is not the highest priority waiter of @lock,
+		 * or its peer when lateral steal is allowed, give up.
 		 */
-		if (waiter != rt_mutex_top_waiter(lock))
+		if (!rt_mutex_steal(lock, waiter, mode))
 			return 0;
-
 		/*
 		 * We can acquire the lock. Remove the waiter from the
 		 * lock waiters tree.
@@ -862,14 +907,12 @@ static int try_to_take_rt_mutex(struct rt_mutex *lock, struct task_struct *task,
 		 */
 		if (rt_mutex_has_waiters(lock)) {
 			/*
-			 * If @task->prio is greater than or equal to
-			 * the top waiter priority (kernel view),
-			 * @task lost.
+			 * If @task->prio is greater than the top waiter
+			 * priority (kernel view), or equal to it when a
+			 * lateral steal is forbidden, @task lost.
 			 */
-			if (!rt_mutex_waiter_less(task_to_waiter(task),
-						  rt_mutex_top_waiter(lock)))
+			if (!rt_mutex_steal(lock, task_to_waiter(task), mode))
 				return 0;
-
 			/*
 			 * The current top waiter stays enqueued. We
 			 * don't have to change anything in the lock
@@ -916,6 +959,329 @@ static int try_to_take_rt_mutex(struct rt_mutex *lock, struct task_struct *task,
 	return 1;
 }
 
+#ifdef CONFIG_PREEMPT_RT
+/*
+ * preemptible spin_lock functions:
+ */
+static inline void rt_spin_lock_fastlock(struct rt_mutex *lock,
+					 void  (*slowfn)(struct rt_mutex *lock))
+{
+	might_sleep_no_state_check();
+
+	if (likely(rt_mutex_cmpxchg_acquire(lock, NULL, current)))
+		return;
+	else
+		slowfn(lock);
+}
+
+static inline void rt_spin_lock_fastunlock(struct rt_mutex *lock,
+					   void  (*slowfn)(struct rt_mutex *lock))
+{
+	if (likely(rt_mutex_cmpxchg_release(lock, current, NULL)))
+		return;
+	else
+		slowfn(lock);
+}
+#ifdef CONFIG_SMP
+/*
+ * Note that owner is a speculative pointer and dereferencing relies
+ * on rcu_read_lock() and the check against the lock owner.
+ */
+static int adaptive_wait(struct rt_mutex *lock,
+			 struct task_struct *owner)
+{
+	int res = 0;
+
+	rcu_read_lock();
+	for (;;) {
+		if (owner != rt_mutex_owner(lock))
+			break;
+		/*
+		 * Ensure that owner->on_cpu is dereferenced _after_
+		 * checking the above to be valid.
+		 */
+		barrier();
+		if (!owner->on_cpu) {
+			res = 1;
+			break;
+		}
+		cpu_relax();
+	}
+	rcu_read_unlock();
+	return res;
+}
+#else
+static int adaptive_wait(struct rt_mutex *lock,
+			 struct task_struct *orig_owner)
+{
+	return 1;
+}
+#endif
+
+static int task_blocks_on_rt_mutex(struct rt_mutex *lock,
+				   struct rt_mutex_waiter *waiter,
+				   struct task_struct *task,
+				   enum rtmutex_chainwalk chwalk);
+/*
+ * Slow path lock function spin_lock style: this variant is very
+ * careful not to miss any non-lock wakeups.
+ *
+ * We store the current state under p->pi_lock in p->saved_state and
+ * the try_to_wake_up() code handles this accordingly.
+ */
+void __sched rt_spin_lock_slowlock_locked(struct rt_mutex *lock,
+					  struct rt_mutex_waiter *waiter,
+					  unsigned long flags)
+{
+	struct task_struct *lock_owner, *self = current;
+	struct rt_mutex_waiter *top_waiter;
+	int ret;
+
+	if (__try_to_take_rt_mutex(lock, self, NULL, STEAL_LATERAL))
+		return;
+
+	BUG_ON(rt_mutex_owner(lock) == self);
+
+	/*
+	 * We save whatever state the task is in and we'll restore it
+	 * after acquiring the lock taking real wakeups into account
+	 * as well. We are serialized via pi_lock against wakeups. See
+	 * try_to_wake_up().
+	 */
+	raw_spin_lock(&self->pi_lock);
+	self->saved_state = self->state;
+	__set_current_state_no_track(TASK_UNINTERRUPTIBLE);
+	raw_spin_unlock(&self->pi_lock);
+
+	ret = task_blocks_on_rt_mutex(lock, waiter, self, RT_MUTEX_MIN_CHAINWALK);
+	BUG_ON(ret);
+
+	for (;;) {
+		/* Try to acquire the lock again. */
+		if (__try_to_take_rt_mutex(lock, self, waiter, STEAL_LATERAL))
+			break;
+
+		top_waiter = rt_mutex_top_waiter(lock);
+		lock_owner = rt_mutex_owner(lock);
+
+		raw_spin_unlock_irqrestore(&lock->wait_lock, flags);
+
+		if (top_waiter != waiter || adaptive_wait(lock, lock_owner))
+			preempt_schedule_lock();
+
+		raw_spin_lock_irqsave(&lock->wait_lock, flags);
+
+		raw_spin_lock(&self->pi_lock);
+		__set_current_state_no_track(TASK_UNINTERRUPTIBLE);
+		raw_spin_unlock(&self->pi_lock);
+	}
+
+	/*
+	 * Restore the task state to current->saved_state. We set it
+	 * to the original state above and the try_to_wake_up() code
+	 * has possibly updated it when a real (non-rtmutex) wakeup
+	 * happened while we were blocked. Clear saved_state so
+	 * try_to_wakeup() does not get confused.
+	 */
+	raw_spin_lock(&self->pi_lock);
+	__set_current_state_no_track(self->saved_state);
+	self->saved_state = TASK_RUNNING;
+	raw_spin_unlock(&self->pi_lock);
+
+	/*
+	 * try_to_take_rt_mutex() sets the waiter bit
+	 * unconditionally. We might have to fix that up:
+	 */
+	fixup_rt_mutex_waiters(lock);
+
+	BUG_ON(rt_mutex_has_waiters(lock) && waiter == rt_mutex_top_waiter(lock));
+	BUG_ON(!RB_EMPTY_NODE(&waiter->tree_entry));
+}
+
+static void noinline __sched rt_spin_lock_slowlock(struct rt_mutex *lock)
+{
+	struct rt_mutex_waiter waiter;
+	unsigned long flags;
+
+	rt_mutex_init_waiter(&waiter, true);
+
+	raw_spin_lock_irqsave(&lock->wait_lock, flags);
+	rt_spin_lock_slowlock_locked(lock, &waiter, flags);
+	raw_spin_unlock_irqrestore(&lock->wait_lock, flags);
+	debug_rt_mutex_free_waiter(&waiter);
+}
+
+static bool __sched __rt_mutex_unlock_common(struct rt_mutex *lock,
+					     struct wake_q_head *wake_q,
+					     struct wake_q_head *wq_sleeper);
+/*
+ * Slow path to release a rt_mutex spin_lock style
+ */
+void __sched rt_spin_lock_slowunlock(struct rt_mutex *lock)
+{
+	unsigned long flags;
+	DEFINE_WAKE_Q(wake_q);
+	DEFINE_WAKE_Q(wake_sleeper_q);
+	bool postunlock;
+
+	raw_spin_lock_irqsave(&lock->wait_lock, flags);
+	postunlock = __rt_mutex_unlock_common(lock, &wake_q, &wake_sleeper_q);
+	raw_spin_unlock_irqrestore(&lock->wait_lock, flags);
+
+	if (postunlock)
+		rt_mutex_postunlock(&wake_q, &wake_sleeper_q);
+}
+
+void __lockfunc rt_spin_lock(spinlock_t *lock)
+{
+	spin_acquire(&lock->dep_map, 0, 0, _RET_IP_);
+	rt_spin_lock_fastlock(&lock->lock, rt_spin_lock_slowlock);
+	rcu_read_lock();
+	migrate_disable();
+}
+EXPORT_SYMBOL(rt_spin_lock);
+
+void __lockfunc __rt_spin_lock(struct rt_mutex *lock)
+{
+	rt_spin_lock_fastlock(lock, rt_spin_lock_slowlock);
+}
+
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+void __lockfunc rt_spin_lock_nested(spinlock_t *lock, int subclass)
+{
+	spin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);
+	rt_spin_lock_fastlock(&lock->lock, rt_spin_lock_slowlock);
+	rcu_read_lock();
+	migrate_disable();
+}
+EXPORT_SYMBOL(rt_spin_lock_nested);
+
+void __lockfunc rt_spin_lock_nest_lock(spinlock_t *lock,
+				       struct lockdep_map *nest_lock)
+{
+	spin_acquire_nest(&lock->dep_map, 0, 0, nest_lock, _RET_IP_);
+	rt_spin_lock_fastlock(&lock->lock, rt_spin_lock_slowlock);
+	rcu_read_lock();
+	migrate_disable();
+}
+EXPORT_SYMBOL(rt_spin_lock_nest_lock);
+#endif
+
+void __lockfunc rt_spin_unlock(spinlock_t *lock)
+{
+	/* NOTE: we always pass in '1' for nested, for simplicity */
+	spin_release(&lock->dep_map, _RET_IP_);
+	migrate_enable();
+	rcu_read_unlock();
+	rt_spin_lock_fastunlock(&lock->lock, rt_spin_lock_slowunlock);
+}
+EXPORT_SYMBOL(rt_spin_unlock);
+
+void __lockfunc __rt_spin_unlock(struct rt_mutex *lock)
+{
+	rt_spin_lock_fastunlock(lock, rt_spin_lock_slowunlock);
+}
+EXPORT_SYMBOL(__rt_spin_unlock);
+
+/*
+ * Wait for the lock to get unlocked: instead of polling for an unlock
+ * (like raw spinlocks do), we lock and unlock, to force the kernel to
+ * schedule if there's contention:
+ */
+void __lockfunc rt_spin_lock_unlock(spinlock_t *lock)
+{
+	spin_lock(lock);
+	spin_unlock(lock);
+}
+EXPORT_SYMBOL(rt_spin_lock_unlock);
+
+int __lockfunc rt_spin_trylock(spinlock_t *lock)
+{
+	int ret;
+
+	ret = __rt_mutex_trylock(&lock->lock);
+	if (ret) {
+		spin_acquire(&lock->dep_map, 0, 1, _RET_IP_);
+		rcu_read_lock();
+		migrate_disable();
+	}
+	return ret;
+}
+EXPORT_SYMBOL(rt_spin_trylock);
+
+int __lockfunc rt_spin_trylock_bh(spinlock_t *lock)
+{
+	int ret;
+
+	local_bh_disable();
+	ret = __rt_mutex_trylock(&lock->lock);
+	if (ret) {
+		spin_acquire(&lock->dep_map, 0, 1, _RET_IP_);
+		rcu_read_lock();
+		migrate_disable();
+	} else {
+		local_bh_enable();
+	}
+	return ret;
+}
+EXPORT_SYMBOL(rt_spin_trylock_bh);
+
+void
+__rt_spin_lock_init(spinlock_t *lock, const char *name, struct lock_class_key *key)
+{
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	/*
+	 * Make sure we are not reinitializing a held lock:
+	 */
+	debug_check_no_locks_freed((void *)lock, sizeof(*lock));
+	lockdep_init_map(&lock->dep_map, name, key, 0);
+#endif
+}
+EXPORT_SYMBOL(__rt_spin_lock_init);
+
+#endif /* PREEMPT_RT */
+
+#ifdef CONFIG_PREEMPT_RT
+	static inline int __sched
+__mutex_lock_check_stamp(struct rt_mutex *lock, struct ww_acquire_ctx *ctx)
+{
+	struct ww_mutex *ww = container_of(lock, struct ww_mutex, base.lock);
+	struct ww_acquire_ctx *hold_ctx = READ_ONCE(ww->ctx);
+
+	if (!hold_ctx)
+		return 0;
+
+	if (unlikely(ctx == hold_ctx))
+		return -EALREADY;
+
+	if (ctx->stamp - hold_ctx->stamp <= LONG_MAX &&
+	    (ctx->stamp != hold_ctx->stamp || ctx > hold_ctx)) {
+#ifdef CONFIG_DEBUG_MUTEXES
+		DEBUG_LOCKS_WARN_ON(ctx->contending_lock);
+		ctx->contending_lock = ww;
+#endif
+		return -EDEADLK;
+	}
+
+	return 0;
+}
+#else
+	static inline int __sched
+__mutex_lock_check_stamp(struct rt_mutex *lock, struct ww_acquire_ctx *ctx)
+{
+	BUG();
+	return 0;
+}
+
+#endif
+
+static inline int
+try_to_take_rt_mutex(struct rt_mutex *lock, struct task_struct *task,
+		     struct rt_mutex_waiter *waiter)
+{
+	return __try_to_take_rt_mutex(lock, task, waiter, STEAL_NORMAL);
+}
+
 /*
  * Task blocks on lock.
  *
@@ -948,6 +1314,22 @@ static int task_blocks_on_rt_mutex(struct rt_mutex *lock,
 		return -EDEADLK;
 
 	raw_spin_lock(&task->pi_lock);
+	/*
+	 * In the case of futex requeue PI, this will be a proxy
+	 * lock. The task will wake unaware that it is enqueueed on
+	 * this lock. Avoid blocking on two locks and corrupting
+	 * pi_blocked_on via the PI_WAKEUP_INPROGRESS
+	 * flag. futex_wait_requeue_pi() sets this when it wakes up
+	 * before requeue (due to a signal or timeout). Do not enqueue
+	 * the task if PI_WAKEUP_INPROGRESS is set.
+	 */
+	if (task != current && task->pi_blocked_on == PI_WAKEUP_INPROGRESS) {
+		raw_spin_unlock(&task->pi_lock);
+		return -EAGAIN;
+	}
+
+       BUG_ON(rt_mutex_real_waiter(task->pi_blocked_on));
+
 	waiter->task = task;
 	waiter->lock = lock;
 	waiter->prio = task->prio;
@@ -971,7 +1353,7 @@ static int task_blocks_on_rt_mutex(struct rt_mutex *lock,
 		rt_mutex_enqueue_pi(owner, waiter);
 
 		rt_mutex_adjust_prio(owner);
-		if (owner->pi_blocked_on)
+		if (rt_mutex_real_waiter(owner->pi_blocked_on))
 			chain_walk = 1;
 	} else if (rt_mutex_cond_detect_deadlock(waiter, chwalk)) {
 		chain_walk = 1;
@@ -1013,6 +1395,7 @@ static int task_blocks_on_rt_mutex(struct rt_mutex *lock,
  * Called with lock->wait_lock held and interrupts disabled.
  */
 static void mark_wakeup_next_waiter(struct wake_q_head *wake_q,
+				    struct wake_q_head *wake_sleeper_q,
 				    struct rt_mutex *lock)
 {
 	struct rt_mutex_waiter *waiter;
@@ -1052,7 +1435,10 @@ static void mark_wakeup_next_waiter(struct wake_q_head *wake_q,
 	 * Pairs with preempt_enable() in rt_mutex_postunlock();
 	 */
 	preempt_disable();
-	wake_q_add(wake_q, waiter->task);
+	if (waiter->savestate)
+		wake_q_add_sleeper(wake_sleeper_q, waiter->task);
+	else
+		wake_q_add(wake_q, waiter->task);
 	raw_spin_unlock(&current->pi_lock);
 }
 
@@ -1067,7 +1453,7 @@ static void remove_waiter(struct rt_mutex *lock,
 {
 	bool is_top_waiter = (waiter == rt_mutex_top_waiter(lock));
 	struct task_struct *owner = rt_mutex_owner(lock);
-	struct rt_mutex *next_lock;
+	struct rt_mutex *next_lock = NULL;
 
 	lockdep_assert_held(&lock->wait_lock);
 
@@ -1093,7 +1479,8 @@ static void remove_waiter(struct rt_mutex *lock,
 	rt_mutex_adjust_prio(owner);
 
 	/* Store the lock on which owner is blocked or NULL */
-	next_lock = task_blocked_on_lock(owner);
+	if (rt_mutex_real_waiter(owner->pi_blocked_on))
+		next_lock = task_blocked_on_lock(owner);
 
 	raw_spin_unlock(&owner->pi_lock);
 
@@ -1129,26 +1516,28 @@ void rt_mutex_adjust_pi(struct task_struct *task)
 	raw_spin_lock_irqsave(&task->pi_lock, flags);
 
 	waiter = task->pi_blocked_on;
-	if (!waiter || rt_mutex_waiter_equal(waiter, task_to_waiter(task))) {
+	if (!rt_mutex_real_waiter(waiter) ||
+	    rt_mutex_waiter_equal(waiter, task_to_waiter(task))) {
 		raw_spin_unlock_irqrestore(&task->pi_lock, flags);
 		return;
 	}
 	next_lock = waiter->lock;
-	raw_spin_unlock_irqrestore(&task->pi_lock, flags);
 
 	/* gets dropped in rt_mutex_adjust_prio_chain()! */
 	get_task_struct(task);
 
+	raw_spin_unlock_irqrestore(&task->pi_lock, flags);
 	rt_mutex_adjust_prio_chain(task, RT_MUTEX_MIN_CHAINWALK, NULL,
 				   next_lock, NULL, task);
 }
 
-void rt_mutex_init_waiter(struct rt_mutex_waiter *waiter)
+void rt_mutex_init_waiter(struct rt_mutex_waiter *waiter, bool savestate)
 {
 	debug_rt_mutex_init_waiter(waiter);
 	RB_CLEAR_NODE(&waiter->pi_tree_entry);
 	RB_CLEAR_NODE(&waiter->tree_entry);
 	waiter->task = NULL;
+	waiter->savestate = savestate;
 }
 
 /**
@@ -1164,7 +1553,8 @@ void rt_mutex_init_waiter(struct rt_mutex_waiter *waiter)
 static int __sched
 __rt_mutex_slowlock(struct rt_mutex *lock, int state,
 		    struct hrtimer_sleeper *timeout,
-		    struct rt_mutex_waiter *waiter)
+		    struct rt_mutex_waiter *waiter,
+		    struct ww_acquire_ctx *ww_ctx)
 {
 	int ret = 0;
 
@@ -1173,24 +1563,23 @@ __rt_mutex_slowlock(struct rt_mutex *lock, int state,
 		if (try_to_take_rt_mutex(lock, current, waiter))
 			break;
 
-		/*
-		 * TASK_INTERRUPTIBLE checks for signals and
-		 * timeout. Ignored otherwise.
-		 */
-		if (likely(state == TASK_INTERRUPTIBLE)) {
-			/* Signal pending? */
-			if (signal_pending(current))
-				ret = -EINTR;
-			if (timeout && !timeout->task)
-				ret = -ETIMEDOUT;
+		if (timeout && !timeout->task) {
+			ret = -ETIMEDOUT;
+			break;
+		}
+		if (signal_pending_state(state, current)) {
+			ret = -EINTR;
+			break;
+		}
+
+		if (ww_ctx && ww_ctx->acquired > 0) {
+			ret = __mutex_lock_check_stamp(lock, ww_ctx);
 			if (ret)
 				break;
 		}
 
 		raw_spin_unlock_irq(&lock->wait_lock);
 
-		debug_rt_mutex_print_deadlock(waiter);
-
 		schedule();
 
 		raw_spin_lock_irq(&lock->wait_lock);
@@ -1211,43 +1600,110 @@ static void rt_mutex_handle_deadlock(int res, int detect_deadlock,
 	if (res != -EDEADLOCK || detect_deadlock)
 		return;
 
-	/*
-	 * Yell lowdly and stop the task right here.
-	 */
-	rt_mutex_print_deadlock(w);
 	while (1) {
 		set_current_state(TASK_INTERRUPTIBLE);
 		schedule();
 	}
 }
 
-/*
- * Slow path lock function:
- */
-static int __sched
-rt_mutex_slowlock(struct rt_mutex *lock, int state,
-		  struct hrtimer_sleeper *timeout,
-		  enum rtmutex_chainwalk chwalk)
+static __always_inline void ww_mutex_lock_acquired(struct ww_mutex *ww,
+						   struct ww_acquire_ctx *ww_ctx)
 {
-	struct rt_mutex_waiter waiter;
-	unsigned long flags;
-	int ret = 0;
+#ifdef CONFIG_DEBUG_MUTEXES
+	/*
+	 * If this WARN_ON triggers, you used ww_mutex_lock to acquire,
+	 * but released with a normal mutex_unlock in this call.
+	 *
+	 * This should never happen, always use ww_mutex_unlock.
+	 */
+	DEBUG_LOCKS_WARN_ON(ww->ctx);
+
+	/*
+	 * Not quite done after calling ww_acquire_done() ?
+	 */
+	DEBUG_LOCKS_WARN_ON(ww_ctx->done_acquire);
 
-	rt_mutex_init_waiter(&waiter);
+	if (ww_ctx->contending_lock) {
+		/*
+		 * After -EDEADLK you tried to
+		 * acquire a different ww_mutex? Bad!
+		 */
+		DEBUG_LOCKS_WARN_ON(ww_ctx->contending_lock != ww);
+
+		/*
+		 * You called ww_mutex_lock after receiving -EDEADLK,
+		 * but 'forgot' to unlock everything else first?
+		 */
+		DEBUG_LOCKS_WARN_ON(ww_ctx->acquired > 0);
+		ww_ctx->contending_lock = NULL;
+	}
 
 	/*
-	 * Technically we could use raw_spin_[un]lock_irq() here, but this can
-	 * be called in early boot if the cmpxchg() fast path is disabled
-	 * (debug, no architecture support). In this case we will acquire the
-	 * rtmutex with lock->wait_lock held. But we cannot unconditionally
-	 * enable interrupts in that early boot case. So we need to use the
-	 * irqsave/restore variants.
+	 * Naughty, using a different class will lead to undefined behavior!
 	 */
-	raw_spin_lock_irqsave(&lock->wait_lock, flags);
+	DEBUG_LOCKS_WARN_ON(ww_ctx->ww_class != ww->ww_class);
+#endif
+	ww_ctx->acquired++;
+}
+
+#ifdef CONFIG_PREEMPT_RT
+static void ww_mutex_account_lock(struct rt_mutex *lock,
+				  struct ww_acquire_ctx *ww_ctx)
+{
+	struct ww_mutex *ww = container_of(lock, struct ww_mutex, base.lock);
+	struct rt_mutex_waiter *waiter, *n;
+
+	/*
+	 * This branch gets optimized out for the common case,
+	 * and is only important for ww_mutex_lock.
+	 */
+	ww_mutex_lock_acquired(ww, ww_ctx);
+	ww->ctx = ww_ctx;
+
+	/*
+	 * Give any possible sleeping processes the chance to wake up,
+	 * so they can recheck if they have to back off.
+	 */
+	rbtree_postorder_for_each_entry_safe(waiter, n, &lock->waiters.rb_root,
+					     tree_entry) {
+		/* XXX debug rt mutex waiter wakeup */
+
+		BUG_ON(waiter->lock != lock);
+		rt_mutex_wake_waiter(waiter);
+	}
+}
+
+#else
+
+static void ww_mutex_account_lock(struct rt_mutex *lock,
+				  struct ww_acquire_ctx *ww_ctx)
+{
+	BUG();
+}
+#endif
+
+int __sched rt_mutex_slowlock_locked(struct rt_mutex *lock, int state,
+				     struct hrtimer_sleeper *timeout,
+				     enum rtmutex_chainwalk chwalk,
+				     struct ww_acquire_ctx *ww_ctx,
+				     struct rt_mutex_waiter *waiter)
+{
+	int ret;
+
+#ifdef CONFIG_PREEMPT_RT
+	if (ww_ctx) {
+		struct ww_mutex *ww;
+
+		ww = container_of(lock, struct ww_mutex, base.lock);
+		if (unlikely(ww_ctx == READ_ONCE(ww->ctx)))
+			return -EALREADY;
+	}
+#endif
 
 	/* Try to acquire the lock again: */
 	if (try_to_take_rt_mutex(lock, current, NULL)) {
-		raw_spin_unlock_irqrestore(&lock->wait_lock, flags);
+		if (ww_ctx)
+			ww_mutex_account_lock(lock, ww_ctx);
 		return 0;
 	}
 
@@ -1257,16 +1713,26 @@ rt_mutex_slowlock(struct rt_mutex *lock, int state,
 	if (unlikely(timeout))
 		hrtimer_start_expires(&timeout->timer, HRTIMER_MODE_ABS);
 
-	ret = task_blocks_on_rt_mutex(lock, &waiter, current, chwalk);
+	ret = task_blocks_on_rt_mutex(lock, waiter, current, chwalk);
 
-	if (likely(!ret))
+	if (likely(!ret)) {
 		/* sleep on the mutex */
-		ret = __rt_mutex_slowlock(lock, state, timeout, &waiter);
+		ret = __rt_mutex_slowlock(lock, state, timeout, waiter,
+					  ww_ctx);
+	} else if (ww_ctx) {
+		/* ww_mutex received EDEADLK, let it become EALREADY */
+		ret = __mutex_lock_check_stamp(lock, ww_ctx);
+		BUG_ON(!ret);
+	}
 
 	if (unlikely(ret)) {
 		__set_current_state(TASK_RUNNING);
-		remove_waiter(lock, &waiter);
-		rt_mutex_handle_deadlock(ret, chwalk, &waiter);
+		remove_waiter(lock, waiter);
+		/* ww_mutex wants to report EDEADLK/EALREADY, let it */
+		if (!ww_ctx)
+			rt_mutex_handle_deadlock(ret, chwalk, waiter);
+	} else if (ww_ctx) {
+		ww_mutex_account_lock(lock, ww_ctx);
 	}
 
 	/*
@@ -1274,6 +1740,36 @@ rt_mutex_slowlock(struct rt_mutex *lock, int state,
 	 * unconditionally. We might have to fix that up.
 	 */
 	fixup_rt_mutex_waiters(lock);
+	return ret;
+}
+
+/*
+ * Slow path lock function:
+ */
+static int __sched
+rt_mutex_slowlock(struct rt_mutex *lock, int state,
+		  struct hrtimer_sleeper *timeout,
+		  enum rtmutex_chainwalk chwalk,
+		  struct ww_acquire_ctx *ww_ctx)
+{
+	struct rt_mutex_waiter waiter;
+	unsigned long flags;
+	int ret = 0;
+
+	rt_mutex_init_waiter(&waiter, false);
+
+	/*
+	 * Technically we could use raw_spin_[un]lock_irq() here, but this can
+	 * be called in early boot if the cmpxchg() fast path is disabled
+	 * (debug, no architecture support). In this case we will acquire the
+	 * rtmutex with lock->wait_lock held. But we cannot unconditionally
+	 * enable interrupts in that early boot case. So we need to use the
+	 * irqsave/restore variants.
+	 */
+	raw_spin_lock_irqsave(&lock->wait_lock, flags);
+
+	ret = rt_mutex_slowlock_locked(lock, state, timeout, chwalk, ww_ctx,
+				       &waiter);
 
 	raw_spin_unlock_irqrestore(&lock->wait_lock, flags);
 
@@ -1334,7 +1830,8 @@ static inline int rt_mutex_slowtrylock(struct rt_mutex *lock)
  * Return whether the current task needs to call rt_mutex_postunlock().
  */
 static bool __sched rt_mutex_slowunlock(struct rt_mutex *lock,
-					struct wake_q_head *wake_q)
+					struct wake_q_head *wake_q,
+					struct wake_q_head *wake_sleeper_q)
 {
 	unsigned long flags;
 
@@ -1388,7 +1885,7 @@ static bool __sched rt_mutex_slowunlock(struct rt_mutex *lock,
 	 *
 	 * Queue the next waiter for wakeup once we release the wait_lock.
 	 */
-	mark_wakeup_next_waiter(wake_q, lock);
+	mark_wakeup_next_waiter(wake_q, wake_sleeper_q, lock);
 	raw_spin_unlock_irqrestore(&lock->wait_lock, flags);
 
 	return true; /* call rt_mutex_postunlock() */
@@ -1402,29 +1899,16 @@ static bool __sched rt_mutex_slowunlock(struct rt_mutex *lock,
  */
 static inline int
 rt_mutex_fastlock(struct rt_mutex *lock, int state,
+		  struct ww_acquire_ctx *ww_ctx,
 		  int (*slowfn)(struct rt_mutex *lock, int state,
 				struct hrtimer_sleeper *timeout,
-				enum rtmutex_chainwalk chwalk))
+				enum rtmutex_chainwalk chwalk,
+				struct ww_acquire_ctx *ww_ctx))
 {
 	if (likely(rt_mutex_cmpxchg_acquire(lock, NULL, current)))
 		return 0;
 
-	return slowfn(lock, state, NULL, RT_MUTEX_MIN_CHAINWALK);
-}
-
-static inline int
-rt_mutex_timed_fastlock(struct rt_mutex *lock, int state,
-			struct hrtimer_sleeper *timeout,
-			enum rtmutex_chainwalk chwalk,
-			int (*slowfn)(struct rt_mutex *lock, int state,
-				      struct hrtimer_sleeper *timeout,
-				      enum rtmutex_chainwalk chwalk))
-{
-	if (chwalk == RT_MUTEX_MIN_CHAINWALK &&
-	    likely(rt_mutex_cmpxchg_acquire(lock, NULL, current)))
-		return 0;
-
-	return slowfn(lock, state, timeout, chwalk);
+	return slowfn(lock, state, NULL, RT_MUTEX_MIN_CHAINWALK, ww_ctx);
 }
 
 static inline int
@@ -1440,9 +1924,11 @@ rt_mutex_fasttrylock(struct rt_mutex *lock,
 /*
  * Performs the wakeup of the the top-waiter and re-enables preemption.
  */
-void rt_mutex_postunlock(struct wake_q_head *wake_q)
+void rt_mutex_postunlock(struct wake_q_head *wake_q,
+			 struct wake_q_head *wake_sleeper_q)
 {
 	wake_up_q(wake_q);
+	wake_up_q_sleeper(wake_sleeper_q);
 
 	/* Pairs with preempt_disable() in rt_mutex_slowunlock() */
 	preempt_enable();
@@ -1451,23 +1937,46 @@ void rt_mutex_postunlock(struct wake_q_head *wake_q)
 static inline void
 rt_mutex_fastunlock(struct rt_mutex *lock,
 		    bool (*slowfn)(struct rt_mutex *lock,
-				   struct wake_q_head *wqh))
+				   struct wake_q_head *wqh,
+				   struct wake_q_head *wq_sleeper))
 {
 	DEFINE_WAKE_Q(wake_q);
+	DEFINE_WAKE_Q(wake_sleeper_q);
 
 	if (likely(rt_mutex_cmpxchg_release(lock, current, NULL)))
 		return;
 
-	if (slowfn(lock, &wake_q))
-		rt_mutex_postunlock(&wake_q);
+	if (slowfn(lock, &wake_q, &wake_sleeper_q))
+		rt_mutex_postunlock(&wake_q, &wake_sleeper_q);
 }
 
-static inline void __rt_mutex_lock(struct rt_mutex *lock, unsigned int subclass)
+int __sched __rt_mutex_lock_state(struct rt_mutex *lock, int state)
 {
 	might_sleep();
+	return rt_mutex_fastlock(lock, state, NULL, rt_mutex_slowlock);
+}
+
+/**
+ * rt_mutex_lock_state - lock a rt_mutex with a given state
+ *
+ * @lock:      The rt_mutex to be locked
+ * @state:     The state to set when blocking on the rt_mutex
+ */
+static inline int __sched rt_mutex_lock_state(struct rt_mutex *lock,
+					      unsigned int subclass, int state)
+{
+	int ret;
 
 	mutex_acquire(&lock->dep_map, subclass, 0, _RET_IP_);
-	rt_mutex_fastlock(lock, TASK_UNINTERRUPTIBLE, rt_mutex_slowlock);
+	ret = __rt_mutex_lock_state(lock, state);
+	if (ret)
+		mutex_release(&lock->dep_map, _RET_IP_);
+	return ret;
+}
+
+static inline void __rt_mutex_lock(struct rt_mutex *lock, unsigned int subclass)
+{
+	rt_mutex_lock_state(lock, subclass, TASK_UNINTERRUPTIBLE);
 }
 
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
@@ -1508,16 +2017,7 @@ EXPORT_SYMBOL_GPL(rt_mutex_lock);
  */
 int __sched rt_mutex_lock_interruptible(struct rt_mutex *lock)
 {
-	int ret;
-
-	might_sleep();
-
-	mutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);
-	ret = rt_mutex_fastlock(lock, TASK_INTERRUPTIBLE, rt_mutex_slowlock);
-	if (ret)
-		mutex_release(&lock->dep_map, _RET_IP_);
-
-	return ret;
+	return rt_mutex_lock_state(lock, 0, TASK_INTERRUPTIBLE);
 }
 EXPORT_SYMBOL_GPL(rt_mutex_lock_interruptible);
 
@@ -1534,36 +2034,17 @@ int __sched __rt_mutex_futex_trylock(struct rt_mutex *lock)
 	return __rt_mutex_slowtrylock(lock);
 }
 
-/**
- * rt_mutex_timed_lock - lock a rt_mutex interruptible
- *			the timeout structure is provided
- *			by the caller
- *
- * @lock:		the rt_mutex to be locked
- * @timeout:		timeout structure or NULL (no timeout)
- *
- * Returns:
- *  0		on success
- * -EINTR	when interrupted by a signal
- * -ETIMEDOUT	when the timeout expired
- */
-int
-rt_mutex_timed_lock(struct rt_mutex *lock, struct hrtimer_sleeper *timeout)
+int __sched __rt_mutex_trylock(struct rt_mutex *lock)
 {
-	int ret;
-
-	might_sleep();
-
-	mutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);
-	ret = rt_mutex_timed_fastlock(lock, TASK_INTERRUPTIBLE, timeout,
-				       RT_MUTEX_MIN_CHAINWALK,
-				       rt_mutex_slowlock);
-	if (ret)
-		mutex_release(&lock->dep_map, _RET_IP_);
+#ifdef CONFIG_PREEMPT_RT
+	if (WARN_ON_ONCE(in_irq() || in_nmi()))
+#else
+	if (WARN_ON_ONCE(in_irq() || in_nmi() || in_serving_softirq()))
+#endif
+		return 0;
 
-	return ret;
+	return rt_mutex_fasttrylock(lock, rt_mutex_slowtrylock);
 }
-EXPORT_SYMBOL_GPL(rt_mutex_timed_lock);
 
 /**
  * rt_mutex_trylock - try to lock a rt_mutex
@@ -1580,10 +2061,7 @@ int __sched rt_mutex_trylock(struct rt_mutex *lock)
 {
 	int ret;
 
-	if (WARN_ON_ONCE(in_irq() || in_nmi() || in_serving_softirq()))
-		return 0;
-
-	ret = rt_mutex_fasttrylock(lock, rt_mutex_slowtrylock);
+	ret = __rt_mutex_trylock(lock);
 	if (ret)
 		mutex_acquire(&lock->dep_map, 0, 1, _RET_IP_);
 
@@ -1591,6 +2069,11 @@ int __sched rt_mutex_trylock(struct rt_mutex *lock)
 }
 EXPORT_SYMBOL_GPL(rt_mutex_trylock);
 
+void __sched __rt_mutex_unlock(struct rt_mutex *lock)
+{
+	rt_mutex_fastunlock(lock, rt_mutex_slowunlock);
+}
+
 /**
  * rt_mutex_unlock - unlock a rt_mutex
  *
@@ -1599,16 +2082,13 @@ EXPORT_SYMBOL_GPL(rt_mutex_trylock);
 void __sched rt_mutex_unlock(struct rt_mutex *lock)
 {
 	mutex_release(&lock->dep_map, _RET_IP_);
-	rt_mutex_fastunlock(lock, rt_mutex_slowunlock);
+	__rt_mutex_unlock(lock);
 }
 EXPORT_SYMBOL_GPL(rt_mutex_unlock);
 
-/**
- * Futex variant, that since futex variants do not use the fast-path, can be
- * simple and will not need to retry.
- */
-bool __sched __rt_mutex_futex_unlock(struct rt_mutex *lock,
-				    struct wake_q_head *wake_q)
+static bool __sched __rt_mutex_unlock_common(struct rt_mutex *lock,
+					     struct wake_q_head *wake_q,
+					     struct wake_q_head *wq_sleeper)
 {
 	lockdep_assert_held(&lock->wait_lock);
 
@@ -1625,23 +2105,35 @@ bool __sched __rt_mutex_futex_unlock(struct rt_mutex *lock,
 	 * avoid inversion prior to the wakeup.  preempt_disable()
 	 * therein pairs with rt_mutex_postunlock().
 	 */
-	mark_wakeup_next_waiter(wake_q, lock);
+	mark_wakeup_next_waiter(wake_q, wq_sleeper, lock);
 
 	return true; /* call postunlock() */
 }
 
+/**
+ * Futex variant, that since futex variants do not use the fast-path, can be
+ * simple and will not need to retry.
+ */
+bool __sched __rt_mutex_futex_unlock(struct rt_mutex *lock,
+				     struct wake_q_head *wake_q,
+				     struct wake_q_head *wq_sleeper)
+{
+	return __rt_mutex_unlock_common(lock, wake_q, wq_sleeper);
+}
+
 void __sched rt_mutex_futex_unlock(struct rt_mutex *lock)
 {
 	DEFINE_WAKE_Q(wake_q);
+	DEFINE_WAKE_Q(wake_sleeper_q);
 	unsigned long flags;
 	bool postunlock;
 
 	raw_spin_lock_irqsave(&lock->wait_lock, flags);
-	postunlock = __rt_mutex_futex_unlock(lock, &wake_q);
+	postunlock = __rt_mutex_futex_unlock(lock, &wake_q, &wake_sleeper_q);
 	raw_spin_unlock_irqrestore(&lock->wait_lock, flags);
 
 	if (postunlock)
-		rt_mutex_postunlock(&wake_q);
+		rt_mutex_postunlock(&wake_q, &wake_sleeper_q);
 }
 
 /**
@@ -1655,9 +2147,6 @@ void __sched rt_mutex_futex_unlock(struct rt_mutex *lock)
 void rt_mutex_destroy(struct rt_mutex *lock)
 {
 	WARN_ON(rt_mutex_is_locked(lock));
-#ifdef CONFIG_DEBUG_RT_MUTEXES
-	lock->magic = NULL;
-#endif
 }
 EXPORT_SYMBOL_GPL(rt_mutex_destroy);
 
@@ -1680,7 +2169,7 @@ void __rt_mutex_init(struct rt_mutex *lock, const char *name,
 	if (name && key)
 		debug_rt_mutex_init(lock, name, key);
 }
-EXPORT_SYMBOL_GPL(__rt_mutex_init);
+EXPORT_SYMBOL(__rt_mutex_init);
 
 /**
  * rt_mutex_init_proxy_locked - initialize and lock a rt_mutex on behalf of a
@@ -1700,6 +2189,14 @@ void rt_mutex_init_proxy_locked(struct rt_mutex *lock,
 				struct task_struct *proxy_owner)
 {
 	__rt_mutex_init(lock, NULL, NULL);
+#ifdef CONFIG_DEBUG_SPINLOCK
+	/*
+	 * get another key class for the wait_lock. LOCK_PI and UNLOCK_PI is
+	 * holding the ->wait_lock of the proxy_lock while unlocking a sleeping
+	 * lock.
+	 */
+	raw_spin_lock_init(&lock->wait_lock);
+#endif
 	debug_rt_mutex_proxy_lock(lock, proxy_owner);
 	rt_mutex_set_owner(lock, proxy_owner);
 }
@@ -1722,6 +2219,26 @@ void rt_mutex_proxy_unlock(struct rt_mutex *lock)
 	rt_mutex_set_owner(lock, NULL);
 }
 
+static void fixup_rt_mutex_blocked(struct rt_mutex *lock)
+{
+	struct task_struct *tsk = current;
+	/*
+	 * RT has a problem here when the wait got interrupted by a timeout
+	 * or a signal. task->pi_blocked_on is still set. The task must
+	 * acquire the hash bucket lock when returning from this function.
+	 *
+	 * If the hash bucket lock is contended then the
+	 * BUG_ON(rt_mutex_real_waiter(task->pi_blocked_on)) in
+	 * task_blocks_on_rt_mutex() will trigger. This can be avoided by
+	 * clearing task->pi_blocked_on which removes the task from the
+	 * boosting chain of the rtmutex. That's correct because the task
+	 * is not longer blocked on it.
+	 */
+	raw_spin_lock(&tsk->pi_lock);
+	tsk->pi_blocked_on = NULL;
+	raw_spin_unlock(&tsk->pi_lock);
+}
+
 /**
  * __rt_mutex_start_proxy_lock() - Start lock acquisition for another task
  * @lock:		the rt_mutex to take
@@ -1752,6 +2269,34 @@ int __rt_mutex_start_proxy_lock(struct rt_mutex *lock,
 	if (try_to_take_rt_mutex(lock, task, NULL))
 		return 1;
 
+#ifdef CONFIG_PREEMPT_RT
+	/*
+	 * In PREEMPT_RT there's an added race.
+	 * If the task, that we are about to requeue, times out,
+	 * it can set the PI_WAKEUP_INPROGRESS. This tells the requeue
+	 * to skip this task. But right after the task sets
+	 * its pi_blocked_on to PI_WAKEUP_INPROGRESS it can then
+	 * block on the spin_lock(&hb->lock), which in RT is an rtmutex.
+	 * This will replace the PI_WAKEUP_INPROGRESS with the actual
+	 * lock that it blocks on. We *must not* place this task
+	 * on this proxy lock in that case.
+	 *
+	 * To prevent this race, we first take the task's pi_lock
+	 * and check if it has updated its pi_blocked_on. If it has,
+	 * we assume that it woke up and we return -EAGAIN.
+	 * Otherwise, we set the task's pi_blocked_on to
+	 * PI_REQUEUE_INPROGRESS, so that if the task is waking up
+	 * it will know that we are in the process of requeuing it.
+	 */
+	raw_spin_lock(&task->pi_lock);
+	if (task->pi_blocked_on) {
+		raw_spin_unlock(&task->pi_lock);
+		return -EAGAIN;
+	}
+	task->pi_blocked_on = PI_REQUEUE_INPROGRESS;
+	raw_spin_unlock(&task->pi_lock);
+#endif
+
 	/* We enforce deadlock detection for futexes */
 	ret = task_blocks_on_rt_mutex(lock, waiter, task,
 				      RT_MUTEX_FULL_CHAINWALK);
@@ -1766,7 +2311,8 @@ int __rt_mutex_start_proxy_lock(struct rt_mutex *lock,
 		ret = 0;
 	}
 
-	debug_rt_mutex_print_deadlock(waiter);
+	if (ret)
+		fixup_rt_mutex_blocked(lock);
 
 	return ret;
 }
@@ -1851,12 +2397,15 @@ int rt_mutex_wait_proxy_lock(struct rt_mutex *lock,
 	raw_spin_lock_irq(&lock->wait_lock);
 	/* sleep on the mutex */
 	set_current_state(TASK_INTERRUPTIBLE);
-	ret = __rt_mutex_slowlock(lock, TASK_INTERRUPTIBLE, to, waiter);
+	ret = __rt_mutex_slowlock(lock, TASK_INTERRUPTIBLE, to, waiter, NULL);
 	/*
 	 * try_to_take_rt_mutex() sets the waiter bit unconditionally. We might
 	 * have to fix that up.
 	 */
 	fixup_rt_mutex_waiters(lock);
+	if (ret)
+		fixup_rt_mutex_blocked(lock);
+
 	raw_spin_unlock_irq(&lock->wait_lock);
 
 	return ret;
@@ -1918,3 +2467,97 @@ bool rt_mutex_cleanup_proxy_lock(struct rt_mutex *lock,
 
 	return cleanup;
 }
+
+static inline int
+ww_mutex_deadlock_injection(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)
+{
+#ifdef CONFIG_DEBUG_WW_MUTEX_SLOWPATH
+	unsigned int tmp;
+
+	if (ctx->deadlock_inject_countdown-- == 0) {
+		tmp = ctx->deadlock_inject_interval;
+		if (tmp > UINT_MAX/4)
+			tmp = UINT_MAX;
+		else
+			tmp = tmp*2 + tmp + tmp/2;
+
+		ctx->deadlock_inject_interval = tmp;
+		ctx->deadlock_inject_countdown = tmp;
+		ctx->contending_lock = lock;
+
+		ww_mutex_unlock(lock);
+
+		return -EDEADLK;
+	}
+#endif
+
+	return 0;
+}
+
+#ifdef CONFIG_PREEMPT_RT
+int __sched
+ww_mutex_lock_interruptible(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)
+{
+	int ret;
+
+	might_sleep();
+
+	mutex_acquire_nest(&lock->base.dep_map, 0, 0,
+			   ctx ? &ctx->dep_map : NULL, _RET_IP_);
+	ret = rt_mutex_slowlock(&lock->base.lock, TASK_INTERRUPTIBLE, NULL, 0,
+				ctx);
+	if (ret)
+		mutex_release(&lock->base.dep_map, _RET_IP_);
+	else if (!ret && ctx && ctx->acquired > 1)
+		return ww_mutex_deadlock_injection(lock, ctx);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(ww_mutex_lock_interruptible);
+
+int __sched
+ww_mutex_lock(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)
+{
+	int ret;
+
+	might_sleep();
+
+	mutex_acquire_nest(&lock->base.dep_map, 0, 0,
+			   ctx ? &ctx->dep_map : NULL, _RET_IP_);
+	ret = rt_mutex_slowlock(&lock->base.lock, TASK_UNINTERRUPTIBLE, NULL, 0,
+				ctx);
+	if (ret)
+		mutex_release(&lock->base.dep_map, _RET_IP_);
+	else if (!ret && ctx && ctx->acquired > 1)
+		return ww_mutex_deadlock_injection(lock, ctx);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(ww_mutex_lock);
+
+void __sched ww_mutex_unlock(struct ww_mutex *lock)
+{
+	/*
+	 * The unlocking fastpath is the 0->1 transition from 'locked'
+	 * into 'unlocked' state:
+	 */
+	if (lock->ctx) {
+#ifdef CONFIG_DEBUG_MUTEXES
+		DEBUG_LOCKS_WARN_ON(!lock->ctx->acquired);
+#endif
+		if (lock->ctx->acquired > 0)
+			lock->ctx->acquired--;
+		lock->ctx = NULL;
+	}
+
+	mutex_release(&lock->base.dep_map, _RET_IP_);
+	__rt_mutex_unlock(&lock->base.lock);
+}
+EXPORT_SYMBOL(ww_mutex_unlock);
+
+int __rt_mutex_owner_current(struct rt_mutex *lock)
+{
+	return rt_mutex_owner(lock) == current;
+}
+EXPORT_SYMBOL(__rt_mutex_owner_current);
+#endif
diff --git a/kernel/locking/rtmutex.h b/kernel/locking/rtmutex.h
index 732f96abf462..338ccd29119a 100644
--- a/kernel/locking/rtmutex.h
+++ b/kernel/locking/rtmutex.h
@@ -19,15 +19,8 @@
 #define debug_rt_mutex_proxy_unlock(l)			do { } while (0)
 #define debug_rt_mutex_unlock(l)			do { } while (0)
 #define debug_rt_mutex_init(m, n, k)			do { } while (0)
-#define debug_rt_mutex_deadlock(d, a ,l)		do { } while (0)
-#define debug_rt_mutex_print_deadlock(w)		do { } while (0)
 #define debug_rt_mutex_reset_waiter(w)			do { } while (0)
 
-static inline void rt_mutex_print_deadlock(struct rt_mutex_waiter *w)
-{
-	WARN(1, "rtmutex deadlock detected\n");
-}
-
 static inline bool debug_rt_mutex_detect_deadlock(struct rt_mutex_waiter *w,
 						  enum rtmutex_chainwalk walk)
 {
diff --git a/kernel/locking/rtmutex_common.h b/kernel/locking/rtmutex_common.h
index ca6fb489007b..248a7d91583b 100644
--- a/kernel/locking/rtmutex_common.h
+++ b/kernel/locking/rtmutex_common.h
@@ -15,6 +15,7 @@
 
 #include <linux/rtmutex.h>
 #include <linux/sched/wake_q.h>
+#include <linux/sched/debug.h>
 
 /*
  * This is the control structure for tasks blocked on a rt_mutex,
@@ -29,12 +30,8 @@ struct rt_mutex_waiter {
 	struct rb_node          pi_tree_entry;
 	struct task_struct	*task;
 	struct rt_mutex		*lock;
-#ifdef CONFIG_DEBUG_RT_MUTEXES
-	unsigned long		ip;
-	struct pid		*deadlock_task_pid;
-	struct rt_mutex		*deadlock_lock;
-#endif
 	int prio;
+	bool			savestate;
 	u64 deadline;
 };
 
@@ -130,11 +127,14 @@ enum rtmutex_chainwalk {
 /*
  * PI-futex support (proxy locking functions, etc.):
  */
+#define PI_WAKEUP_INPROGRESS	((struct rt_mutex_waiter *) 1)
+#define PI_REQUEUE_INPROGRESS	((struct rt_mutex_waiter *) 2)
+
 extern struct task_struct *rt_mutex_next_owner(struct rt_mutex *lock);
 extern void rt_mutex_init_proxy_locked(struct rt_mutex *lock,
 				       struct task_struct *proxy_owner);
 extern void rt_mutex_proxy_unlock(struct rt_mutex *lock);
-extern void rt_mutex_init_waiter(struct rt_mutex_waiter *waiter);
+extern void rt_mutex_init_waiter(struct rt_mutex_waiter *waiter, bool savetate);
 extern int __rt_mutex_start_proxy_lock(struct rt_mutex *lock,
 				     struct rt_mutex_waiter *waiter,
 				     struct task_struct *task);
@@ -152,9 +152,27 @@ extern int __rt_mutex_futex_trylock(struct rt_mutex *l);
 
 extern void rt_mutex_futex_unlock(struct rt_mutex *lock);
 extern bool __rt_mutex_futex_unlock(struct rt_mutex *lock,
-				 struct wake_q_head *wqh);
-
-extern void rt_mutex_postunlock(struct wake_q_head *wake_q);
+				 struct wake_q_head *wqh,
+				 struct wake_q_head *wq_sleeper);
+
+extern void rt_mutex_postunlock(struct wake_q_head *wake_q,
+				struct wake_q_head *wake_sleeper_q);
+
+/* RW semaphore special interface */
+struct ww_acquire_ctx;
+
+extern int __rt_mutex_lock_state(struct rt_mutex *lock, int state);
+extern int __rt_mutex_trylock(struct rt_mutex *lock);
+extern void __rt_mutex_unlock(struct rt_mutex *lock);
+int __sched rt_mutex_slowlock_locked(struct rt_mutex *lock, int state,
+				     struct hrtimer_sleeper *timeout,
+				     enum rtmutex_chainwalk chwalk,
+				     struct ww_acquire_ctx *ww_ctx,
+				     struct rt_mutex_waiter *waiter);
+void __sched rt_spin_lock_slowlock_locked(struct rt_mutex *lock,
+					  struct rt_mutex_waiter *waiter,
+					  unsigned long flags);
+void __sched rt_spin_lock_slowunlock(struct rt_mutex *lock);
 
 #ifdef CONFIG_DEBUG_RT_MUTEXES
 # include "rtmutex-debug.h"
diff --git a/kernel/locking/rwlock-rt.c b/kernel/locking/rwlock-rt.c
new file mode 100644
index 000000000000..3d2d1f14b513
--- /dev/null
+++ b/kernel/locking/rwlock-rt.c
@@ -0,0 +1,334 @@
+// SPDX-License-Identifier: GPL-2.0-only
+#include <linux/sched/debug.h>
+#include <linux/export.h>
+
+#include "rtmutex_common.h"
+#include <linux/rwlock_types_rt.h>
+
+/*
+ * RT-specific reader/writer locks
+ *
+ * write_lock()
+ *  1) Lock lock->rtmutex
+ *  2) Remove the reader BIAS to force readers into the slow path
+ *  3) Wait until all readers have left the critical region
+ *  4) Mark it write locked
+ *
+ * write_unlock()
+ *  1) Remove the write locked marker
+ *  2) Set the reader BIAS so readers can use the fast path again
+ *  3) Unlock lock->rtmutex to release blocked readers
+ *
+ * read_lock()
+ *  1) Try fast path acquisition (reader BIAS is set)
+ *  2) Take lock->rtmutex.wait_lock which protects the writelocked flag
+ *  3) If !writelocked, acquire it for read
+ *  4) If writelocked, block on lock->rtmutex
+ *  5) unlock lock->rtmutex, goto 1)
+ *
+ * read_unlock()
+ *  1) Try fast path release (reader count != 1)
+ *  2) Wake the writer waiting in write_lock()#3
+ *
+ * read_lock()#3 has the consequence, that rw locks on RT are not writer
+ * fair, but writers, which should be avoided in RT tasks (think tasklist
+ * lock), are subject to the rtmutex priority/DL inheritance mechanism.
+ *
+ * It's possible to make the rw locks writer fair by keeping a list of
+ * active readers. A blocked writer would force all newly incoming readers
+ * to block on the rtmutex, but the rtmutex would have to be proxy locked
+ * for one reader after the other. We can't use multi-reader inheritance
+ * because there is no way to support that with
+ * SCHED_DEADLINE. Implementing the one by one reader boosting/handover
+ * mechanism is a major surgery for a very dubious value.
+ *
+ * The risk of writer starvation is there, but the pathological use cases
+ * which trigger it are not necessarily the typical RT workloads.
+ */
+
+void __rwlock_biased_rt_init(struct rt_rw_lock *lock, const char *name,
+			     struct lock_class_key *key)
+{
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	/*
+	 * Make sure we are not reinitializing a held semaphore:
+	 */
+	debug_check_no_locks_freed((void *)lock, sizeof(*lock));
+	lockdep_init_map(&lock->dep_map, name, key, 0);
+#endif
+	atomic_set(&lock->readers, READER_BIAS);
+	rt_mutex_init(&lock->rtmutex);
+	lock->rtmutex.save_state = 1;
+}
+
+static int __read_rt_trylock(struct rt_rw_lock *lock)
+{
+	int r, old;
+
+	/*
+	 * Increment reader count, if lock->readers < 0, i.e. READER_BIAS is
+	 * set.
+	 */
+	for (r = atomic_read(&lock->readers); r < 0;) {
+		old = atomic_cmpxchg(&lock->readers, r, r + 1);
+		if (likely(old == r))
+			return 1;
+		r = old;
+	}
+	return 0;
+}
+
+static void __read_rt_lock(struct rt_rw_lock *lock)
+{
+	struct rt_mutex *m = &lock->rtmutex;
+	struct rt_mutex_waiter waiter;
+	unsigned long flags;
+
+	if (__read_rt_trylock(lock))
+		return;
+
+	raw_spin_lock_irqsave(&m->wait_lock, flags);
+	/*
+	 * Allow readers as long as the writer has not completely
+	 * acquired the semaphore for write.
+	 */
+	if (atomic_read(&lock->readers) != WRITER_BIAS) {
+		atomic_inc(&lock->readers);
+		raw_spin_unlock_irqrestore(&m->wait_lock, flags);
+		return;
+	}
+
+	/*
+	 * Call into the slow lock path with the rtmutex->wait_lock
+	 * held, so this can't result in the following race:
+	 *
+	 * Reader1		Reader2		Writer
+	 *			read_lock()
+	 *					write_lock()
+	 *					rtmutex_lock(m)
+	 *					swait()
+	 * read_lock()
+	 * unlock(m->wait_lock)
+	 *			read_unlock()
+	 *			swake()
+	 *					lock(m->wait_lock)
+	 *					lock->writelocked=true
+	 *					unlock(m->wait_lock)
+	 *
+	 *					write_unlock()
+	 *					lock->writelocked=false
+	 *					rtmutex_unlock(m)
+	 *			read_lock()
+	 *					write_lock()
+	 *					rtmutex_lock(m)
+	 *					swait()
+	 * rtmutex_lock(m)
+	 *
+	 * That would put Reader1 behind the writer waiting on
+	 * Reader2 to call read_unlock() which might be unbound.
+	 */
+	rt_mutex_init_waiter(&waiter, true);
+	rt_spin_lock_slowlock_locked(m, &waiter, flags);
+	/*
+	 * The slowlock() above is guaranteed to return with the rtmutex is
+	 * now held, so there can't be a writer active. Increment the reader
+	 * count and immediately drop the rtmutex again.
+	 */
+	atomic_inc(&lock->readers);
+	raw_spin_unlock_irqrestore(&m->wait_lock, flags);
+	rt_spin_lock_slowunlock(m);
+
+	debug_rt_mutex_free_waiter(&waiter);
+}
+
+static void __read_rt_unlock(struct rt_rw_lock *lock)
+{
+	struct rt_mutex *m = &lock->rtmutex;
+	struct task_struct *tsk;
+
+	/*
+	 * sem->readers can only hit 0 when a writer is waiting for the
+	 * active readers to leave the critical region.
+	 */
+	if (!atomic_dec_and_test(&lock->readers))
+		return;
+
+	raw_spin_lock_irq(&m->wait_lock);
+	/*
+	 * Wake the writer, i.e. the rtmutex owner. It might release the
+	 * rtmutex concurrently in the fast path, but to clean up the rw
+	 * lock it needs to acquire m->wait_lock. The worst case which can
+	 * happen is a spurious wakeup.
+	 */
+	tsk = rt_mutex_owner(m);
+	if (tsk)
+		wake_up_process(tsk);
+
+	raw_spin_unlock_irq(&m->wait_lock);
+}
+
+static void __write_unlock_common(struct rt_rw_lock *lock, int bias,
+				  unsigned long flags)
+{
+	struct rt_mutex *m = &lock->rtmutex;
+
+	atomic_add(READER_BIAS - bias, &lock->readers);
+	raw_spin_unlock_irqrestore(&m->wait_lock, flags);
+	rt_spin_lock_slowunlock(m);
+}
+
+static void __write_rt_lock(struct rt_rw_lock *lock)
+{
+	struct rt_mutex *m = &lock->rtmutex;
+	struct task_struct *self = current;
+	unsigned long flags;
+
+	/* Take the rtmutex as a first step */
+	__rt_spin_lock(m);
+
+	/* Force readers into slow path */
+	atomic_sub(READER_BIAS, &lock->readers);
+
+	raw_spin_lock_irqsave(&m->wait_lock, flags);
+
+	raw_spin_lock(&self->pi_lock);
+	self->saved_state = self->state;
+	__set_current_state_no_track(TASK_UNINTERRUPTIBLE);
+	raw_spin_unlock(&self->pi_lock);
+
+	for (;;) {
+		/* Have all readers left the critical region? */
+		if (!atomic_read(&lock->readers)) {
+			atomic_set(&lock->readers, WRITER_BIAS);
+			raw_spin_lock(&self->pi_lock);
+			__set_current_state_no_track(self->saved_state);
+			self->saved_state = TASK_RUNNING;
+			raw_spin_unlock(&self->pi_lock);
+			raw_spin_unlock_irqrestore(&m->wait_lock, flags);
+			return;
+		}
+
+		raw_spin_unlock_irqrestore(&m->wait_lock, flags);
+
+		if (atomic_read(&lock->readers) != 0)
+			preempt_schedule_lock();
+
+		raw_spin_lock_irqsave(&m->wait_lock, flags);
+
+		raw_spin_lock(&self->pi_lock);
+		__set_current_state_no_track(TASK_UNINTERRUPTIBLE);
+		raw_spin_unlock(&self->pi_lock);
+	}
+}
+
+static int __write_rt_trylock(struct rt_rw_lock *lock)
+{
+	struct rt_mutex *m = &lock->rtmutex;
+	unsigned long flags;
+
+	if (!__rt_mutex_trylock(m))
+		return 0;
+
+	atomic_sub(READER_BIAS, &lock->readers);
+
+	raw_spin_lock_irqsave(&m->wait_lock, flags);
+	if (!atomic_read(&lock->readers)) {
+		atomic_set(&lock->readers, WRITER_BIAS);
+		raw_spin_unlock_irqrestore(&m->wait_lock, flags);
+		return 1;
+	}
+	__write_unlock_common(lock, 0, flags);
+	return 0;
+}
+
+static void __write_rt_unlock(struct rt_rw_lock *lock)
+{
+	struct rt_mutex *m = &lock->rtmutex;
+	unsigned long flags;
+
+	raw_spin_lock_irqsave(&m->wait_lock, flags);
+	__write_unlock_common(lock, WRITER_BIAS, flags);
+}
+
+int __lockfunc rt_read_can_lock(rwlock_t *rwlock)
+{
+	return  atomic_read(&rwlock->readers) < 0;
+}
+
+int __lockfunc rt_write_can_lock(rwlock_t *rwlock)
+{
+	return atomic_read(&rwlock->readers) == READER_BIAS;
+}
+
+/*
+ * The common functions which get wrapped into the rwlock API.
+ */
+int __lockfunc rt_read_trylock(rwlock_t *rwlock)
+{
+	int ret;
+
+	ret = __read_rt_trylock(rwlock);
+	if (ret) {
+		rwlock_acquire_read(&rwlock->dep_map, 0, 1, _RET_IP_);
+		rcu_read_lock();
+		migrate_disable();
+	}
+	return ret;
+}
+EXPORT_SYMBOL(rt_read_trylock);
+
+int __lockfunc rt_write_trylock(rwlock_t *rwlock)
+{
+	int ret;
+
+	ret = __write_rt_trylock(rwlock);
+	if (ret) {
+		rwlock_acquire(&rwlock->dep_map, 0, 1, _RET_IP_);
+		rcu_read_lock();
+		migrate_disable();
+	}
+	return ret;
+}
+EXPORT_SYMBOL(rt_write_trylock);
+
+void __lockfunc rt_read_lock(rwlock_t *rwlock)
+{
+	rwlock_acquire_read(&rwlock->dep_map, 0, 0, _RET_IP_);
+	__read_rt_lock(rwlock);
+	rcu_read_lock();
+	migrate_disable();
+}
+EXPORT_SYMBOL(rt_read_lock);
+
+void __lockfunc rt_write_lock(rwlock_t *rwlock)
+{
+	rwlock_acquire(&rwlock->dep_map, 0, 0, _RET_IP_);
+	__write_rt_lock(rwlock);
+	rcu_read_lock();
+	migrate_disable();
+}
+EXPORT_SYMBOL(rt_write_lock);
+
+void __lockfunc rt_read_unlock(rwlock_t *rwlock)
+{
+	rwlock_release(&rwlock->dep_map, _RET_IP_);
+	migrate_enable();
+	rcu_read_unlock();
+	__read_rt_unlock(rwlock);
+}
+EXPORT_SYMBOL(rt_read_unlock);
+
+void __lockfunc rt_write_unlock(rwlock_t *rwlock)
+{
+	rwlock_release(&rwlock->dep_map, _RET_IP_);
+	migrate_enable();
+	rcu_read_unlock();
+	__write_rt_unlock(rwlock);
+}
+EXPORT_SYMBOL(rt_write_unlock);
+
+void __rt_rwlock_init(rwlock_t *rwlock, char *name, struct lock_class_key *key)
+{
+	__rwlock_biased_rt_init(rwlock, name, key);
+}
+EXPORT_SYMBOL(__rt_rwlock_init);
diff --git a/kernel/locking/rwsem-rt.c b/kernel/locking/rwsem-rt.c
new file mode 100644
index 000000000000..274172d5bb3a
--- /dev/null
+++ b/kernel/locking/rwsem-rt.c
@@ -0,0 +1,318 @@
+// SPDX-License-Identifier: GPL-2.0-only
+#include <linux/rwsem.h>
+#include <linux/sched/debug.h>
+#include <linux/sched/signal.h>
+#include <linux/export.h>
+#include <linux/blkdev.h>
+
+#include "rtmutex_common.h"
+
+/*
+ * RT-specific reader/writer semaphores
+ *
+ * down_write()
+ *  1) Lock sem->rtmutex
+ *  2) Remove the reader BIAS to force readers into the slow path
+ *  3) Wait until all readers have left the critical region
+ *  4) Mark it write locked
+ *
+ * up_write()
+ *  1) Remove the write locked marker
+ *  2) Set the reader BIAS so readers can use the fast path again
+ *  3) Unlock sem->rtmutex to release blocked readers
+ *
+ * down_read()
+ *  1) Try fast path acquisition (reader BIAS is set)
+ *  2) Take sem->rtmutex.wait_lock which protects the writelocked flag
+ *  3) If !writelocked, acquire it for read
+ *  4) If writelocked, block on sem->rtmutex
+ *  5) unlock sem->rtmutex, goto 1)
+ *
+ * up_read()
+ *  1) Try fast path release (reader count != 1)
+ *  2) Wake the writer waiting in down_write()#3
+ *
+ * down_read()#3 has the consequence, that rw semaphores on RT are not writer
+ * fair, but writers, which should be avoided in RT tasks (think mmap_sem),
+ * are subject to the rtmutex priority/DL inheritance mechanism.
+ *
+ * It's possible to make the rw semaphores writer fair by keeping a list of
+ * active readers. A blocked writer would force all newly incoming readers to
+ * block on the rtmutex, but the rtmutex would have to be proxy locked for one
+ * reader after the other. We can't use multi-reader inheritance because there
+ * is no way to support that with SCHED_DEADLINE. Implementing the one by one
+ * reader boosting/handover mechanism is a major surgery for a very dubious
+ * value.
+ *
+ * The risk of writer starvation is there, but the pathological use cases
+ * which trigger it are not necessarily the typical RT workloads.
+ */
+
+void __rwsem_init(struct rw_semaphore *sem, const char *name,
+		  struct lock_class_key *key)
+{
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	/*
+	 * Make sure we are not reinitializing a held semaphore:
+	 */
+	debug_check_no_locks_freed((void *)sem, sizeof(*sem));
+	lockdep_init_map(&sem->dep_map, name, key, 0);
+#endif
+	atomic_set(&sem->readers, READER_BIAS);
+}
+EXPORT_SYMBOL(__rwsem_init);
+
+int __down_read_trylock(struct rw_semaphore *sem)
+{
+	int r, old;
+
+	/*
+	 * Increment reader count, if sem->readers < 0, i.e. READER_BIAS is
+	 * set.
+	 */
+	for (r = atomic_read(&sem->readers); r < 0;) {
+		old = atomic_cmpxchg(&sem->readers, r, r + 1);
+		if (likely(old == r))
+			return 1;
+		r = old;
+	}
+	return 0;
+}
+
+static int __sched __down_read_common(struct rw_semaphore *sem, int state)
+{
+	struct rt_mutex *m = &sem->rtmutex;
+	struct rt_mutex_waiter waiter;
+	int ret;
+
+	if (__down_read_trylock(sem))
+		return 0;
+
+	/*
+	 * Flush blk before ->pi_blocked_on is set. At schedule() time it is too
+	 * late if one of the callbacks needs to acquire a sleeping lock.
+	 */
+	if (blk_needs_flush_plug(current))
+		blk_schedule_flush_plug(current);
+
+	might_sleep();
+	raw_spin_lock_irq(&m->wait_lock);
+	/*
+	 * Allow readers as long as the writer has not completely
+	 * acquired the semaphore for write.
+	 */
+	if (atomic_read(&sem->readers) != WRITER_BIAS) {
+		atomic_inc(&sem->readers);
+		raw_spin_unlock_irq(&m->wait_lock);
+		return 0;
+	}
+
+	/*
+	 * Call into the slow lock path with the rtmutex->wait_lock
+	 * held, so this can't result in the following race:
+	 *
+	 * Reader1		Reader2		Writer
+	 *			down_read()
+	 *					down_write()
+	 *					rtmutex_lock(m)
+	 *					swait()
+	 * down_read()
+	 * unlock(m->wait_lock)
+	 *			up_read()
+	 *			swake()
+	 *					lock(m->wait_lock)
+	 *					sem->writelocked=true
+	 *					unlock(m->wait_lock)
+	 *
+	 *					up_write()
+	 *					sem->writelocked=false
+	 *					rtmutex_unlock(m)
+	 *			down_read()
+	 *					down_write()
+	 *					rtmutex_lock(m)
+	 *					swait()
+	 * rtmutex_lock(m)
+	 *
+	 * That would put Reader1 behind the writer waiting on
+	 * Reader2 to call up_read() which might be unbound.
+	 */
+	rt_mutex_init_waiter(&waiter, false);
+	ret = rt_mutex_slowlock_locked(m, state, NULL, RT_MUTEX_MIN_CHAINWALK,
+				       NULL, &waiter);
+	/*
+	 * The slowlock() above is guaranteed to return with the rtmutex (for
+	 * ret = 0) is now held, so there can't be a writer active. Increment
+	 * the reader count and immediately drop the rtmutex again.
+	 * For ret != 0 we don't hold the rtmutex and need unlock the wait_lock.
+	 * We don't own the lock then.
+	 */
+	if (!ret)
+		atomic_inc(&sem->readers);
+	raw_spin_unlock_irq(&m->wait_lock);
+	if (!ret)
+		__rt_mutex_unlock(m);
+
+	debug_rt_mutex_free_waiter(&waiter);
+	return ret;
+}
+
+void __down_read(struct rw_semaphore *sem)
+{
+	int ret;
+
+	ret = __down_read_common(sem, TASK_UNINTERRUPTIBLE);
+	WARN_ON_ONCE(ret);
+}
+
+int __down_read_interruptible(struct rw_semaphore *sem)
+{
+	int ret;
+
+	ret = __down_read_common(sem, TASK_INTERRUPTIBLE);
+	if (likely(!ret))
+		return ret;
+	WARN_ONCE(ret != -EINTR, "Unexpected state: %d\n", ret);
+	return -EINTR;
+}
+
+int __down_read_killable(struct rw_semaphore *sem)
+{
+	int ret;
+
+	ret = __down_read_common(sem, TASK_KILLABLE);
+	if (likely(!ret))
+		return ret;
+	WARN_ONCE(ret != -EINTR, "Unexpected state: %d\n", ret);
+	return -EINTR;
+}
+
+void __up_read(struct rw_semaphore *sem)
+{
+	struct rt_mutex *m = &sem->rtmutex;
+	struct task_struct *tsk;
+
+	/*
+	 * sem->readers can only hit 0 when a writer is waiting for the
+	 * active readers to leave the critical region.
+	 */
+	if (!atomic_dec_and_test(&sem->readers))
+		return;
+
+	might_sleep();
+	raw_spin_lock_irq(&m->wait_lock);
+	/*
+	 * Wake the writer, i.e. the rtmutex owner. It might release the
+	 * rtmutex concurrently in the fast path (due to a signal), but to
+	 * clean up the rwsem it needs to acquire m->wait_lock. The worst
+	 * case which can happen is a spurious wakeup.
+	 */
+	tsk = rt_mutex_owner(m);
+	if (tsk)
+		wake_up_process(tsk);
+
+	raw_spin_unlock_irq(&m->wait_lock);
+}
+
+static void __up_write_unlock(struct rw_semaphore *sem, int bias,
+			      unsigned long flags)
+{
+	struct rt_mutex *m = &sem->rtmutex;
+
+	atomic_add(READER_BIAS - bias, &sem->readers);
+	raw_spin_unlock_irqrestore(&m->wait_lock, flags);
+	__rt_mutex_unlock(m);
+}
+
+static int __sched __down_write_common(struct rw_semaphore *sem, int state)
+{
+	struct rt_mutex *m = &sem->rtmutex;
+	unsigned long flags;
+
+	/*
+	 * Flush blk before ->pi_blocked_on is set. At schedule() time it is too
+	 * late if one of the callbacks needs to acquire a sleeping lock.
+	 */
+	if (blk_needs_flush_plug(current))
+		blk_schedule_flush_plug(current);
+
+	/* Take the rtmutex as a first step */
+	if (__rt_mutex_lock_state(m, state))
+		return -EINTR;
+
+	/* Force readers into slow path */
+	atomic_sub(READER_BIAS, &sem->readers);
+	might_sleep();
+
+	set_current_state(state);
+	for (;;) {
+		raw_spin_lock_irqsave(&m->wait_lock, flags);
+		/* Have all readers left the critical region? */
+		if (!atomic_read(&sem->readers)) {
+			atomic_set(&sem->readers, WRITER_BIAS);
+			__set_current_state(TASK_RUNNING);
+			raw_spin_unlock_irqrestore(&m->wait_lock, flags);
+			return 0;
+		}
+
+		if (signal_pending_state(state, current)) {
+			__set_current_state(TASK_RUNNING);
+			__up_write_unlock(sem, 0, flags);
+			return -EINTR;
+		}
+		raw_spin_unlock_irqrestore(&m->wait_lock, flags);
+
+		if (atomic_read(&sem->readers) != 0) {
+			schedule();
+			set_current_state(state);
+		}
+	}
+}
+
+void __sched __down_write(struct rw_semaphore *sem)
+{
+	__down_write_common(sem, TASK_UNINTERRUPTIBLE);
+}
+
+int __sched __down_write_killable(struct rw_semaphore *sem)
+{
+	return __down_write_common(sem, TASK_KILLABLE);
+}
+
+int __down_write_trylock(struct rw_semaphore *sem)
+{
+	struct rt_mutex *m = &sem->rtmutex;
+	unsigned long flags;
+
+	if (!__rt_mutex_trylock(m))
+		return 0;
+
+	atomic_sub(READER_BIAS, &sem->readers);
+
+	raw_spin_lock_irqsave(&m->wait_lock, flags);
+	if (!atomic_read(&sem->readers)) {
+		atomic_set(&sem->readers, WRITER_BIAS);
+		raw_spin_unlock_irqrestore(&m->wait_lock, flags);
+		return 1;
+	}
+	__up_write_unlock(sem, 0, flags);
+	return 0;
+}
+
+void __up_write(struct rw_semaphore *sem)
+{
+	struct rt_mutex *m = &sem->rtmutex;
+	unsigned long flags;
+
+	raw_spin_lock_irqsave(&m->wait_lock, flags);
+	__up_write_unlock(sem, WRITER_BIAS, flags);
+}
+
+void __downgrade_write(struct rw_semaphore *sem)
+{
+	struct rt_mutex *m = &sem->rtmutex;
+	unsigned long flags;
+
+	raw_spin_lock_irqsave(&m->wait_lock, flags);
+	/* Release it and account current as reader */
+	__up_write_unlock(sem, WRITER_BIAS - 1, flags);
+}
diff --git a/kernel/locking/rwsem.c b/kernel/locking/rwsem.c
index a163542d178e..3821c7204d10 100644
--- a/kernel/locking/rwsem.c
+++ b/kernel/locking/rwsem.c
@@ -28,6 +28,7 @@
 #include <linux/rwsem.h>
 #include <linux/atomic.h>
 
+#ifndef CONFIG_PREEMPT_RT
 #include "lock_events.h"
 
 /*
@@ -1494,6 +1495,7 @@ static inline void __downgrade_write(struct rw_semaphore *sem)
 	if (tmp & RWSEM_FLAG_WAITERS)
 		rwsem_downgrade_wake(sem);
 }
+#endif
 
 /*
  * lock for reading
@@ -1657,7 +1659,9 @@ void down_read_non_owner(struct rw_semaphore *sem)
 {
 	might_sleep();
 	__down_read(sem);
+#ifndef CONFIG_PREEMPT_RT
 	__rwsem_set_reader_owned(sem, NULL);
+#endif
 }
 EXPORT_SYMBOL(down_read_non_owner);
 
@@ -1686,7 +1690,9 @@ EXPORT_SYMBOL(down_write_killable_nested);
 
 void up_read_non_owner(struct rw_semaphore *sem)
 {
+#ifndef CONFIG_PREEMPT_RT
 	DEBUG_RWSEMS_WARN_ON(!is_rwsem_reader_owned(sem), sem);
+#endif
 	__up_read(sem);
 }
 EXPORT_SYMBOL(up_read_non_owner);
diff --git a/kernel/locking/spinlock.c b/kernel/locking/spinlock.c
index 0ff08380f531..45445a2f1799 100644
--- a/kernel/locking/spinlock.c
+++ b/kernel/locking/spinlock.c
@@ -124,8 +124,11 @@ void __lockfunc __raw_##op##_lock_bh(locktype##_t *lock)		\
  *         __[spin|read|write]_lock_bh()
  */
 BUILD_LOCK_OPS(spin, raw_spinlock);
+
+#ifndef CONFIG_PREEMPT_RT
 BUILD_LOCK_OPS(read, rwlock);
 BUILD_LOCK_OPS(write, rwlock);
+#endif
 
 #endif
 
@@ -209,6 +212,8 @@ void __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)
 EXPORT_SYMBOL(_raw_spin_unlock_bh);
 #endif
 
+#ifndef CONFIG_PREEMPT_RT
+
 #ifndef CONFIG_INLINE_READ_TRYLOCK
 int __lockfunc _raw_read_trylock(rwlock_t *lock)
 {
@@ -353,6 +358,8 @@ void __lockfunc _raw_write_unlock_bh(rwlock_t *lock)
 EXPORT_SYMBOL(_raw_write_unlock_bh);
 #endif
 
+#endif /* !PREEMPT_RT */
+
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 
 void __lockfunc _raw_spin_lock_nested(raw_spinlock_t *lock, int subclass)
diff --git a/kernel/locking/spinlock_debug.c b/kernel/locking/spinlock_debug.c
index b9d93087ee66..72e306e0e8a3 100644
--- a/kernel/locking/spinlock_debug.c
+++ b/kernel/locking/spinlock_debug.c
@@ -31,6 +31,7 @@ void __raw_spin_lock_init(raw_spinlock_t *lock, const char *name,
 
 EXPORT_SYMBOL(__raw_spin_lock_init);
 
+#ifndef CONFIG_PREEMPT_RT
 void __rwlock_init(rwlock_t *lock, const char *name,
 		   struct lock_class_key *key)
 {
@@ -48,6 +49,7 @@ void __rwlock_init(rwlock_t *lock, const char *name,
 }
 
 EXPORT_SYMBOL(__rwlock_init);
+#endif
 
 static void spin_dump(raw_spinlock_t *lock, const char *msg)
 {
@@ -139,6 +141,7 @@ void do_raw_spin_unlock(raw_spinlock_t *lock)
 	arch_spin_unlock(&lock->raw_lock);
 }
 
+#ifndef CONFIG_PREEMPT_RT
 static void rwlock_bug(rwlock_t *lock, const char *msg)
 {
 	if (!debug_locks_off())
@@ -228,3 +231,5 @@ void do_raw_write_unlock(rwlock_t *lock)
 	debug_write_unlock(lock);
 	arch_write_unlock(&lock->raw_lock);
 }
+
+#endif
diff --git a/kernel/notifier.c b/kernel/notifier.c
index 1b019cbca594..c20782f07643 100644
--- a/kernel/notifier.c
+++ b/kernel/notifier.c
@@ -142,9 +142,9 @@ int atomic_notifier_chain_register(struct atomic_notifier_head *nh,
 	unsigned long flags;
 	int ret;
 
-	spin_lock_irqsave(&nh->lock, flags);
+	raw_spin_lock_irqsave(&nh->lock, flags);
 	ret = notifier_chain_register(&nh->head, n);
-	spin_unlock_irqrestore(&nh->lock, flags);
+	raw_spin_unlock_irqrestore(&nh->lock, flags);
 	return ret;
 }
 EXPORT_SYMBOL_GPL(atomic_notifier_chain_register);
@@ -164,9 +164,9 @@ int atomic_notifier_chain_unregister(struct atomic_notifier_head *nh,
 	unsigned long flags;
 	int ret;
 
-	spin_lock_irqsave(&nh->lock, flags);
+	raw_spin_lock_irqsave(&nh->lock, flags);
 	ret = notifier_chain_unregister(&nh->head, n);
-	spin_unlock_irqrestore(&nh->lock, flags);
+	raw_spin_unlock_irqrestore(&nh->lock, flags);
 	synchronize_rcu();
 	return ret;
 }
@@ -182,9 +182,9 @@ int atomic_notifier_call_chain_robust(struct atomic_notifier_head *nh,
 	 * Musn't use RCU; because then the notifier list can
 	 * change between the up and down traversal.
 	 */
-	spin_lock_irqsave(&nh->lock, flags);
+	raw_spin_lock_irqsave(&nh->lock, flags);
 	ret = notifier_call_chain_robust(&nh->head, val_up, val_down, v);
-	spin_unlock_irqrestore(&nh->lock, flags);
+	raw_spin_unlock_irqrestore(&nh->lock, flags);
 
 	return ret;
 }
diff --git a/kernel/panic.c b/kernel/panic.c
index 332736a72a58..a14e2f5a9f55 100644
--- a/kernel/panic.c
+++ b/kernel/panic.c
@@ -177,12 +177,28 @@ static void panic_print_sys_info(void)
 void panic(const char *fmt, ...)
 {
 	static char buf[1024];
+	va_list args2;
 	va_list args;
 	long i, i_next = 0, len;
 	int state = 0;
 	int old_cpu, this_cpu;
 	bool _crash_kexec_post_notifiers = crash_kexec_post_notifiers;
 
+	console_verbose();
+	pr_emerg("Kernel panic - not syncing:\n");
+	va_start(args2, fmt);
+	va_copy(args, args2);
+	vprintk(fmt, args2);
+	va_end(args2);
+#ifdef CONFIG_DEBUG_BUGVERBOSE
+	/*
+	 * Avoid nested stack-dumping if a panic occurs during oops processing
+	 */
+	if (!test_taint(TAINT_DIE) && oops_in_progress <= 1)
+		dump_stack();
+#endif
+	pr_flush(1000, true);
+
 	/*
 	 * Disable local interrupts. This will prevent panic_smp_self_stop
 	 * from deadlocking the first cpu that invokes the panic, since
@@ -213,24 +229,13 @@ void panic(const char *fmt, ...)
 	if (old_cpu != PANIC_CPU_INVALID && old_cpu != this_cpu)
 		panic_smp_self_stop();
 
-	console_verbose();
 	bust_spinlocks(1);
-	va_start(args, fmt);
 	len = vscnprintf(buf, sizeof(buf), fmt, args);
 	va_end(args);
 
 	if (len && buf[len - 1] == '\n')
 		buf[len - 1] = '\0';
 
-	pr_emerg("Kernel panic - not syncing: %s\n", buf);
-#ifdef CONFIG_DEBUG_BUGVERBOSE
-	/*
-	 * Avoid nested stack-dumping if a panic occurs during oops processing
-	 */
-	if (!test_taint(TAINT_DIE) && oops_in_progress <= 1)
-		dump_stack();
-#endif
-
 	/*
 	 * If kgdb is enabled, give it a chance to run before we stop all
 	 * the other CPUs or else we won't be able to debug processes left
@@ -247,7 +252,6 @@ void panic(const char *fmt, ...)
 	 * Bypass the panic_cpu check and call __crash_kexec directly.
 	 */
 	if (!_crash_kexec_post_notifiers) {
-		printk_safe_flush_on_panic();
 		__crash_kexec(NULL);
 
 		/*
@@ -271,8 +275,6 @@ void panic(const char *fmt, ...)
 	 */
 	atomic_notifier_call_chain(&panic_notifier_list, 0, buf);
 
-	/* Call flush even twice. It tries harder with a single online CPU */
-	printk_safe_flush_on_panic();
 	kmsg_dump(KMSG_DUMP_PANIC);
 
 	/*
@@ -542,9 +544,11 @@ static u64 oops_id;
 
 static int init_oops_id(void)
 {
+#ifndef CONFIG_PREEMPT_RT
 	if (!oops_id)
 		get_random_bytes(&oops_id, sizeof(oops_id));
 	else
+#endif
 		oops_id++;
 
 	return 0;
@@ -555,6 +559,7 @@ static void print_oops_end_marker(void)
 {
 	init_oops_id();
 	pr_warn("---[ end trace %016llx ]---\n", (unsigned long long)oops_id);
+	pr_flush(1000, true);
 }
 
 /*
diff --git a/kernel/printk/Makefile b/kernel/printk/Makefile
index eee3dc9b60a9..59cb24e25f00 100644
--- a/kernel/printk/Makefile
+++ b/kernel/printk/Makefile
@@ -1,5 +1,4 @@
 # SPDX-License-Identifier: GPL-2.0-only
 obj-y	= printk.o
-obj-$(CONFIG_PRINTK)	+= printk_safe.o
 obj-$(CONFIG_A11Y_BRAILLE_CONSOLE)	+= braille.o
 obj-$(CONFIG_PRINTK)	+= printk_ringbuffer.o
diff --git a/kernel/printk/internal.h b/kernel/printk/internal.h
deleted file mode 100644
index 3a8fd491758c..000000000000
--- a/kernel/printk/internal.h
+++ /dev/null
@@ -1,74 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0-or-later */
-/*
- * internal.h - printk internal definitions
- */
-#include <linux/percpu.h>
-
-#ifdef CONFIG_PRINTK
-
-#define PRINTK_SAFE_CONTEXT_MASK	0x007ffffff
-#define PRINTK_NMI_DIRECT_CONTEXT_MASK	0x008000000
-#define PRINTK_NMI_CONTEXT_MASK		0xff0000000
-
-#define PRINTK_NMI_CONTEXT_OFFSET	0x010000000
-
-extern raw_spinlock_t logbuf_lock;
-
-__printf(4, 0)
-int vprintk_store(int facility, int level,
-		  const struct dev_printk_info *dev_info,
-		  const char *fmt, va_list args);
-
-__printf(1, 0) int vprintk_default(const char *fmt, va_list args);
-__printf(1, 0) int vprintk_deferred(const char *fmt, va_list args);
-__printf(1, 0) int vprintk_func(const char *fmt, va_list args);
-void __printk_safe_enter(void);
-void __printk_safe_exit(void);
-
-void printk_safe_init(void);
-bool printk_percpu_data_ready(void);
-
-#define printk_safe_enter_irqsave(flags)	\
-	do {					\
-		local_irq_save(flags);		\
-		__printk_safe_enter();		\
-	} while (0)
-
-#define printk_safe_exit_irqrestore(flags)	\
-	do {					\
-		__printk_safe_exit();		\
-		local_irq_restore(flags);	\
-	} while (0)
-
-#define printk_safe_enter_irq()		\
-	do {					\
-		local_irq_disable();		\
-		__printk_safe_enter();		\
-	} while (0)
-
-#define printk_safe_exit_irq()			\
-	do {					\
-		__printk_safe_exit();		\
-		local_irq_enable();		\
-	} while (0)
-
-void defer_console_output(void);
-
-#else
-
-__printf(1, 0) int vprintk_func(const char *fmt, va_list args) { return 0; }
-
-/*
- * In !PRINTK builds we still export logbuf_lock spin_lock, console_sem
- * semaphore and some of console functions (console_unlock()/etc.), so
- * printk-safe must preserve the existing local IRQ guarantees.
- */
-#define printk_safe_enter_irqsave(flags) local_irq_save(flags)
-#define printk_safe_exit_irqrestore(flags) local_irq_restore(flags)
-
-#define printk_safe_enter_irq() local_irq_disable()
-#define printk_safe_exit_irq() local_irq_enable()
-
-static inline void printk_safe_init(void) { }
-static inline bool printk_percpu_data_ready(void) { return false; }
-#endif /* CONFIG_PRINTK */
diff --git a/kernel/printk/printk.c b/kernel/printk/printk.c
index d0df95346ab3..f56fd2e34cc7 100644
--- a/kernel/printk/printk.c
+++ b/kernel/printk/printk.c
@@ -44,6 +44,9 @@
 #include <linux/irq_work.h>
 #include <linux/ctype.h>
 #include <linux/uio.h>
+#include <linux/kthread.h>
+#include <linux/kdb.h>
+#include <linux/clocksource.h>
 #include <linux/sched/clock.h>
 #include <linux/sched/debug.h>
 #include <linux/sched/task_stack.h>
@@ -58,7 +61,6 @@
 #include "printk_ringbuffer.h"
 #include "console_cmdline.h"
 #include "braille.h"
-#include "internal.h"
 
 int console_printk[4] = {
 	CONSOLE_LOGLEVEL_DEFAULT,	/* console_loglevel */
@@ -225,19 +227,7 @@ static int nr_ext_console_drivers;
 
 static int __down_trylock_console_sem(unsigned long ip)
 {
-	int lock_failed;
-	unsigned long flags;
-
-	/*
-	 * Here and in __up_console_sem() we need to be in safe mode,
-	 * because spindump/WARN/etc from under console ->lock will
-	 * deadlock in printk()->down_trylock_console_sem() otherwise.
-	 */
-	printk_safe_enter_irqsave(flags);
-	lock_failed = down_trylock(&console_sem);
-	printk_safe_exit_irqrestore(flags);
-
-	if (lock_failed)
+	if (down_trylock(&console_sem))
 		return 1;
 	mutex_acquire(&console_lock_dep_map, 0, 1, ip);
 	return 0;
@@ -246,13 +236,9 @@ static int __down_trylock_console_sem(unsigned long ip)
 
 static void __up_console_sem(unsigned long ip)
 {
-	unsigned long flags;
-
 	mutex_release(&console_lock_dep_map, ip);
 
-	printk_safe_enter_irqsave(flags);
 	up(&console_sem);
-	printk_safe_exit_irqrestore(flags);
 }
 #define up_console_sem() __up_console_sem(_RET_IP_)
 
@@ -266,11 +252,6 @@ static void __up_console_sem(unsigned long ip)
  */
 static int console_locked, console_suspended;
 
-/*
- * If exclusive_console is non-NULL then only this console is to be printed to.
- */
-static struct console *exclusive_console;
-
 /*
  *	Array of consoles built from command line options (console=)
  */
@@ -355,61 +336,43 @@ enum log_flags {
 	LOG_CONT	= 8,	/* text is a fragment of a continuation line */
 };
 
-/*
- * The logbuf_lock protects kmsg buffer, indices, counters.  This can be taken
- * within the scheduler's rq lock. It must be released before calling
- * console_unlock() or anything else that might wake up a process.
- */
-DEFINE_RAW_SPINLOCK(logbuf_lock);
+#ifdef CONFIG_PRINTK
+/* syslog_lock protects syslog_* variables and write access to clear_seq. */
+static DEFINE_SPINLOCK(syslog_lock);
 
-/*
- * Helper macros to lock/unlock logbuf_lock and switch between
- * printk-safe/unsafe modes.
- */
-#define logbuf_lock_irq()				\
-	do {						\
-		printk_safe_enter_irq();		\
-		raw_spin_lock(&logbuf_lock);		\
-	} while (0)
-
-#define logbuf_unlock_irq()				\
-	do {						\
-		raw_spin_unlock(&logbuf_lock);		\
-		printk_safe_exit_irq();			\
-	} while (0)
-
-#define logbuf_lock_irqsave(flags)			\
-	do {						\
-		printk_safe_enter_irqsave(flags);	\
-		raw_spin_lock(&logbuf_lock);		\
-	} while (0)
-
-#define logbuf_unlock_irqrestore(flags)		\
-	do {						\
-		raw_spin_unlock(&logbuf_lock);		\
-		printk_safe_exit_irqrestore(flags);	\
-	} while (0)
+/* Set to enable sync mode. Once set, it is never cleared. */
+static bool sync_mode;
 
-#ifdef CONFIG_PRINTK
 DECLARE_WAIT_QUEUE_HEAD(log_wait);
+/* All 3 protected by @syslog_lock. */
 /* the next printk record to read by syslog(READ) or /proc/kmsg */
 static u64 syslog_seq;
 static size_t syslog_partial;
 static bool syslog_time;
 
-/* the next printk record to write to the console */
-static u64 console_seq;
-static u64 exclusive_console_stop_seq;
-static unsigned long console_dropped;
+struct latched_seq {
+	seqcount_latch_t	latch;
+	u64			val[2];
+};
 
-/* the next printk record to read after the last 'clear' command */
-static u64 clear_seq;
+/*
+ * The next printk record to read after the last 'clear' command. There are
+ * two copies (updated with seqcount_latch) so that reads can locklessly
+ * access a valid value. Writers are synchronized by @syslog_lock.
+ */
+static struct latched_seq clear_seq = {
+	.latch		= SEQCNT_LATCH_ZERO(clear_seq.latch),
+	.val[0]		= 0,
+	.val[1]		= 0,
+};
 
 #ifdef CONFIG_PRINTK_CALLER
 #define PREFIX_MAX		48
 #else
 #define PREFIX_MAX		32
 #endif
+
+/* the maximum size allowed to be reserved for a record */
 #define LOG_LINE_MAX		(1024 - PREFIX_MAX)
 
 #define LOG_LEVEL(v)		((v) & 0x07)
@@ -447,11 +410,36 @@ static struct printk_ringbuffer *prb = &printk_rb_static;
  */
 static bool __printk_percpu_data_ready __read_mostly;
 
-bool printk_percpu_data_ready(void)
+static bool printk_percpu_data_ready(void)
 {
 	return __printk_percpu_data_ready;
 }
 
+/* Must be called under syslog_lock. */
+static void latched_seq_write(struct latched_seq *ls, u64 val)
+{
+	raw_write_seqcount_latch(&ls->latch);
+	ls->val[0] = val;
+	raw_write_seqcount_latch(&ls->latch);
+	ls->val[1] = val;
+}
+
+/* Can be called from any context. */
+static u64 latched_seq_read_nolock(struct latched_seq *ls)
+{
+	unsigned int seq;
+	unsigned int idx;
+	u64 val;
+
+	do {
+		seq = raw_read_seqcount_latch(&ls->latch);
+		idx = seq & 0x1;
+		val = ls->val[idx];
+	} while (read_seqcount_latch_retry(&ls->latch, seq));
+
+	return val;
+}
+
 /* Return log buffer address */
 char *log_buf_addr_get(void)
 {
@@ -491,52 +479,6 @@ static void truncate_msg(u16 *text_len, u16 *trunc_msg_len)
 		*trunc_msg_len = 0;
 }
 
-/* insert record into the buffer, discard old ones, update heads */
-static int log_store(u32 caller_id, int facility, int level,
-		     enum log_flags flags, u64 ts_nsec,
-		     const struct dev_printk_info *dev_info,
-		     const char *text, u16 text_len)
-{
-	struct prb_reserved_entry e;
-	struct printk_record r;
-	u16 trunc_msg_len = 0;
-
-	prb_rec_init_wr(&r, text_len);
-
-	if (!prb_reserve(&e, prb, &r)) {
-		/* truncate the message if it is too long for empty buffer */
-		truncate_msg(&text_len, &trunc_msg_len);
-		prb_rec_init_wr(&r, text_len + trunc_msg_len);
-		/* survive when the log buffer is too small for trunc_msg */
-		if (!prb_reserve(&e, prb, &r))
-			return 0;
-	}
-
-	/* fill message */
-	memcpy(&r.text_buf[0], text, text_len);
-	if (trunc_msg_len)
-		memcpy(&r.text_buf[text_len], trunc_msg, trunc_msg_len);
-	r.info->text_len = text_len + trunc_msg_len;
-	r.info->facility = facility;
-	r.info->level = level & 7;
-	r.info->flags = flags & 0x1f;
-	if (ts_nsec > 0)
-		r.info->ts_nsec = ts_nsec;
-	else
-		r.info->ts_nsec = local_clock();
-	r.info->caller_id = caller_id;
-	if (dev_info)
-		memcpy(&r.info->dev_info, dev_info, sizeof(r.info->dev_info));
-
-	/* A message without a trailing newline can be continued. */
-	if (!(flags & LOG_NEWLINE))
-		prb_commit(&e);
-	else
-		prb_final_commit(&e);
-
-	return (text_len + trunc_msg_len);
-}
-
 int dmesg_restrict = IS_ENABLED(CONFIG_SECURITY_DMESG_RESTRICT);
 
 static int syslog_action_restricted(int type)
@@ -665,7 +607,7 @@ static ssize_t msg_print_ext_body(char *buf, size_t size,
 
 /* /dev/kmsg - userspace message inject/listen interface */
 struct devkmsg_user {
-	u64 seq;
+	atomic64_t seq;
 	struct ratelimit_state rs;
 	struct mutex lock;
 	char buf[CONSOLE_EXT_LOG_MAX];
@@ -766,27 +708,22 @@ static ssize_t devkmsg_read(struct file *file, char __user *buf,
 	if (ret)
 		return ret;
 
-	logbuf_lock_irq();
-	if (!prb_read_valid(prb, user->seq, r)) {
+	if (!prb_read_valid(prb, atomic64_read(&user->seq), r)) {
 		if (file->f_flags & O_NONBLOCK) {
 			ret = -EAGAIN;
-			logbuf_unlock_irq();
 			goto out;
 		}
 
-		logbuf_unlock_irq();
 		ret = wait_event_interruptible(log_wait,
-					prb_read_valid(prb, user->seq, r));
+				prb_read_valid(prb, atomic64_read(&user->seq), r));
 		if (ret)
 			goto out;
-		logbuf_lock_irq();
 	}
 
-	if (r->info->seq != user->seq) {
+	if (r->info->seq != atomic64_read(&user->seq)) {
 		/* our last seen message is gone, return error and reset */
-		user->seq = r->info->seq;
+		atomic64_set(&user->seq, r->info->seq);
 		ret = -EPIPE;
-		logbuf_unlock_irq();
 		goto out;
 	}
 
@@ -795,8 +732,7 @@ static ssize_t devkmsg_read(struct file *file, char __user *buf,
 				  &r->text_buf[0], r->info->text_len,
 				  &r->info->dev_info);
 
-	user->seq = r->info->seq + 1;
-	logbuf_unlock_irq();
+	atomic64_set(&user->seq, r->info->seq + 1);
 
 	if (len > count) {
 		ret = -EINVAL;
@@ -831,11 +767,10 @@ static loff_t devkmsg_llseek(struct file *file, loff_t offset, int whence)
 	if (offset)
 		return -ESPIPE;
 
-	logbuf_lock_irq();
 	switch (whence) {
 	case SEEK_SET:
 		/* the first record */
-		user->seq = prb_first_valid_seq(prb);
+		atomic64_set(&user->seq, prb_first_valid_seq(prb));
 		break;
 	case SEEK_DATA:
 		/*
@@ -843,16 +778,15 @@ static loff_t devkmsg_llseek(struct file *file, loff_t offset, int whence)
 		 * like issued by 'dmesg -c'. Reading /dev/kmsg itself
 		 * changes no global state, and does not clear anything.
 		 */
-		user->seq = clear_seq;
+		atomic64_set(&user->seq, latched_seq_read_nolock(&clear_seq));
 		break;
 	case SEEK_END:
 		/* after the last record */
-		user->seq = prb_next_seq(prb);
+		atomic64_set(&user->seq, prb_next_seq(prb));
 		break;
 	default:
 		ret = -EINVAL;
 	}
-	logbuf_unlock_irq();
 	return ret;
 }
 
@@ -867,15 +801,13 @@ static __poll_t devkmsg_poll(struct file *file, poll_table *wait)
 
 	poll_wait(file, &log_wait, wait);
 
-	logbuf_lock_irq();
-	if (prb_read_valid_info(prb, user->seq, &info, NULL)) {
+	if (prb_read_valid_info(prb, atomic64_read(&user->seq), &info, NULL)) {
 		/* return error when data has vanished underneath us */
-		if (info.seq != user->seq)
+		if (info.seq != atomic64_read(&user->seq))
 			ret = EPOLLIN|EPOLLRDNORM|EPOLLERR|EPOLLPRI;
 		else
 			ret = EPOLLIN|EPOLLRDNORM;
 	}
-	logbuf_unlock_irq();
 
 	return ret;
 }
@@ -908,9 +840,7 @@ static int devkmsg_open(struct inode *inode, struct file *file)
 	prb_rec_init_rd(&user->record, &user->info,
 			&user->text_buf[0], sizeof(user->text_buf));
 
-	logbuf_lock_irq();
-	user->seq = prb_first_valid_seq(prb);
-	logbuf_unlock_irq();
+	atomic64_set(&user->seq, prb_first_valid_seq(prb));
 
 	file->private_data = user;
 	return 0;
@@ -1002,6 +932,9 @@ void log_buf_vmcoreinfo_setup(void)
 
 	VMCOREINFO_SIZE(atomic_long_t);
 	VMCOREINFO_TYPE_OFFSET(atomic_long_t, counter);
+
+	VMCOREINFO_STRUCT_SIZE(latched_seq);
+	VMCOREINFO_OFFSET(latched_seq, val);
 }
 #endif
 
@@ -1073,9 +1006,6 @@ static inline void log_buf_add_cpu(void) {}
 
 static void __init set_percpu_data_ready(void)
 {
-	printk_safe_init();
-	/* Make sure we set this flag only after printk_safe() init is done */
-	barrier();
 	__printk_percpu_data_ready = true;
 }
 
@@ -1115,7 +1045,6 @@ void __init setup_log_buf(int early)
 	struct printk_record r;
 	size_t new_descs_size;
 	size_t new_infos_size;
-	unsigned long flags;
 	char *new_log_buf;
 	unsigned int free;
 	u64 seq;
@@ -1173,8 +1102,6 @@ void __init setup_log_buf(int early)
 		 new_descs, ilog2(new_descs_count),
 		 new_infos);
 
-	logbuf_lock_irqsave(flags);
-
 	log_buf_len = new_log_buf_len;
 	log_buf = new_log_buf;
 	new_log_buf_len = 0;
@@ -1190,8 +1117,6 @@ void __init setup_log_buf(int early)
 	 */
 	prb = &printk_rb_dynamic;
 
-	logbuf_unlock_irqrestore(flags);
-
 	if (seq != prb_next_seq(&printk_rb_static)) {
 		pr_err("dropped %llu messages\n",
 		       prb_next_seq(&printk_rb_static) - seq);
@@ -1468,6 +1393,50 @@ static size_t get_record_print_text_size(struct printk_info *info,
 	return ((prefix_len * line_count) + info->text_len + 1);
 }
 
+/*
+ * Beginning with @start_seq, find the first record where it and all following
+ * records up to (but not including) @max_seq fit into @size.
+ *
+ * @max_seq is simply an upper bound and does not need to exist. If the caller
+ * does not require an upper bound, -1 can be used for @max_seq.
+ */
+static u64 find_first_fitting_seq(u64 start_seq, u64 max_seq, size_t size,
+				  bool syslog, bool time)
+{
+	struct printk_info info;
+	unsigned int line_count;
+	size_t len = 0;
+	u64 seq;
+
+	/* Determine the size of the records up to @max_seq. */
+	prb_for_each_info(start_seq, prb, seq, &info, &line_count) {
+		if (info.seq >= max_seq)
+			break;
+		len += get_record_print_text_size(&info, line_count, syslog, time);
+	}
+
+	/*
+	 * Adjust the upper bound for the next loop to avoid subtracting
+	 * lengths that were never added.
+	 */
+	if (seq < max_seq)
+		max_seq = seq;
+
+	/*
+	 * Move first record forward until length fits into the buffer. Ignore
+	 * newest messages that were not counted in the above cycle. Messages
+	 * might appear and get lost in the meantime. This is a best effort
+	 * that prevents an infinite loop that could occur with a retry.
+	 */
+	prb_for_each_info(start_seq, prb, seq, &info, &line_count) {
+		if (len <= size || info.seq >= max_seq)
+			break;
+		len -= get_record_print_text_size(&info, line_count, syslog, time);
+	}
+
+	return seq;
+}
+
 static int syslog_print(char __user *buf, int size)
 {
 	struct printk_info info;
@@ -1475,19 +1444,19 @@ static int syslog_print(char __user *buf, int size)
 	char *text;
 	int len = 0;
 
-	text = kmalloc(LOG_LINE_MAX + PREFIX_MAX, GFP_KERNEL);
+	text = kmalloc(CONSOLE_LOG_MAX, GFP_KERNEL);
 	if (!text)
 		return -ENOMEM;
 
-	prb_rec_init_rd(&r, &info, text, LOG_LINE_MAX + PREFIX_MAX);
+	prb_rec_init_rd(&r, &info, text, CONSOLE_LOG_MAX);
 
 	while (size > 0) {
 		size_t n;
 		size_t skip;
 
-		logbuf_lock_irq();
+		spin_lock_irq(&syslog_lock);
 		if (!prb_read_valid(prb, syslog_seq, &r)) {
-			logbuf_unlock_irq();
+			spin_unlock_irq(&syslog_lock);
 			break;
 		}
 		if (r.info->seq != syslog_seq) {
@@ -1516,7 +1485,7 @@ static int syslog_print(char __user *buf, int size)
 			syslog_partial += n;
 		} else
 			n = 0;
-		logbuf_unlock_irq();
+		spin_unlock_irq(&syslog_lock);
 
 		if (!n)
 			break;
@@ -1539,34 +1508,25 @@ static int syslog_print(char __user *buf, int size)
 static int syslog_print_all(char __user *buf, int size, bool clear)
 {
 	struct printk_info info;
-	unsigned int line_count;
 	struct printk_record r;
 	char *text;
 	int len = 0;
 	u64 seq;
 	bool time;
 
-	text = kmalloc(LOG_LINE_MAX + PREFIX_MAX, GFP_KERNEL);
+	text = kmalloc(CONSOLE_LOG_MAX, GFP_KERNEL);
 	if (!text)
 		return -ENOMEM;
 
 	time = printk_time;
-	logbuf_lock_irq();
 	/*
 	 * Find first record that fits, including all following records,
 	 * into the user-provided buffer for this dump.
 	 */
-	prb_for_each_info(clear_seq, prb, seq, &info, &line_count)
-		len += get_record_print_text_size(&info, line_count, true, time);
-
-	/* move first record forward until length fits into the buffer */
-	prb_for_each_info(clear_seq, prb, seq, &info, &line_count) {
-		if (len <= size)
-			break;
-		len -= get_record_print_text_size(&info, line_count, true, time);
-	}
+	seq = find_first_fitting_seq(latched_seq_read_nolock(&clear_seq), -1,
+				     size, true, time);
 
-	prb_rec_init_rd(&r, &info, text, LOG_LINE_MAX + PREFIX_MAX);
+	prb_rec_init_rd(&r, &info, text, CONSOLE_LOG_MAX);
 
 	len = 0;
 	prb_for_each_record(seq, prb, seq, &r) {
@@ -1579,20 +1539,20 @@ static int syslog_print_all(char __user *buf, int size, bool clear)
 			break;
 		}
 
-		logbuf_unlock_irq();
 		if (copy_to_user(buf + len, text, textlen))
 			len = -EFAULT;
 		else
 			len += textlen;
-		logbuf_lock_irq();
 
 		if (len < 0)
 			break;
 	}
 
-	if (clear)
-		clear_seq = seq;
-	logbuf_unlock_irq();
+	if (clear) {
+		spin_lock_irq(&syslog_lock);
+		latched_seq_write(&clear_seq, seq);
+		spin_unlock_irq(&syslog_lock);
+	}
 
 	kfree(text);
 	return len;
@@ -1600,9 +1560,21 @@ static int syslog_print_all(char __user *buf, int size, bool clear)
 
 static void syslog_clear(void)
 {
-	logbuf_lock_irq();
-	clear_seq = prb_next_seq(prb);
-	logbuf_unlock_irq();
+	spin_lock_irq(&syslog_lock);
+	latched_seq_write(&clear_seq, prb_next_seq(prb));
+	spin_unlock_irq(&syslog_lock);
+}
+
+/* Return a consistent copy of @syslog_seq. */
+static u64 read_syslog_seq_irq(void)
+{
+	u64 seq;
+
+	spin_lock_irq(&syslog_lock);
+	seq = syslog_seq;
+	spin_unlock_irq(&syslog_lock);
+
+	return seq;
 }
 
 int do_syslog(int type, char __user *buf, int len, int source)
@@ -1628,8 +1600,9 @@ int do_syslog(int type, char __user *buf, int len, int source)
 			return 0;
 		if (!access_ok(buf, len))
 			return -EFAULT;
+
 		error = wait_event_interruptible(log_wait,
-				prb_read_valid(prb, syslog_seq, NULL));
+				prb_read_valid(prb, read_syslog_seq_irq(), NULL));
 		if (error)
 			return error;
 		error = syslog_print(buf, len);
@@ -1677,10 +1650,10 @@ int do_syslog(int type, char __user *buf, int len, int source)
 		break;
 	/* Number of chars in the log buffer */
 	case SYSLOG_ACTION_SIZE_UNREAD:
-		logbuf_lock_irq();
+		spin_lock_irq(&syslog_lock);
 		if (!prb_read_valid_info(prb, syslog_seq, &info, NULL)) {
 			/* No unread messages. */
-			logbuf_unlock_irq();
+			spin_unlock_irq(&syslog_lock);
 			return 0;
 		}
 		if (info.seq != syslog_seq) {
@@ -1708,7 +1681,7 @@ int do_syslog(int type, char __user *buf, int len, int source)
 			}
 			error -= syslog_partial;
 		}
-		logbuf_unlock_irq();
+		spin_unlock_irq(&syslog_lock);
 		break;
 	/* Size of the log buffer */
 	case SYSLOG_ACTION_SIZE_BUFFER:
@@ -1727,202 +1700,172 @@ SYSCALL_DEFINE3(syslog, int, type, char __user *, buf, int, len)
 	return do_syslog(type, buf, len, SYSLOG_FROM_READER);
 }
 
-/*
- * Special console_lock variants that help to reduce the risk of soft-lockups.
- * They allow to pass console_lock to another printk() call using a busy wait.
- */
-
-#ifdef CONFIG_LOCKDEP
-static struct lockdep_map console_owner_dep_map = {
-	.name = "console_owner"
-};
-#endif
-
-static DEFINE_RAW_SPINLOCK(console_owner_lock);
-static struct task_struct *console_owner;
-static bool console_waiter;
+int printk_delay_msec __read_mostly;
 
-/**
- * console_lock_spinning_enable - mark beginning of code where another
- *	thread might safely busy wait
- *
- * This basically converts console_lock into a spinlock. This marks
- * the section where the console_lock owner can not sleep, because
- * there may be a waiter spinning (like a spinlock). Also it must be
- * ready to hand over the lock at the end of the section.
- */
-static void console_lock_spinning_enable(void)
+static inline void printk_delay(int level)
 {
-	raw_spin_lock(&console_owner_lock);
-	console_owner = current;
-	raw_spin_unlock(&console_owner_lock);
+	boot_delay_msec(level);
+
+	if (unlikely(printk_delay_msec)) {
+		int m = printk_delay_msec;
 
-	/* The waiter may spin on us after setting console_owner */
-	spin_acquire(&console_owner_dep_map, 0, 0, _THIS_IP_);
+		while (m--) {
+			mdelay(1);
+			touch_nmi_watchdog();
+		}
+	}
 }
 
-/**
- * console_lock_spinning_disable_and_check - mark end of code where another
- *	thread was able to busy wait and check if there is a waiter
- *
- * This is called at the end of the section where spinning is allowed.
- * It has two functions. First, it is a signal that it is no longer
- * safe to start busy waiting for the lock. Second, it checks if
- * there is a busy waiter and passes the lock rights to her.
- *
- * Important: Callers lose the lock if there was a busy waiter.
- *	They must not touch items synchronized by console_lock
- *	in this case.
- *
- * Return: 1 if the lock rights were passed, 0 otherwise.
- */
-static int console_lock_spinning_disable_and_check(void)
+static bool kernel_sync_mode(void)
 {
-	int waiter;
+	return (oops_in_progress || sync_mode);
+}
 
-	raw_spin_lock(&console_owner_lock);
-	waiter = READ_ONCE(console_waiter);
-	console_owner = NULL;
-	raw_spin_unlock(&console_owner_lock);
+static bool console_can_sync(struct console *con)
+{
+	if (!(con->flags & CON_ENABLED))
+		return false;
+	if (con->write_atomic && kernel_sync_mode())
+		return true;
+	if (con->write_atomic && (con->flags & CON_HANDOVER) && !con->thread)
+		return true;
+	if (con->write && (con->flags & CON_BOOT) && !con->thread)
+		return true;
+	return false;
+}
 
-	if (!waiter) {
-		spin_release(&console_owner_dep_map, _THIS_IP_);
-		return 0;
-	}
+static bool call_sync_console_driver(struct console *con, const char *text, size_t text_len)
+{
+	if (!(con->flags & CON_ENABLED))
+		return false;
+	if (con->write_atomic && kernel_sync_mode())
+		con->write_atomic(con, text, text_len);
+	else if (con->write_atomic && (con->flags & CON_HANDOVER) && !con->thread)
+		con->write_atomic(con, text, text_len);
+	else if (con->write && (con->flags & CON_BOOT) && !con->thread)
+		con->write(con, text, text_len);
+	else
+		return false;
 
-	/* The waiter is now free to continue */
-	WRITE_ONCE(console_waiter, false);
+	return true;
+}
 
-	spin_release(&console_owner_dep_map, _THIS_IP_);
+static bool have_atomic_console(void)
+{
+	struct console *con;
 
-	/*
-	 * Hand off console_lock to waiter. The waiter will perform
-	 * the up(). After this, the waiter is the console_lock owner.
-	 */
-	mutex_release(&console_lock_dep_map, _THIS_IP_);
-	return 1;
+	for_each_console(con) {
+		if (!(con->flags & CON_ENABLED))
+			continue;
+		if (con->write_atomic)
+			return true;
+	}
+	return false;
 }
 
-/**
- * console_trylock_spinning - try to get console_lock by busy waiting
- *
- * This allows to busy wait for the console_lock when the current
- * owner is running in specially marked sections. It means that
- * the current owner is running and cannot reschedule until it
- * is ready to lose the lock.
- *
- * Return: 1 if we got the lock, 0 othrewise
- */
-static int console_trylock_spinning(void)
+static bool print_sync(struct console *con, u64 *seq)
 {
-	struct task_struct *owner = NULL;
-	bool waiter;
-	bool spin = false;
-	unsigned long flags;
+	struct printk_info info;
+	struct printk_record r;
+	size_t text_len;
 
-	if (console_trylock())
-		return 1;
+	prb_rec_init_rd(&r, &info, &con->sync_buf[0], sizeof(con->sync_buf));
 
-	printk_safe_enter_irqsave(flags);
+	if (!prb_read_valid(prb, *seq, &r))
+		return false;
 
-	raw_spin_lock(&console_owner_lock);
-	owner = READ_ONCE(console_owner);
-	waiter = READ_ONCE(console_waiter);
-	if (!waiter && owner && owner != current) {
-		WRITE_ONCE(console_waiter, true);
-		spin = true;
-	}
-	raw_spin_unlock(&console_owner_lock);
+	text_len = record_print_text(&r, console_msg_format & MSG_FORMAT_SYSLOG, printk_time);
 
-	/*
-	 * If there is an active printk() writing to the
-	 * consoles, instead of having it write our data too,
-	 * see if we can offload that load from the active
-	 * printer, and do some printing ourselves.
-	 * Go into a spin only if there isn't already a waiter
-	 * spinning, and there is an active printer, and
-	 * that active printer isn't us (recursive printk?).
-	 */
-	if (!spin) {
-		printk_safe_exit_irqrestore(flags);
-		return 0;
-	}
+	if (!call_sync_console_driver(con, &con->sync_buf[0], text_len))
+		return false;
 
-	/* We spin waiting for the owner to release us */
-	spin_acquire(&console_owner_dep_map, 0, 0, _THIS_IP_);
-	/* Owner will clear console_waiter on hand off */
-	while (READ_ONCE(console_waiter))
-		cpu_relax();
-	spin_release(&console_owner_dep_map, _THIS_IP_);
+	*seq = r.info->seq;
 
-	printk_safe_exit_irqrestore(flags);
-	/*
-	 * The owner passed the console lock to us.
-	 * Since we did not spin on console lock, annotate
-	 * this as a trylock. Otherwise lockdep will
-	 * complain.
-	 */
-	mutex_acquire(&console_lock_dep_map, 0, 1, _THIS_IP_);
+	touch_softlockup_watchdog_sync();
+	clocksource_touch_watchdog();
+	rcu_cpu_stall_reset();
+	touch_nmi_watchdog();
 
-	return 1;
+	if (text_len)
+		printk_delay(r.info->level);
+
+	return true;
 }
 
-/*
- * Call the console drivers, asking them to write out
- * log_buf[start] to log_buf[end - 1].
- * The console_lock must be held.
- */
-static void call_console_drivers(const char *ext_text, size_t ext_len,
-				 const char *text, size_t len)
+static void print_sync_until(struct console *con, u64 seq)
 {
-	static char dropped_text[64];
-	size_t dropped_len = 0;
-	struct console *con;
+	unsigned int flags;
+	u64 printk_seq;
+
+	console_atomic_lock(&flags);
+	for (;;) {
+		printk_seq = atomic64_read(&con->printk_seq);
+		if (printk_seq >= seq)
+			break;
+		if (!print_sync(con, &printk_seq))
+			break;
+		atomic64_set(&con->printk_seq, printk_seq + 1);
+	}
+	console_atomic_unlock(flags);
+}
 
-	trace_console_rcuidle(text, len);
+#ifdef CONFIG_PRINTK_NMI
+#define NUM_RECURSION_CTX 2
+#else
+#define NUM_RECURSION_CTX 1
+#endif
 
-	if (!console_drivers)
-		return;
+struct printk_recursion {
+	char	count[NUM_RECURSION_CTX];
+};
 
-	if (console_dropped) {
-		dropped_len = snprintf(dropped_text, sizeof(dropped_text),
-				       "** %lu printk messages dropped **\n",
-				       console_dropped);
-		console_dropped = 0;
-	}
+static DEFINE_PER_CPU(struct printk_recursion, percpu_printk_recursion);
+static char printk_recursion_count[NUM_RECURSION_CTX];
 
-	for_each_console(con) {
-		if (exclusive_console && con != exclusive_console)
-			continue;
-		if (!(con->flags & CON_ENABLED))
-			continue;
-		if (!con->write)
-			continue;
-		if (!cpu_online(smp_processor_id()) &&
-		    !(con->flags & CON_ANYTIME))
-			continue;
-		if (con->flags & CON_EXTENDED)
-			con->write(con, ext_text, ext_len);
-		else {
-			if (dropped_len)
-				con->write(con, dropped_text, dropped_len);
-			con->write(con, text, len);
-		}
+static char *printk_recursion_counter(void)
+{
+	struct printk_recursion *rec;
+	char *count;
+
+	if (!printk_percpu_data_ready()) {
+		count = &printk_recursion_count[0];
+	} else {
+		rec = this_cpu_ptr(&percpu_printk_recursion);
+
+		count = &rec->count[0];
 	}
-}
 
-int printk_delay_msec __read_mostly;
+#ifdef CONFIG_PRINTK_NMI
+	if (in_nmi())
+		count++;
+#endif
+
+	return count;
+}
 
-static inline void printk_delay(void)
+static bool printk_enter_irqsave(unsigned long *flags)
 {
-	if (unlikely(printk_delay_msec)) {
-		int m = printk_delay_msec;
+	char *count;
 
-		while (m--) {
-			mdelay(1);
-			touch_nmi_watchdog();
-		}
+	local_irq_save(*flags);
+	count = printk_recursion_counter();
+	/* Only 1 level of recursion allowed. */
+	if (*count > 1) {
+		local_irq_restore(*flags);
+		return false;
 	}
+	(*count)++;
+
+	return true;
+}
+
+static void printk_exit_irqrestore(unsigned long flags)
+{
+	char *count;
+
+	count = printk_recursion_counter();
+	(*count)--;
+	local_irq_restore(flags);
 }
 
 static inline u32 printk_caller_id(void)
@@ -1931,144 +1874,248 @@ static inline u32 printk_caller_id(void)
 		0x80000000 + raw_smp_processor_id();
 }
 
-static size_t log_output(int facility, int level, enum log_flags lflags,
-			 const struct dev_printk_info *dev_info,
-			 char *text, size_t text_len)
+/**
+ * parse_prefix - Parse level and control flags.
+ *
+ * @text:     The terminated text message.
+ * @level:    A pointer to the current level value, will be updated.
+ * @lflags:   A pointer to the current log flags, will be updated.
+ *
+ * @level may be NULL if the caller is not interested in the parsed value.
+ * Otherwise the variable pointed to by @level must be set to
+ * LOGLEVEL_DEFAULT in order to be updated with the parsed value.
+ *
+ * @lflags may be NULL if the caller is not interested in the parsed value.
+ * Otherwise the variable pointed to by @lflags will be OR'd with the parsed
+ * value.
+ *
+ * Return: The length of the parsed level and control flags.
+ */
+static u16 parse_prefix(char *text, int *level, enum log_flags *lflags)
 {
-	const u32 caller_id = printk_caller_id();
+	u16 prefix_len = 0;
+	int kern_level;
 
-	if (lflags & LOG_CONT) {
-		struct prb_reserved_entry e;
-		struct printk_record r;
+	while (*text) {
+		kern_level = printk_get_level(text);
+		if (!kern_level)
+			break;
 
-		prb_rec_init_wr(&r, text_len);
-		if (prb_reserve_in_last(&e, prb, &r, caller_id, LOG_LINE_MAX)) {
-			memcpy(&r.text_buf[r.info->text_len], text, text_len);
-			r.info->text_len += text_len;
-			if (lflags & LOG_NEWLINE) {
-				r.info->flags |= LOG_NEWLINE;
-				prb_final_commit(&e);
-			} else {
-				prb_commit(&e);
-			}
-			return text_len;
+		switch (kern_level) {
+		case '0' ... '7':
+			if (level && *level == LOGLEVEL_DEFAULT)
+				*level = kern_level - '0';
+			break;
+		case 'c':	/* KERN_CONT */
+			if (lflags)
+				*lflags |= LOG_CONT;
 		}
+
+		prefix_len += 2;
+		text += 2;
 	}
 
-	/* Store it in the record log */
-	return log_store(caller_id, facility, level, lflags, 0,
-			 dev_info, text, text_len);
+	return prefix_len;
 }
 
-/* Must be called under logbuf_lock. */
-int vprintk_store(int facility, int level,
-		  const struct dev_printk_info *dev_info,
-		  const char *fmt, va_list args)
+static u16 printk_sprint(char *text, u16 size, int facility, enum log_flags *lflags,
+			 const char *fmt, va_list args)
 {
-	static char textbuf[LOG_LINE_MAX];
-	char *text = textbuf;
-	size_t text_len;
-	enum log_flags lflags = 0;
+	u16 text_len;
 
-	/*
-	 * The printf needs to come first; we need the syslog
-	 * prefix which might be passed-in as a parameter.
-	 */
-	text_len = vscnprintf(text, sizeof(textbuf), fmt, args);
+	text_len = vscnprintf(text, size, fmt, args);
 
-	/* mark and strip a trailing newline */
-	if (text_len && text[text_len-1] == '\n') {
+	/* Mark and strip a trailing newline. */
+	if (text_len && text[text_len - 1] == '\n') {
 		text_len--;
-		lflags |= LOG_NEWLINE;
+		*lflags |= LOG_NEWLINE;
 	}
 
-	/* strip kernel syslog prefix and extract log level or control flags */
+	/* Strip log level and control flags. */
 	if (facility == 0) {
-		int kern_level;
-
-		while ((kern_level = printk_get_level(text)) != 0) {
-			switch (kern_level) {
-			case '0' ... '7':
-				if (level == LOGLEVEL_DEFAULT)
-					level = kern_level - '0';
-				break;
-			case 'c':	/* KERN_CONT */
-				lflags |= LOG_CONT;
-			}
+		u16 prefix_len;
 
-			text_len -= 2;
-			text += 2;
+		prefix_len = parse_prefix(text, NULL, NULL);
+		if (prefix_len) {
+			text_len -= prefix_len;
+			memmove(text, text + prefix_len, text_len);
 		}
 	}
 
-	if (level == LOGLEVEL_DEFAULT)
-		level = default_message_loglevel;
-
-	if (dev_info)
-		lflags |= LOG_NEWLINE;
-
-	return log_output(facility, level, lflags, dev_info, text, text_len);
+	return text_len;
 }
 
-asmlinkage int vprintk_emit(int facility, int level,
-			    const struct dev_printk_info *dev_info,
-			    const char *fmt, va_list args)
+__printf(4, 0)
+static int vprintk_store(int facility, int level,
+			 const struct dev_printk_info *dev_info,
+			 const char *fmt, va_list args)
 {
-	int printed_len;
-	bool in_sched = false;
-	unsigned long flags;
-
-	/* Suppress unimportant messages after panic happens */
-	if (unlikely(suppress_printk))
+	const u32 caller_id = printk_caller_id();
+	struct prb_reserved_entry e;
+	enum log_flags lflags = 0;
+	bool final_commit = false;
+	struct printk_record r;
+	unsigned long irqflags;
+	u16 trunc_msg_len = 0;
+	char prefix_buf[8];
+	u16 reserve_size;
+	va_list args2;
+	u16 text_len;
+	int ret = 0;
+	u64 ts_nsec;
+	u64 seq;
+
+	/*
+	 * Since the duration of printk() can vary depending on the message
+	 * and state of the ringbuffer, grab the timestamp now so that it is
+	 * close to the call of printk(). This provides a more deterministic
+	 * timestamp with respect to the caller.
+	 */
+	ts_nsec = local_clock();
+
+	if (!printk_enter_irqsave(&irqflags))
 		return 0;
 
-	if (level == LOGLEVEL_SCHED) {
-		level = LOGLEVEL_DEFAULT;
-		in_sched = true;
+	/*
+	 * The sprintf needs to come first since the syslog prefix might be
+	 * passed in as a parameter. An extra byte must be reserved so that
+	 * later the vscnprintf() into the reserved buffer has room for the
+	 * terminating '\0', which is not counted by vsnprintf().
+	 */
+	va_copy(args2, args);
+	reserve_size = vsnprintf(&prefix_buf[0], sizeof(prefix_buf), fmt, args2) + 1;
+	va_end(args2);
+
+	if (reserve_size > LOG_LINE_MAX)
+		reserve_size = LOG_LINE_MAX;
+
+	/* Extract log level or control flags. */
+	if (facility == 0)
+		parse_prefix(&prefix_buf[0], &level, &lflags);
+
+	if (level == LOGLEVEL_DEFAULT)
+		level = default_message_loglevel;
+
+	if (dev_info)
+		lflags |= LOG_NEWLINE;
+
+	if (lflags & LOG_CONT) {
+		prb_rec_init_wr(&r, reserve_size);
+		if (prb_reserve_in_last(&e, prb, &r, caller_id, LOG_LINE_MAX)) {
+			seq = r.info->seq;
+			text_len = printk_sprint(&r.text_buf[r.info->text_len], reserve_size,
+						 facility, &lflags, fmt, args);
+			r.info->text_len += text_len;
+
+			if (lflags & LOG_NEWLINE) {
+				r.info->flags |= LOG_NEWLINE;
+				prb_final_commit(&e);
+				final_commit = true;
+			} else {
+				prb_commit(&e);
+			}
+
+			ret = text_len;
+			goto out;
+		}
 	}
 
-	boot_delay_msec(level);
-	printk_delay();
+	/*
+	 * Explicitly initialize the record before every prb_reserve() call.
+	 * prb_reserve_in_last() and prb_reserve() purposely invalidate the
+	 * structure when they fail.
+	 */
+	prb_rec_init_wr(&r, reserve_size);
+	if (!prb_reserve(&e, prb, &r)) {
+		/* truncate the message if it is too long for empty buffer */
+		truncate_msg(&reserve_size, &trunc_msg_len);
 
-	/* This stops the holder of console_sem just where we want him */
-	logbuf_lock_irqsave(flags);
-	printed_len = vprintk_store(facility, level, dev_info, fmt, args);
-	logbuf_unlock_irqrestore(flags);
+		prb_rec_init_wr(&r, reserve_size + trunc_msg_len);
+		if (!prb_reserve(&e, prb, &r))
+			goto out;
+	}
 
-	/* If called from the scheduler, we can not call up(). */
-	if (!in_sched) {
-		/*
-		 * Disable preemption to avoid being preempted while holding
-		 * console_sem which would prevent anyone from printing to
-		 * console
-		 */
-		preempt_disable();
-		/*
-		 * Try to acquire and then immediately release the console
-		 * semaphore.  The release will print out buffers and wake up
-		 * /dev/kmsg and syslog() users.
-		 */
-		if (console_trylock_spinning())
-			console_unlock();
-		preempt_enable();
+	seq = r.info->seq;
+
+	/* fill message */
+	text_len = printk_sprint(&r.text_buf[0], reserve_size, facility, &lflags, fmt, args);
+	if (trunc_msg_len)
+		memcpy(&r.text_buf[text_len], trunc_msg, trunc_msg_len);
+	r.info->text_len = text_len + trunc_msg_len;
+	r.info->facility = facility;
+	r.info->level = level & 7;
+	r.info->flags = lflags & 0x1f;
+	r.info->ts_nsec = ts_nsec;
+	r.info->caller_id = caller_id;
+	if (dev_info)
+		memcpy(&r.info->dev_info, dev_info, sizeof(r.info->dev_info));
+
+	/* A message without a trailing newline can be continued. */
+	if (!(lflags & LOG_NEWLINE)) {
+		prb_commit(&e);
+	} else {
+		prb_final_commit(&e);
+		final_commit = true;
 	}
 
+	ret = text_len + trunc_msg_len;
+out:
+	/* only the kernel may perform synchronous printing */
+	if (facility == 0 && final_commit) {
+		struct console *con;
+
+		for_each_console(con) {
+			if (console_can_sync(con))
+				print_sync_until(con, seq + 1);
+		}
+	}
+
+	printk_exit_irqrestore(irqflags);
+	return ret;
+}
+
+asmlinkage int vprintk_emit(int facility, int level,
+			    const struct dev_printk_info *dev_info,
+			    const char *fmt, va_list args)
+{
+	int printed_len;
+
+	/* Suppress unimportant messages after panic happens */
+	if (unlikely(suppress_printk))
+		return 0;
+
+	if (level == LOGLEVEL_SCHED)
+		level = LOGLEVEL_DEFAULT;
+
+	printed_len = vprintk_store(facility, level, dev_info, fmt, args);
+
 	wake_up_klogd();
 	return printed_len;
 }
 EXPORT_SYMBOL(vprintk_emit);
 
-asmlinkage int vprintk(const char *fmt, va_list args)
+__printf(1, 0)
+static int vprintk_default(const char *fmt, va_list args)
 {
-	return vprintk_func(fmt, args);
+	return vprintk_emit(0, LOGLEVEL_DEFAULT, NULL, fmt, args);
 }
-EXPORT_SYMBOL(vprintk);
 
-int vprintk_default(const char *fmt, va_list args)
+__printf(1, 0)
+static int vprintk_func(const char *fmt, va_list args)
 {
-	return vprintk_emit(0, LOGLEVEL_DEFAULT, NULL, fmt, args);
+#ifdef CONFIG_KGDB_KDB
+	/* Allow to pass printk() to kdb but avoid a recursion. */
+	if (unlikely(kdb_trap_printk && kdb_printf_cpu < 0))
+		return vkdb_printf(KDB_MSGSRC_PRINTK, fmt, args);
+#endif
+	return vprintk_default(fmt, args);
+}
+
+asmlinkage int vprintk(const char *fmt, va_list args)
+{
+	return vprintk_func(fmt, args);
 }
-EXPORT_SYMBOL_GPL(vprintk_default);
+EXPORT_SYMBOL(vprintk);
 
 /**
  * printk - print a kernel message
@@ -2104,38 +2151,158 @@ asmlinkage __visible int printk(const char *fmt, ...)
 }
 EXPORT_SYMBOL(printk);
 
-#else /* CONFIG_PRINTK */
+static int printk_kthread_func(void *data)
+{
+	struct console *con = data;
+	unsigned long dropped = 0;
+	char *dropped_text = NULL;
+	struct printk_info info;
+	struct printk_record r;
+	char *ext_text = NULL;
+	size_t dropped_len;
+	int ret = -ENOMEM;
+	char *text = NULL;
+	char *write_text;
+	u64 printk_seq;
+	size_t len;
+	int error;
+	u64 seq;
 
-#define LOG_LINE_MAX		0
-#define PREFIX_MAX		0
-#define printk_time		false
+	if (con->flags & CON_EXTENDED) {
+		ext_text = kmalloc(CONSOLE_EXT_LOG_MAX, GFP_KERNEL);
+		if (!ext_text)
+			goto out;
+	}
+	text = kmalloc(LOG_LINE_MAX + PREFIX_MAX, GFP_KERNEL);
+	dropped_text = kmalloc(64, GFP_KERNEL);
+	if (!text || !dropped_text)
+		goto out;
 
-#define prb_read_valid(rb, seq, r)	false
-#define prb_first_valid_seq(rb)		0
+	if (con->flags & CON_EXTENDED)
+		write_text = ext_text;
+	else
+		write_text = text;
 
-static u64 syslog_seq;
-static u64 console_seq;
-static u64 exclusive_console_stop_seq;
-static unsigned long console_dropped;
+	seq = atomic64_read(&con->printk_seq);
+
+	prb_rec_init_rd(&r, &info, text, LOG_LINE_MAX + PREFIX_MAX);
+
+	for (;;) {
+		error = wait_event_interruptible(log_wait,
+				prb_read_valid(prb, seq, &r) || kthread_should_stop());
 
-static size_t record_print_text(const struct printk_record *r,
-				bool syslog, bool time)
+		if (kthread_should_stop())
+			break;
+
+		if (error)
+			continue;
+
+		if (seq != r.info->seq) {
+			dropped += r.info->seq - seq;
+			seq = r.info->seq;
+		}
+
+		seq++;
+
+		if (!(con->flags & CON_ENABLED))
+			continue;
+
+		if (suppress_message_printing(r.info->level))
+			continue;
+
+		if (con->flags & CON_EXTENDED) {
+			len = info_print_ext_header(ext_text,
+				CONSOLE_EXT_LOG_MAX,
+				r.info);
+			len += msg_print_ext_body(ext_text + len,
+				CONSOLE_EXT_LOG_MAX - len,
+				&r.text_buf[0], r.info->text_len,
+				&r.info->dev_info);
+		} else {
+			len = record_print_text(&r,
+				console_msg_format & MSG_FORMAT_SYSLOG,
+				printk_time);
+		}
+
+		printk_seq = atomic64_read(&con->printk_seq);
+
+		console_lock();
+		console_may_schedule = 0;
+
+		if (kernel_sync_mode() && con->write_atomic) {
+			console_unlock();
+			break;
+		}
+
+		if (!(con->flags & CON_EXTENDED) && dropped) {
+			dropped_len = snprintf(dropped_text, 64,
+					       "** %lu printk messages dropped **\n",
+					       dropped);
+			dropped = 0;
+
+			con->write(con, dropped_text, dropped_len);
+			printk_delay(r.info->level);
+		}
+
+		con->write(con, write_text, len);
+		if (len)
+			printk_delay(r.info->level);
+
+		atomic64_cmpxchg_relaxed(&con->printk_seq, printk_seq, seq);
+
+		console_unlock();
+	}
+out:
+	kfree(dropped_text);
+	kfree(text);
+	kfree(ext_text);
+	pr_info("%sconsole [%s%d]: printing thread stopped\n",
+		(con->flags & CON_BOOT) ? "boot" : "",
+		con->name, con->index);
+	return ret;
+}
+
+/* Must be called within console_lock(). */
+static void start_printk_kthread(struct console *con)
 {
-	return 0;
+	con->thread = kthread_run(printk_kthread_func, con,
+				  "pr/%s%d", con->name, con->index);
+	if (IS_ERR(con->thread)) {
+		pr_err("%sconsole [%s%d]: unable to start printing thread\n",
+			(con->flags & CON_BOOT) ? "boot" : "",
+			con->name, con->index);
+		return;
+	}
+	pr_info("%sconsole [%s%d]: printing thread started\n",
+		(con->flags & CON_BOOT) ? "boot" : "",
+		con->name, con->index);
 }
-static ssize_t info_print_ext_header(char *buf, size_t size,
-				     struct printk_info *info)
+
+/* protected by console_lock */
+static bool kthreads_started;
+
+/* Must be called within console_lock(). */
+static void console_try_thread(struct console *con)
 {
-	return 0;
+	if (kthreads_started) {
+		start_printk_kthread(con);
+		return;
+	}
+
+	/*
+	 * The printing threads have not been started yet. If this console
+	 * can print synchronously, print all unprinted messages.
+	 */
+	if (console_can_sync(con))
+		print_sync_until(con, prb_next_seq(prb));
 }
-static ssize_t msg_print_ext_body(char *buf, size_t size,
-				  char *text, size_t text_len,
-				  struct dev_printk_info *dev_info) { return 0; }
-static void console_lock_spinning_enable(void) { }
-static int console_lock_spinning_disable_and_check(void) { return 0; }
-static void call_console_drivers(const char *ext_text, size_t ext_len,
-				 const char *text, size_t len) {}
-static bool suppress_message_printing(int level) { return false; }
+
+#else /* CONFIG_PRINTK */
+
+#define prb_first_valid_seq(rb)		0
+#define prb_next_seq(rb)		0
+
+#define console_try_thread(con)
 
 #endif /* CONFIG_PRINTK */
 
@@ -2373,34 +2540,6 @@ int is_console_locked(void)
 }
 EXPORT_SYMBOL(is_console_locked);
 
-/*
- * Check if we have any console that is capable of printing while cpu is
- * booting or shutting down. Requires console_sem.
- */
-static int have_callable_console(void)
-{
-	struct console *con;
-
-	for_each_console(con)
-		if ((con->flags & CON_ENABLED) &&
-				(con->flags & CON_ANYTIME))
-			return 1;
-
-	return 0;
-}
-
-/*
- * Can we actually use the console at this time on this cpu?
- *
- * Console drivers may assume that per-cpu resources have been allocated. So
- * unless they're explicitly marked as being able to cope (CON_ANYTIME) don't
- * call them until this CPU is officially up.
- */
-static inline int can_use_console(void)
-{
-	return cpu_online(raw_smp_processor_id()) || have_callable_console();
-}
-
 /**
  * console_unlock - unlock the console system
  *
@@ -2417,142 +2556,14 @@ static inline int can_use_console(void)
  */
 void console_unlock(void)
 {
-	static char ext_text[CONSOLE_EXT_LOG_MAX];
-	static char text[LOG_LINE_MAX + PREFIX_MAX];
-	unsigned long flags;
-	bool do_cond_resched, retry;
-	struct printk_info info;
-	struct printk_record r;
-
 	if (console_suspended) {
 		up_console_sem();
 		return;
 	}
 
-	prb_rec_init_rd(&r, &info, text, sizeof(text));
-
-	/*
-	 * Console drivers are called with interrupts disabled, so
-	 * @console_may_schedule should be cleared before; however, we may
-	 * end up dumping a lot of lines, for example, if called from
-	 * console registration path, and should invoke cond_resched()
-	 * between lines if allowable.  Not doing so can cause a very long
-	 * scheduling stall on a slow console leading to RCU stall and
-	 * softlockup warnings which exacerbate the issue with more
-	 * messages practically incapacitating the system.
-	 *
-	 * console_trylock() is not able to detect the preemptive
-	 * context reliably. Therefore the value must be stored before
-	 * and cleared after the "again" goto label.
-	 */
-	do_cond_resched = console_may_schedule;
-again:
-	console_may_schedule = 0;
-
-	/*
-	 * We released the console_sem lock, so we need to recheck if
-	 * cpu is online and (if not) is there at least one CON_ANYTIME
-	 * console.
-	 */
-	if (!can_use_console()) {
-		console_locked = 0;
-		up_console_sem();
-		return;
-	}
-
-	for (;;) {
-		size_t ext_len = 0;
-		size_t len;
-
-		printk_safe_enter_irqsave(flags);
-		raw_spin_lock(&logbuf_lock);
-skip:
-		if (!prb_read_valid(prb, console_seq, &r))
-			break;
-
-		if (console_seq != r.info->seq) {
-			console_dropped += r.info->seq - console_seq;
-			console_seq = r.info->seq;
-		}
-
-		if (suppress_message_printing(r.info->level)) {
-			/*
-			 * Skip record we have buffered and already printed
-			 * directly to the console when we received it, and
-			 * record that has level above the console loglevel.
-			 */
-			console_seq++;
-			goto skip;
-		}
-
-		/* Output to all consoles once old messages replayed. */
-		if (unlikely(exclusive_console &&
-			     console_seq >= exclusive_console_stop_seq)) {
-			exclusive_console = NULL;
-		}
-
-		/*
-		 * Handle extended console text first because later
-		 * record_print_text() will modify the record buffer in-place.
-		 */
-		if (nr_ext_console_drivers) {
-			ext_len = info_print_ext_header(ext_text,
-						sizeof(ext_text),
-						r.info);
-			ext_len += msg_print_ext_body(ext_text + ext_len,
-						sizeof(ext_text) - ext_len,
-						&r.text_buf[0],
-						r.info->text_len,
-						&r.info->dev_info);
-		}
-		len = record_print_text(&r,
-				console_msg_format & MSG_FORMAT_SYSLOG,
-				printk_time);
-		console_seq++;
-		raw_spin_unlock(&logbuf_lock);
-
-		/*
-		 * While actively printing out messages, if another printk()
-		 * were to occur on another CPU, it may wait for this one to
-		 * finish. This task can not be preempted if there is a
-		 * waiter waiting to take over.
-		 */
-		console_lock_spinning_enable();
-
-		stop_critical_timings();	/* don't trace print latency */
-		call_console_drivers(ext_text, ext_len, text, len);
-		start_critical_timings();
-
-		if (console_lock_spinning_disable_and_check()) {
-			printk_safe_exit_irqrestore(flags);
-			return;
-		}
-
-		printk_safe_exit_irqrestore(flags);
-
-		if (do_cond_resched)
-			cond_resched();
-	}
-
 	console_locked = 0;
 
-	raw_spin_unlock(&logbuf_lock);
-
 	up_console_sem();
-
-	/*
-	 * Someone could have filled up the buffer again, so re-check if there's
-	 * something to flush. In case we cannot trylock the console_sem again,
-	 * there's a new owner and the console_unlock() from them will do the
-	 * flush, no worries.
-	 */
-	raw_spin_lock(&logbuf_lock);
-	retry = prb_read_valid(prb, console_seq, NULL);
-	raw_spin_unlock(&logbuf_lock);
-	printk_safe_exit_irqrestore(flags);
-
-	if (retry && console_trylock())
-		goto again;
 }
 EXPORT_SYMBOL(console_unlock);
 
@@ -2602,23 +2613,20 @@ void console_unblank(void)
  */
 void console_flush_on_panic(enum con_flush_mode mode)
 {
-	/*
-	 * If someone else is holding the console lock, trylock will fail
-	 * and may_schedule may be set.  Ignore and proceed to unlock so
-	 * that messages are flushed out.  As this can be called from any
-	 * context and we don't want to get preempted while flushing,
-	 * ensure may_schedule is cleared.
-	 */
-	console_trylock();
+	struct console *c;
+	u64 seq;
+
+	if (!console_trylock())
+		return;
+
 	console_may_schedule = 0;
 
 	if (mode == CONSOLE_REPLAY_ALL) {
-		unsigned long flags;
-
-		logbuf_lock_irqsave(flags);
-		console_seq = prb_first_valid_seq(prb);
-		logbuf_unlock_irqrestore(flags);
+		seq = prb_first_valid_seq(prb);
+		for_each_console(c)
+			atomic64_set(&c->printk_seq, seq);
 	}
+
 	console_unlock();
 }
 
@@ -2753,7 +2761,6 @@ static int try_enable_new_console(struct console *newcon, bool user_specified)
  */
 void register_console(struct console *newcon)
 {
-	unsigned long flags;
 	struct console *bcon = NULL;
 	int err;
 
@@ -2777,6 +2784,8 @@ void register_console(struct console *newcon)
 		}
 	}
 
+	newcon->thread = NULL;
+
 	if (console_drivers && console_drivers->flags & CON_BOOT)
 		bcon = console_drivers;
 
@@ -2818,8 +2827,10 @@ void register_console(struct console *newcon)
 	 * the real console are the same physical device, it's annoying to
 	 * see the beginning boot messages twice
 	 */
-	if (bcon && ((newcon->flags & (CON_CONSDEV | CON_BOOT)) == CON_CONSDEV))
+	if (bcon && ((newcon->flags & (CON_CONSDEV | CON_BOOT)) == CON_CONSDEV)) {
 		newcon->flags &= ~CON_PRINTBUFFER;
+		newcon->flags |= CON_HANDOVER;
+	}
 
 	/*
 	 *	Put this console in the list - keep the
@@ -2841,26 +2852,12 @@ void register_console(struct console *newcon)
 	if (newcon->flags & CON_EXTENDED)
 		nr_ext_console_drivers++;
 
-	if (newcon->flags & CON_PRINTBUFFER) {
-		/*
-		 * console_unlock(); will print out the buffered messages
-		 * for us.
-		 */
-		logbuf_lock_irqsave(flags);
-		/*
-		 * We're about to replay the log buffer.  Only do this to the
-		 * just-registered console to avoid excessive message spam to
-		 * the already-registered consoles.
-		 *
-		 * Set exclusive_console with disabled interrupts to reduce
-		 * race window with eventual console_flush_on_panic() that
-		 * ignores console_lock.
-		 */
-		exclusive_console = newcon;
-		exclusive_console_stop_seq = console_seq;
-		console_seq = syslog_seq;
-		logbuf_unlock_irqrestore(flags);
-	}
+	if (newcon->flags & CON_PRINTBUFFER)
+		atomic64_set(&newcon->printk_seq, 0);
+	else
+		atomic64_set(&newcon->printk_seq, prb_next_seq(prb));
+
+	console_try_thread(newcon);
 	console_unlock();
 	console_sysfs_notify();
 
@@ -2934,6 +2931,9 @@ int unregister_console(struct console *console)
 	console_unlock();
 	console_sysfs_notify();
 
+	if (console->thread && !IS_ERR(console->thread))
+		kthread_stop(console->thread);
+
 	if (console->exit)
 		res = console->exit(console);
 
@@ -3016,6 +3016,15 @@ static int __init printk_late_init(void)
 			unregister_console(con);
 		}
 	}
+
+#ifdef CONFIG_PRINTK
+	console_lock();
+	for_each_console(con)
+		start_printk_kthread(con);
+	kthreads_started = true;
+	console_unlock();
+#endif
+
 	ret = cpuhp_setup_state_nocalls(CPUHP_PRINTK_DEAD, "printk:dead", NULL,
 					console_cpu_notify);
 	WARN_ON(ret < 0);
@@ -3031,7 +3040,6 @@ late_initcall(printk_late_init);
  * Delayed printk version, for scheduler-internal messages:
  */
 #define PRINTK_PENDING_WAKEUP	0x01
-#define PRINTK_PENDING_OUTPUT	0x02
 
 static DEFINE_PER_CPU(int, printk_pending);
 
@@ -3039,14 +3047,8 @@ static void wake_up_klogd_work_func(struct irq_work *irq_work)
 {
 	int pending = __this_cpu_xchg(printk_pending, 0);
 
-	if (pending & PRINTK_PENDING_OUTPUT) {
-		/* If trylock fails, someone else is doing the printing */
-		if (console_trylock())
-			console_unlock();
-	}
-
 	if (pending & PRINTK_PENDING_WAKEUP)
-		wake_up_interruptible(&log_wait);
+		wake_up_interruptible_all(&log_wait);
 }
 
 static DEFINE_PER_CPU(struct irq_work, wake_up_klogd_work) = {
@@ -3067,25 +3069,10 @@ void wake_up_klogd(void)
 	preempt_enable();
 }
 
-void defer_console_output(void)
-{
-	if (!printk_percpu_data_ready())
-		return;
-
-	preempt_disable();
-	__this_cpu_or(printk_pending, PRINTK_PENDING_OUTPUT);
-	irq_work_queue(this_cpu_ptr(&wake_up_klogd_work));
-	preempt_enable();
-}
-
-int vprintk_deferred(const char *fmt, va_list args)
+__printf(1, 0)
+static int vprintk_deferred(const char *fmt, va_list args)
 {
-	int r;
-
-	r = vprintk_emit(0, LOGLEVEL_SCHED, NULL, fmt, args);
-	defer_console_output();
-
-	return r;
+	return vprintk_emit(0, LOGLEVEL_DEFAULT, NULL, fmt, args);
 }
 
 int printk_deferred(const char *fmt, ...)
@@ -3224,8 +3211,26 @@ EXPORT_SYMBOL_GPL(kmsg_dump_reason_str);
  */
 void kmsg_dump(enum kmsg_dump_reason reason)
 {
+	struct kmsg_dumper_iter iter;
 	struct kmsg_dumper *dumper;
-	unsigned long flags;
+
+	if (!oops_in_progress) {
+		/*
+		 * If atomic consoles are available, activate kernel sync mode
+		 * to make sure any final messages are visible. The trailing
+		 * printk message is important to flush any pending messages.
+		 */
+		if (have_atomic_console()) {
+			sync_mode = true;
+			pr_info("enabled sync mode\n");
+		}
+
+		/*
+		 * Give the printing threads time to flush, allowing up to
+		 * 1s of no printing forward progress before giving up.
+		 */
+		pr_flush(1000, true);
+	}
 
 	rcu_read_lock();
 	list_for_each_entry_rcu(dumper, &dump_list, list) {
@@ -3243,25 +3248,18 @@ void kmsg_dump(enum kmsg_dump_reason reason)
 			continue;
 
 		/* initialize iterator with data about the stored records */
-		dumper->active = true;
-
-		logbuf_lock_irqsave(flags);
-		dumper->cur_seq = clear_seq;
-		dumper->next_seq = prb_next_seq(prb);
-		logbuf_unlock_irqrestore(flags);
+		iter.active = true;
+		kmsg_dump_rewind(&iter);
 
 		/* invoke dumper which will iterate over records */
-		dumper->dump(dumper, reason);
-
-		/* reset iterator */
-		dumper->active = false;
+		dumper->dump(dumper, reason, &iter);
 	}
 	rcu_read_unlock();
 }
 
 /**
- * kmsg_dump_get_line_nolock - retrieve one kmsg log line (unlocked version)
- * @dumper: registered kmsg dumper
+ * kmsg_dump_get_line - retrieve one kmsg log line
+ * @iter: kmsg dumper iterator
  * @syslog: include the "<4>" prefixes
  * @line: buffer to copy the line to
  * @size: maximum size of the buffer
@@ -3275,11 +3273,9 @@ void kmsg_dump(enum kmsg_dump_reason reason)
  *
  * A return value of FALSE indicates that there are no more records to
  * read.
- *
- * The function is similar to kmsg_dump_get_line(), but grabs no locks.
  */
-bool kmsg_dump_get_line_nolock(struct kmsg_dumper *dumper, bool syslog,
-			       char *line, size_t size, size_t *len)
+bool kmsg_dump_get_line(struct kmsg_dumper_iter *iter, bool syslog,
+			char *line, size_t size, size_t *len)
 {
 	struct printk_info info;
 	unsigned int line_count;
@@ -3289,16 +3285,16 @@ bool kmsg_dump_get_line_nolock(struct kmsg_dumper *dumper, bool syslog,
 
 	prb_rec_init_rd(&r, &info, line, size);
 
-	if (!dumper->active)
+	if (!iter->active)
 		goto out;
 
 	/* Read text or count text lines? */
 	if (line) {
-		if (!prb_read_valid(prb, dumper->cur_seq, &r))
+		if (!prb_read_valid(prb, iter->cur_seq, &r))
 			goto out;
 		l = record_print_text(&r, syslog, printk_time);
 	} else {
-		if (!prb_read_valid_info(prb, dumper->cur_seq,
+		if (!prb_read_valid_info(prb, iter->cur_seq,
 					 &info, &line_count)) {
 			goto out;
 		}
@@ -3307,48 +3303,18 @@ bool kmsg_dump_get_line_nolock(struct kmsg_dumper *dumper, bool syslog,
 
 	}
 
-	dumper->cur_seq = r.info->seq + 1;
+	iter->cur_seq = r.info->seq + 1;
 	ret = true;
 out:
 	if (len)
 		*len = l;
 	return ret;
 }
-
-/**
- * kmsg_dump_get_line - retrieve one kmsg log line
- * @dumper: registered kmsg dumper
- * @syslog: include the "<4>" prefixes
- * @line: buffer to copy the line to
- * @size: maximum size of the buffer
- * @len: length of line placed into buffer
- *
- * Start at the beginning of the kmsg buffer, with the oldest kmsg
- * record, and copy one record into the provided buffer.
- *
- * Consecutive calls will return the next available record moving
- * towards the end of the buffer with the youngest messages.
- *
- * A return value of FALSE indicates that there are no more records to
- * read.
- */
-bool kmsg_dump_get_line(struct kmsg_dumper *dumper, bool syslog,
-			char *line, size_t size, size_t *len)
-{
-	unsigned long flags;
-	bool ret;
-
-	logbuf_lock_irqsave(flags);
-	ret = kmsg_dump_get_line_nolock(dumper, syslog, line, size, len);
-	logbuf_unlock_irqrestore(flags);
-
-	return ret;
-}
 EXPORT_SYMBOL_GPL(kmsg_dump_get_line);
 
 /**
  * kmsg_dump_get_buffer - copy kmsg log lines
- * @dumper: registered kmsg dumper
+ * @iter: kmsg dumper iterator
  * @syslog: include the "<4>" prefixes
  * @buf: buffer to copy the line to
  * @size: maximum size of the buffer
@@ -3365,116 +3331,254 @@ EXPORT_SYMBOL_GPL(kmsg_dump_get_line);
  * A return value of FALSE indicates that there are no more records to
  * read.
  */
-bool kmsg_dump_get_buffer(struct kmsg_dumper *dumper, bool syslog,
-			  char *buf, size_t size, size_t *len)
+bool kmsg_dump_get_buffer(struct kmsg_dumper_iter *iter, bool syslog,
+			  char *buf, size_t size, size_t *len_out)
 {
 	struct printk_info info;
-	unsigned int line_count;
 	struct printk_record r;
-	unsigned long flags;
 	u64 seq;
 	u64 next_seq;
-	size_t l = 0;
+	size_t len = 0;
 	bool ret = false;
 	bool time = printk_time;
 
-	prb_rec_init_rd(&r, &info, buf, size);
-
-	if (!dumper->active || !buf || !size)
+	if (!iter->active || !buf || !size)
 		goto out;
 
-	logbuf_lock_irqsave(flags);
-	if (prb_read_valid_info(prb, dumper->cur_seq, &info, NULL)) {
-		if (info.seq != dumper->cur_seq) {
+	if (prb_read_valid_info(prb, iter->cur_seq, &info, NULL)) {
+		if (info.seq != iter->cur_seq) {
 			/* messages are gone, move to first available one */
-			dumper->cur_seq = info.seq;
+			iter->cur_seq = info.seq;
 		}
 	}
 
 	/* last entry */
-	if (dumper->cur_seq >= dumper->next_seq) {
-		logbuf_unlock_irqrestore(flags);
+	if (iter->cur_seq >= iter->next_seq)
 		goto out;
-	}
-
-	/* calculate length of entire buffer */
-	seq = dumper->cur_seq;
-	while (prb_read_valid_info(prb, seq, &info, &line_count)) {
-		if (r.info->seq >= dumper->next_seq)
-			break;
-		l += get_record_print_text_size(&info, line_count, syslog, time);
-		seq = r.info->seq + 1;
-	}
 
-	/* move first record forward until length fits into the buffer */
-	seq = dumper->cur_seq;
-	while (l >= size && prb_read_valid_info(prb, seq,
-						&info, &line_count)) {
-		if (r.info->seq >= dumper->next_seq)
-			break;
-		l -= get_record_print_text_size(&info, line_count, syslog, time);
-		seq = r.info->seq + 1;
-	}
+	/*
+	 * Find first record that fits, including all following records,
+	 * into the user-provided buffer for this dump. Pass in size-1
+	 * because this function (by way of record_print_text()) will
+	 * not write more than size-1 bytes of text into @buf.
+	 */
+	seq = find_first_fitting_seq(iter->cur_seq, iter->next_seq,
+				     size - 1, syslog, time);
 
-	/* last message in next interation */
+	/*
+	 * Next kmsg_dump_get_buffer() invocation will dump block of
+	 * older records stored right before this one.
+	 */
 	next_seq = seq;
 
-	/* actually read text into the buffer now */
-	l = 0;
-	while (prb_read_valid(prb, seq, &r)) {
-		if (r.info->seq >= dumper->next_seq)
-			break;
+	prb_rec_init_rd(&r, &info, buf, size);
 
-		l += record_print_text(&r, syslog, time);
+	len = 0;
+	prb_for_each_record(seq, prb, seq, &r) {
+		if (r.info->seq >= iter->next_seq)
+			break;
 
-		/* adjust record to store to remaining buffer space */
-		prb_rec_init_rd(&r, &info, buf + l, size - l);
+		len += record_print_text(&r, syslog, time);
 
-		seq = r.info->seq + 1;
+		/* Adjust record to store to remaining buffer space. */
+		prb_rec_init_rd(&r, &info, buf + len, size - len);
 	}
 
-	dumper->next_seq = next_seq;
+	iter->next_seq = next_seq;
 	ret = true;
-	logbuf_unlock_irqrestore(flags);
 out:
-	if (len)
-		*len = l;
+	if (len_out)
+		*len_out = len;
 	return ret;
 }
 EXPORT_SYMBOL_GPL(kmsg_dump_get_buffer);
 
 /**
- * kmsg_dump_rewind_nolock - reset the iterator (unlocked version)
- * @dumper: registered kmsg dumper
+ * kmsg_dump_rewind - reset the iterator
+ * @iter: kmsg dumper iterator
  *
  * Reset the dumper's iterator so that kmsg_dump_get_line() and
  * kmsg_dump_get_buffer() can be called again and used multiple
  * times within the same dumper.dump() callback.
+ */
+void kmsg_dump_rewind(struct kmsg_dumper_iter *iter)
+{
+	iter->cur_seq = latched_seq_read_nolock(&clear_seq);
+	iter->next_seq = prb_next_seq(prb);
+}
+EXPORT_SYMBOL_GPL(kmsg_dump_rewind);
+
+#endif
+
+struct prb_cpulock {
+	atomic_t owner;
+	unsigned long __percpu *irqflags;
+};
+
+#define DECLARE_STATIC_PRINTKRB_CPULOCK(name)				\
+static DEFINE_PER_CPU(unsigned long, _##name##_percpu_irqflags);	\
+static struct prb_cpulock name = {					\
+	.owner = ATOMIC_INIT(-1),					\
+	.irqflags = &_##name##_percpu_irqflags,				\
+}
+
+static bool __prb_trylock(struct prb_cpulock *cpu_lock,
+			  unsigned int *cpu_store)
+{
+	unsigned long *flags;
+	unsigned int cpu;
+
+	cpu = get_cpu();
+
+	*cpu_store = atomic_read(&cpu_lock->owner);
+	/* memory barrier to ensure the current lock owner is visible */
+	smp_rmb();
+	if (*cpu_store == -1) {
+		flags = per_cpu_ptr(cpu_lock->irqflags, cpu);
+		local_irq_save(*flags);
+		if (atomic_try_cmpxchg_acquire(&cpu_lock->owner,
+					       cpu_store, cpu)) {
+			return true;
+		}
+		local_irq_restore(*flags);
+	} else if (*cpu_store == cpu) {
+		return true;
+	}
+
+	put_cpu();
+	return false;
+}
+
+/*
+ * prb_lock: Perform a processor-reentrant spin lock.
+ * @cpu_lock: A pointer to the lock object.
+ * @cpu_store: A "flags" pointer to store lock status information.
+ *
+ * If no processor has the lock, the calling processor takes the lock and
+ * becomes the owner. If the calling processor is already the owner of the
+ * lock, this function succeeds immediately. If lock is locked by another
+ * processor, this function spins until the calling processor becomes the
+ * owner.
+ *
+ * It is safe to call this function from any context and state.
+ */
+static void prb_lock(struct prb_cpulock *cpu_lock, unsigned int *cpu_store)
+{
+	for (;;) {
+		if (__prb_trylock(cpu_lock, cpu_store))
+			break;
+		cpu_relax();
+	}
+}
+
+/*
+ * prb_unlock: Perform a processor-reentrant spin unlock.
+ * @cpu_lock: A pointer to the lock object.
+ * @cpu_store: A "flags" object storing lock status information.
  *
- * The function is similar to kmsg_dump_rewind(), but grabs no locks.
+ * Release the lock. The calling processor must be the owner of the lock.
+ *
+ * It is safe to call this function from any context and state.
  */
-void kmsg_dump_rewind_nolock(struct kmsg_dumper *dumper)
+static void prb_unlock(struct prb_cpulock *cpu_lock, unsigned int cpu_store)
 {
-	dumper->cur_seq = clear_seq;
-	dumper->next_seq = prb_next_seq(prb);
+	unsigned long *flags;
+	unsigned int cpu;
+
+	cpu = atomic_read(&cpu_lock->owner);
+	atomic_set_release(&cpu_lock->owner, cpu_store);
+
+	if (cpu_store == -1) {
+		flags = per_cpu_ptr(cpu_lock->irqflags, cpu);
+		local_irq_restore(*flags);
+	}
+
+	put_cpu();
+}
+
+DECLARE_STATIC_PRINTKRB_CPULOCK(printk_cpulock);
+
+void console_atomic_lock(unsigned int *flags)
+{
+	prb_lock(&printk_cpulock, flags);
+}
+EXPORT_SYMBOL(console_atomic_lock);
+
+void console_atomic_unlock(unsigned int flags)
+{
+	prb_unlock(&printk_cpulock, flags);
+}
+EXPORT_SYMBOL(console_atomic_unlock);
+
+static void pr_msleep(bool may_sleep, int ms)
+{
+	if (may_sleep) {
+		msleep(ms);
+	} else {
+		while (ms--)
+			udelay(1000);
+	}
 }
 
 /**
- * kmsg_dump_rewind - reset the iterator
- * @dumper: registered kmsg dumper
+ * pr_flush() - Wait for printing threads to catch up.
  *
- * Reset the dumper's iterator so that kmsg_dump_get_line() and
- * kmsg_dump_get_buffer() can be called again and used multiple
- * times within the same dumper.dump() callback.
+ * @timeout_ms:        The maximum time (in ms) to wait.
+ * @reset_on_progress: Reset the timeout if forward progress is seen.
+ *
+ * A value of 0 for @timeout_ms means no waiting will occur. A value of -1
+ * represents infinite waiting.
+ *
+ * If @reset_on_progress is true, the timeout will be reset whenever any
+ * printer has been seen to make some forward progress.
+ *
+ * Context: Any context.
+ * Return: true if all enabled printers are caught up.
  */
-void kmsg_dump_rewind(struct kmsg_dumper *dumper)
+bool pr_flush(int timeout_ms, bool reset_on_progress)
 {
-	unsigned long flags;
+	int remaining = timeout_ms;
+	struct console *con;
+	u64 last_diff = 0;
+	bool may_sleep;
+	u64 printk_seq;
+	u64 diff;
+	u64 seq;
 
-	logbuf_lock_irqsave(flags);
-	kmsg_dump_rewind_nolock(dumper);
-	logbuf_unlock_irqrestore(flags);
-}
-EXPORT_SYMBOL_GPL(kmsg_dump_rewind);
+	may_sleep = (preemptible() && !in_softirq());
 
-#endif
+	seq = prb_next_seq(prb);
+
+	for (;;) {
+		diff = 0;
+
+		for_each_console(con) {
+			if (!(con->flags & CON_ENABLED))
+				continue;
+			printk_seq = atomic64_read(&con->printk_seq);
+			if (printk_seq < seq)
+				diff += seq - printk_seq;
+		}
+
+		if (diff != last_diff && reset_on_progress)
+			remaining = timeout_ms;
+
+		if (!diff || remaining == 0)
+			break;
+
+		if (remaining < 0) {
+			pr_msleep(may_sleep, 100);
+		} else if (remaining < 100) {
+			pr_msleep(may_sleep, remaining);
+			remaining = 0;
+		} else {
+			pr_msleep(may_sleep, 100);
+			remaining -= 100;
+		}
+
+		last_diff = diff;
+	}
+
+	return (diff == 0);
+}
+EXPORT_SYMBOL(pr_flush);
diff --git a/kernel/printk/printk_safe.c b/kernel/printk/printk_safe.c
deleted file mode 100644
index 2e9e3ed7d63e..000000000000
--- a/kernel/printk/printk_safe.c
+++ /dev/null
@@ -1,422 +0,0 @@
-// SPDX-License-Identifier: GPL-2.0-or-later
-/*
- * printk_safe.c - Safe printk for printk-deadlock-prone contexts
- */
-
-#include <linux/preempt.h>
-#include <linux/spinlock.h>
-#include <linux/debug_locks.h>
-#include <linux/kdb.h>
-#include <linux/smp.h>
-#include <linux/cpumask.h>
-#include <linux/irq_work.h>
-#include <linux/printk.h>
-#include <linux/kprobes.h>
-
-#include "internal.h"
-
-/*
- * printk() could not take logbuf_lock in NMI context. Instead,
- * it uses an alternative implementation that temporary stores
- * the strings into a per-CPU buffer. The content of the buffer
- * is later flushed into the main ring buffer via IRQ work.
- *
- * The alternative implementation is chosen transparently
- * by examining current printk() context mask stored in @printk_context
- * per-CPU variable.
- *
- * The implementation allows to flush the strings also from another CPU.
- * There are situations when we want to make sure that all buffers
- * were handled or when IRQs are blocked.
- */
-
-#define SAFE_LOG_BUF_LEN ((1 << CONFIG_PRINTK_SAFE_LOG_BUF_SHIFT) -	\
-				sizeof(atomic_t) -			\
-				sizeof(atomic_t) -			\
-				sizeof(struct irq_work))
-
-struct printk_safe_seq_buf {
-	atomic_t		len;	/* length of written data */
-	atomic_t		message_lost;
-	struct irq_work		work;	/* IRQ work that flushes the buffer */
-	unsigned char		buffer[SAFE_LOG_BUF_LEN];
-};
-
-static DEFINE_PER_CPU(struct printk_safe_seq_buf, safe_print_seq);
-static DEFINE_PER_CPU(int, printk_context);
-
-static DEFINE_RAW_SPINLOCK(safe_read_lock);
-
-#ifdef CONFIG_PRINTK_NMI
-static DEFINE_PER_CPU(struct printk_safe_seq_buf, nmi_print_seq);
-#endif
-
-/* Get flushed in a more safe context. */
-static void queue_flush_work(struct printk_safe_seq_buf *s)
-{
-	if (printk_percpu_data_ready())
-		irq_work_queue(&s->work);
-}
-
-/*
- * Add a message to per-CPU context-dependent buffer. NMI and printk-safe
- * have dedicated buffers, because otherwise printk-safe preempted by
- * NMI-printk would have overwritten the NMI messages.
- *
- * The messages are flushed from irq work (or from panic()), possibly,
- * from other CPU, concurrently with printk_safe_log_store(). Should this
- * happen, printk_safe_log_store() will notice the buffer->len mismatch
- * and repeat the write.
- */
-static __printf(2, 0) int printk_safe_log_store(struct printk_safe_seq_buf *s,
-						const char *fmt, va_list args)
-{
-	int add;
-	size_t len;
-	va_list ap;
-
-again:
-	len = atomic_read(&s->len);
-
-	/* The trailing '\0' is not counted into len. */
-	if (len >= sizeof(s->buffer) - 1) {
-		atomic_inc(&s->message_lost);
-		queue_flush_work(s);
-		return 0;
-	}
-
-	/*
-	 * Make sure that all old data have been read before the buffer
-	 * was reset. This is not needed when we just append data.
-	 */
-	if (!len)
-		smp_rmb();
-
-	va_copy(ap, args);
-	add = vscnprintf(s->buffer + len, sizeof(s->buffer) - len, fmt, ap);
-	va_end(ap);
-	if (!add)
-		return 0;
-
-	/*
-	 * Do it once again if the buffer has been flushed in the meantime.
-	 * Note that atomic_cmpxchg() is an implicit memory barrier that
-	 * makes sure that the data were written before updating s->len.
-	 */
-	if (atomic_cmpxchg(&s->len, len, len + add) != len)
-		goto again;
-
-	queue_flush_work(s);
-	return add;
-}
-
-static inline void printk_safe_flush_line(const char *text, int len)
-{
-	/*
-	 * Avoid any console drivers calls from here, because we may be
-	 * in NMI or printk_safe context (when in panic). The messages
-	 * must go only into the ring buffer at this stage.  Consoles will
-	 * get explicitly called later when a crashdump is not generated.
-	 */
-	printk_deferred("%.*s", len, text);
-}
-
-/* printk part of the temporary buffer line by line */
-static int printk_safe_flush_buffer(const char *start, size_t len)
-{
-	const char *c, *end;
-	bool header;
-
-	c = start;
-	end = start + len;
-	header = true;
-
-	/* Print line by line. */
-	while (c < end) {
-		if (*c == '\n') {
-			printk_safe_flush_line(start, c - start + 1);
-			start = ++c;
-			header = true;
-			continue;
-		}
-
-		/* Handle continuous lines or missing new line. */
-		if ((c + 1 < end) && printk_get_level(c)) {
-			if (header) {
-				c = printk_skip_level(c);
-				continue;
-			}
-
-			printk_safe_flush_line(start, c - start);
-			start = c++;
-			header = true;
-			continue;
-		}
-
-		header = false;
-		c++;
-	}
-
-	/* Check if there was a partial line. Ignore pure header. */
-	if (start < end && !header) {
-		static const char newline[] = KERN_CONT "\n";
-
-		printk_safe_flush_line(start, end - start);
-		printk_safe_flush_line(newline, strlen(newline));
-	}
-
-	return len;
-}
-
-static void report_message_lost(struct printk_safe_seq_buf *s)
-{
-	int lost = atomic_xchg(&s->message_lost, 0);
-
-	if (lost)
-		printk_deferred("Lost %d message(s)!\n", lost);
-}
-
-/*
- * Flush data from the associated per-CPU buffer. The function
- * can be called either via IRQ work or independently.
- */
-static void __printk_safe_flush(struct irq_work *work)
-{
-	struct printk_safe_seq_buf *s =
-		container_of(work, struct printk_safe_seq_buf, work);
-	unsigned long flags;
-	size_t len;
-	int i;
-
-	/*
-	 * The lock has two functions. First, one reader has to flush all
-	 * available message to make the lockless synchronization with
-	 * writers easier. Second, we do not want to mix messages from
-	 * different CPUs. This is especially important when printing
-	 * a backtrace.
-	 */
-	raw_spin_lock_irqsave(&safe_read_lock, flags);
-
-	i = 0;
-more:
-	len = atomic_read(&s->len);
-
-	/*
-	 * This is just a paranoid check that nobody has manipulated
-	 * the buffer an unexpected way. If we printed something then
-	 * @len must only increase. Also it should never overflow the
-	 * buffer size.
-	 */
-	if ((i && i >= len) || len > sizeof(s->buffer)) {
-		const char *msg = "printk_safe_flush: internal error\n";
-
-		printk_safe_flush_line(msg, strlen(msg));
-		len = 0;
-	}
-
-	if (!len)
-		goto out; /* Someone else has already flushed the buffer. */
-
-	/* Make sure that data has been written up to the @len */
-	smp_rmb();
-	i += printk_safe_flush_buffer(s->buffer + i, len - i);
-
-	/*
-	 * Check that nothing has got added in the meantime and truncate
-	 * the buffer. Note that atomic_cmpxchg() is an implicit memory
-	 * barrier that makes sure that the data were copied before
-	 * updating s->len.
-	 */
-	if (atomic_cmpxchg(&s->len, len, 0) != len)
-		goto more;
-
-out:
-	report_message_lost(s);
-	raw_spin_unlock_irqrestore(&safe_read_lock, flags);
-}
-
-/**
- * printk_safe_flush - flush all per-cpu nmi buffers.
- *
- * The buffers are flushed automatically via IRQ work. This function
- * is useful only when someone wants to be sure that all buffers have
- * been flushed at some point.
- */
-void printk_safe_flush(void)
-{
-	int cpu;
-
-	for_each_possible_cpu(cpu) {
-#ifdef CONFIG_PRINTK_NMI
-		__printk_safe_flush(&per_cpu(nmi_print_seq, cpu).work);
-#endif
-		__printk_safe_flush(&per_cpu(safe_print_seq, cpu).work);
-	}
-}
-
-/**
- * printk_safe_flush_on_panic - flush all per-cpu nmi buffers when the system
- *	goes down.
- *
- * Similar to printk_safe_flush() but it can be called even in NMI context when
- * the system goes down. It does the best effort to get NMI messages into
- * the main ring buffer.
- *
- * Note that it could try harder when there is only one CPU online.
- */
-void printk_safe_flush_on_panic(void)
-{
-	/*
-	 * Make sure that we could access the main ring buffer.
-	 * Do not risk a double release when more CPUs are up.
-	 */
-	if (raw_spin_is_locked(&logbuf_lock)) {
-		if (num_online_cpus() > 1)
-			return;
-
-		debug_locks_off();
-		raw_spin_lock_init(&logbuf_lock);
-	}
-
-	if (raw_spin_is_locked(&safe_read_lock)) {
-		if (num_online_cpus() > 1)
-			return;
-
-		debug_locks_off();
-		raw_spin_lock_init(&safe_read_lock);
-	}
-
-	printk_safe_flush();
-}
-
-#ifdef CONFIG_PRINTK_NMI
-/*
- * Safe printk() for NMI context. It uses a per-CPU buffer to
- * store the message. NMIs are not nested, so there is always only
- * one writer running. But the buffer might get flushed from another
- * CPU, so we need to be careful.
- */
-static __printf(1, 0) int vprintk_nmi(const char *fmt, va_list args)
-{
-	struct printk_safe_seq_buf *s = this_cpu_ptr(&nmi_print_seq);
-
-	return printk_safe_log_store(s, fmt, args);
-}
-
-void noinstr printk_nmi_enter(void)
-{
-	this_cpu_add(printk_context, PRINTK_NMI_CONTEXT_OFFSET);
-}
-
-void noinstr printk_nmi_exit(void)
-{
-	this_cpu_sub(printk_context, PRINTK_NMI_CONTEXT_OFFSET);
-}
-
-/*
- * Marks a code that might produce many messages in NMI context
- * and the risk of losing them is more critical than eventual
- * reordering.
- *
- * It has effect only when called in NMI context. Then printk()
- * will try to store the messages into the main logbuf directly
- * and use the per-CPU buffers only as a fallback when the lock
- * is not available.
- */
-void printk_nmi_direct_enter(void)
-{
-	if (this_cpu_read(printk_context) & PRINTK_NMI_CONTEXT_MASK)
-		this_cpu_or(printk_context, PRINTK_NMI_DIRECT_CONTEXT_MASK);
-}
-
-void printk_nmi_direct_exit(void)
-{
-	this_cpu_and(printk_context, ~PRINTK_NMI_DIRECT_CONTEXT_MASK);
-}
-
-#else
-
-static __printf(1, 0) int vprintk_nmi(const char *fmt, va_list args)
-{
-	return 0;
-}
-
-#endif /* CONFIG_PRINTK_NMI */
-
-/*
- * Lock-less printk(), to avoid deadlocks should the printk() recurse
- * into itself. It uses a per-CPU buffer to store the message, just like
- * NMI.
- */
-static __printf(1, 0) int vprintk_safe(const char *fmt, va_list args)
-{
-	struct printk_safe_seq_buf *s = this_cpu_ptr(&safe_print_seq);
-
-	return printk_safe_log_store(s, fmt, args);
-}
-
-/* Can be preempted by NMI. */
-void __printk_safe_enter(void)
-{
-	this_cpu_inc(printk_context);
-}
-
-/* Can be preempted by NMI. */
-void __printk_safe_exit(void)
-{
-	this_cpu_dec(printk_context);
-}
-
-__printf(1, 0) int vprintk_func(const char *fmt, va_list args)
-{
-#ifdef CONFIG_KGDB_KDB
-	/* Allow to pass printk() to kdb but avoid a recursion. */
-	if (unlikely(kdb_trap_printk && kdb_printf_cpu < 0))
-		return vkdb_printf(KDB_MSGSRC_PRINTK, fmt, args);
-#endif
-
-	/*
-	 * Try to use the main logbuf even in NMI. But avoid calling console
-	 * drivers that might have their own locks.
-	 */
-	if ((this_cpu_read(printk_context) & PRINTK_NMI_DIRECT_CONTEXT_MASK) &&
-	    raw_spin_trylock(&logbuf_lock)) {
-		int len;
-
-		len = vprintk_store(0, LOGLEVEL_DEFAULT, NULL, fmt, args);
-		raw_spin_unlock(&logbuf_lock);
-		defer_console_output();
-		return len;
-	}
-
-	/* Use extra buffer in NMI when logbuf_lock is taken or in safe mode. */
-	if (this_cpu_read(printk_context) & PRINTK_NMI_CONTEXT_MASK)
-		return vprintk_nmi(fmt, args);
-
-	/* Use extra buffer to prevent a recursion deadlock in safe mode. */
-	if (this_cpu_read(printk_context) & PRINTK_SAFE_CONTEXT_MASK)
-		return vprintk_safe(fmt, args);
-
-	/* No obstacles. */
-	return vprintk_default(fmt, args);
-}
-
-void __init printk_safe_init(void)
-{
-	int cpu;
-
-	for_each_possible_cpu(cpu) {
-		struct printk_safe_seq_buf *s;
-
-		s = &per_cpu(safe_print_seq, cpu);
-		init_irq_work(&s->work, __printk_safe_flush);
-
-#ifdef CONFIG_PRINTK_NMI
-		s = &per_cpu(nmi_print_seq, cpu);
-		init_irq_work(&s->work, __printk_safe_flush);
-#endif
-	}
-
-	/* Flush pending messages that did not have scheduled IRQ works. */
-	printk_safe_flush();
-}
diff --git a/kernel/ptrace.c b/kernel/ptrace.c
index 79de1294f8eb..2f09a6dbe140 100644
--- a/kernel/ptrace.c
+++ b/kernel/ptrace.c
@@ -180,7 +180,14 @@ static bool ptrace_freeze_traced(struct task_struct *task)
 
 	spin_lock_irq(&task->sighand->siglock);
 	if (task_is_traced(task) && !__fatal_signal_pending(task)) {
-		task->state = __TASK_TRACED;
+		unsigned long flags;
+
+		raw_spin_lock_irqsave(&task->pi_lock, flags);
+		if (task->state & __TASK_TRACED)
+			task->state = __TASK_TRACED;
+		else
+			task->saved_state = __TASK_TRACED;
+		raw_spin_unlock_irqrestore(&task->pi_lock, flags);
 		ret = true;
 	}
 	spin_unlock_irq(&task->sighand->siglock);
@@ -190,8 +197,8 @@ static bool ptrace_freeze_traced(struct task_struct *task)
 
 static void ptrace_unfreeze_traced(struct task_struct *task)
 {
-	if (task->state != __TASK_TRACED)
-		return;
+	unsigned long flags;
+	bool frozen = true;
 
 	WARN_ON(!task->ptrace || task->parent != current);
 
@@ -200,12 +207,19 @@ static void ptrace_unfreeze_traced(struct task_struct *task)
 	 * Recheck state under the lock to close this race.
 	 */
 	spin_lock_irq(&task->sighand->siglock);
-	if (task->state == __TASK_TRACED) {
-		if (__fatal_signal_pending(task))
-			wake_up_state(task, __TASK_TRACED);
-		else
-			task->state = TASK_TRACED;
-	}
+
+	raw_spin_lock_irqsave(&task->pi_lock, flags);
+	if (task->state == __TASK_TRACED)
+		task->state = TASK_TRACED;
+	else if (task->saved_state == __TASK_TRACED)
+		task->saved_state = TASK_TRACED;
+	else
+		frozen = false;
+	raw_spin_unlock_irqrestore(&task->pi_lock, flags);
+
+	if (frozen && __fatal_signal_pending(task))
+		wake_up_state(task, __TASK_TRACED);
+
 	spin_unlock_irq(&task->sighand->siglock);
 }
 
diff --git a/kernel/rcu/Kconfig b/kernel/rcu/Kconfig
index b71e21f73c40..a2534cd1765a 100644
--- a/kernel/rcu/Kconfig
+++ b/kernel/rcu/Kconfig
@@ -188,8 +188,8 @@ config RCU_FAST_NO_HZ
 
 config RCU_BOOST
 	bool "Enable RCU priority boosting"
-	depends on RT_MUTEXES && PREEMPT_RCU && RCU_EXPERT
-	default n
+	depends on (RT_MUTEXES && PREEMPT_RCU && RCU_EXPERT) || PREEMPT_RT
+	default y if PREEMPT_RT
 	help
 	  This option boosts the priority of preempted RCU readers that
 	  block the current preemptible RCU grace period for too long.
diff --git a/kernel/rcu/rcutorture.c b/kernel/rcu/rcutorture.c
index 916ea4f66e4b..eb089d3035d5 100644
--- a/kernel/rcu/rcutorture.c
+++ b/kernel/rcu/rcutorture.c
@@ -61,10 +61,13 @@ MODULE_AUTHOR("Paul E. McKenney <paulmck@linux.ibm.com> and Josh Triplett <josh@
 #define RCUTORTURE_RDR_RBH	 0x08	/*  ... rcu_read_lock_bh(). */
 #define RCUTORTURE_RDR_SCHED	 0x10	/*  ... rcu_read_lock_sched(). */
 #define RCUTORTURE_RDR_RCU	 0x20	/*  ... entering another RCU reader. */
-#define RCUTORTURE_RDR_NBITS	 6	/* Number of bits defined above. */
+#define RCUTORTURE_RDR_ATOM_BH	 0x40	/*  ... disabling bh while atomic */
+#define RCUTORTURE_RDR_ATOM_RBH	 0x80	/*  ... RBH while atomic */
+#define RCUTORTURE_RDR_NBITS	 8	/* Number of bits defined above. */
 #define RCUTORTURE_MAX_EXTEND	 \
 	(RCUTORTURE_RDR_BH | RCUTORTURE_RDR_IRQ | RCUTORTURE_RDR_PREEMPT | \
-	 RCUTORTURE_RDR_RBH | RCUTORTURE_RDR_SCHED)
+	 RCUTORTURE_RDR_RBH | RCUTORTURE_RDR_SCHED | \
+	 RCUTORTURE_RDR_ATOM_BH | RCUTORTURE_RDR_ATOM_RBH)
 #define RCUTORTURE_RDR_MAX_LOOPS 0x7	/* Maximum reader extensions. */
 					/* Must be power of two minus one. */
 #define RCUTORTURE_RDR_MAX_SEGS (RCUTORTURE_RDR_MAX_LOOPS + 3)
@@ -1235,31 +1238,53 @@ static void rcutorture_one_extend(int *readstate, int newstate,
 	WARN_ON_ONCE((idxold >> RCUTORTURE_RDR_SHIFT) > 1);
 	rtrsp->rt_readstate = newstate;
 
-	/* First, put new protection in place to avoid critical-section gap. */
+	/*
+	 * First, put new protection in place to avoid critical-section gap.
+	 * Disable preemption around the ATOM disables to ensure that
+	 * in_atomic() is true.
+	 */
 	if (statesnew & RCUTORTURE_RDR_BH)
 		local_bh_disable();
+	if (statesnew & RCUTORTURE_RDR_RBH)
+		rcu_read_lock_bh();
 	if (statesnew & RCUTORTURE_RDR_IRQ)
 		local_irq_disable();
 	if (statesnew & RCUTORTURE_RDR_PREEMPT)
 		preempt_disable();
-	if (statesnew & RCUTORTURE_RDR_RBH)
-		rcu_read_lock_bh();
 	if (statesnew & RCUTORTURE_RDR_SCHED)
 		rcu_read_lock_sched();
+	preempt_disable();
+	if (statesnew & RCUTORTURE_RDR_ATOM_BH)
+		local_bh_disable();
+	if (statesnew & RCUTORTURE_RDR_ATOM_RBH)
+		rcu_read_lock_bh();
+	preempt_enable();
 	if (statesnew & RCUTORTURE_RDR_RCU)
 		idxnew = cur_ops->readlock() << RCUTORTURE_RDR_SHIFT;
 
-	/* Next, remove old protection, irq first due to bh conflict. */
+	/*
+	 * Next, remove old protection, in decreasing order of strength
+	 * to avoid unlock paths that aren't safe in the stronger
+	 * context.  Disable preemption around the ATOM enables in
+	 * case the context was only atomic due to IRQ disabling.
+	 */
+	preempt_disable();
 	if (statesold & RCUTORTURE_RDR_IRQ)
 		local_irq_enable();
-	if (statesold & RCUTORTURE_RDR_BH)
+	if (statesold & RCUTORTURE_RDR_ATOM_BH)
 		local_bh_enable();
+	if (statesold & RCUTORTURE_RDR_ATOM_RBH)
+		rcu_read_unlock_bh();
+	preempt_enable();
 	if (statesold & RCUTORTURE_RDR_PREEMPT)
 		preempt_enable();
-	if (statesold & RCUTORTURE_RDR_RBH)
-		rcu_read_unlock_bh();
 	if (statesold & RCUTORTURE_RDR_SCHED)
 		rcu_read_unlock_sched();
+	if (statesold & RCUTORTURE_RDR_BH)
+		local_bh_enable();
+	if (statesold & RCUTORTURE_RDR_RBH)
+		rcu_read_unlock_bh();
+
 	if (statesold & RCUTORTURE_RDR_RCU) {
 		bool lockit = !statesnew && !(torture_random(trsp) & 0xffff);
 
@@ -1302,6 +1327,12 @@ rcutorture_extend_mask(int oldmask, struct torture_random_state *trsp)
 	int mask = rcutorture_extend_mask_max();
 	unsigned long randmask1 = torture_random(trsp) >> 8;
 	unsigned long randmask2 = randmask1 >> 3;
+	unsigned long preempts = RCUTORTURE_RDR_PREEMPT | RCUTORTURE_RDR_SCHED;
+	unsigned long preempts_irq = preempts | RCUTORTURE_RDR_IRQ;
+	unsigned long nonatomic_bhs = RCUTORTURE_RDR_BH | RCUTORTURE_RDR_RBH;
+	unsigned long atomic_bhs = RCUTORTURE_RDR_ATOM_BH |
+				   RCUTORTURE_RDR_ATOM_RBH;
+	unsigned long tmp;
 
 	WARN_ON_ONCE(mask >> RCUTORTURE_RDR_SHIFT);
 	/* Mostly only one bit (need preemption!), sometimes lots of bits. */
@@ -1309,11 +1340,49 @@ rcutorture_extend_mask(int oldmask, struct torture_random_state *trsp)
 		mask = mask & randmask2;
 	else
 		mask = mask & (1 << (randmask2 % RCUTORTURE_RDR_NBITS));
-	/* Can't enable bh w/irq disabled. */
-	if ((mask & RCUTORTURE_RDR_IRQ) &&
-	    ((!(mask & RCUTORTURE_RDR_BH) && (oldmask & RCUTORTURE_RDR_BH)) ||
-	     (!(mask & RCUTORTURE_RDR_RBH) && (oldmask & RCUTORTURE_RDR_RBH))))
-		mask |= RCUTORTURE_RDR_BH | RCUTORTURE_RDR_RBH;
+
+	/*
+	 * Can't enable bh w/irq disabled.
+	 */
+	tmp = atomic_bhs | nonatomic_bhs;
+	if (mask & RCUTORTURE_RDR_IRQ)
+		mask |= oldmask & tmp;
+
+	/*
+	 * Ideally these sequences would be detected in debug builds
+	 * (regardless of RT), but until then don't stop testing
+	 * them on non-RT.
+	 */
+	if (IS_ENABLED(CONFIG_PREEMPT_RT)) {
+		/*
+		 * Can't release the outermost rcu lock in an irq disabled
+		 * section without preemption also being disabled, if irqs
+		 * had ever been enabled during this RCU critical section
+		 * (could leak a special flag and delay reporting the qs).
+		 */
+		if ((oldmask & RCUTORTURE_RDR_RCU) &&
+		    (mask & RCUTORTURE_RDR_IRQ) &&
+		    !(mask & preempts))
+			mask |= RCUTORTURE_RDR_RCU;
+
+		/* Can't modify atomic bh in non-atomic context */
+		if ((oldmask & atomic_bhs) && (mask & atomic_bhs) &&
+		    !(mask & preempts_irq)) {
+			mask |= oldmask & preempts_irq;
+			if (mask & RCUTORTURE_RDR_IRQ)
+				mask |= oldmask & tmp;
+		}
+		if ((mask & atomic_bhs) && !(mask & preempts_irq))
+			mask |= RCUTORTURE_RDR_PREEMPT;
+
+		/* Can't modify non-atomic bh in atomic context */
+		tmp = nonatomic_bhs;
+		if (oldmask & preempts_irq)
+			mask &= ~tmp;
+		if ((oldmask | mask) & preempts_irq)
+			mask |= oldmask & tmp;
+	}
+
 	return mask ?: RCUTORTURE_RDR_RCU;
 }
 
diff --git a/kernel/rcu/tree.c b/kernel/rcu/tree.c
index 5dc36c6e80fd..782a3152bafc 100644
--- a/kernel/rcu/tree.c
+++ b/kernel/rcu/tree.c
@@ -100,8 +100,10 @@ static struct rcu_state rcu_state = {
 static bool dump_tree;
 module_param(dump_tree, bool, 0444);
 /* By default, use RCU_SOFTIRQ instead of rcuc kthreads. */
-static bool use_softirq = true;
+static bool use_softirq = !IS_ENABLED(CONFIG_PREEMPT_RT);
+#ifndef CONFIG_PREEMPT_RT
 module_param(use_softirq, bool, 0444);
+#endif
 /* Control rcu_node-tree auto-balancing at boot time. */
 static bool rcu_fanout_exact;
 module_param(rcu_fanout_exact, bool, 0444);
diff --git a/kernel/rcu/update.c b/kernel/rcu/update.c
index 39334d2d2b37..b95ae86c40a7 100644
--- a/kernel/rcu/update.c
+++ b/kernel/rcu/update.c
@@ -56,8 +56,10 @@
 #ifndef CONFIG_TINY_RCU
 module_param(rcu_expedited, int, 0);
 module_param(rcu_normal, int, 0);
-static int rcu_normal_after_boot;
+static int rcu_normal_after_boot = IS_ENABLED(CONFIG_PREEMPT_RT);
+#ifndef CONFIG_PREEMPT_RT
 module_param(rcu_normal_after_boot, int, 0);
+#endif
 #endif /* #ifndef CONFIG_TINY_RCU */
 
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 3a150445e0cb..f6b931d82443 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -64,7 +64,11 @@ const_debug unsigned int sysctl_sched_features =
  * Number of tasks to iterate in a single balance run.
  * Limited because this is done with IRQs disabled.
  */
+#ifdef CONFIG_PREEMPT_RT
+const_debug unsigned int sysctl_sched_nr_migrate = 8;
+#else
 const_debug unsigned int sysctl_sched_nr_migrate = 32;
+#endif
 
 /*
  * period over which we measure -rt task CPU usage in us.
@@ -510,9 +514,15 @@ static bool set_nr_if_polling(struct task_struct *p)
 #endif
 #endif
 
-static bool __wake_q_add(struct wake_q_head *head, struct task_struct *task)
+static bool __wake_q_add(struct wake_q_head *head, struct task_struct *task,
+			 bool sleeper)
 {
-	struct wake_q_node *node = &task->wake_q;
+	struct wake_q_node *node;
+
+	if (sleeper)
+		node = &task->wake_q_sleeper;
+	else
+		node = &task->wake_q;
 
 	/*
 	 * Atomically grab the task, if ->wake_q is !nil already it means
@@ -548,7 +558,13 @@ static bool __wake_q_add(struct wake_q_head *head, struct task_struct *task)
  */
 void wake_q_add(struct wake_q_head *head, struct task_struct *task)
 {
-	if (__wake_q_add(head, task))
+	if (__wake_q_add(head, task, false))
+		get_task_struct(task);
+}
+
+void wake_q_add_sleeper(struct wake_q_head *head, struct task_struct *task)
+{
+	if (__wake_q_add(head, task, true))
 		get_task_struct(task);
 }
 
@@ -571,28 +587,39 @@ void wake_q_add(struct wake_q_head *head, struct task_struct *task)
  */
 void wake_q_add_safe(struct wake_q_head *head, struct task_struct *task)
 {
-	if (!__wake_q_add(head, task))
+	if (!__wake_q_add(head, task, false))
 		put_task_struct(task);
 }
 
-void wake_up_q(struct wake_q_head *head)
+void __wake_up_q(struct wake_q_head *head, bool sleeper)
 {
 	struct wake_q_node *node = head->first;
 
 	while (node != WAKE_Q_TAIL) {
 		struct task_struct *task;
 
-		task = container_of(node, struct task_struct, wake_q);
+		if (sleeper)
+			task = container_of(node, struct task_struct, wake_q_sleeper);
+		else
+			task = container_of(node, struct task_struct, wake_q);
+
 		BUG_ON(!task);
 		/* Task can safely be re-inserted now: */
 		node = node->next;
-		task->wake_q.next = NULL;
 
+		if (sleeper)
+			task->wake_q_sleeper.next = NULL;
+		else
+			task->wake_q.next = NULL;
 		/*
 		 * wake_up_process() executes a full barrier, which pairs with
 		 * the queueing in wake_q_add() so as not to miss wakeups.
 		 */
-		wake_up_process(task);
+		if (sleeper)
+			wake_up_lock_sleeper(task);
+		else
+			wake_up_process(task);
+
 		put_task_struct(task);
 	}
 }
@@ -628,6 +655,48 @@ void resched_curr(struct rq *rq)
 		trace_sched_wake_idle_without_ipi(cpu);
 }
 
+#ifdef CONFIG_PREEMPT_LAZY
+
+static int tsk_is_polling(struct task_struct *p)
+{
+#ifdef TIF_POLLING_NRFLAG
+	return test_tsk_thread_flag(p, TIF_POLLING_NRFLAG);
+#else
+	return 0;
+#endif
+}
+
+void resched_curr_lazy(struct rq *rq)
+{
+	struct task_struct *curr = rq->curr;
+	int cpu;
+
+	if (!sched_feat(PREEMPT_LAZY)) {
+		resched_curr(rq);
+		return;
+	}
+
+	lockdep_assert_held(&rq->lock);
+
+	if (test_tsk_need_resched(curr))
+		return;
+
+	if (test_tsk_need_resched_lazy(curr))
+		return;
+
+	set_tsk_need_resched_lazy(curr);
+
+	cpu = cpu_of(rq);
+	if (cpu == smp_processor_id())
+		return;
+
+	/* NEED_RESCHED_LAZY must be visible before we test polling */
+	smp_mb();
+	if (!tsk_is_polling(curr))
+		smp_send_reschedule(cpu);
+}
+#endif
+
 void resched_cpu(int cpu)
 {
 	struct rq *rq = cpu_rq(cpu);
@@ -1694,6 +1763,82 @@ void check_preempt_curr(struct rq *rq, struct task_struct *p, int flags)
 
 #ifdef CONFIG_SMP
 
+static void
+__do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask, u32 flags);
+
+static int __set_cpus_allowed_ptr(struct task_struct *p,
+				  const struct cpumask *new_mask,
+				  u32 flags);
+
+static void migrate_disable_switch(struct rq *rq, struct task_struct *p)
+{
+	if (likely(!p->migration_disabled))
+		return;
+
+	if (p->cpus_ptr != &p->cpus_mask)
+		return;
+
+	/*
+	 * Violates locking rules! see comment in __do_set_cpus_allowed().
+	 */
+	__do_set_cpus_allowed(p, cpumask_of(rq->cpu), SCA_MIGRATE_DISABLE);
+}
+
+void migrate_disable(void)
+{
+	struct task_struct *p = current;
+
+	if (p->migration_disabled) {
+		p->migration_disabled++;
+		return;
+	}
+
+	trace_sched_migrate_disable_tp(p);
+
+	preempt_disable();
+	this_rq()->nr_pinned++;
+	p->migration_disabled = 1;
+	preempt_lazy_disable();
+	preempt_enable();
+}
+EXPORT_SYMBOL_GPL(migrate_disable);
+
+void migrate_enable(void)
+{
+	struct task_struct *p = current;
+
+	if (p->migration_disabled > 1) {
+		p->migration_disabled--;
+		return;
+	}
+
+	/*
+	 * Ensure stop_task runs either before or after this, and that
+	 * __set_cpus_allowed_ptr(SCA_MIGRATE_ENABLE) doesn't schedule().
+	 */
+	preempt_disable();
+	if (p->cpus_ptr != &p->cpus_mask)
+		__set_cpus_allowed_ptr(p, &p->cpus_mask, SCA_MIGRATE_ENABLE);
+	/*
+	 * Mustn't clear migration_disabled() until cpus_ptr points back at the
+	 * regular cpus_mask, otherwise things that race (eg.
+	 * select_fallback_rq) get confused.
+	 */
+	barrier();
+	p->migration_disabled = 0;
+	this_rq()->nr_pinned--;
+	preempt_lazy_enable();
+	preempt_enable();
+
+	trace_sched_migrate_enable_tp(p);
+}
+EXPORT_SYMBOL_GPL(migrate_enable);
+
+static inline bool rq_has_pinned_tasks(struct rq *rq)
+{
+	return rq->nr_pinned;
+}
+
 /*
  * Per-CPU kthreads are allowed to run on !active && online CPUs, see
  * __set_cpus_allowed_ptr() and select_fallback_rq().
@@ -1703,7 +1848,7 @@ static inline bool is_cpu_allowed(struct task_struct *p, int cpu)
 	if (!cpumask_test_cpu(cpu, p->cpus_ptr))
 		return false;
 
-	if (is_per_cpu_kthread(p))
+	if (is_per_cpu_kthread(p) || is_migration_disabled(p))
 		return cpu_online(cpu);
 
 	return cpu_active(cpu);
@@ -1748,8 +1893,16 @@ static struct rq *move_queued_task(struct rq *rq, struct rq_flags *rf,
 }
 
 struct migration_arg {
-	struct task_struct *task;
-	int dest_cpu;
+	struct task_struct		*task;
+	int				dest_cpu;
+	struct set_affinity_pending	*pending;
+};
+
+struct set_affinity_pending {
+	refcount_t		refs;
+	struct completion	done;
+	struct cpu_stop_work	stop_work;
+	struct migration_arg	arg;
 };
 
 /*
@@ -1781,16 +1934,19 @@ static struct rq *__migrate_task(struct rq *rq, struct rq_flags *rf,
  */
 static int migration_cpu_stop(void *data)
 {
+	struct set_affinity_pending *pending;
 	struct migration_arg *arg = data;
 	struct task_struct *p = arg->task;
+	int dest_cpu = arg->dest_cpu;
 	struct rq *rq = this_rq();
+	bool complete = false;
 	struct rq_flags rf;
 
 	/*
 	 * The original target CPU might have gone down and we might
 	 * be on another CPU but it doesn't matter.
 	 */
-	local_irq_disable();
+	local_irq_save(rf.flags);
 	/*
 	 * We need to explicitly wake pending tasks before running
 	 * __migrate_task() such that we will not miss enforcing cpus_ptr
@@ -1800,21 +1956,137 @@ static int migration_cpu_stop(void *data)
 
 	raw_spin_lock(&p->pi_lock);
 	rq_lock(rq, &rf);
+
+	pending = p->migration_pending;
 	/*
 	 * If task_rq(p) != rq, it cannot be migrated here, because we're
 	 * holding rq->lock, if p->on_rq == 0 it cannot get enqueued because
 	 * we're holding p->pi_lock.
 	 */
 	if (task_rq(p) == rq) {
+		if (is_migration_disabled(p))
+			goto out;
+
+		if (pending) {
+			p->migration_pending = NULL;
+			complete = true;
+		}
+
+		/* migrate_enable() --  we must not race against SCA */
+		if (dest_cpu < 0) {
+			/*
+			 * When this was migrate_enable() but we no longer
+			 * have a @pending, a concurrent SCA 'fixed' things
+			 * and we should be valid again. Nothing to do.
+			 */
+			if (!pending) {
+				WARN_ON_ONCE(!cpumask_test_cpu(task_cpu(p), &p->cpus_mask));
+				goto out;
+			}
+
+			dest_cpu = cpumask_any_distribute(&p->cpus_mask);
+		}
+
 		if (task_on_rq_queued(p))
-			rq = __migrate_task(rq, &rf, p, arg->dest_cpu);
+			rq = __migrate_task(rq, &rf, p, dest_cpu);
 		else
-			p->wake_cpu = arg->dest_cpu;
+			p->wake_cpu = dest_cpu;
+
+	} else if (dest_cpu < 0 || pending) {
+		/*
+		 * This happens when we get migrated between migrate_enable()'s
+		 * preempt_enable() and scheduling the stopper task. At that
+		 * point we're a regular task again and not current anymore.
+		 *
+		 * A !PREEMPT kernel has a giant hole here, which makes it far
+		 * more likely.
+		 */
+
+		/*
+		 * The task moved before the stopper got to run. We're holding
+		 * ->pi_lock, so the allowed mask is stable - if it got
+		 * somewhere allowed, we're done.
+		 */
+		if (pending && cpumask_test_cpu(task_cpu(p), p->cpus_ptr)) {
+			p->migration_pending = NULL;
+			complete = true;
+			goto out;
+		}
+
+		/*
+		 * When this was migrate_enable() but we no longer have an
+		 * @pending, a concurrent SCA 'fixed' things and we should be
+		 * valid again. Nothing to do.
+		 */
+		if (!pending) {
+			WARN_ON_ONCE(!cpumask_test_cpu(task_cpu(p), &p->cpus_mask));
+			goto out;
+		}
+
+		/*
+		 * When migrate_enable() hits a rq mis-match we can't reliably
+		 * determine is_migration_disabled() and so have to chase after
+		 * it.
+		 */
+		task_rq_unlock(rq, p, &rf);
+		stop_one_cpu_nowait(task_cpu(p), migration_cpu_stop,
+				    &pending->arg, &pending->stop_work);
+		return 0;
 	}
-	rq_unlock(rq, &rf);
-	raw_spin_unlock(&p->pi_lock);
+out:
+	task_rq_unlock(rq, p, &rf);
+
+	if (complete)
+		complete_all(&pending->done);
+
+	/* For pending->{arg,stop_work} */
+	pending = arg->pending;
+	if (pending && refcount_dec_and_test(&pending->refs))
+		wake_up_var(&pending->refs);
 
-	local_irq_enable();
+	return 0;
+}
+
+int push_cpu_stop(void *arg)
+{
+	struct rq *lowest_rq = NULL, *rq = this_rq();
+	struct task_struct *p = arg;
+
+	raw_spin_lock_irq(&p->pi_lock);
+	raw_spin_lock(&rq->lock);
+
+	if (task_rq(p) != rq)
+		goto out_unlock;
+
+	if (is_migration_disabled(p)) {
+		p->migration_flags |= MDF_PUSH;
+		goto out_unlock;
+	}
+
+	p->migration_flags &= ~MDF_PUSH;
+
+	if (p->sched_class->find_lock_rq)
+		lowest_rq = p->sched_class->find_lock_rq(p, rq);
+
+	if (!lowest_rq)
+		goto out_unlock;
+
+	// XXX validate p is still the highest prio task
+	if (task_rq(p) == rq) {
+		deactivate_task(rq, p, 0);
+		set_task_cpu(p, lowest_rq->cpu);
+		activate_task(lowest_rq, p, 0);
+		resched_curr(lowest_rq);
+	}
+
+	double_unlock_balance(rq, lowest_rq);
+
+out_unlock:
+	rq->push_busy = false;
+	raw_spin_unlock(&rq->lock);
+	raw_spin_unlock_irq(&p->pi_lock);
+
+	put_task_struct(p);
 	return 0;
 }
 
@@ -1822,18 +2094,39 @@ static int migration_cpu_stop(void *data)
  * sched_class::set_cpus_allowed must do the below, but is not required to
  * actually call this function.
  */
-void set_cpus_allowed_common(struct task_struct *p, const struct cpumask *new_mask)
+void set_cpus_allowed_common(struct task_struct *p, const struct cpumask *new_mask, u32 flags)
 {
+	if (flags & (SCA_MIGRATE_ENABLE | SCA_MIGRATE_DISABLE)) {
+		p->cpus_ptr = new_mask;
+		return;
+	}
+
 	cpumask_copy(&p->cpus_mask, new_mask);
 	p->nr_cpus_allowed = cpumask_weight(new_mask);
 }
 
-void do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask)
+static void
+__do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask, u32 flags)
 {
 	struct rq *rq = task_rq(p);
 	bool queued, running;
 
-	lockdep_assert_held(&p->pi_lock);
+	/*
+	 * This here violates the locking rules for affinity, since we're only
+	 * supposed to change these variables while holding both rq->lock and
+	 * p->pi_lock.
+	 *
+	 * HOWEVER, it magically works, because ttwu() is the only code that
+	 * accesses these variables under p->pi_lock and only does so after
+	 * smp_cond_load_acquire(&p->on_cpu, !VAL), and we're in __schedule()
+	 * before finish_task().
+	 *
+	 * XXX do further audits, this smells like something putrid.
+	 */
+	if (flags & SCA_MIGRATE_DISABLE)
+		SCHED_WARN_ON(!p->on_cpu);
+	else
+		lockdep_assert_held(&p->pi_lock);
 
 	queued = task_on_rq_queued(p);
 	running = task_current(rq, p);
@@ -1849,7 +2142,7 @@ void do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask)
 	if (running)
 		put_prev_task(rq, p);
 
-	p->sched_class->set_cpus_allowed(p, new_mask);
+	p->sched_class->set_cpus_allowed(p, new_mask, flags);
 
 	if (queued)
 		enqueue_task(rq, p, ENQUEUE_RESTORE | ENQUEUE_NOCLOCK);
@@ -1857,6 +2150,208 @@ void do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask)
 		set_next_task(rq, p);
 }
 
+void do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask)
+{
+	__do_set_cpus_allowed(p, new_mask, 0);
+}
+
+/*
+ * This function is wildly self concurrent; here be dragons.
+ *
+ *
+ * When given a valid mask, __set_cpus_allowed_ptr() must block until the
+ * designated task is enqueued on an allowed CPU. If that task is currently
+ * running, we have to kick it out using the CPU stopper.
+ *
+ * Migrate-Disable comes along and tramples all over our nice sandcastle.
+ * Consider:
+ *
+ *     Initial conditions: P0->cpus_mask = [0, 1]
+ *
+ *     P0@CPU0                  P1
+ *
+ *     migrate_disable();
+ *     <preempted>
+ *                              set_cpus_allowed_ptr(P0, [1]);
+ *
+ * P1 *cannot* return from this set_cpus_allowed_ptr() call until P0 executes
+ * its outermost migrate_enable() (i.e. it exits its Migrate-Disable region).
+ * This means we need the following scheme:
+ *
+ *     P0@CPU0                  P1
+ *
+ *     migrate_disable();
+ *     <preempted>
+ *                              set_cpus_allowed_ptr(P0, [1]);
+ *                                <blocks>
+ *     <resumes>
+ *     migrate_enable();
+ *       __set_cpus_allowed_ptr();
+ *       <wakes local stopper>
+ *                         `--> <woken on migration completion>
+ *
+ * Now the fun stuff: there may be several P1-like tasks, i.e. multiple
+ * concurrent set_cpus_allowed_ptr(P0, [*]) calls. CPU affinity changes of any
+ * task p are serialized by p->pi_lock, which we can leverage: the one that
+ * should come into effect at the end of the Migrate-Disable region is the last
+ * one. This means we only need to track a single cpumask (i.e. p->cpus_mask),
+ * but we still need to properly signal those waiting tasks at the appropriate
+ * moment.
+ *
+ * This is implemented using struct set_affinity_pending. The first
+ * __set_cpus_allowed_ptr() caller within a given Migrate-Disable region will
+ * setup an instance of that struct and install it on the targeted task_struct.
+ * Any and all further callers will reuse that instance. Those then wait for
+ * a completion signaled at the tail of the CPU stopper callback (1), triggered
+ * on the end of the Migrate-Disable region (i.e. outermost migrate_enable()).
+ *
+ *
+ * (1) In the cases covered above. There is one more where the completion is
+ * signaled within affine_move_task() itself: when a subsequent affinity request
+ * cancels the need for an active migration. Consider:
+ *
+ *     Initial conditions: P0->cpus_mask = [0, 1]
+ *
+ *     P0@CPU0            P1                             P2
+ *
+ *     migrate_disable();
+ *     <preempted>
+ *                        set_cpus_allowed_ptr(P0, [1]);
+ *                          <blocks>
+ *                                                       set_cpus_allowed_ptr(P0, [0, 1]);
+ *                                                         <signal completion>
+ *                          <awakes>
+ *
+ * Note that the above is safe vs a concurrent migrate_enable(), as any
+ * pending affinity completion is preceded an uninstallion of
+ * p->migration_pending done with p->pi_lock held.
+ */
+static int affine_move_task(struct rq *rq, struct task_struct *p, struct rq_flags *rf,
+			    int dest_cpu, unsigned int flags)
+{
+	struct set_affinity_pending my_pending = { }, *pending = NULL;
+	struct migration_arg arg = {
+		.task = p,
+		.dest_cpu = dest_cpu,
+	};
+	bool complete = false;
+
+	/* Can the task run on the task's current CPU? If so, we're done */
+	if (cpumask_test_cpu(task_cpu(p), &p->cpus_mask)) {
+		struct task_struct *push_task = NULL;
+
+		if ((flags & SCA_MIGRATE_ENABLE) &&
+		    (p->migration_flags & MDF_PUSH) && !rq->push_busy) {
+			rq->push_busy = true;
+			push_task = get_task_struct(p);
+		}
+
+		pending = p->migration_pending;
+		if (pending) {
+			refcount_inc(&pending->refs);
+			p->migration_pending = NULL;
+			complete = true;
+		}
+		task_rq_unlock(rq, p, rf);
+
+		if (push_task) {
+			stop_one_cpu_nowait(rq->cpu, push_cpu_stop,
+					    p, &rq->push_work);
+		}
+
+		if (complete)
+			goto do_complete;
+
+		return 0;
+	}
+
+	if (!(flags & SCA_MIGRATE_ENABLE)) {
+		/* serialized by p->pi_lock */
+		if (!p->migration_pending) {
+			/* Install the request */
+			refcount_set(&my_pending.refs, 1);
+			init_completion(&my_pending.done);
+			p->migration_pending = &my_pending;
+		} else {
+			pending = p->migration_pending;
+			refcount_inc(&pending->refs);
+		}
+	}
+	pending = p->migration_pending;
+	/*
+	 * - !MIGRATE_ENABLE:
+	 *   we'll have installed a pending if there wasn't one already.
+	 *
+	 * - MIGRATE_ENABLE:
+	 *   we're here because the current CPU isn't matching anymore,
+	 *   the only way that can happen is because of a concurrent
+	 *   set_cpus_allowed_ptr() call, which should then still be
+	 *   pending completion.
+	 *
+	 * Either way, we really should have a @pending here.
+	 */
+	if (WARN_ON_ONCE(!pending)) {
+		task_rq_unlock(rq, p, rf);
+		return -EINVAL;
+	}
+
+	if (flags & SCA_MIGRATE_ENABLE) {
+
+		refcount_inc(&pending->refs); /* pending->{arg,stop_work} */
+		p->migration_flags &= ~MDF_PUSH;
+		task_rq_unlock(rq, p, rf);
+
+		pending->arg = (struct migration_arg) {
+			.task = p,
+			.dest_cpu = -1,
+			.pending = pending,
+		};
+
+		stop_one_cpu_nowait(cpu_of(rq), migration_cpu_stop,
+				    &pending->arg, &pending->stop_work);
+
+		return 0;
+	}
+
+	if (task_running(rq, p) || p->state == TASK_WAKING) {
+		/*
+		 * Lessen races (and headaches) by delegating
+		 * is_migration_disabled(p) checks to the stopper, which will
+		 * run on the same CPU as said p.
+		 */
+		task_rq_unlock(rq, p, rf);
+		stop_one_cpu(cpu_of(rq), migration_cpu_stop, &arg);
+
+	} else {
+
+		if (!is_migration_disabled(p)) {
+			if (task_on_rq_queued(p))
+				rq = move_queued_task(rq, rf, p, dest_cpu);
+
+			p->migration_pending = NULL;
+			complete = true;
+		}
+		task_rq_unlock(rq, p, rf);
+
+do_complete:
+		if (complete)
+			complete_all(&pending->done);
+	}
+
+	wait_for_completion(&pending->done);
+
+	if (refcount_dec_and_test(&pending->refs))
+		wake_up_var(&pending->refs);
+
+	/*
+	 * Block the original owner of &pending until all subsequent callers
+	 * have seen the completion and decremented the refcount
+	 */
+	wait_var_event(&my_pending.refs, !refcount_read(&my_pending.refs));
+
+	return 0;
+}
+
 /*
  * Change a given task's CPU affinity. Migrate the thread to a
  * proper CPU and schedule it away if the CPU it's executing on
@@ -1867,7 +2362,8 @@ void do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask)
  * call is not atomic; no spinlocks may be held.
  */
 static int __set_cpus_allowed_ptr(struct task_struct *p,
-				  const struct cpumask *new_mask, bool check)
+				  const struct cpumask *new_mask,
+				  u32 flags)
 {
 	const struct cpumask *cpu_valid_mask = cpu_active_mask;
 	unsigned int dest_cpu;
@@ -1878,9 +2374,14 @@ static int __set_cpus_allowed_ptr(struct task_struct *p,
 	rq = task_rq_lock(p, &rf);
 	update_rq_clock(rq);
 
-	if (p->flags & PF_KTHREAD) {
+	if (p->flags & PF_KTHREAD || is_migration_disabled(p)) {
 		/*
-		 * Kernel threads are allowed on online && !active CPUs
+		 * Kernel threads are allowed on online && !active CPUs.
+		 *
+		 * Specifically, migration_disabled() tasks must not fail the
+		 * cpumask_any_and_distribute() pick below, esp. so on
+		 * SCA_MIGRATE_ENABLE, otherwise we'll not call
+		 * set_cpus_allowed_common() and actually reset p->cpus_ptr.
 		 */
 		cpu_valid_mask = cpu_online_mask;
 	}
@@ -1889,13 +2390,22 @@ static int __set_cpus_allowed_ptr(struct task_struct *p,
 	 * Must re-check here, to close a race against __kthread_bind(),
 	 * sched_setaffinity() is not guaranteed to observe the flag.
 	 */
-	if (check && (p->flags & PF_NO_SETAFFINITY)) {
+	if ((flags & SCA_CHECK) && (p->flags & PF_NO_SETAFFINITY)) {
 		ret = -EINVAL;
 		goto out;
 	}
 
-	if (cpumask_equal(&p->cpus_mask, new_mask))
-		goto out;
+	if (!(flags & SCA_MIGRATE_ENABLE)) {
+		if (cpumask_equal(&p->cpus_mask, new_mask))
+			goto out;
+
+		if (WARN_ON_ONCE(p == current &&
+				 is_migration_disabled(p) &&
+				 !cpumask_test_cpu(task_cpu(p), new_mask))) {
+			ret = -EBUSY;
+			goto out;
+		}
+	}
 
 	/*
 	 * Picking a ~random cpu helps in cases where we are changing affinity
@@ -1908,7 +2418,7 @@ static int __set_cpus_allowed_ptr(struct task_struct *p,
 		goto out;
 	}
 
-	do_set_cpus_allowed(p, new_mask);
+	__do_set_cpus_allowed(p, new_mask, flags);
 
 	if (p->flags & PF_KTHREAD) {
 		/*
@@ -1920,23 +2430,8 @@ static int __set_cpus_allowed_ptr(struct task_struct *p,
 			p->nr_cpus_allowed != 1);
 	}
 
-	/* Can the task run on the task's current CPU? If so, we're done */
-	if (cpumask_test_cpu(task_cpu(p), new_mask))
-		goto out;
+	return affine_move_task(rq, p, &rf, dest_cpu, flags);
 
-	if (task_running(rq, p) || p->state == TASK_WAKING) {
-		struct migration_arg arg = { p, dest_cpu };
-		/* Need help from migration thread: drop lock and wait. */
-		task_rq_unlock(rq, p, &rf);
-		stop_one_cpu(cpu_of(rq), migration_cpu_stop, &arg);
-		return 0;
-	} else if (task_on_rq_queued(p)) {
-		/*
-		 * OK, since we're going to drop the lock immediately
-		 * afterwards anyway.
-		 */
-		rq = move_queued_task(rq, &rf, p, dest_cpu);
-	}
 out:
 	task_rq_unlock(rq, p, &rf);
 
@@ -1945,7 +2440,7 @@ static int __set_cpus_allowed_ptr(struct task_struct *p,
 
 int set_cpus_allowed_ptr(struct task_struct *p, const struct cpumask *new_mask)
 {
-	return __set_cpus_allowed_ptr(p, new_mask, false);
+	return __set_cpus_allowed_ptr(p, new_mask, 0);
 }
 EXPORT_SYMBOL_GPL(set_cpus_allowed_ptr);
 
@@ -1986,6 +2481,8 @@ void set_task_cpu(struct task_struct *p, unsigned int new_cpu)
 	 * Clearly, migrating tasks to offline CPUs is a fairly daft thing.
 	 */
 	WARN_ON_ONCE(!cpu_online(new_cpu));
+
+	WARN_ON_ONCE(is_migration_disabled(p));
 #endif
 
 	trace_sched_migrate_task(p, new_cpu);
@@ -2118,6 +2615,18 @@ int migrate_swap(struct task_struct *cur, struct task_struct *p,
 }
 #endif /* CONFIG_NUMA_BALANCING */
 
+static bool check_task_state(struct task_struct *p, long match_state)
+{
+	bool match = false;
+
+	raw_spin_lock_irq(&p->pi_lock);
+	if (p->state == match_state || p->saved_state == match_state)
+		match = true;
+	raw_spin_unlock_irq(&p->pi_lock);
+
+	return match;
+}
+
 /*
  * wait_task_inactive - wait for a thread to unschedule.
  *
@@ -2162,7 +2671,7 @@ unsigned long wait_task_inactive(struct task_struct *p, long match_state)
 		 * is actually now running somewhere else!
 		 */
 		while (task_running(rq, p)) {
-			if (match_state && unlikely(p->state != match_state))
+			if (match_state && !check_task_state(p, match_state))
 				return 0;
 			cpu_relax();
 		}
@@ -2177,7 +2686,8 @@ unsigned long wait_task_inactive(struct task_struct *p, long match_state)
 		running = task_running(rq, p);
 		queued = task_on_rq_queued(p);
 		ncsw = 0;
-		if (!match_state || p->state == match_state)
+		if (!match_state || p->state == match_state ||
+		    p->saved_state == match_state)
 			ncsw = p->nvcsw | LONG_MIN; /* sets MSB */
 		task_rq_unlock(rq, p, &rf);
 
@@ -2316,6 +2826,12 @@ static int select_fallback_rq(int cpu, struct task_struct *p)
 			}
 			fallthrough;
 		case possible:
+			/*
+			 * XXX When called from select_task_rq() we only
+			 * hold p->pi_lock and again violate locking order.
+			 *
+			 * More yuck to audit.
+			 */
 			do_set_cpus_allowed(p, cpu_possible_mask);
 			state = fail;
 			break;
@@ -2350,7 +2866,7 @@ int select_task_rq(struct task_struct *p, int cpu, int sd_flags, int wake_flags)
 {
 	lockdep_assert_held(&p->pi_lock);
 
-	if (p->nr_cpus_allowed > 1)
+	if (p->nr_cpus_allowed > 1 && !is_migration_disabled(p))
 		cpu = p->sched_class->select_task_rq(p, cpu, sd_flags, wake_flags);
 	else
 		cpu = cpumask_any(p->cpus_ptr);
@@ -2373,6 +2889,7 @@ int select_task_rq(struct task_struct *p, int cpu, int sd_flags, int wake_flags)
 
 void sched_set_stop_task(int cpu, struct task_struct *stop)
 {
+	static struct lock_class_key stop_pi_lock;
 	struct sched_param param = { .sched_priority = MAX_RT_PRIO - 1 };
 	struct task_struct *old_stop = cpu_rq(cpu)->stop;
 
@@ -2388,6 +2905,20 @@ void sched_set_stop_task(int cpu, struct task_struct *stop)
 		sched_setscheduler_nocheck(stop, SCHED_FIFO, &param);
 
 		stop->sched_class = &stop_sched_class;
+
+		/*
+		 * The PI code calls rt_mutex_setprio() with ->pi_lock held to
+		 * adjust the effective priority of a task. As a result,
+		 * rt_mutex_setprio() can trigger (RT) balancing operations,
+		 * which can then trigger wakeups of the stop thread to push
+		 * around the current task.
+		 *
+		 * The stop task itself will never be part of the PI-chain, it
+		 * never blocks, therefore that ->pi_lock recursion is safe.
+		 * Tell lockdep about this by placing the stop->pi_lock in its
+		 * own class.
+		 */
+		lockdep_set_class(&stop->pi_lock, &stop_pi_lock);
 	}
 
 	cpu_rq(cpu)->stop = stop;
@@ -2401,15 +2932,23 @@ void sched_set_stop_task(int cpu, struct task_struct *stop)
 	}
 }
 
-#else
+#else /* CONFIG_SMP */
 
 static inline int __set_cpus_allowed_ptr(struct task_struct *p,
-					 const struct cpumask *new_mask, bool check)
+					 const struct cpumask *new_mask,
+					 u32 flags)
 {
 	return set_cpus_allowed_ptr(p, new_mask);
 }
 
-#endif /* CONFIG_SMP */
+static inline void migrate_disable_switch(struct rq *rq, struct task_struct *p) { }
+
+static inline bool rq_has_pinned_tasks(struct rq *rq)
+{
+	return false;
+}
+
+#endif /* !CONFIG_SMP */
 
 static void
 ttwu_stat(struct task_struct *p, int cpu, int wake_flags)
@@ -2827,7 +3366,7 @@ try_to_wake_up(struct task_struct *p, unsigned int state, int wake_flags)
 	int cpu, success = 0;
 
 	preempt_disable();
-	if (p == current) {
+	if (!IS_ENABLED(CONFIG_PREEMPT_RT) && p == current) {
 		/*
 		 * We're waking current, this means 'p->on_rq' and 'task_cpu(p)
 		 * == smp_processor_id()'. Together this means we can special
@@ -2857,8 +3396,26 @@ try_to_wake_up(struct task_struct *p, unsigned int state, int wake_flags)
 	 */
 	raw_spin_lock_irqsave(&p->pi_lock, flags);
 	smp_mb__after_spinlock();
-	if (!(p->state & state))
+	if (!(p->state & state)) {
+		/*
+		 * The task might be running due to a spinlock sleeper
+		 * wakeup. Check the saved state and set it to running
+		 * if the wakeup condition is true.
+		 */
+		if (!(wake_flags & WF_LOCK_SLEEPER)) {
+			if (p->saved_state & state) {
+				p->saved_state = TASK_RUNNING;
+				success = 1;
+			}
+		}
 		goto unlock;
+	}
+	/*
+	 * If this is a regular wakeup, then we can unconditionally
+	 * clear the saved state of a "lock sleeper".
+	 */
+	if (!(wake_flags & WF_LOCK_SLEEPER))
+		p->saved_state = TASK_RUNNING;
 
 	trace_sched_waking(p);
 
@@ -3047,6 +3604,18 @@ int wake_up_process(struct task_struct *p)
 }
 EXPORT_SYMBOL(wake_up_process);
 
+/**
+ * wake_up_lock_sleeper - Wake up a specific process blocked on a "sleeping lock"
+ * @p: The process to be woken up.
+ *
+ * Same as wake_up_process() above, but wake_flags=WF_LOCK_SLEEPER to indicate
+ * the nature of the wakeup.
+ */
+int wake_up_lock_sleeper(struct task_struct *p)
+{
+	return try_to_wake_up(p, TASK_UNINTERRUPTIBLE, WF_LOCK_SLEEPER);
+}
+
 int wake_up_state(struct task_struct *p, unsigned int state)
 {
 	return try_to_wake_up(p, state, 0);
@@ -3100,6 +3669,7 @@ static void __sched_fork(unsigned long clone_flags, struct task_struct *p)
 	init_numa_balancing(clone_flags, p);
 #ifdef CONFIG_SMP
 	p->wake_entry.u_flags = CSD_TYPE_TTWU;
+	p->migration_pending = NULL;
 #endif
 }
 
@@ -3293,6 +3863,9 @@ int sched_fork(unsigned long clone_flags, struct task_struct *p)
 	p->on_cpu = 0;
 #endif
 	init_task_preempt_count(p);
+#ifdef CONFIG_HAVE_PREEMPT_LAZY
+	task_thread_info(p)->preempt_lazy_count = 0;
+#endif
 #ifdef CONFIG_SMP
 	plist_node_init(&p->pushable_tasks, MAX_PRIO);
 	RB_CLEAR_NODE(&p->pushable_dl_tasks);
@@ -3444,49 +4017,133 @@ fire_sched_out_preempt_notifiers(struct task_struct *curr,
 
 #else /* !CONFIG_PREEMPT_NOTIFIERS */
 
-static inline void fire_sched_in_preempt_notifiers(struct task_struct *curr)
+static inline void fire_sched_in_preempt_notifiers(struct task_struct *curr)
+{
+}
+
+static inline void
+fire_sched_out_preempt_notifiers(struct task_struct *curr,
+				 struct task_struct *next)
+{
+}
+
+#endif /* CONFIG_PREEMPT_NOTIFIERS */
+
+static inline void prepare_task(struct task_struct *next)
+{
+#ifdef CONFIG_SMP
+	/*
+	 * Claim the task as running, we do this before switching to it
+	 * such that any running task will have this set.
+	 *
+	 * See the ttwu() WF_ON_CPU case and its ordering comment.
+	 */
+	WRITE_ONCE(next->on_cpu, 1);
+#endif
+}
+
+static inline void finish_task(struct task_struct *prev)
+{
+#ifdef CONFIG_SMP
+	/*
+	 * This must be the very last reference to @prev from this CPU. After
+	 * p->on_cpu is cleared, the task can be moved to a different CPU. We
+	 * must ensure this doesn't happen until the switch is completely
+	 * finished.
+	 *
+	 * In particular, the load of prev->state in finish_task_switch() must
+	 * happen before this.
+	 *
+	 * Pairs with the smp_cond_load_acquire() in try_to_wake_up().
+	 */
+	smp_store_release(&prev->on_cpu, 0);
+#endif
+}
+
+#ifdef CONFIG_SMP
+
+static void do_balance_callbacks(struct rq *rq, struct callback_head *head)
+{
+	void (*func)(struct rq *rq);
+	struct callback_head *next;
+
+	lockdep_assert_held(&rq->lock);
+
+	while (head) {
+		func = (void (*)(struct rq *))head->func;
+		next = head->next;
+		head->next = NULL;
+		head = next;
+
+		func(rq);
+	}
+}
+
+static inline struct callback_head *splice_balance_callbacks(struct rq *rq)
+{
+	struct callback_head *head = rq->balance_callback;
+
+	lockdep_assert_held(&rq->lock);
+	if (head) {
+		rq->balance_callback = NULL;
+		rq->balance_flags &= ~BALANCE_WORK;
+	}
+
+	return head;
+}
+
+static void __balance_callbacks(struct rq *rq)
+{
+	do_balance_callbacks(rq, splice_balance_callbacks(rq));
+}
+
+static inline void balance_callbacks(struct rq *rq, struct callback_head *head)
+{
+	unsigned long flags;
+
+	if (unlikely(head)) {
+		raw_spin_lock_irqsave(&rq->lock, flags);
+		do_balance_callbacks(rq, head);
+		raw_spin_unlock_irqrestore(&rq->lock, flags);
+	}
+}
+
+static void balance_push(struct rq *rq);
+
+static inline void balance_switch(struct rq *rq)
+{
+	if (likely(!rq->balance_flags))
+		return;
+
+	if (rq->balance_flags & BALANCE_PUSH) {
+		balance_push(rq);
+		return;
+	}
+
+	__balance_callbacks(rq);
+}
+
+#else
+
+static inline void __balance_callbacks(struct rq *rq)
 {
 }
 
-static inline void
-fire_sched_out_preempt_notifiers(struct task_struct *curr,
-				 struct task_struct *next)
+static inline struct callback_head *splice_balance_callbacks(struct rq *rq)
 {
+	return NULL;
 }
 
-#endif /* CONFIG_PREEMPT_NOTIFIERS */
-
-static inline void prepare_task(struct task_struct *next)
+static inline void balance_callbacks(struct rq *rq, struct callback_head *head)
 {
-#ifdef CONFIG_SMP
-	/*
-	 * Claim the task as running, we do this before switching to it
-	 * such that any running task will have this set.
-	 *
-	 * See the ttwu() WF_ON_CPU case and its ordering comment.
-	 */
-	WRITE_ONCE(next->on_cpu, 1);
-#endif
 }
 
-static inline void finish_task(struct task_struct *prev)
+static inline void balance_switch(struct rq *rq)
 {
-#ifdef CONFIG_SMP
-	/*
-	 * This must be the very last reference to @prev from this CPU. After
-	 * p->on_cpu is cleared, the task can be moved to a different CPU. We
-	 * must ensure this doesn't happen until the switch is completely
-	 * finished.
-	 *
-	 * In particular, the load of prev->state in finish_task_switch() must
-	 * happen before this.
-	 *
-	 * Pairs with the smp_cond_load_acquire() in try_to_wake_up().
-	 */
-	smp_store_release(&prev->on_cpu, 0);
-#endif
 }
 
+#endif
+
 static inline void
 prepare_lock_switch(struct rq *rq, struct task_struct *next, struct rq_flags *rf)
 {
@@ -3512,6 +4169,7 @@ static inline void finish_lock_switch(struct rq *rq)
 	 * prev into current:
 	 */
 	spin_acquire(&rq->lock.dep_map, 0, 0, _THIS_IP_);
+	balance_switch(rq);
 	raw_spin_unlock_irq(&rq->lock);
 }
 
@@ -3527,6 +4185,22 @@ static inline void finish_lock_switch(struct rq *rq)
 # define finish_arch_post_lock_switch()	do { } while (0)
 #endif
 
+static inline void kmap_local_sched_out(void)
+{
+#ifdef CONFIG_KMAP_LOCAL
+	if (unlikely(current->kmap_ctrl.idx))
+		__kmap_local_sched_out();
+#endif
+}
+
+static inline void kmap_local_sched_in(void)
+{
+#ifdef CONFIG_KMAP_LOCAL
+	if (unlikely(current->kmap_ctrl.idx))
+		__kmap_local_sched_in();
+#endif
+}
+
 /**
  * prepare_task_switch - prepare to switch tasks
  * @rq: the runqueue preparing to switch
@@ -3549,6 +4223,7 @@ prepare_task_switch(struct rq *rq, struct task_struct *prev,
 	perf_event_task_sched_out(prev, next);
 	rseq_preempt(prev);
 	fire_sched_out_preempt_notifiers(prev, next);
+	kmap_local_sched_out();
 	prepare_task(next);
 	prepare_arch_switch(next);
 }
@@ -3615,6 +4290,7 @@ static struct rq *finish_task_switch(struct task_struct *prev)
 	finish_lock_switch(rq);
 	finish_arch_post_lock_switch();
 	kcov_finish_switch(current);
+	kmap_local_sched_in();
 
 	fire_sched_in_preempt_notifiers(current);
 	/*
@@ -3629,23 +4305,18 @@ static struct rq *finish_task_switch(struct task_struct *prev)
 	 *   provided by mmdrop(),
 	 * - a sync_core for SYNC_CORE.
 	 */
+	/*
+	 * We use mmdrop_delayed() here so we don't have to do the
+	 * full __mmdrop() when we are the last user.
+	 */
 	if (mm) {
 		membarrier_mm_sync_core_before_usermode(mm);
-		mmdrop(mm);
+		mmdrop_delayed(mm);
 	}
 	if (unlikely(prev_state == TASK_DEAD)) {
 		if (prev->sched_class->task_dead)
 			prev->sched_class->task_dead(prev);
 
-		/*
-		 * Remove function-return probe instances associated with this
-		 * task and put them back on the free list.
-		 */
-		kprobe_flush_task(prev);
-
-		/* Task is done with its stack. */
-		put_task_stack(prev);
-
 		put_task_struct_rcu_user(prev);
 	}
 
@@ -3653,43 +4324,6 @@ static struct rq *finish_task_switch(struct task_struct *prev)
 	return rq;
 }
 
-#ifdef CONFIG_SMP
-
-/* rq->lock is NOT held, but preemption is disabled */
-static void __balance_callback(struct rq *rq)
-{
-	struct callback_head *head, *next;
-	void (*func)(struct rq *rq);
-	unsigned long flags;
-
-	raw_spin_lock_irqsave(&rq->lock, flags);
-	head = rq->balance_callback;
-	rq->balance_callback = NULL;
-	while (head) {
-		func = (void (*)(struct rq *))head->func;
-		next = head->next;
-		head->next = NULL;
-		head = next;
-
-		func(rq);
-	}
-	raw_spin_unlock_irqrestore(&rq->lock, flags);
-}
-
-static inline void balance_callback(struct rq *rq)
-{
-	if (unlikely(rq->balance_callback))
-		__balance_callback(rq);
-}
-
-#else
-
-static inline void balance_callback(struct rq *rq)
-{
-}
-
-#endif
-
 /**
  * schedule_tail - first thing a freshly forked thread must call.
  * @prev: the thread we just switched away from.
@@ -3709,7 +4343,6 @@ asmlinkage __visible void schedule_tail(struct task_struct *prev)
 	 */
 
 	rq = finish_task_switch(prev);
-	balance_callback(rq);
 	preempt_enable();
 
 	if (current->set_child_tid)
@@ -4404,7 +5037,7 @@ pick_next_task(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
  *
  * WARNING: must be called with preemption disabled!
  */
-static void __sched notrace __schedule(bool preempt)
+static void __sched notrace __schedule(bool preempt, bool spinning_lock)
 {
 	struct task_struct *prev, *next;
 	unsigned long *switch_count;
@@ -4457,7 +5090,7 @@ static void __sched notrace __schedule(bool preempt)
 	 *  - ptrace_{,un}freeze_traced() can change ->state underneath us.
 	 */
 	prev_state = prev->state;
-	if (!preempt && prev_state) {
+	if ((!preempt || spinning_lock) && prev_state) {
 		if (signal_pending_state(prev_state, prev)) {
 			prev->state = TASK_RUNNING;
 		} else {
@@ -4492,6 +5125,7 @@ static void __sched notrace __schedule(bool preempt)
 
 	next = pick_next_task(rq, prev, &rf);
 	clear_tsk_need_resched(prev);
+	clear_tsk_need_resched_lazy(prev);
 	clear_preempt_need_resched();
 
 	if (likely(prev != next)) {
@@ -4517,6 +5151,7 @@ static void __sched notrace __schedule(bool preempt)
 		 */
 		++*switch_count;
 
+		migrate_disable_switch(rq, prev);
 		psi_sched_switch(prev, next, !task_on_rq_queued(prev));
 
 		trace_sched_switch(preempt, prev, next);
@@ -4525,10 +5160,11 @@ static void __sched notrace __schedule(bool preempt)
 		rq = context_switch(rq, prev, next, &rf);
 	} else {
 		rq->clock_update_flags &= ~(RQCF_ACT_SKIP|RQCF_REQ_SKIP);
-		rq_unlock_irq(rq, &rf);
-	}
 
-	balance_callback(rq);
+		rq_unpin_lock(rq, &rf);
+		__balance_callbacks(rq);
+		raw_spin_unlock_irq(&rq->lock);
+	}
 }
 
 void __noreturn do_task_dead(void)
@@ -4539,7 +5175,7 @@ void __noreturn do_task_dead(void)
 	/* Tell freezer to ignore us: */
 	current->flags |= PF_NOFREEZE;
 
-	__schedule(false);
+	__schedule(false, false);
 	BUG();
 
 	/* Avoid "noreturn function does return" - but don't continue if BUG() is a NOP: */
@@ -4572,9 +5208,6 @@ static inline void sched_submit_work(struct task_struct *tsk)
 		preempt_enable_no_resched();
 	}
 
-	if (tsk_is_pi_blocked(tsk))
-		return;
-
 	/*
 	 * If we are going to sleep and we have plugged IO queued,
 	 * make sure to submit it to avoid deadlocks.
@@ -4600,7 +5233,7 @@ asmlinkage __visible void __sched schedule(void)
 	sched_submit_work(tsk);
 	do {
 		preempt_disable();
-		__schedule(false);
+		__schedule(false, false);
 		sched_preempt_enable_no_resched();
 	} while (need_resched());
 	sched_update_worker(tsk);
@@ -4628,7 +5261,7 @@ void __sched schedule_idle(void)
 	 */
 	WARN_ON_ONCE(current->state);
 	do {
-		__schedule(false);
+		__schedule(false, false);
 	} while (need_resched());
 }
 
@@ -4681,7 +5314,7 @@ static void __sched notrace preempt_schedule_common(void)
 		 */
 		preempt_disable_notrace();
 		preempt_latency_start(1);
-		__schedule(true);
+		__schedule(true, false);
 		preempt_latency_stop(1);
 		preempt_enable_no_resched_notrace();
 
@@ -4692,6 +5325,30 @@ static void __sched notrace preempt_schedule_common(void)
 	} while (need_resched());
 }
 
+#ifdef CONFIG_PREEMPT_LAZY
+/*
+ * If TIF_NEED_RESCHED is then we allow to be scheduled away since this is
+ * set by a RT task. Oterwise we try to avoid beeing scheduled out as long as
+ * preempt_lazy_count counter >0.
+ */
+static __always_inline int preemptible_lazy(void)
+{
+	if (test_thread_flag(TIF_NEED_RESCHED))
+		return 1;
+	if (current_thread_info()->preempt_lazy_count)
+		return 0;
+	return 1;
+}
+
+#else
+
+static inline int preemptible_lazy(void)
+{
+	return 1;
+}
+
+#endif
+
 #ifdef CONFIG_PREEMPTION
 /*
  * This is the entry point to schedule() from in-kernel preemption
@@ -4705,12 +5362,26 @@ asmlinkage __visible void __sched notrace preempt_schedule(void)
 	 */
 	if (likely(!preemptible()))
 		return;
-
+	if (!preemptible_lazy())
+		return;
 	preempt_schedule_common();
 }
 NOKPROBE_SYMBOL(preempt_schedule);
 EXPORT_SYMBOL(preempt_schedule);
 
+#ifdef CONFIG_PREEMPT_RT
+void __sched notrace preempt_schedule_lock(void)
+{
+	do {
+		preempt_disable();
+		__schedule(true, true);
+		sched_preempt_enable_no_resched();
+	} while (need_resched());
+}
+NOKPROBE_SYMBOL(preempt_schedule_lock);
+EXPORT_SYMBOL(preempt_schedule_lock);
+#endif
+
 /**
  * preempt_schedule_notrace - preempt_schedule called by tracing
  *
@@ -4732,6 +5403,9 @@ asmlinkage __visible void __sched notrace preempt_schedule_notrace(void)
 	if (likely(!preemptible()))
 		return;
 
+	if (!preemptible_lazy())
+		return;
+
 	do {
 		/*
 		 * Because the function tracer can trace preempt_count_sub()
@@ -4754,7 +5428,7 @@ asmlinkage __visible void __sched notrace preempt_schedule_notrace(void)
 		 * an infinite recursion.
 		 */
 		prev_ctx = exception_enter();
-		__schedule(true);
+		__schedule(true, false);
 		exception_exit(prev_ctx);
 
 		preempt_latency_stop(1);
@@ -4783,7 +5457,7 @@ asmlinkage __visible void __sched preempt_schedule_irq(void)
 	do {
 		preempt_disable();
 		local_irq_enable();
-		__schedule(true);
+		__schedule(true, false);
 		local_irq_disable();
 		sched_preempt_enable_no_resched();
 	} while (need_resched());
@@ -4940,9 +5614,11 @@ void rt_mutex_setprio(struct task_struct *p, struct task_struct *pi_task)
 out_unlock:
 	/* Avoid rq from going away on us: */
 	preempt_disable();
-	__task_rq_unlock(rq, &rf);
 
-	balance_callback(rq);
+	rq_unpin_lock(rq, &rf);
+	__balance_callbacks(rq);
+	raw_spin_unlock(&rq->lock);
+
 	preempt_enable();
 }
 #else
@@ -5216,6 +5892,7 @@ static int __sched_setscheduler(struct task_struct *p,
 	int retval, oldprio, oldpolicy = -1, queued, running;
 	int new_effective_prio, policy = attr->sched_policy;
 	const struct sched_class *prev_class;
+	struct callback_head *head;
 	struct rq_flags rf;
 	int reset_on_fork;
 	int queue_flags = DEQUEUE_SAVE | DEQUEUE_MOVE | DEQUEUE_NOCLOCK;
@@ -5454,6 +6131,7 @@ static int __sched_setscheduler(struct task_struct *p,
 
 	/* Avoid rq from going away on us: */
 	preempt_disable();
+	head = splice_balance_callbacks(rq);
 	task_rq_unlock(rq, p, &rf);
 
 	if (pi) {
@@ -5462,7 +6140,7 @@ static int __sched_setscheduler(struct task_struct *p,
 	}
 
 	/* Run balance callbacks after we've adjusted the PI chain: */
-	balance_callback(rq);
+	balance_callbacks(rq, head);
 	preempt_enable();
 
 	return 0;
@@ -5957,7 +6635,7 @@ long sched_setaffinity(pid_t pid, const struct cpumask *in_mask)
 	}
 #endif
 again:
-	retval = __set_cpus_allowed_ptr(p, new_mask, true);
+	retval = __set_cpus_allowed_ptr(p, new_mask, SCA_CHECK);
 
 	if (!retval) {
 		cpuset_cpus_allowed(p, cpus_allowed);
@@ -6536,7 +7214,7 @@ void init_idle(struct task_struct *idle, int cpu)
 	 *
 	 * And since this is boot we can forgo the serialization.
 	 */
-	set_cpus_allowed_common(idle, cpumask_of(cpu));
+	set_cpus_allowed_common(idle, cpumask_of(cpu), 0);
 #endif
 	/*
 	 * We're having a chicken and egg problem, even though we are
@@ -6563,7 +7241,9 @@ void init_idle(struct task_struct *idle, int cpu)
 
 	/* Set the preempt count _outside_ the spinlocks! */
 	init_idle_preempt_count(idle, cpu);
-
+#ifdef CONFIG_HAVE_PREEMPT_LAZY
+	task_thread_info(idle)->preempt_lazy_count = 0;
+#endif
 	/*
 	 * The idle tasks have their own, simple scheduling class:
 	 */
@@ -6668,6 +7348,7 @@ void sched_setnuma(struct task_struct *p, int nid)
 #endif /* CONFIG_NUMA_BALANCING */
 
 #ifdef CONFIG_HOTPLUG_CPU
+
 /*
  * Ensure that the idle task is using init_mm right before its CPU goes
  * offline.
@@ -6687,119 +7368,126 @@ void idle_task_exit(void)
 	/* finish_cpu(), as ran on the BP, will clean up the active_mm state */
 }
 
-/*
- * Since this CPU is going 'away' for a while, fold any nr_active delta
- * we might have. Assumes we're called after migrate_tasks() so that the
- * nr_active count is stable. We need to take the teardown thread which
- * is calling this into account, so we hand in adjust = 1 to the load
- * calculation.
- *
- * Also see the comment "Global load-average calculations".
- */
-static void calc_load_migrate(struct rq *rq)
+static int __balance_push_cpu_stop(void *arg)
 {
-	long delta = calc_load_fold_active(rq, 1);
-	if (delta)
-		atomic_long_add(delta, &calc_load_tasks);
-}
+	struct task_struct *p = arg;
+	struct rq *rq = this_rq();
+	struct rq_flags rf;
+	int cpu;
 
-static struct task_struct *__pick_migrate_task(struct rq *rq)
-{
-	const struct sched_class *class;
-	struct task_struct *next;
+	raw_spin_lock_irq(&p->pi_lock);
+	rq_lock(rq, &rf);
 
-	for_each_class(class) {
-		next = class->pick_next_task(rq);
-		if (next) {
-			next->sched_class->put_prev_task(rq, next);
-			return next;
-		}
+	update_rq_clock(rq);
+
+	if (task_rq(p) == rq && task_on_rq_queued(p)) {
+		cpu = select_fallback_rq(rq->cpu, p);
+		rq = __migrate_task(rq, &rf, p, cpu);
 	}
 
-	/* The idle class should always have a runnable task */
-	BUG();
+	rq_unlock(rq, &rf);
+	raw_spin_unlock_irq(&p->pi_lock);
+
+	put_task_struct(p);
+
+	return 0;
 }
 
+static DEFINE_PER_CPU(struct cpu_stop_work, push_work);
+
 /*
- * Migrate all tasks from the rq, sleeping tasks will be migrated by
- * try_to_wake_up()->select_task_rq().
- *
- * Called with rq->lock held even though we'er in stop_machine() and
- * there's no concurrency possible, we hold the required locks anyway
- * because of lock validation efforts.
+ * Ensure we only run per-cpu kthreads once the CPU goes !active.
  */
-static void migrate_tasks(struct rq *dead_rq, struct rq_flags *rf)
+static void balance_push(struct rq *rq)
 {
-	struct rq *rq = dead_rq;
-	struct task_struct *next, *stop = rq->stop;
-	struct rq_flags orf = *rf;
-	int dest_cpu;
+	struct task_struct *push_task = rq->curr;
+
+	lockdep_assert_held(&rq->lock);
+	SCHED_WARN_ON(rq->cpu != smp_processor_id());
 
 	/*
-	 * Fudge the rq selection such that the below task selection loop
-	 * doesn't get stuck on the currently eligible stop task.
-	 *
-	 * We're currently inside stop_machine() and the rq is either stuck
-	 * in the stop_machine_cpu_stop() loop, or we're executing this code,
-	 * either way we should never end up calling schedule() until we're
-	 * done here.
+	 * Both the cpu-hotplug and stop task are in this case and are
+	 * required to complete the hotplug process.
 	 */
-	rq->stop = NULL;
+	if (is_per_cpu_kthread(push_task) || is_migration_disabled(push_task)) {
+		/*
+		 * If this is the idle task on the outgoing CPU try to wake
+		 * up the hotplug control thread which might wait for the
+		 * last task to vanish. The rcuwait_active() check is
+		 * accurate here because the waiter is pinned on this CPU
+		 * and can't obviously be running in parallel.
+		 *
+		 * On RT kernels this also has to check whether there are
+		 * pinned and scheduled out tasks on the runqueue. They
+		 * need to leave the migrate disabled section first.
+		 */
+		if (!rq->nr_running && !rq_has_pinned_tasks(rq) &&
+		    rcuwait_active(&rq->hotplug_wait)) {
+			raw_spin_unlock(&rq->lock);
+			rcuwait_wake_up(&rq->hotplug_wait);
+			raw_spin_lock(&rq->lock);
+		}
+		return;
+	}
 
+	get_task_struct(push_task);
 	/*
-	 * put_prev_task() and pick_next_task() sched
-	 * class method both need to have an up-to-date
-	 * value of rq->clock[_task]
+	 * Temporarily drop rq->lock such that we can wake-up the stop task.
+	 * Both preemption and IRQs are still disabled.
 	 */
-	update_rq_clock(rq);
+	raw_spin_unlock(&rq->lock);
+	stop_one_cpu_nowait(rq->cpu, __balance_push_cpu_stop, push_task,
+			    this_cpu_ptr(&push_work));
+	/*
+	 * At this point need_resched() is true and we'll take the loop in
+	 * schedule(). The next pick is obviously going to be the stop task
+	 * which is_per_cpu_kthread() and will push this task away.
+	 */
+	raw_spin_lock(&rq->lock);
+}
 
-	for (;;) {
-		/*
-		 * There's this thread running, bail when that's the only
-		 * remaining thread:
-		 */
-		if (rq->nr_running == 1)
-			break;
+static void balance_push_set(int cpu, bool on)
+{
+	struct rq *rq = cpu_rq(cpu);
+	struct rq_flags rf;
 
-		next = __pick_migrate_task(rq);
+	rq_lock_irqsave(rq, &rf);
+	if (on)
+		rq->balance_flags |= BALANCE_PUSH;
+	else
+		rq->balance_flags &= ~BALANCE_PUSH;
+	rq_unlock_irqrestore(rq, &rf);
+}
 
-		/*
-		 * Rules for changing task_struct::cpus_mask are holding
-		 * both pi_lock and rq->lock, such that holding either
-		 * stabilizes the mask.
-		 *
-		 * Drop rq->lock is not quite as disastrous as it usually is
-		 * because !cpu_active at this point, which means load-balance
-		 * will not interfere. Also, stop-machine.
-		 */
-		rq_unlock(rq, rf);
-		raw_spin_lock(&next->pi_lock);
-		rq_relock(rq, rf);
+/*
+ * Invoked from a CPUs hotplug control thread after the CPU has been marked
+ * inactive. All tasks which are not per CPU kernel threads are either
+ * pushed off this CPU now via balance_push() or placed on a different CPU
+ * during wakeup. Wait until the CPU is quiescent.
+ */
+static void balance_hotplug_wait(void)
+{
+	struct rq *rq = this_rq();
 
-		/*
-		 * Since we're inside stop-machine, _nothing_ should have
-		 * changed the task, WARN if weird stuff happened, because in
-		 * that case the above rq->lock drop is a fail too.
-		 */
-		if (WARN_ON(task_rq(next) != rq || !task_on_rq_queued(next))) {
-			raw_spin_unlock(&next->pi_lock);
-			continue;
-		}
+	rcuwait_wait_event(&rq->hotplug_wait,
+			   rq->nr_running == 1 && !rq_has_pinned_tasks(rq),
+			   TASK_UNINTERRUPTIBLE);
+}
 
-		/* Find suitable destination for @next, with force if needed. */
-		dest_cpu = select_fallback_rq(dead_rq->cpu, next);
-		rq = __migrate_task(rq, rf, next, dest_cpu);
-		if (rq != dead_rq) {
-			rq_unlock(rq, rf);
-			rq = dead_rq;
-			*rf = orf;
-			rq_relock(rq, rf);
-		}
-		raw_spin_unlock(&next->pi_lock);
-	}
+#else
+
+static inline void balance_push(struct rq *rq)
+{
+}
 
-	rq->stop = stop;
+static inline void balance_push_set(int cpu, bool on)
+{
+}
+
+static inline void balance_hotplug_wait(void)
+{
 }
+
 #endif /* CONFIG_HOTPLUG_CPU */
 
 void set_rq_online(struct rq *rq)
@@ -6885,6 +7573,8 @@ int sched_cpu_activate(unsigned int cpu)
 	struct rq *rq = cpu_rq(cpu);
 	struct rq_flags rf;
 
+	balance_push_set(cpu, false);
+
 #ifdef CONFIG_SCHED_SMT
 	/*
 	 * When going up, increment the number of cores with SMT present.
@@ -6920,6 +7610,8 @@ int sched_cpu_activate(unsigned int cpu)
 
 int sched_cpu_deactivate(unsigned int cpu)
 {
+	struct rq *rq = cpu_rq(cpu);
+	struct rq_flags rf;
 	int ret;
 
 	set_cpu_active(cpu, false);
@@ -6932,6 +7624,16 @@ int sched_cpu_deactivate(unsigned int cpu)
 	 */
 	synchronize_rcu();
 
+	balance_push_set(cpu, true);
+
+	rq_lock_irqsave(rq, &rf);
+	if (rq->rd) {
+		update_rq_clock(rq);
+		BUG_ON(!cpumask_test_cpu(cpu, rq->rd->span));
+		set_rq_offline(rq);
+	}
+	rq_unlock_irqrestore(rq, &rf);
+
 #ifdef CONFIG_SCHED_SMT
 	/*
 	 * When going down, decrement the number of cores with SMT present.
@@ -6945,6 +7647,7 @@ int sched_cpu_deactivate(unsigned int cpu)
 
 	ret = cpuset_cpu_inactive(cpu);
 	if (ret) {
+		balance_push_set(cpu, false);
 		set_cpu_active(cpu, true);
 		return ret;
 	}
@@ -6968,6 +7671,41 @@ int sched_cpu_starting(unsigned int cpu)
 }
 
 #ifdef CONFIG_HOTPLUG_CPU
+
+/*
+ * Invoked immediately before the stopper thread is invoked to bring the
+ * CPU down completely. At this point all per CPU kthreads except the
+ * hotplug thread (current) and the stopper thread (inactive) have been
+ * either parked or have been unbound from the outgoing CPU. Ensure that
+ * any of those which might be on the way out are gone.
+ *
+ * If after this point a bound task is being woken on this CPU then the
+ * responsible hotplug callback has failed to do it's job.
+ * sched_cpu_dying() will catch it with the appropriate fireworks.
+ */
+int sched_cpu_wait_empty(unsigned int cpu)
+{
+	balance_hotplug_wait();
+	return 0;
+}
+
+/*
+ * Since this CPU is going 'away' for a while, fold any nr_active delta we
+ * might have. Called from the CPU stopper task after ensuring that the
+ * stopper is the last running task on the CPU, so nr_active count is
+ * stable. We need to take the teardown thread which is calling this into
+ * account, so we hand in adjust = 1 to the load calculation.
+ *
+ * Also see the comment "Global load-average calculations".
+ */
+static void calc_load_migrate(struct rq *rq)
+{
+	long delta = calc_load_fold_active(rq, 1);
+
+	if (delta)
+		atomic_long_add(delta, &calc_load_tasks);
+}
+
 int sched_cpu_dying(unsigned int cpu)
 {
 	struct rq *rq = cpu_rq(cpu);
@@ -6977,12 +7715,7 @@ int sched_cpu_dying(unsigned int cpu)
 	sched_tick_stop(cpu);
 
 	rq_lock_irqsave(rq, &rf);
-	if (rq->rd) {
-		BUG_ON(!cpumask_test_cpu(cpu, rq->rd->span));
-		set_rq_offline(rq);
-	}
-	migrate_tasks(rq, &rf);
-	BUG_ON(rq->nr_running != 1);
+	BUG_ON(rq->nr_running != 1 || rq_has_pinned_tasks(rq));
 	rq_unlock_irqrestore(rq, &rf);
 
 	calc_load_migrate(rq);
@@ -7189,6 +7922,9 @@ void __init sched_init(void)
 
 		rq_csd_init(rq, &rq->nohz_csd, nohz_csd_func);
 #endif
+#ifdef CONFIG_HOTPLUG_CPU
+		rcuwait_init(&rq->hotplug_wait);
+#endif
 #endif /* CONFIG_SMP */
 		hrtick_rq_init(rq);
 		atomic_set(&rq->nr_iowait, 0);
@@ -7229,7 +7965,7 @@ void __init sched_init(void)
 #ifdef CONFIG_DEBUG_ATOMIC_SLEEP
 static inline int preempt_count_equals(int preempt_offset)
 {
-	int nested = preempt_count() + rcu_preempt_depth();
+	int nested = preempt_count() + sched_rcu_preempt_depth();
 
 	return (nested == preempt_offset);
 }
@@ -7326,6 +8062,39 @@ void __cant_sleep(const char *file, int line, int preempt_offset)
 	add_taint(TAINT_WARN, LOCKDEP_STILL_OK);
 }
 EXPORT_SYMBOL_GPL(__cant_sleep);
+
+#ifdef CONFIG_SMP
+void __cant_migrate(const char *file, int line)
+{
+	static unsigned long prev_jiffy;
+
+	if (irqs_disabled())
+		return;
+
+	if (is_migration_disabled(current))
+		return;
+
+	if (!IS_ENABLED(CONFIG_PREEMPT_COUNT))
+		return;
+
+	if (preempt_count() > 0)
+		return;
+
+	if (time_before(jiffies, prev_jiffy + HZ) && prev_jiffy)
+		return;
+	prev_jiffy = jiffies;
+
+	pr_err("BUG: assuming non migratable context at %s:%d\n", file, line);
+	pr_err("in_atomic(): %d, irqs_disabled(): %d, migration_disabled() %u pid: %d, name: %s\n",
+	       in_atomic(), irqs_disabled(), is_migration_disabled(current),
+	       current->pid, current->comm);
+
+	debug_show_held_locks(current);
+	dump_stack();
+	add_taint(TAINT_WARN, LOCKDEP_STILL_OK);
+}
+EXPORT_SYMBOL_GPL(__cant_migrate);
+#endif
 #endif
 
 #ifdef CONFIG_MAGIC_SYSRQ
diff --git a/kernel/sched/cpudeadline.c b/kernel/sched/cpudeadline.c
index 8cb06c8c7eb1..ceb03d76c0cc 100644
--- a/kernel/sched/cpudeadline.c
+++ b/kernel/sched/cpudeadline.c
@@ -120,7 +120,7 @@ int cpudl_find(struct cpudl *cp, struct task_struct *p,
 	const struct sched_dl_entity *dl_se = &p->dl;
 
 	if (later_mask &&
-	    cpumask_and(later_mask, cp->free_cpus, p->cpus_ptr)) {
+	    cpumask_and(later_mask, cp->free_cpus, &p->cpus_mask)) {
 		unsigned long cap, max_cap = 0;
 		int cpu, max_cpu = -1;
 
@@ -151,7 +151,7 @@ int cpudl_find(struct cpudl *cp, struct task_struct *p,
 
 		WARN_ON(best_cpu != -1 && !cpu_present(best_cpu));
 
-		if (cpumask_test_cpu(best_cpu, p->cpus_ptr) &&
+		if (cpumask_test_cpu(best_cpu, &p->cpus_mask) &&
 		    dl_time_before(dl_se->deadline, cp->elements[0].dl)) {
 			if (later_mask)
 				cpumask_set_cpu(best_cpu, later_mask);
diff --git a/kernel/sched/cpupri.c b/kernel/sched/cpupri.c
index 0033731a0797..11c4df2010de 100644
--- a/kernel/sched/cpupri.c
+++ b/kernel/sched/cpupri.c
@@ -73,11 +73,11 @@ static inline int __cpupri_find(struct cpupri *cp, struct task_struct *p,
 	if (skip)
 		return 0;
 
-	if (cpumask_any_and(p->cpus_ptr, vec->mask) >= nr_cpu_ids)
+	if (cpumask_any_and(&p->cpus_mask, vec->mask) >= nr_cpu_ids)
 		return 0;
 
 	if (lowest_mask) {
-		cpumask_and(lowest_mask, p->cpus_ptr, vec->mask);
+		cpumask_and(lowest_mask, &p->cpus_mask, vec->mask);
 
 		/*
 		 * We have to ensure that we have at least one bit
diff --git a/kernel/sched/cputime.c b/kernel/sched/cputime.c
index 5a55d2300452..2c36a5fad589 100644
--- a/kernel/sched/cputime.c
+++ b/kernel/sched/cputime.c
@@ -44,12 +44,13 @@ static void irqtime_account_delta(struct irqtime *irqtime, u64 delta,
 }
 
 /*
- * Called before incrementing preempt_count on {soft,}irq_enter
+ * Called after incrementing preempt_count on {soft,}irq_enter
  * and before decrementing preempt_count on {soft,}irq_exit.
  */
-void irqtime_account_irq(struct task_struct *curr)
+void irqtime_account_irq(struct task_struct *curr, unsigned int offset)
 {
 	struct irqtime *irqtime = this_cpu_ptr(&cpu_irqtime);
+	unsigned int pc;
 	s64 delta;
 	int cpu;
 
@@ -59,6 +60,7 @@ void irqtime_account_irq(struct task_struct *curr)
 	cpu = smp_processor_id();
 	delta = sched_clock_cpu(cpu) - irqtime->irq_start_time;
 	irqtime->irq_start_time += delta;
+	pc = irq_count() - offset;
 
 	/*
 	 * We do not account for softirq time from ksoftirqd here.
@@ -66,12 +68,11 @@ void irqtime_account_irq(struct task_struct *curr)
 	 * in that case, so as not to confuse scheduler with a special task
 	 * that do not consume any time, but still wants to run.
 	 */
-	if (hardirq_count())
+	if (pc & HARDIRQ_MASK)
 		irqtime_account_delta(irqtime, delta, CPUTIME_IRQ);
-	else if (in_serving_softirq() && curr != this_cpu_ksoftirqd())
+	else if ((pc & SOFTIRQ_OFFSET) && curr != this_cpu_ksoftirqd())
 		irqtime_account_delta(irqtime, delta, CPUTIME_SOFTIRQ);
 }
-EXPORT_SYMBOL_GPL(irqtime_account_irq);
 
 static u64 irqtime_tick_accounted(u64 maxtime)
 {
@@ -418,24 +419,21 @@ void vtime_task_switch(struct task_struct *prev)
 }
 # endif
 
-/*
- * Archs that account the whole time spent in the idle task
- * (outside irq) as idle time can rely on this and just implement
- * vtime_account_kernel() and vtime_account_idle(). Archs that
- * have other meaning of the idle time (s390 only includes the
- * time spent by the CPU when it's in low power mode) must override
- * vtime_account().
- */
-#ifndef __ARCH_HAS_VTIME_ACCOUNT
-void vtime_account_irq_enter(struct task_struct *tsk)
+void vtime_account_irq(struct task_struct *tsk, unsigned int offset)
 {
-	if (!in_interrupt() && is_idle_task(tsk))
+	unsigned int pc = irq_count() - offset;
+
+	if (pc & HARDIRQ_OFFSET) {
+		vtime_account_hardirq(tsk);
+	} else if (pc & SOFTIRQ_OFFSET) {
+		vtime_account_softirq(tsk);
+	} else if (!IS_ENABLED(CONFIG_HAVE_VIRT_CPU_ACCOUNTING_IDLE) &&
+		   is_idle_task(tsk)) {
 		vtime_account_idle(tsk);
-	else
+	} else {
 		vtime_account_kernel(tsk);
+	}
 }
-EXPORT_SYMBOL_GPL(vtime_account_irq_enter);
-#endif /* __ARCH_HAS_VTIME_ACCOUNT */
 
 void cputime_adjust(struct task_cputime *curr, struct prev_cputime *prev,
 		    u64 *ut, u64 *st)
diff --git a/kernel/sched/deadline.c b/kernel/sched/deadline.c
index 8d06d1f4e2f7..56faef8c9238 100644
--- a/kernel/sched/deadline.c
+++ b/kernel/sched/deadline.c
@@ -565,7 +565,7 @@ static int push_dl_task(struct rq *rq);
 
 static inline bool need_pull_dl_task(struct rq *rq, struct task_struct *prev)
 {
-	return dl_task(prev);
+	return rq->online && dl_task(prev);
 }
 
 static DEFINE_PER_CPU(struct callback_head, dl_push_head);
@@ -1918,7 +1918,7 @@ static void task_fork_dl(struct task_struct *p)
 static int pick_dl_task(struct rq *rq, struct task_struct *p, int cpu)
 {
 	if (!task_running(rq, p) &&
-	    cpumask_test_cpu(cpu, p->cpus_ptr))
+	    cpumask_test_cpu(cpu, &p->cpus_mask))
 		return 1;
 	return 0;
 }
@@ -2008,8 +2008,8 @@ static int find_later_rq(struct task_struct *task)
 				return this_cpu;
 			}
 
-			best_cpu = cpumask_first_and(later_mask,
-							sched_domain_span(sd));
+			best_cpu = cpumask_any_and_distribute(later_mask,
+							      sched_domain_span(sd));
 			/*
 			 * Last chance: if a CPU being in both later_mask
 			 * and current sd span is valid, that becomes our
@@ -2031,7 +2031,7 @@ static int find_later_rq(struct task_struct *task)
 	if (this_cpu != -1)
 		return this_cpu;
 
-	cpu = cpumask_any(later_mask);
+	cpu = cpumask_any_distribute(later_mask);
 	if (cpu < nr_cpu_ids)
 		return cpu;
 
@@ -2068,7 +2068,7 @@ static struct rq *find_lock_later_rq(struct task_struct *task, struct rq *rq)
 		/* Retry if something changed. */
 		if (double_lock_balance(rq, later_rq)) {
 			if (unlikely(task_rq(task) != rq ||
-				     !cpumask_test_cpu(later_rq->cpu, task->cpus_ptr) ||
+				     !cpumask_test_cpu(later_rq->cpu, &task->cpus_mask) ||
 				     task_running(rq, task) ||
 				     !dl_task(task) ||
 				     !task_on_rq_queued(task))) {
@@ -2135,6 +2135,9 @@ static int push_dl_task(struct rq *rq)
 		return 0;
 
 retry:
+	if (is_migration_disabled(next_task))
+		return 0;
+
 	if (WARN_ON(next_task == rq->curr))
 		return 0;
 
@@ -2212,7 +2215,7 @@ static void push_dl_tasks(struct rq *rq)
 static void pull_dl_task(struct rq *this_rq)
 {
 	int this_cpu = this_rq->cpu, cpu;
-	struct task_struct *p;
+	struct task_struct *p, *push_task;
 	bool resched = false;
 	struct rq *src_rq;
 	u64 dmin = LONG_MAX;
@@ -2242,6 +2245,7 @@ static void pull_dl_task(struct rq *this_rq)
 			continue;
 
 		/* Might drop this_rq->lock */
+		push_task = NULL;
 		double_lock_balance(this_rq, src_rq);
 
 		/*
@@ -2273,17 +2277,28 @@ static void pull_dl_task(struct rq *this_rq)
 					   src_rq->curr->dl.deadline))
 				goto skip;
 
-			resched = true;
-
-			deactivate_task(src_rq, p, 0);
-			set_task_cpu(p, this_cpu);
-			activate_task(this_rq, p, 0);
-			dmin = p->dl.deadline;
+			if (is_migration_disabled(p)) {
+				trace_sched_migrate_pull_tp(p);
+				push_task = get_push_task(src_rq);
+			} else {
+				deactivate_task(src_rq, p, 0);
+				set_task_cpu(p, this_cpu);
+				activate_task(this_rq, p, 0);
+				dmin = p->dl.deadline;
+				resched = true;
+			}
 
 			/* Is there any other task even earlier? */
 		}
 skip:
 		double_unlock_balance(this_rq, src_rq);
+
+		if (push_task) {
+			raw_spin_unlock(&this_rq->lock);
+			stop_one_cpu_nowait(src_rq->cpu, push_cpu_stop,
+					    push_task, &src_rq->push_work);
+			raw_spin_lock(&this_rq->lock);
+		}
 	}
 
 	if (resched)
@@ -2307,7 +2322,8 @@ static void task_woken_dl(struct rq *rq, struct task_struct *p)
 }
 
 static void set_cpus_allowed_dl(struct task_struct *p,
-				const struct cpumask *new_mask)
+				const struct cpumask *new_mask,
+				u32 flags)
 {
 	struct root_domain *src_rd;
 	struct rq *rq;
@@ -2336,7 +2352,7 @@ static void set_cpus_allowed_dl(struct task_struct *p,
 		raw_spin_unlock(&src_dl_b->lock);
 	}
 
-	set_cpus_allowed_common(p, new_mask);
+	set_cpus_allowed_common(p, new_mask, flags);
 }
 
 /* Assumes rq->lock is held */
@@ -2529,6 +2545,7 @@ const struct sched_class dl_sched_class
 	.rq_online              = rq_online_dl,
 	.rq_offline             = rq_offline_dl,
 	.task_woken		= task_woken_dl,
+	.find_lock_rq		= find_lock_later_rq,
 #endif
 
 	.task_tick		= task_tick_dl,
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 348605306027..e7d6ae7882c1 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -4372,7 +4372,7 @@ check_preempt_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr)
 	ideal_runtime = sched_slice(cfs_rq, curr);
 	delta_exec = curr->sum_exec_runtime - curr->prev_sum_exec_runtime;
 	if (delta_exec > ideal_runtime) {
-		resched_curr(rq_of(cfs_rq));
+		resched_curr_lazy(rq_of(cfs_rq));
 		/*
 		 * The current task ran long enough, ensure it doesn't get
 		 * re-elected due to buddy favours.
@@ -4396,7 +4396,7 @@ check_preempt_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr)
 		return;
 
 	if (delta > ideal_runtime)
-		resched_curr(rq_of(cfs_rq));
+		resched_curr_lazy(rq_of(cfs_rq));
 }
 
 static void
@@ -4539,7 +4539,7 @@ entity_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr, int queued)
 	 * validating it and just reschedule.
 	 */
 	if (queued) {
-		resched_curr(rq_of(cfs_rq));
+		resched_curr_lazy(rq_of(cfs_rq));
 		return;
 	}
 	/*
@@ -4676,7 +4676,7 @@ static void __account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec)
 	 * hierarchy can be throttled
 	 */
 	if (!assign_cfs_rq_runtime(cfs_rq) && likely(cfs_rq->curr))
-		resched_curr(rq_of(cfs_rq));
+		resched_curr_lazy(rq_of(cfs_rq));
 }
 
 static __always_inline
@@ -5411,7 +5411,7 @@ static void hrtick_start_fair(struct rq *rq, struct task_struct *p)
 
 		if (delta < 0) {
 			if (rq->curr == p)
-				resched_curr(rq);
+				resched_curr_lazy(rq);
 			return;
 		}
 		hrtick_start(rq, delta);
@@ -6992,7 +6992,7 @@ static void check_preempt_wakeup(struct rq *rq, struct task_struct *p, int wake_
 	return;
 
 preempt:
-	resched_curr(rq);
+	resched_curr_lazy(rq);
 	/*
 	 * Only set the backward buddy when the current task is still
 	 * on the rq. This can happen when a wakeup gets interleaved
@@ -10749,7 +10749,7 @@ static void task_fork_fair(struct task_struct *p)
 		 * 'current' within the tree based on its new key value.
 		 */
 		swap(curr->vruntime, se->vruntime);
-		resched_curr(rq);
+		resched_curr_lazy(rq);
 	}
 
 	se->vruntime -= cfs_rq->min_vruntime;
@@ -10776,7 +10776,7 @@ prio_changed_fair(struct rq *rq, struct task_struct *p, int oldprio)
 	 */
 	if (rq->curr == p) {
 		if (p->prio > oldprio)
-			resched_curr(rq);
+			resched_curr_lazy(rq);
 	} else
 		check_preempt_curr(rq, p, 0);
 }
diff --git a/kernel/sched/features.h b/kernel/sched/features.h
index 68d369cba9e4..5a2e27297126 100644
--- a/kernel/sched/features.h
+++ b/kernel/sched/features.h
@@ -45,11 +45,19 @@ SCHED_FEAT(DOUBLE_TICK, false)
  */
 SCHED_FEAT(NONTASK_CAPACITY, true)
 
+#ifdef CONFIG_PREEMPT_RT
+SCHED_FEAT(TTWU_QUEUE, false)
+# ifdef CONFIG_PREEMPT_LAZY
+SCHED_FEAT(PREEMPT_LAZY, true)
+# endif
+#else
+
 /*
  * Queue remote wakeups on the target CPU and process them
  * using the scheduler IPI. Reduces rq->lock contention/bounces.
  */
 SCHED_FEAT(TTWU_QUEUE, true)
+#endif
 
 /*
  * When doing wakeups, attempt to limit superfluous scans of the LLC domain.
diff --git a/kernel/sched/rt.c b/kernel/sched/rt.c
index 49ec096a8aa1..1ed7e3dfee9e 100644
--- a/kernel/sched/rt.c
+++ b/kernel/sched/rt.c
@@ -265,7 +265,7 @@ static void pull_rt_task(struct rq *this_rq);
 static inline bool need_pull_rt_task(struct rq *rq, struct task_struct *prev)
 {
 	/* Try to pull RT tasks here if we lower this rq's prio */
-	return rq->rt.highest_prio.curr > prev->prio;
+	return rq->online && rq->rt.highest_prio.curr > prev->prio;
 }
 
 static inline int rt_overloaded(struct rq *rq)
@@ -1658,7 +1658,7 @@ static void put_prev_task_rt(struct rq *rq, struct task_struct *p)
 static int pick_rt_task(struct rq *rq, struct task_struct *p, int cpu)
 {
 	if (!task_running(rq, p) &&
-	    cpumask_test_cpu(cpu, p->cpus_ptr))
+	    cpumask_test_cpu(cpu, &p->cpus_mask))
 		return 1;
 
 	return 0;
@@ -1752,8 +1752,8 @@ static int find_lowest_rq(struct task_struct *task)
 				return this_cpu;
 			}
 
-			best_cpu = cpumask_first_and(lowest_mask,
-						     sched_domain_span(sd));
+			best_cpu = cpumask_any_and_distribute(lowest_mask,
+							      sched_domain_span(sd));
 			if (best_cpu < nr_cpu_ids) {
 				rcu_read_unlock();
 				return best_cpu;
@@ -1770,7 +1770,7 @@ static int find_lowest_rq(struct task_struct *task)
 	if (this_cpu != -1)
 		return this_cpu;
 
-	cpu = cpumask_any(lowest_mask);
+	cpu = cpumask_any_distribute(lowest_mask);
 	if (cpu < nr_cpu_ids)
 		return cpu;
 
@@ -1811,7 +1811,7 @@ static struct rq *find_lock_lowest_rq(struct task_struct *task, struct rq *rq)
 			 * Also make sure that it wasn't scheduled on its rq.
 			 */
 			if (unlikely(task_rq(task) != rq ||
-				     !cpumask_test_cpu(lowest_rq->cpu, task->cpus_ptr) ||
+				     !cpumask_test_cpu(lowest_rq->cpu, &task->cpus_mask) ||
 				     task_running(rq, task) ||
 				     !rt_task(task) ||
 				     !task_on_rq_queued(task))) {
@@ -1859,7 +1859,7 @@ static struct task_struct *pick_next_pushable_task(struct rq *rq)
  * running task can migrate over to a CPU that is running a task
  * of lesser priority.
  */
-static int push_rt_task(struct rq *rq)
+static int push_rt_task(struct rq *rq, bool pull)
 {
 	struct task_struct *next_task;
 	struct rq *lowest_rq;
@@ -1873,6 +1873,39 @@ static int push_rt_task(struct rq *rq)
 		return 0;
 
 retry:
+	if (is_migration_disabled(next_task)) {
+		struct task_struct *push_task = NULL;
+		int cpu;
+
+		if (!pull)
+			return 0;
+
+		trace_sched_migrate_pull_tp(next_task);
+
+		if (rq->push_busy)
+			return 0;
+
+		cpu = find_lowest_rq(rq->curr);
+		if (cpu == -1 || cpu == rq->cpu)
+			return 0;
+
+		/*
+		 * Given we found a CPU with lower priority than @next_task,
+		 * therefore it should be running. However we cannot migrate it
+		 * to this other CPU, instead attempt to push the current
+		 * running task on this CPU away.
+		 */
+		push_task = get_push_task(rq);
+		if (push_task) {
+			raw_spin_unlock(&rq->lock);
+			stop_one_cpu_nowait(rq->cpu, push_cpu_stop,
+					    push_task, &rq->push_work);
+			raw_spin_lock(&rq->lock);
+		}
+
+		return 0;
+	}
+
 	if (WARN_ON(next_task == rq->curr))
 		return 0;
 
@@ -1927,12 +1960,10 @@ static int push_rt_task(struct rq *rq)
 	deactivate_task(rq, next_task, 0);
 	set_task_cpu(next_task, lowest_rq->cpu);
 	activate_task(lowest_rq, next_task, 0);
-	ret = 1;
-
 	resched_curr(lowest_rq);
+	ret = 1;
 
 	double_unlock_balance(rq, lowest_rq);
-
 out:
 	put_task_struct(next_task);
 
@@ -1942,7 +1973,7 @@ static int push_rt_task(struct rq *rq)
 static void push_rt_tasks(struct rq *rq)
 {
 	/* push_rt_task will return true if it moved an RT */
-	while (push_rt_task(rq))
+	while (push_rt_task(rq, false))
 		;
 }
 
@@ -2095,7 +2126,8 @@ void rto_push_irq_work_func(struct irq_work *work)
 	 */
 	if (has_pushable_tasks(rq)) {
 		raw_spin_lock(&rq->lock);
-		push_rt_tasks(rq);
+		while (push_rt_task(rq, true))
+			;
 		raw_spin_unlock(&rq->lock);
 	}
 
@@ -2120,7 +2152,7 @@ static void pull_rt_task(struct rq *this_rq)
 {
 	int this_cpu = this_rq->cpu, cpu;
 	bool resched = false;
-	struct task_struct *p;
+	struct task_struct *p, *push_task;
 	struct rq *src_rq;
 	int rt_overload_count = rt_overloaded(this_rq);
 
@@ -2167,6 +2199,7 @@ static void pull_rt_task(struct rq *this_rq)
 		 * double_lock_balance, and another CPU could
 		 * alter this_rq
 		 */
+		push_task = NULL;
 		double_lock_balance(this_rq, src_rq);
 
 		/*
@@ -2194,11 +2227,15 @@ static void pull_rt_task(struct rq *this_rq)
 			if (p->prio < src_rq->curr->prio)
 				goto skip;
 
-			resched = true;
-
-			deactivate_task(src_rq, p, 0);
-			set_task_cpu(p, this_cpu);
-			activate_task(this_rq, p, 0);
+			if (is_migration_disabled(p)) {
+				trace_sched_migrate_pull_tp(p);
+				push_task = get_push_task(src_rq);
+			} else {
+				deactivate_task(src_rq, p, 0);
+				set_task_cpu(p, this_cpu);
+				activate_task(this_rq, p, 0);
+				resched = true;
+			}
 			/*
 			 * We continue with the search, just in
 			 * case there's an even higher prio task
@@ -2208,6 +2245,13 @@ static void pull_rt_task(struct rq *this_rq)
 		}
 skip:
 		double_unlock_balance(this_rq, src_rq);
+
+		if (push_task) {
+			raw_spin_unlock(&this_rq->lock);
+			stop_one_cpu_nowait(src_rq->cpu, push_cpu_stop,
+					    push_task, &src_rq->push_work);
+			raw_spin_lock(&this_rq->lock);
+		}
 	}
 
 	if (resched)
@@ -2449,6 +2493,7 @@ const struct sched_class rt_sched_class
 	.rq_offline             = rq_offline_rt,
 	.task_woken		= task_woken_rt,
 	.switched_from		= switched_from_rt,
+	.find_lock_rq		= find_lock_lowest_rq,
 #endif
 
 	.task_tick		= task_tick_rt,
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index fac1b121d113..a6dc180ae5ef 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -967,6 +967,7 @@ struct rq {
 	unsigned long		cpu_capacity_orig;
 
 	struct callback_head	*balance_callback;
+	unsigned char		balance_flags;
 
 	unsigned char		nohz_idle_balance;
 	unsigned char		idle_balance;
@@ -997,6 +998,10 @@ struct rq {
 
 	/* This is used to determine avg_idle's max value */
 	u64			max_idle_balance_cost;
+
+#ifdef CONFIG_HOTPLUG_CPU
+	struct rcuwait		hotplug_wait;
+#endif
 #endif /* CONFIG_SMP */
 
 #ifdef CONFIG_IRQ_TIME_ACCOUNTING
@@ -1043,6 +1048,12 @@ struct rq {
 	/* Must be inspected within a rcu lock section */
 	struct cpuidle_state	*idle_state;
 #endif
+
+#ifdef CONFIG_SMP
+	unsigned int		nr_pinned;
+#endif
+	unsigned int		push_busy;
+	struct cpu_stop_work	push_work;
 };
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
@@ -1070,6 +1081,16 @@ static inline int cpu_of(struct rq *rq)
 #endif
 }
 
+#define MDF_PUSH	0x01
+
+static inline bool is_migration_disabled(struct task_struct *p)
+{
+#ifdef CONFIG_SMP
+	return p->migration_disabled;
+#else
+	return false;
+#endif
+}
 
 #ifdef CONFIG_SCHED_SMT
 extern void __update_idle_core(struct rq *rq);
@@ -1216,6 +1237,9 @@ static inline void rq_pin_lock(struct rq *rq, struct rq_flags *rf)
 	rq->clock_update_flags &= (RQCF_REQ_SKIP|RQCF_ACT_SKIP);
 	rf->clock_update_flags = 0;
 #endif
+#ifdef CONFIG_SMP
+	SCHED_WARN_ON(rq->balance_callback);
+#endif
 }
 
 static inline void rq_unpin_lock(struct rq *rq, struct rq_flags *rf)
@@ -1377,6 +1401,9 @@ init_numa_balancing(unsigned long clone_flags, struct task_struct *p)
 
 #ifdef CONFIG_SMP
 
+#define BALANCE_WORK	0x01
+#define BALANCE_PUSH	0x02
+
 static inline void
 queue_balance_callback(struct rq *rq,
 		       struct callback_head *head,
@@ -1384,12 +1411,13 @@ queue_balance_callback(struct rq *rq,
 {
 	lockdep_assert_held(&rq->lock);
 
-	if (unlikely(head->next))
+	if (unlikely(head->next || (rq->balance_flags & BALANCE_PUSH)))
 		return;
 
 	head->func = (void (*)(struct callback_head *))func;
 	head->next = rq->balance_callback;
 	rq->balance_callback = head;
+	rq->balance_flags |= BALANCE_WORK;
 }
 
 #define rcu_dereference_check_sched_domain(p) \
@@ -1716,6 +1744,7 @@ static inline int task_on_rq_migrating(struct task_struct *p)
 #define WF_FORK			0x02		/* Child wakeup after fork */
 #define WF_MIGRATED		0x04		/* Internal use, task got migrated */
 #define WF_ON_CPU		0x08		/* Wakee is on_cpu */
+#define WF_LOCK_SLEEPER		0x10		/* Wakeup spinlock "sleeper" */
 
 /*
  * To aid in avoiding the subversion of "niceness" due to uneven distribution
@@ -1797,10 +1826,13 @@ struct sched_class {
 	void (*task_woken)(struct rq *this_rq, struct task_struct *task);
 
 	void (*set_cpus_allowed)(struct task_struct *p,
-				 const struct cpumask *newmask);
+				 const struct cpumask *newmask,
+				 u32 flags);
 
 	void (*rq_online)(struct rq *rq);
 	void (*rq_offline)(struct rq *rq);
+
+	struct rq *(*find_lock_rq)(struct task_struct *p, struct rq *rq);
 #endif
 
 	void (*task_tick)(struct rq *rq, struct task_struct *p, int queued);
@@ -1884,13 +1916,35 @@ static inline bool sched_fair_runnable(struct rq *rq)
 extern struct task_struct *pick_next_task_fair(struct rq *rq, struct task_struct *prev, struct rq_flags *rf);
 extern struct task_struct *pick_next_task_idle(struct rq *rq);
 
+#define SCA_CHECK		0x01
+#define SCA_MIGRATE_DISABLE	0x02
+#define SCA_MIGRATE_ENABLE	0x04
+
 #ifdef CONFIG_SMP
 
 extern void update_group_capacity(struct sched_domain *sd, int cpu);
 
 extern void trigger_load_balance(struct rq *rq);
 
-extern void set_cpus_allowed_common(struct task_struct *p, const struct cpumask *new_mask);
+extern void set_cpus_allowed_common(struct task_struct *p, const struct cpumask *new_mask, u32 flags);
+
+static inline struct task_struct *get_push_task(struct rq *rq)
+{
+	struct task_struct *p = rq->curr;
+
+	lockdep_assert_held(&rq->lock);
+
+	if (rq->push_busy)
+		return NULL;
+
+	if (p->nr_cpus_allowed == 1)
+		return NULL;
+
+	rq->push_busy = true;
+	return get_task_struct(p);
+}
+
+extern int push_cpu_stop(void *arg);
 
 #endif
 
@@ -1934,6 +1988,15 @@ extern void reweight_task(struct task_struct *p, int prio);
 extern void resched_curr(struct rq *rq);
 extern void resched_cpu(int cpu);
 
+#ifdef CONFIG_PREEMPT_LAZY
+extern void resched_curr_lazy(struct rq *rq);
+#else
+static inline void resched_curr_lazy(struct rq *rq)
+{
+	resched_curr(rq);
+}
+#endif
+
 extern struct rt_bandwidth def_rt_bandwidth;
 extern void init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime);
 
diff --git a/kernel/sched/swait.c b/kernel/sched/swait.c
index e1c655f928c7..f230b1ac7f91 100644
--- a/kernel/sched/swait.c
+++ b/kernel/sched/swait.c
@@ -64,6 +64,7 @@ void swake_up_all(struct swait_queue_head *q)
 	struct swait_queue *curr;
 	LIST_HEAD(tmp);
 
+	WARN_ON(irqs_disabled());
 	raw_spin_lock_irq(&q->lock);
 	list_splice_init(&q->task_list, &tmp);
 	while (!list_empty(&tmp)) {
diff --git a/kernel/sched/topology.c b/kernel/sched/topology.c
index dd7770226086..c2d1d1ede0f4 100644
--- a/kernel/sched/topology.c
+++ b/kernel/sched/topology.c
@@ -514,6 +514,7 @@ static int init_rootdomain(struct root_domain *rd)
 	rd->rto_cpu = -1;
 	raw_spin_lock_init(&rd->rto_lock);
 	init_irq_work(&rd->rto_push_work, rto_push_irq_work_func);
+	atomic_or(IRQ_WORK_HARD_IRQ, &rd->rto_push_work.flags);
 #endif
 
 	init_dl_bw(&rd->dl_bw);
diff --git a/kernel/signal.c b/kernel/signal.c
index ef8f2a28d37c..0be3c40c5662 100644
--- a/kernel/signal.c
+++ b/kernel/signal.c
@@ -20,6 +20,7 @@
 #include <linux/sched/task.h>
 #include <linux/sched/task_stack.h>
 #include <linux/sched/cputime.h>
+#include <linux/sched/rt.h>
 #include <linux/file.h>
 #include <linux/fs.h>
 #include <linux/proc_fs.h>
@@ -404,13 +405,30 @@ void task_join_group_stop(struct task_struct *task)
 	task_set_jobctl_pending(task, mask | JOBCTL_STOP_PENDING);
 }
 
+static inline struct sigqueue *get_task_cache(struct task_struct *t)
+{
+	struct sigqueue *q = t->sigqueue_cache;
+
+	if (cmpxchg(&t->sigqueue_cache, q, NULL) != q)
+		return NULL;
+	return q;
+}
+
+static inline int put_task_cache(struct task_struct *t, struct sigqueue *q)
+{
+	if (cmpxchg(&t->sigqueue_cache, NULL, q) == NULL)
+		return 0;
+	return 1;
+}
+
 /*
  * allocate a new signal queue record
  * - this may be called without locks if and only if t == current, otherwise an
  *   appropriate lock must be held to stop the target task from exiting
  */
 static struct sigqueue *
-__sigqueue_alloc(int sig, struct task_struct *t, gfp_t flags, int override_rlimit)
+__sigqueue_do_alloc(int sig, struct task_struct *t, gfp_t flags,
+		    int override_rlimit, int fromslab)
 {
 	struct sigqueue *q = NULL;
 	struct user_struct *user;
@@ -432,7 +450,10 @@ __sigqueue_alloc(int sig, struct task_struct *t, gfp_t flags, int override_rlimi
 	rcu_read_unlock();
 
 	if (override_rlimit || likely(sigpending <= task_rlimit(t, RLIMIT_SIGPENDING))) {
-		q = kmem_cache_alloc(sigqueue_cachep, flags);
+		if (!fromslab)
+			q = get_task_cache(t);
+		if (!q)
+			q = kmem_cache_alloc(sigqueue_cachep, flags);
 	} else {
 		print_dropped_signal(sig);
 	}
@@ -449,6 +470,13 @@ __sigqueue_alloc(int sig, struct task_struct *t, gfp_t flags, int override_rlimi
 	return q;
 }
 
+static struct sigqueue *
+__sigqueue_alloc(int sig, struct task_struct *t, gfp_t flags,
+		 int override_rlimit)
+{
+	return __sigqueue_do_alloc(sig, t, flags, override_rlimit, 0);
+}
+
 static void __sigqueue_free(struct sigqueue *q)
 {
 	if (q->flags & SIGQUEUE_PREALLOC)
@@ -458,6 +486,21 @@ static void __sigqueue_free(struct sigqueue *q)
 	kmem_cache_free(sigqueue_cachep, q);
 }
 
+static void sigqueue_free_current(struct sigqueue *q)
+{
+	struct user_struct *up;
+
+	if (q->flags & SIGQUEUE_PREALLOC)
+		return;
+
+	up = q->user;
+	if (rt_prio(current->normal_prio) && !put_task_cache(current, q)) {
+		if (atomic_dec_and_test(&up->sigpending))
+			free_uid(up);
+	} else
+		  __sigqueue_free(q);
+}
+
 void flush_sigqueue(struct sigpending *queue)
 {
 	struct sigqueue *q;
@@ -470,6 +513,21 @@ void flush_sigqueue(struct sigpending *queue)
 	}
 }
 
+/*
+ * Called from __exit_signal. Flush tsk->pending and
+ * tsk->sigqueue_cache
+ */
+void flush_task_sigqueue(struct task_struct *tsk)
+{
+	struct sigqueue *q;
+
+	flush_sigqueue(&tsk->pending);
+
+	q = get_task_cache(tsk);
+	if (q)
+		kmem_cache_free(sigqueue_cachep, q);
+}
+
 /*
  * Flush all pending signals for this kthread.
  */
@@ -594,7 +652,7 @@ static void collect_signal(int sig, struct sigpending *list, kernel_siginfo_t *i
 			(info->si_code == SI_TIMER) &&
 			(info->si_sys_private);
 
-		__sigqueue_free(first);
+		sigqueue_free_current(first);
 	} else {
 		/*
 		 * Ok, it wasn't in the queue.  This must be
@@ -631,6 +689,8 @@ int dequeue_signal(struct task_struct *tsk, sigset_t *mask, kernel_siginfo_t *in
 	bool resched_timer = false;
 	int signr;
 
+	WARN_ON_ONCE(tsk != current);
+
 	/* We only dequeue private signals from ourselves, we don't let
 	 * signalfd steal them
 	 */
@@ -1314,6 +1374,34 @@ force_sig_info_to_task(struct kernel_siginfo *info, struct task_struct *t)
 	struct k_sigaction *action;
 	int sig = info->si_signo;
 
+	/*
+	 * On some archs, PREEMPT_RT has to delay sending a signal from a trap
+	 * since it can not enable preemption, and the signal code's spin_locks
+	 * turn into mutexes. Instead, it must set TIF_NOTIFY_RESUME which will
+	 * send the signal on exit of the trap.
+	 */
+#ifdef ARCH_RT_DELAYS_SIGNAL_SEND
+	if (in_atomic()) {
+		struct task_struct *t = current;
+
+		if (WARN_ON_ONCE(t->forced_info.si_signo))
+			return 0;
+
+		if (is_si_special(info)) {
+			WARN_ON_ONCE(info != SEND_SIG_PRIV);
+			t->forced_info.si_signo = info->si_signo;
+			t->forced_info.si_errno = 0;
+			t->forced_info.si_code = SI_KERNEL;
+			t->forced_info.si_pid = 0;
+			t->forced_info.si_uid = 0;
+		} else {
+			t->forced_info = *info;
+		}
+
+		set_tsk_thread_flag(t, TIF_NOTIFY_RESUME);
+		return 0;
+	}
+#endif
 	spin_lock_irqsave(&t->sighand->siglock, flags);
 	action = &t->sighand->action[sig-1];
 	ignored = action->sa.sa_handler == SIG_IGN;
@@ -1807,7 +1895,8 @@ EXPORT_SYMBOL(kill_pid);
  */
 struct sigqueue *sigqueue_alloc(void)
 {
-	struct sigqueue *q = __sigqueue_alloc(-1, current, GFP_KERNEL, 0);
+	/* Preallocated sigqueue objects always from the slabcache ! */
+	struct sigqueue *q = __sigqueue_do_alloc(-1, current, GFP_KERNEL, 0, 1);
 
 	if (q)
 		q->flags |= SIGQUEUE_PREALLOC;
@@ -2203,16 +2292,8 @@ static void ptrace_stop(int exit_code, int why, int clear_code, kernel_siginfo_t
 		if (gstop_done && ptrace_reparented(current))
 			do_notify_parent_cldstop(current, false, why);
 
-		/*
-		 * Don't want to allow preemption here, because
-		 * sys_ptrace() needs this task to be inactive.
-		 *
-		 * XXX: implement read_unlock_no_resched().
-		 */
-		preempt_disable();
 		read_unlock(&tasklist_lock);
 		cgroup_enter_frozen();
-		preempt_enable_no_resched();
 		freezable_schedule();
 		cgroup_leave_frozen(true);
 	} else {
diff --git a/kernel/smp.c b/kernel/smp.c
index 25240fb2df94..23778281aaa7 100644
--- a/kernel/smp.c
+++ b/kernel/smp.c
@@ -450,8 +450,18 @@ void flush_smp_call_function_from_idle(void)
 
 	local_irq_save(flags);
 	flush_smp_call_function_queue(true);
-	if (local_softirq_pending())
-		do_softirq();
+
+	if (local_softirq_pending()) {
+
+		if (!IS_ENABLED(CONFIG_PREEMPT_RT)) {
+			do_softirq();
+		} else {
+			struct task_struct *ksoftirqd = this_cpu_ksoftirqd();
+
+			if (ksoftirqd && ksoftirqd->state != TASK_RUNNING)
+				wake_up_process(ksoftirqd);
+		}
+	}
 
 	local_irq_restore(flags);
 }
diff --git a/kernel/softirq.c b/kernel/softirq.c
index 09229ad82209..2a71c053a228 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -13,6 +13,7 @@
 #include <linux/kernel_stat.h>
 #include <linux/interrupt.h>
 #include <linux/init.h>
+#include <linux/local_lock.h>
 #include <linux/mm.h>
 #include <linux/notifier.h>
 #include <linux/percpu.h>
@@ -25,6 +26,7 @@
 #include <linux/smpboot.h>
 #include <linux/tick.h>
 #include <linux/irq.h>
+#include <linux/wait_bit.h>
 
 #define CREATE_TRACE_POINTS
 #include <trace/events/irq.h>
@@ -81,7 +83,9 @@ static void wakeup_softirqd(void)
  * right now. Let ksoftirqd handle this at its own rate, to get fairness,
  * unless we're doing some of the synchronous softirqs.
  */
-#define SOFTIRQ_NOW_MASK ((1 << HI_SOFTIRQ) | (1 << TASKLET_SOFTIRQ))
+#define SOFTIRQ_NOW_MASK                                                       \
+	((1 << HI_SOFTIRQ) | (1 << TASKLET_SOFTIRQ) | (1 << NET_RX_SOFTIRQ) |  \
+	 (1 << NET_TX_SOFTIRQ))
 static bool ksoftirqd_running(unsigned long pending)
 {
 	struct task_struct *tsk = __this_cpu_read(ksoftirqd);
@@ -92,27 +96,212 @@ static bool ksoftirqd_running(unsigned long pending)
 		!__kthread_should_park(tsk);
 }
 
+#ifdef CONFIG_TRACE_IRQFLAGS
+DEFINE_PER_CPU(int, hardirqs_enabled);
+DEFINE_PER_CPU(int, hardirq_context);
+EXPORT_PER_CPU_SYMBOL_GPL(hardirqs_enabled);
+EXPORT_PER_CPU_SYMBOL_GPL(hardirq_context);
+#endif
+
 /*
- * preempt_count and SOFTIRQ_OFFSET usage:
- * - preempt_count is changed by SOFTIRQ_OFFSET on entering or leaving
- *   softirq processing.
- * - preempt_count is changed by SOFTIRQ_DISABLE_OFFSET (= 2 * SOFTIRQ_OFFSET)
+ * SOFTIRQ_OFFSET usage:
+ *
+ * On !RT kernels 'count' is the preempt counter, on RT kernels this applies
+ * to a per CPU counter and to task::softirqs_disabled_cnt.
+ *
+ * - count is changed by SOFTIRQ_OFFSET on entering or leaving softirq
+ *   processing.
+ *
+ * - count is changed by SOFTIRQ_DISABLE_OFFSET (= 2 * SOFTIRQ_OFFSET)
  *   on local_bh_disable or local_bh_enable.
+ *
  * This lets us distinguish between whether we are currently processing
  * softirq and whether we just have bh disabled.
  */
+#ifdef CONFIG_PREEMPT_RT
 
 /*
- * This one is for softirq.c-internal use,
- * where hardirqs are disabled legitimately:
+ * RT accounts for BH disabled sections in task::softirqs_disabled_cnt and
+ * also in per CPU softirq_ctrl::cnt. This is necessary to allow tasks in a
+ * softirq disabled section to be preempted.
+ *
+ * The per task counter is used for softirq_count(), in_softirq() and
+ * in_serving_softirqs() because these counts are only valid when the task
+ * holding softirq_ctrl::lock is running.
+ *
+ * The per CPU counter prevents pointless wakeups of ksoftirqd in case that
+ * the task which is in a softirq disabled section is preempted or blocks.
  */
-#ifdef CONFIG_TRACE_IRQFLAGS
+struct softirq_ctrl {
+	local_lock_t	lock;
+	int		cnt;
+};
 
-DEFINE_PER_CPU(int, hardirqs_enabled);
-DEFINE_PER_CPU(int, hardirq_context);
-EXPORT_PER_CPU_SYMBOL_GPL(hardirqs_enabled);
-EXPORT_PER_CPU_SYMBOL_GPL(hardirq_context);
+static DEFINE_PER_CPU(struct softirq_ctrl, softirq_ctrl) = {
+	.lock	= INIT_LOCAL_LOCK(softirq_ctrl.lock),
+};
+
+/**
+ * local_bh_blocked() - Check for idle whether BH processing is blocked
+ *
+ * Returns false if the per CPU softirq::cnt is 0 otherwise true.
+ *
+ * This is invoked from the idle task to guard against false positive
+ * softirq pending warnings, which would happen when the task which holds
+ * softirq_ctrl::lock was the only running task on the CPU and blocks on
+ * some other lock.
+ */
+bool local_bh_blocked(void)
+{
+	return __this_cpu_read(softirq_ctrl.cnt) != 0;
+}
+
+void __local_bh_disable_ip(unsigned long ip, unsigned int cnt)
+{
+	unsigned long flags;
+	int newcnt;
+
+	WARN_ON_ONCE(in_hardirq());
+
+	/* First entry of a task into a BH disabled section? */
+	if (!current->softirq_disable_cnt) {
+		if (preemptible()) {
+			local_lock(&softirq_ctrl.lock);
+			/* Required to meet the RCU bottomhalf requirements. */
+			rcu_read_lock();
+		} else {
+			DEBUG_LOCKS_WARN_ON(this_cpu_read(softirq_ctrl.cnt));
+		}
+	}
+
+	/*
+	 * Track the per CPU softirq disabled state. On RT this is per CPU
+	 * state to allow preemption of bottom half disabled sections.
+	 */
+	newcnt = __this_cpu_add_return(softirq_ctrl.cnt, cnt);
+	/*
+	 * Reflect the result in the task state to prevent recursion on the
+	 * local lock and to make softirq_count() & al work.
+	 */
+	current->softirq_disable_cnt = newcnt;
+
+	if (IS_ENABLED(CONFIG_TRACE_IRQFLAGS) && newcnt == cnt) {
+		raw_local_irq_save(flags);
+		lockdep_softirqs_off(ip);
+		raw_local_irq_restore(flags);
+	}
+}
+EXPORT_SYMBOL(__local_bh_disable_ip);
+
+static void __local_bh_enable(unsigned int cnt, bool unlock)
+{
+	unsigned long flags;
+	int newcnt;
+
+	DEBUG_LOCKS_WARN_ON(current->softirq_disable_cnt !=
+			    this_cpu_read(softirq_ctrl.cnt));
+
+	if (IS_ENABLED(CONFIG_TRACE_IRQFLAGS) && softirq_count() == cnt) {
+		raw_local_irq_save(flags);
+		lockdep_softirqs_on(_RET_IP_);
+		raw_local_irq_restore(flags);
+	}
+
+	newcnt = __this_cpu_sub_return(softirq_ctrl.cnt, cnt);
+	current->softirq_disable_cnt = newcnt;
+
+	if (!newcnt && unlock) {
+		rcu_read_unlock();
+		local_unlock(&softirq_ctrl.lock);
+	}
+}
+
+void __local_bh_enable_ip(unsigned long ip, unsigned int cnt)
+{
+	bool preempt_on = preemptible();
+	unsigned long flags;
+	u32 pending;
+	int curcnt;
+
+	WARN_ON_ONCE(in_irq());
+	lockdep_assert_irqs_enabled();
+
+	local_irq_save(flags);
+	curcnt = __this_cpu_read(softirq_ctrl.cnt);
+
+	/*
+	 * If this is not reenabling soft interrupts, no point in trying to
+	 * run pending ones.
+	 */
+	if (curcnt != cnt)
+		goto out;
+
+	pending = local_softirq_pending();
+	if (!pending || ksoftirqd_running(pending))
+		goto out;
+
+	/*
+	 * If this was called from non preemptible context, wake up the
+	 * softirq daemon.
+	 */
+	if (!preempt_on) {
+		wakeup_softirqd();
+		goto out;
+	}
+
+	/*
+	 * Adjust softirq count to SOFTIRQ_OFFSET which makes
+	 * in_serving_softirq() become true.
+	 */
+	cnt = SOFTIRQ_OFFSET;
+	__local_bh_enable(cnt, false);
+	__do_softirq();
+
+out:
+	__local_bh_enable(cnt, preempt_on);
+	local_irq_restore(flags);
+}
+EXPORT_SYMBOL(__local_bh_enable_ip);
+
+/*
+ * Invoked from ksoftirqd_run() outside of the interrupt disabled section
+ * to acquire the per CPU local lock for reentrancy protection.
+ */
+static inline void ksoftirqd_run_begin(void)
+{
+	__local_bh_disable_ip(_RET_IP_, SOFTIRQ_OFFSET);
+	local_irq_disable();
+}
+
+/* Counterpart to ksoftirqd_run_begin() */
+static inline void ksoftirqd_run_end(void)
+{
+	__local_bh_enable(SOFTIRQ_OFFSET, true);
+	WARN_ON_ONCE(in_interrupt());
+	local_irq_enable();
+}
+
+static inline void softirq_handle_begin(void) { }
+static inline void softirq_handle_end(void) { }
 
+static inline bool should_wake_ksoftirqd(void)
+{
+	return !this_cpu_read(softirq_ctrl.cnt);
+}
+
+static inline void invoke_softirq(void)
+{
+	if (should_wake_ksoftirqd())
+		wakeup_softirqd();
+}
+
+#else /* CONFIG_PREEMPT_RT */
+
+/*
+ * This one is for softirq.c-internal use, where hardirqs are disabled
+ * legitimately:
+ */
+#ifdef CONFIG_TRACE_IRQFLAGS
 void __local_bh_disable_ip(unsigned long ip, unsigned int cnt)
 {
 	unsigned long flags;
@@ -203,6 +392,78 @@ void __local_bh_enable_ip(unsigned long ip, unsigned int cnt)
 }
 EXPORT_SYMBOL(__local_bh_enable_ip);
 
+static inline void softirq_handle_begin(void)
+{
+	__local_bh_disable_ip(_RET_IP_, SOFTIRQ_OFFSET);
+}
+
+static inline void softirq_handle_end(void)
+{
+	__local_bh_enable(SOFTIRQ_OFFSET);
+	WARN_ON_ONCE(in_interrupt());
+}
+
+static inline void ksoftirqd_run_begin(void)
+{
+	local_irq_disable();
+}
+
+static inline void ksoftirqd_run_end(void)
+{
+	local_irq_enable();
+}
+
+static inline bool should_wake_ksoftirqd(void)
+{
+	return true;
+}
+
+static inline void invoke_softirq(void)
+{
+	if (ksoftirqd_running(local_softirq_pending()))
+		return;
+
+	if (!force_irqthreads) {
+#ifdef CONFIG_HAVE_IRQ_EXIT_ON_IRQ_STACK
+		/*
+		 * We can safely execute softirq on the current stack if
+		 * it is the irq stack, because it should be near empty
+		 * at this stage.
+		 */
+		__do_softirq();
+#else
+		/*
+		 * Otherwise, irq_exit() is called on the task stack that can
+		 * be potentially deep already. So call softirq in its own stack
+		 * to prevent from any overrun.
+		 */
+		do_softirq_own_stack();
+#endif
+	} else {
+		wakeup_softirqd();
+	}
+}
+
+asmlinkage __visible void do_softirq(void)
+{
+	__u32 pending;
+	unsigned long flags;
+
+	if (in_interrupt())
+		return;
+
+	local_irq_save(flags);
+
+	pending = local_softirq_pending();
+
+	if (pending && !ksoftirqd_running(pending))
+		do_softirq_own_stack();
+
+	local_irq_restore(flags);
+}
+
+#endif /* !CONFIG_PREEMPT_RT */
+
 /*
  * We restart softirq processing for at most MAX_SOFTIRQ_RESTART times,
  * but break the loop if need_resched() is set or after 2 ms.
@@ -270,10 +531,10 @@ asmlinkage __visible void __softirq_entry __do_softirq(void)
 	current->flags &= ~PF_MEMALLOC;
 
 	pending = local_softirq_pending();
-	account_irq_enter_time(current);
 
-	__local_bh_disable_ip(_RET_IP_, SOFTIRQ_OFFSET);
+	softirq_handle_begin();
 	in_hardirq = lockdep_softirq_start();
+	account_softirq_enter(current);
 
 restart:
 	/* Reset the pending bitmask before enabling irqs */
@@ -307,8 +568,10 @@ asmlinkage __visible void __softirq_entry __do_softirq(void)
 		pending >>= softirq_bit;
 	}
 
-	if (__this_cpu_read(ksoftirqd) == current)
+	if (!IS_ENABLED(CONFIG_PREEMPT_RT) &&
+	    __this_cpu_read(ksoftirqd) == current)
 		rcu_softirq_qs();
+
 	local_irq_disable();
 
 	pending = local_softirq_pending();
@@ -320,46 +583,23 @@ asmlinkage __visible void __softirq_entry __do_softirq(void)
 		wakeup_softirqd();
 	}
 
+	account_softirq_exit(current);
 	lockdep_softirq_end(in_hardirq);
-	account_irq_exit_time(current);
-	__local_bh_enable(SOFTIRQ_OFFSET);
-	WARN_ON_ONCE(in_interrupt());
+	softirq_handle_end();
 	current_restore_flags(old_flags, PF_MEMALLOC);
 }
 
-asmlinkage __visible void do_softirq(void)
-{
-	__u32 pending;
-	unsigned long flags;
-
-	if (in_interrupt())
-		return;
-
-	local_irq_save(flags);
-
-	pending = local_softirq_pending();
-
-	if (pending && !ksoftirqd_running(pending))
-		do_softirq_own_stack();
-
-	local_irq_restore(flags);
-}
-
 /**
  * irq_enter_rcu - Enter an interrupt context with RCU watching
  */
 void irq_enter_rcu(void)
 {
-	if (is_idle_task(current) && !in_interrupt()) {
-		/*
-		 * Prevent raise_softirq from needlessly waking up ksoftirqd
-		 * here, as softirq will be serviced on return from interrupt.
-		 */
-		local_bh_disable();
+	__irq_enter_raw();
+
+	if (is_idle_task(current) && (irq_count() == HARDIRQ_OFFSET))
 		tick_irq_enter();
-		_local_bh_enable();
-	}
-	__irq_enter();
+
+	account_hardirq_enter(current);
 }
 
 /**
@@ -371,32 +611,6 @@ void irq_enter(void)
 	irq_enter_rcu();
 }
 
-static inline void invoke_softirq(void)
-{
-	if (ksoftirqd_running(local_softirq_pending()))
-		return;
-
-	if (!force_irqthreads) {
-#ifdef CONFIG_HAVE_IRQ_EXIT_ON_IRQ_STACK
-		/*
-		 * We can safely execute softirq on the current stack if
-		 * it is the irq stack, because it should be near empty
-		 * at this stage.
-		 */
-		__do_softirq();
-#else
-		/*
-		 * Otherwise, irq_exit() is called on the task stack that can
-		 * be potentially deep already. So call softirq in its own stack
-		 * to prevent from any overrun.
-		 */
-		do_softirq_own_stack();
-#endif
-	} else {
-		wakeup_softirqd();
-	}
-}
-
 static inline void tick_irq_exit(void)
 {
 #ifdef CONFIG_NO_HZ_COMMON
@@ -417,7 +631,7 @@ static inline void __irq_exit_rcu(void)
 #else
 	lockdep_assert_irqs_disabled();
 #endif
-	account_irq_exit_time(current);
+	account_hardirq_exit(current);
 	preempt_count_sub(HARDIRQ_OFFSET);
 	if (!in_interrupt() && local_softirq_pending())
 		invoke_softirq();
@@ -466,7 +680,7 @@ inline void raise_softirq_irqoff(unsigned int nr)
 	 * Otherwise we wake up ksoftirqd to make sure we
 	 * schedule the softirq soon.
 	 */
-	if (!in_interrupt())
+	if (!in_interrupt() && should_wake_ksoftirqd())
 		wakeup_softirqd();
 }
 
@@ -532,6 +746,16 @@ void __tasklet_hi_schedule(struct tasklet_struct *t)
 }
 EXPORT_SYMBOL(__tasklet_hi_schedule);
 
+static inline bool tasklet_clear_sched(struct tasklet_struct *t)
+{
+	if (test_and_clear_bit(TASKLET_STATE_SCHED, &t->state)) {
+		wake_up_var(&t->state);
+		return true;
+	}
+
+	return false;
+}
+
 static void tasklet_action_common(struct softirq_action *a,
 				  struct tasklet_head *tl_head,
 				  unsigned int softirq_nr)
@@ -551,8 +775,7 @@ static void tasklet_action_common(struct softirq_action *a,
 
 		if (tasklet_trylock(t)) {
 			if (!atomic_read(&t->count)) {
-				if (!test_and_clear_bit(TASKLET_STATE_SCHED,
-							&t->state))
+				if (!tasklet_clear_sched(t))
 					BUG();
 				if (t->use_callback)
 					t->callback(t);
@@ -607,21 +830,62 @@ void tasklet_init(struct tasklet_struct *t,
 }
 EXPORT_SYMBOL(tasklet_init);
 
+#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT_RT)
+/*
+ * Do not use in new code. Waiting for tasklets from atomic contexts is
+ * error prone and should be avoided.
+ */
+void tasklet_unlock_spin_wait(struct tasklet_struct *t)
+{
+	while (test_bit(TASKLET_STATE_RUN, &(t)->state)) {
+		if (IS_ENABLED(CONFIG_PREEMPT_RT)) {
+			/*
+			 * Prevent a live lock when current preempted soft
+			 * interrupt processing or prevents ksoftirqd from
+			 * running. If the tasklet runs on a different CPU
+			 * then this has no effect other than doing the BH
+			 * disable/enable dance for nothing.
+			 */
+			local_bh_disable();
+			local_bh_enable();
+		} else {
+			cpu_relax();
+		}
+	}
+}
+EXPORT_SYMBOL(tasklet_unlock_spin_wait);
+#endif
+
 void tasklet_kill(struct tasklet_struct *t)
 {
 	if (in_interrupt())
 		pr_notice("Attempt to kill tasklet from interrupt\n");
 
-	while (test_and_set_bit(TASKLET_STATE_SCHED, &t->state)) {
-		do {
-			yield();
-		} while (test_bit(TASKLET_STATE_SCHED, &t->state));
-	}
+	while (test_and_set_bit(TASKLET_STATE_SCHED, &t->state))
+		wait_var_event(&t->state, !test_bit(TASKLET_STATE_SCHED, &t->state));
+
 	tasklet_unlock_wait(t);
-	clear_bit(TASKLET_STATE_SCHED, &t->state);
+	tasklet_clear_sched(t);
 }
 EXPORT_SYMBOL(tasklet_kill);
 
+#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT_RT)
+void tasklet_unlock(struct tasklet_struct *t)
+{
+	smp_mb__before_atomic();
+	clear_bit(TASKLET_STATE_RUN, &t->state);
+	smp_mb__after_atomic();
+	wake_up_var(&t->state);
+}
+EXPORT_SYMBOL_GPL(tasklet_unlock);
+
+void tasklet_unlock_wait(struct tasklet_struct *t)
+{
+	wait_var_event(&t->state, !test_bit(TASKLET_STATE_RUN, &t->state));
+}
+EXPORT_SYMBOL_GPL(tasklet_unlock_wait);
+#endif
+
 void __init softirq_init(void)
 {
 	int cpu;
@@ -644,18 +908,18 @@ static int ksoftirqd_should_run(unsigned int cpu)
 
 static void run_ksoftirqd(unsigned int cpu)
 {
-	local_irq_disable();
+	ksoftirqd_run_begin();
 	if (local_softirq_pending()) {
 		/*
 		 * We can safely run softirq on inline stack, as we are not deep
 		 * in the task stack here.
 		 */
 		__do_softirq();
-		local_irq_enable();
+		ksoftirqd_run_end();
 		cond_resched();
 		return;
 	}
-	local_irq_enable();
+	ksoftirqd_run_end();
 }
 
 #ifdef CONFIG_HOTPLUG_CPU
diff --git a/kernel/stop_machine.c b/kernel/stop_machine.c
index 890b79cf0e7c..dbf585cf4b9f 100644
--- a/kernel/stop_machine.c
+++ b/kernel/stop_machine.c
@@ -42,11 +42,23 @@ struct cpu_stopper {
 	struct list_head	works;		/* list of pending works */
 
 	struct cpu_stop_work	stop_work;	/* for stop_cpus */
+	unsigned long		caller;
+	cpu_stop_fn_t		fn;
 };
 
 static DEFINE_PER_CPU(struct cpu_stopper, cpu_stopper);
 static bool stop_machine_initialized = false;
 
+void print_stop_info(const char *log_lvl, struct task_struct *task)
+{
+	struct cpu_stopper *stopper = this_cpu_ptr(&cpu_stopper);
+
+	if (task != stopper->thread)
+		return;
+
+	printk("%sStopper: %pS <- %pS\n", log_lvl, stopper->fn, (void *)stopper->caller);
+}
+
 /* static data for stop_cpus */
 static DEFINE_MUTEX(stop_cpus_mutex);
 static bool stop_cpus_in_progress;
@@ -123,7 +135,7 @@ static bool cpu_stop_queue_work(unsigned int cpu, struct cpu_stop_work *work)
 int stop_one_cpu(unsigned int cpu, cpu_stop_fn_t fn, void *arg)
 {
 	struct cpu_stop_done done;
-	struct cpu_stop_work work = { .fn = fn, .arg = arg, .done = &done };
+	struct cpu_stop_work work = { .fn = fn, .arg = arg, .done = &done, .caller = _RET_IP_ };
 
 	cpu_stop_init_done(&done, 1);
 	if (!cpu_stop_queue_work(cpu, &work))
@@ -331,7 +343,8 @@ int stop_two_cpus(unsigned int cpu1, unsigned int cpu2, cpu_stop_fn_t fn, void *
 	work1 = work2 = (struct cpu_stop_work){
 		.fn = multi_cpu_stop,
 		.arg = &msdata,
-		.done = &done
+		.done = &done,
+		.caller = _RET_IP_,
 	};
 
 	cpu_stop_init_done(&done, 2);
@@ -367,7 +380,7 @@ int stop_two_cpus(unsigned int cpu1, unsigned int cpu2, cpu_stop_fn_t fn, void *
 bool stop_one_cpu_nowait(unsigned int cpu, cpu_stop_fn_t fn, void *arg,
 			struct cpu_stop_work *work_buf)
 {
-	*work_buf = (struct cpu_stop_work){ .fn = fn, .arg = arg, };
+	*work_buf = (struct cpu_stop_work){ .fn = fn, .arg = arg, .caller = _RET_IP_, };
 	return cpu_stop_queue_work(cpu, work_buf);
 }
 
@@ -487,6 +500,8 @@ static void cpu_stopper_thread(unsigned int cpu)
 		int ret;
 
 		/* cpu stop callbacks must not sleep, make in_atomic() == T */
+		stopper->caller = work->caller;
+		stopper->fn = fn;
 		preempt_count_inc();
 		ret = fn(arg);
 		if (done) {
@@ -495,6 +510,8 @@ static void cpu_stopper_thread(unsigned int cpu)
 			cpu_stop_signal_done(done);
 		}
 		preempt_count_dec();
+		stopper->fn = NULL;
+		stopper->caller = 0;
 		WARN_ONCE(preempt_count(),
 			  "cpu_stop: %ps(%p) leaked preempt count\n", fn, arg);
 		goto repeat;
diff --git a/kernel/time/hrtimer.c b/kernel/time/hrtimer.c
index 387b4bef7dd1..02f43f663ad5 100644
--- a/kernel/time/hrtimer.c
+++ b/kernel/time/hrtimer.c
@@ -1988,6 +1988,36 @@ SYSCALL_DEFINE2(nanosleep_time32, struct old_timespec32 __user *, rqtp,
 }
 #endif
 
+#ifdef CONFIG_PREEMPT_RT
+/*
+ * Sleep for 1 ms in hope whoever holds what we want will let it go.
+ */
+void cpu_chill(void)
+{
+	unsigned int freeze_flag = current->flags & PF_NOFREEZE;
+	struct task_struct *self = current;
+	ktime_t chill_time;
+
+	raw_spin_lock_irq(&self->pi_lock);
+	self->saved_state = self->state;
+	__set_current_state_no_track(TASK_UNINTERRUPTIBLE);
+	raw_spin_unlock_irq(&self->pi_lock);
+
+	chill_time = ktime_set(0, NSEC_PER_MSEC);
+
+	current->flags |= PF_NOFREEZE;
+	schedule_hrtimeout(&chill_time, HRTIMER_MODE_REL_HARD);
+	if (!freeze_flag)
+		current->flags &= ~PF_NOFREEZE;
+
+	raw_spin_lock_irq(&self->pi_lock);
+	__set_current_state_no_track(self->saved_state);
+	self->saved_state = TASK_RUNNING;
+	raw_spin_unlock_irq(&self->pi_lock);
+}
+EXPORT_SYMBOL(cpu_chill);
+#endif
+
 /*
  * Functions related to boot-time initialization:
  */
diff --git a/kernel/time/tick-sched.c b/kernel/time/tick-sched.c
index e8d351b7f9b0..3e3130a7d44e 100644
--- a/kernel/time/tick-sched.c
+++ b/kernel/time/tick-sched.c
@@ -925,7 +925,7 @@ static bool can_stop_idle_tick(int cpu, struct tick_sched *ts)
 	if (unlikely(local_softirq_pending())) {
 		static int ratelimit;
 
-		if (ratelimit < 10 &&
+		if (ratelimit < 10 && !local_bh_blocked() &&
 		    (local_softirq_pending() & SOFTIRQ_STOP_IDLE_MASK)) {
 			pr_warn("NOHZ tick-stop error: Non-RCU local softirq work is pending, handler #%02x!!!\n",
 				(unsigned int) local_softirq_pending());
diff --git a/kernel/time/timer.c b/kernel/time/timer.c
index c3ad64fb9d8b..b6477db234e6 100644
--- a/kernel/time/timer.c
+++ b/kernel/time/timer.c
@@ -1263,8 +1263,10 @@ static inline void timer_base_unlock_expiry(struct timer_base *base)
 static void timer_sync_wait_running(struct timer_base *base)
 {
 	if (atomic_read(&base->timer_waiters)) {
+		raw_spin_unlock_irq(&base->lock);
 		spin_unlock(&base->expiry_lock);
 		spin_lock(&base->expiry_lock);
+		raw_spin_lock_irq(&base->lock);
 	}
 }
 
@@ -1283,7 +1285,7 @@ static void del_timer_wait_running(struct timer_list *timer)
 	u32 tf;
 
 	tf = READ_ONCE(timer->flags);
-	if (!(tf & TIMER_MIGRATING)) {
+	if (!(tf & (TIMER_MIGRATING | TIMER_IRQSAFE))) {
 		struct timer_base *base = get_timer_base(tf);
 
 		/*
@@ -1367,6 +1369,13 @@ int del_timer_sync(struct timer_list *timer)
 	 */
 	WARN_ON(in_irq() && !(timer->flags & TIMER_IRQSAFE));
 
+	/*
+	 * Must be able to sleep on PREEMPT_RT because of the slowpath in
+	 * del_timer_wait_running().
+	 */
+	if (IS_ENABLED(CONFIG_PREEMPT_RT) && !(timer->flags & TIMER_IRQSAFE))
+		lockdep_assert_preemption_enabled();
+
 	do {
 		ret = try_to_del_timer_sync(timer);
 
@@ -1448,14 +1457,14 @@ static void expire_timers(struct timer_base *base, struct hlist_head *head)
 		if (timer->flags & TIMER_IRQSAFE) {
 			raw_spin_unlock(&base->lock);
 			call_timer_fn(timer, fn, baseclk);
-			base->running_timer = NULL;
 			raw_spin_lock(&base->lock);
+			base->running_timer = NULL;
 		} else {
 			raw_spin_unlock_irq(&base->lock);
 			call_timer_fn(timer, fn, baseclk);
+			raw_spin_lock_irq(&base->lock);
 			base->running_timer = NULL;
 			timer_sync_wait_running(base);
-			raw_spin_lock_irq(&base->lock);
 		}
 	}
 }
@@ -1757,6 +1766,8 @@ static __latent_entropy void run_timer_softirq(struct softirq_action *h)
 {
 	struct timer_base *base = this_cpu_ptr(&timer_bases[BASE_STD]);
 
+	irq_work_tick_soft();
+
 	__run_timers(base);
 	if (IS_ENABLED(CONFIG_NO_HZ_COMMON))
 		__run_timers(this_cpu_ptr(&timer_bases[BASE_DEF]));
diff --git a/kernel/trace/blktrace.c b/kernel/trace/blktrace.c
index f1022945e346..c300ac337573 100644
--- a/kernel/trace/blktrace.c
+++ b/kernel/trace/blktrace.c
@@ -72,17 +72,17 @@ static void trace_note(struct blk_trace *bt, pid_t pid, int action,
 	struct blk_io_trace *t;
 	struct ring_buffer_event *event = NULL;
 	struct trace_buffer *buffer = NULL;
-	int pc = 0;
+	unsigned int trace_ctx = 0;
 	int cpu = smp_processor_id();
 	bool blk_tracer = blk_tracer_enabled;
 	ssize_t cgid_len = cgid ? sizeof(cgid) : 0;
 
 	if (blk_tracer) {
 		buffer = blk_tr->array_buffer.buffer;
-		pc = preempt_count();
+		trace_ctx = tracing_gen_ctx_flags(0);
 		event = trace_buffer_lock_reserve(buffer, TRACE_BLK,
 						  sizeof(*t) + len + cgid_len,
-						  0, pc);
+						  trace_ctx);
 		if (!event)
 			return;
 		t = ring_buffer_event_data(event);
@@ -107,7 +107,7 @@ static void trace_note(struct blk_trace *bt, pid_t pid, int action,
 		memcpy((void *) t + sizeof(*t) + cgid_len, data, len);
 
 		if (blk_tracer)
-			trace_buffer_unlock_commit(blk_tr, buffer, event, 0, pc);
+			trace_buffer_unlock_commit(blk_tr, buffer, event, trace_ctx);
 	}
 }
 
@@ -222,8 +222,9 @@ static void __blk_add_trace(struct blk_trace *bt, sector_t sector, int bytes,
 	struct blk_io_trace *t;
 	unsigned long flags = 0;
 	unsigned long *sequence;
+	unsigned int trace_ctx = 0;
 	pid_t pid;
-	int cpu, pc = 0;
+	int cpu;
 	bool blk_tracer = blk_tracer_enabled;
 	ssize_t cgid_len = cgid ? sizeof(cgid) : 0;
 
@@ -252,10 +253,10 @@ static void __blk_add_trace(struct blk_trace *bt, sector_t sector, int bytes,
 		tracing_record_cmdline(current);
 
 		buffer = blk_tr->array_buffer.buffer;
-		pc = preempt_count();
+		trace_ctx = tracing_gen_ctx_flags(0);
 		event = trace_buffer_lock_reserve(buffer, TRACE_BLK,
 						  sizeof(*t) + pdu_len + cgid_len,
-						  0, pc);
+						  trace_ctx);
 		if (!event)
 			return;
 		t = ring_buffer_event_data(event);
@@ -301,7 +302,7 @@ static void __blk_add_trace(struct blk_trace *bt, sector_t sector, int bytes,
 			memcpy((void *)t + sizeof(*t) + cgid_len, pdu_data, pdu_len);
 
 		if (blk_tracer) {
-			trace_buffer_unlock_commit(blk_tr, buffer, event, 0, pc);
+			trace_buffer_unlock_commit(blk_tr, buffer, event, trace_ctx);
 			return;
 		}
 	}
diff --git a/kernel/trace/trace.c b/kernel/trace/trace.c
index ee4be813ba85..24166ab83304 100644
--- a/kernel/trace/trace.c
+++ b/kernel/trace/trace.c
@@ -176,7 +176,7 @@ static union trace_eval_map_item *trace_eval_maps;
 int tracing_set_tracer(struct trace_array *tr, const char *buf);
 static void ftrace_trace_userstack(struct trace_array *tr,
 				   struct trace_buffer *buffer,
-				   unsigned long flags, int pc);
+				   unsigned int trace_ctx);
 
 #define MAX_TRACER_SIZE		100
 static char bootup_tracer_buf[MAX_TRACER_SIZE] __initdata;
@@ -905,23 +905,23 @@ static inline void trace_access_lock_init(void)
 
 #ifdef CONFIG_STACKTRACE
 static void __ftrace_trace_stack(struct trace_buffer *buffer,
-				 unsigned long flags,
-				 int skip, int pc, struct pt_regs *regs);
+				 unsigned int trace_ctx,
+				 int skip, struct pt_regs *regs);
 static inline void ftrace_trace_stack(struct trace_array *tr,
 				      struct trace_buffer *buffer,
-				      unsigned long flags,
-				      int skip, int pc, struct pt_regs *regs);
+				      unsigned int trace_ctx,
+				      int skip, struct pt_regs *regs);
 
 #else
 static inline void __ftrace_trace_stack(struct trace_buffer *buffer,
-					unsigned long flags,
-					int skip, int pc, struct pt_regs *regs)
+					unsigned int trace_ctx,
+					int skip, struct pt_regs *regs)
 {
 }
 static inline void ftrace_trace_stack(struct trace_array *tr,
 				      struct trace_buffer *buffer,
-				      unsigned long flags,
-				      int skip, int pc, struct pt_regs *regs)
+				      unsigned long trace_ctx,
+				      int skip, struct pt_regs *regs)
 {
 }
 
@@ -929,24 +929,24 @@ static inline void ftrace_trace_stack(struct trace_array *tr,
 
 static __always_inline void
 trace_event_setup(struct ring_buffer_event *event,
-		  int type, unsigned long flags, int pc)
+		  int type, unsigned int trace_ctx)
 {
 	struct trace_entry *ent = ring_buffer_event_data(event);
 
-	tracing_generic_entry_update(ent, type, flags, pc);
+	tracing_generic_entry_update(ent, type, trace_ctx);
 }
 
 static __always_inline struct ring_buffer_event *
 __trace_buffer_lock_reserve(struct trace_buffer *buffer,
 			  int type,
 			  unsigned long len,
-			  unsigned long flags, int pc)
+			  unsigned int trace_ctx)
 {
 	struct ring_buffer_event *event;
 
 	event = ring_buffer_lock_reserve(buffer, len);
 	if (event != NULL)
-		trace_event_setup(event, type, flags, pc);
+		trace_event_setup(event, type, trace_ctx);
 
 	return event;
 }
@@ -1007,25 +1007,22 @@ int __trace_puts(unsigned long ip, const char *str, int size)
 	struct ring_buffer_event *event;
 	struct trace_buffer *buffer;
 	struct print_entry *entry;
-	unsigned long irq_flags;
+	unsigned int trace_ctx;
 	int alloc;
-	int pc;
 
 	if (!(global_trace.trace_flags & TRACE_ITER_PRINTK))
 		return 0;
 
-	pc = preempt_count();
-
 	if (unlikely(tracing_selftest_running || tracing_disabled))
 		return 0;
 
 	alloc = sizeof(*entry) + size + 2; /* possible \n added */
 
-	local_save_flags(irq_flags);
+	trace_ctx = tracing_gen_ctx();
 	buffer = global_trace.array_buffer.buffer;
 	ring_buffer_nest_start(buffer);
-	event = __trace_buffer_lock_reserve(buffer, TRACE_PRINT, alloc, 
-					    irq_flags, pc);
+	event = __trace_buffer_lock_reserve(buffer, TRACE_PRINT, alloc,
+					    trace_ctx);
 	if (!event) {
 		size = 0;
 		goto out;
@@ -1044,7 +1041,7 @@ int __trace_puts(unsigned long ip, const char *str, int size)
 		entry->buf[size] = '\0';
 
 	__buffer_unlock_commit(buffer, event);
-	ftrace_trace_stack(&global_trace, buffer, irq_flags, 4, pc, NULL);
+	ftrace_trace_stack(&global_trace, buffer, trace_ctx, 4, NULL);
  out:
 	ring_buffer_nest_end(buffer);
 	return size;
@@ -1061,25 +1058,22 @@ int __trace_bputs(unsigned long ip, const char *str)
 	struct ring_buffer_event *event;
 	struct trace_buffer *buffer;
 	struct bputs_entry *entry;
-	unsigned long irq_flags;
+	unsigned int trace_ctx;
 	int size = sizeof(struct bputs_entry);
 	int ret = 0;
-	int pc;
 
 	if (!(global_trace.trace_flags & TRACE_ITER_PRINTK))
 		return 0;
 
-	pc = preempt_count();
-
 	if (unlikely(tracing_selftest_running || tracing_disabled))
 		return 0;
 
-	local_save_flags(irq_flags);
+	trace_ctx = tracing_gen_ctx();
 	buffer = global_trace.array_buffer.buffer;
 
 	ring_buffer_nest_start(buffer);
 	event = __trace_buffer_lock_reserve(buffer, TRACE_BPUTS, size,
-					    irq_flags, pc);
+					    trace_ctx);
 	if (!event)
 		goto out;
 
@@ -1088,7 +1082,7 @@ int __trace_bputs(unsigned long ip, const char *str)
 	entry->str			= str;
 
 	__buffer_unlock_commit(buffer, event);
-	ftrace_trace_stack(&global_trace, buffer, irq_flags, 4, pc, NULL);
+	ftrace_trace_stack(&global_trace, buffer, trace_ctx, 4, NULL);
 
 	ret = 1;
  out:
@@ -2584,36 +2578,52 @@ enum print_line_t trace_handle_return(struct trace_seq *s)
 }
 EXPORT_SYMBOL_GPL(trace_handle_return);
 
-void
-tracing_generic_entry_update(struct trace_entry *entry, unsigned short type,
-			     unsigned long flags, int pc)
+static unsigned short migration_disable_value(void)
 {
-	struct task_struct *tsk = current;
-
-	entry->preempt_count		= pc & 0xff;
-	entry->pid			= (tsk) ? tsk->pid : 0;
-	entry->type			= type;
-	entry->flags =
-#ifdef CONFIG_TRACE_IRQFLAGS_SUPPORT
-		(irqs_disabled_flags(flags) ? TRACE_FLAG_IRQS_OFF : 0) |
+#if defined(CONFIG_SMP) && defined(CONFIG_PREEMPT_RT)
+	return current->migration_disabled;
 #else
-		TRACE_FLAG_IRQS_NOSUPPORT |
+	return 0;
+#endif
+}
+
+unsigned int tracing_gen_ctx_irq_test(unsigned int irqs_status)
+{
+	unsigned int trace_flags = irqs_status;
+	unsigned int pc;
+
+	pc = preempt_count();
+
+	if (pc & NMI_MASK)
+		trace_flags |= TRACE_FLAG_NMI;
+	if (pc & HARDIRQ_MASK)
+		trace_flags |= TRACE_FLAG_HARDIRQ;
+	if (in_serving_softirq())
+		trace_flags |= TRACE_FLAG_SOFTIRQ;
+
+	if (tif_need_resched())
+		trace_flags |= TRACE_FLAG_NEED_RESCHED;
+	if (test_preempt_need_resched())
+		trace_flags |= TRACE_FLAG_PREEMPT_RESCHED;
+
+#ifdef CONFIG_PREEMPT_LAZY
+	if (need_resched_lazy())
+		trace_flags |= TRACE_FLAG_NEED_RESCHED_LAZY;
 #endif
-		((pc & NMI_MASK    ) ? TRACE_FLAG_NMI     : 0) |
-		((pc & HARDIRQ_MASK) ? TRACE_FLAG_HARDIRQ : 0) |
-		((pc & SOFTIRQ_OFFSET) ? TRACE_FLAG_SOFTIRQ : 0) |
-		(tif_need_resched() ? TRACE_FLAG_NEED_RESCHED : 0) |
-		(test_preempt_need_resched() ? TRACE_FLAG_PREEMPT_RESCHED : 0);
+
+	return (pc & 0xff) |
+		(migration_disable_value() & 0xff) << 8 |
+		(preempt_lazy_count() & 0xff) << 16 |
+		(trace_flags << 24);
 }
-EXPORT_SYMBOL_GPL(tracing_generic_entry_update);
 
 struct ring_buffer_event *
 trace_buffer_lock_reserve(struct trace_buffer *buffer,
 			  int type,
 			  unsigned long len,
-			  unsigned long flags, int pc)
+			  unsigned int trace_ctx)
 {
-	return __trace_buffer_lock_reserve(buffer, type, len, flags, pc);
+	return __trace_buffer_lock_reserve(buffer, type, len, trace_ctx);
 }
 
 DEFINE_PER_CPU(struct ring_buffer_event *, trace_buffered_event);
@@ -2733,7 +2743,7 @@ struct ring_buffer_event *
 trace_event_buffer_lock_reserve(struct trace_buffer **current_rb,
 			  struct trace_event_file *trace_file,
 			  int type, unsigned long len,
-			  unsigned long flags, int pc)
+			  unsigned int trace_ctx)
 {
 	struct ring_buffer_event *entry;
 	int val;
@@ -2746,7 +2756,7 @@ trace_event_buffer_lock_reserve(struct trace_buffer **current_rb,
 		/* Try to use the per cpu buffer first */
 		val = this_cpu_inc_return(trace_buffered_event_cnt);
 		if ((len < (PAGE_SIZE - sizeof(*entry))) && val == 1) {
-			trace_event_setup(entry, type, flags, pc);
+			trace_event_setup(entry, type, trace_ctx);
 			entry->array[0] = len;
 			return entry;
 		}
@@ -2754,7 +2764,7 @@ trace_event_buffer_lock_reserve(struct trace_buffer **current_rb,
 	}
 
 	entry = __trace_buffer_lock_reserve(*current_rb,
-					    type, len, flags, pc);
+					    type, len, trace_ctx);
 	/*
 	 * If tracing is off, but we have triggers enabled
 	 * we still need to look at the event data. Use the temp_buffer
@@ -2763,8 +2773,8 @@ trace_event_buffer_lock_reserve(struct trace_buffer **current_rb,
 	 */
 	if (!entry && trace_file->flags & EVENT_FILE_FL_TRIGGER_COND) {
 		*current_rb = temp_buffer;
-		entry = __trace_buffer_lock_reserve(*current_rb,
-						    type, len, flags, pc);
+		entry = __trace_buffer_lock_reserve(*current_rb, type, len,
+						    trace_ctx);
 	}
 	return entry;
 }
@@ -2850,7 +2860,7 @@ void trace_event_buffer_commit(struct trace_event_buffer *fbuffer)
 		ftrace_exports(fbuffer->event, TRACE_EXPORT_EVENT);
 	event_trigger_unlock_commit_regs(fbuffer->trace_file, fbuffer->buffer,
 				    fbuffer->event, fbuffer->entry,
-				    fbuffer->flags, fbuffer->pc, fbuffer->regs);
+				    fbuffer->trace_ctx, fbuffer->regs);
 }
 EXPORT_SYMBOL_GPL(trace_event_buffer_commit);
 
@@ -2866,7 +2876,7 @@ EXPORT_SYMBOL_GPL(trace_event_buffer_commit);
 void trace_buffer_unlock_commit_regs(struct trace_array *tr,
 				     struct trace_buffer *buffer,
 				     struct ring_buffer_event *event,
-				     unsigned long flags, int pc,
+				     unsigned int trace_ctx,
 				     struct pt_regs *regs)
 {
 	__buffer_unlock_commit(buffer, event);
@@ -2877,8 +2887,8 @@ void trace_buffer_unlock_commit_regs(struct trace_array *tr,
 	 * and mmiotrace, but that's ok if they lose a function or
 	 * two. They are not that meaningful.
 	 */
-	ftrace_trace_stack(tr, buffer, flags, regs ? 0 : STACK_SKIP, pc, regs);
-	ftrace_trace_userstack(tr, buffer, flags, pc);
+	ftrace_trace_stack(tr, buffer, trace_ctx, regs ? 0 : STACK_SKIP, regs);
+	ftrace_trace_userstack(tr, buffer, trace_ctx);
 }
 
 /*
@@ -2892,9 +2902,8 @@ trace_buffer_unlock_commit_nostack(struct trace_buffer *buffer,
 }
 
 void
-trace_function(struct trace_array *tr,
-	       unsigned long ip, unsigned long parent_ip, unsigned long flags,
-	       int pc)
+trace_function(struct trace_array *tr, unsigned long ip, unsigned long
+	       parent_ip, unsigned int trace_ctx)
 {
 	struct trace_event_call *call = &event_function;
 	struct trace_buffer *buffer = tr->array_buffer.buffer;
@@ -2902,7 +2911,7 @@ trace_function(struct trace_array *tr,
 	struct ftrace_entry *entry;
 
 	event = __trace_buffer_lock_reserve(buffer, TRACE_FN, sizeof(*entry),
-					    flags, pc);
+					    trace_ctx);
 	if (!event)
 		return;
 	entry	= ring_buffer_event_data(event);
@@ -2936,8 +2945,8 @@ static DEFINE_PER_CPU(struct ftrace_stacks, ftrace_stacks);
 static DEFINE_PER_CPU(int, ftrace_stack_reserve);
 
 static void __ftrace_trace_stack(struct trace_buffer *buffer,
-				 unsigned long flags,
-				 int skip, int pc, struct pt_regs *regs)
+				 unsigned int trace_ctx,
+				 int skip, struct pt_regs *regs)
 {
 	struct trace_event_call *call = &event_kernel_stack;
 	struct ring_buffer_event *event;
@@ -2984,7 +2993,7 @@ static void __ftrace_trace_stack(struct trace_buffer *buffer,
 
 	size = nr_entries * sizeof(unsigned long);
 	event = __trace_buffer_lock_reserve(buffer, TRACE_STACK,
-					    sizeof(*entry) + size, flags, pc);
+					    sizeof(*entry) + size, trace_ctx);
 	if (!event)
 		goto out;
 	entry = ring_buffer_event_data(event);
@@ -3005,22 +3014,22 @@ static void __ftrace_trace_stack(struct trace_buffer *buffer,
 
 static inline void ftrace_trace_stack(struct trace_array *tr,
 				      struct trace_buffer *buffer,
-				      unsigned long flags,
-				      int skip, int pc, struct pt_regs *regs)
+				      unsigned int trace_ctx,
+				      int skip, struct pt_regs *regs)
 {
 	if (!(tr->trace_flags & TRACE_ITER_STACKTRACE))
 		return;
 
-	__ftrace_trace_stack(buffer, flags, skip, pc, regs);
+	__ftrace_trace_stack(buffer, trace_ctx, skip, regs);
 }
 
-void __trace_stack(struct trace_array *tr, unsigned long flags, int skip,
-		   int pc)
+void __trace_stack(struct trace_array *tr, unsigned int trace_ctx,
+		   int skip)
 {
 	struct trace_buffer *buffer = tr->array_buffer.buffer;
 
 	if (rcu_is_watching()) {
-		__ftrace_trace_stack(buffer, flags, skip, pc, NULL);
+		__ftrace_trace_stack(buffer, trace_ctx, skip, NULL);
 		return;
 	}
 
@@ -3034,7 +3043,7 @@ void __trace_stack(struct trace_array *tr, unsigned long flags, int skip,
 		return;
 
 	rcu_irq_enter_irqson();
-	__ftrace_trace_stack(buffer, flags, skip, pc, NULL);
+	__ftrace_trace_stack(buffer, trace_ctx, skip, NULL);
 	rcu_irq_exit_irqson();
 }
 
@@ -3044,19 +3053,15 @@ void __trace_stack(struct trace_array *tr, unsigned long flags, int skip,
  */
 void trace_dump_stack(int skip)
 {
-	unsigned long flags;
-
 	if (tracing_disabled || tracing_selftest_running)
 		return;
 
-	local_save_flags(flags);
-
 #ifndef CONFIG_UNWINDER_ORC
 	/* Skip 1 to skip this function. */
 	skip++;
 #endif
 	__ftrace_trace_stack(global_trace.array_buffer.buffer,
-			     flags, skip, preempt_count(), NULL);
+			     tracing_gen_ctx(), skip, NULL);
 }
 EXPORT_SYMBOL_GPL(trace_dump_stack);
 
@@ -3065,7 +3070,7 @@ static DEFINE_PER_CPU(int, user_stack_count);
 
 static void
 ftrace_trace_userstack(struct trace_array *tr,
-		       struct trace_buffer *buffer, unsigned long flags, int pc)
+		       struct trace_buffer *buffer, unsigned int trace_ctx)
 {
 	struct trace_event_call *call = &event_user_stack;
 	struct ring_buffer_event *event;
@@ -3092,7 +3097,7 @@ ftrace_trace_userstack(struct trace_array *tr,
 	__this_cpu_inc(user_stack_count);
 
 	event = __trace_buffer_lock_reserve(buffer, TRACE_USER_STACK,
-					    sizeof(*entry), flags, pc);
+					    sizeof(*entry), trace_ctx);
 	if (!event)
 		goto out_drop_count;
 	entry	= ring_buffer_event_data(event);
@@ -3112,7 +3117,7 @@ ftrace_trace_userstack(struct trace_array *tr,
 #else /* CONFIG_USER_STACKTRACE_SUPPORT */
 static void ftrace_trace_userstack(struct trace_array *tr,
 				   struct trace_buffer *buffer,
-				   unsigned long flags, int pc)
+				   unsigned int trace_ctx)
 {
 }
 #endif /* !CONFIG_USER_STACKTRACE_SUPPORT */
@@ -3242,9 +3247,9 @@ int trace_vbprintk(unsigned long ip, const char *fmt, va_list args)
 	struct trace_buffer *buffer;
 	struct trace_array *tr = &global_trace;
 	struct bprint_entry *entry;
-	unsigned long flags;
+	unsigned int trace_ctx;
 	char *tbuffer;
-	int len = 0, size, pc;
+	int len = 0, size;
 
 	if (unlikely(tracing_selftest_running || tracing_disabled))
 		return 0;
@@ -3252,7 +3257,7 @@ int trace_vbprintk(unsigned long ip, const char *fmt, va_list args)
 	/* Don't pollute graph traces with trace_vprintk internals */
 	pause_graph_tracing();
 
-	pc = preempt_count();
+	trace_ctx = tracing_gen_ctx();
 	preempt_disable_notrace();
 
 	tbuffer = get_trace_buf();
@@ -3266,12 +3271,11 @@ int trace_vbprintk(unsigned long ip, const char *fmt, va_list args)
 	if (len > TRACE_BUF_SIZE/sizeof(int) || len < 0)
 		goto out_put;
 
-	local_save_flags(flags);
 	size = sizeof(*entry) + sizeof(u32) * len;
 	buffer = tr->array_buffer.buffer;
 	ring_buffer_nest_start(buffer);
 	event = __trace_buffer_lock_reserve(buffer, TRACE_BPRINT, size,
-					    flags, pc);
+					    trace_ctx);
 	if (!event)
 		goto out;
 	entry = ring_buffer_event_data(event);
@@ -3281,7 +3285,7 @@ int trace_vbprintk(unsigned long ip, const char *fmt, va_list args)
 	memcpy(entry->buf, tbuffer, sizeof(u32) * len);
 	if (!call_filter_check_discard(call, entry, buffer, event)) {
 		__buffer_unlock_commit(buffer, event);
-		ftrace_trace_stack(tr, buffer, flags, 6, pc, NULL);
+		ftrace_trace_stack(tr, buffer, trace_ctx, 6, NULL);
 	}
 
 out:
@@ -3304,9 +3308,9 @@ __trace_array_vprintk(struct trace_buffer *buffer,
 {
 	struct trace_event_call *call = &event_print;
 	struct ring_buffer_event *event;
-	int len = 0, size, pc;
+	int len = 0, size;
 	struct print_entry *entry;
-	unsigned long flags;
+	unsigned int trace_ctx;
 	char *tbuffer;
 
 	if (tracing_disabled || tracing_selftest_running)
@@ -3315,7 +3319,7 @@ __trace_array_vprintk(struct trace_buffer *buffer,
 	/* Don't pollute graph traces with trace_vprintk internals */
 	pause_graph_tracing();
 
-	pc = preempt_count();
+	trace_ctx = tracing_gen_ctx();
 	preempt_disable_notrace();
 
 
@@ -3327,11 +3331,10 @@ __trace_array_vprintk(struct trace_buffer *buffer,
 
 	len = vscnprintf(tbuffer, TRACE_BUF_SIZE, fmt, args);
 
-	local_save_flags(flags);
 	size = sizeof(*entry) + len + 1;
 	ring_buffer_nest_start(buffer);
 	event = __trace_buffer_lock_reserve(buffer, TRACE_PRINT, size,
-					    flags, pc);
+					    trace_ctx);
 	if (!event)
 		goto out;
 	entry = ring_buffer_event_data(event);
@@ -3340,7 +3343,7 @@ __trace_array_vprintk(struct trace_buffer *buffer,
 	memcpy(&entry->buf, tbuffer, len + 1);
 	if (!call_filter_check_discard(call, entry, buffer, event)) {
 		__buffer_unlock_commit(buffer, event);
-		ftrace_trace_stack(&global_trace, buffer, flags, 6, pc, NULL);
+		ftrace_trace_stack(&global_trace, buffer, trace_ctx, 6, NULL);
 	}
 
 out:
@@ -3812,14 +3815,17 @@ unsigned long trace_total_entries(struct trace_array *tr)
 
 static void print_lat_help_header(struct seq_file *m)
 {
-	seq_puts(m, "#                    _------=> CPU#            \n"
-		    "#                   / _-----=> irqs-off        \n"
-		    "#                  | / _----=> need-resched    \n"
-		    "#                  || / _---=> hardirq/softirq \n"
-		    "#                  ||| / _--=> preempt-depth   \n"
-		    "#                  |||| /     delay            \n"
-		    "#  cmd     pid     ||||| time  |   caller      \n"
-		    "#     \\   /        |||||  \\    |   /         \n");
+	seq_puts(m, "#                    _--------=> CPU#            \n"
+		    "#                   / _-------=> irqs-off        \n"
+		    "#                  | / _------=> need-resched    \n"
+		    "#                  || / _-----=> need-resched-lazy\n"
+		    "#                  ||| / _----=> hardirq/softirq \n"
+		    "#                  |||| / _---=> preempt-depth   \n"
+		    "#                  ||||| / _--=> preempt-lazy-depth\n"
+		    "#                  |||||| / _-=> migrate-disable \n"
+		    "#                  ||||||| /     delay           \n"
+		    "#  cmd     pid     |||||||| time  |   caller     \n"
+		    "#     \\   /        ||||||||  \\    |    /       \n");
 }
 
 static void print_event_info(struct array_buffer *buf, struct seq_file *m)
@@ -3853,13 +3859,16 @@ static void print_func_help_header_irq(struct array_buffer *buf, struct seq_file
 
 	print_event_info(buf, m);
 
-	seq_printf(m, "#                            %.*s  _-----=> irqs-off\n", prec, space);
-	seq_printf(m, "#                            %.*s / _----=> need-resched\n", prec, space);
-	seq_printf(m, "#                            %.*s| / _---=> hardirq/softirq\n", prec, space);
-	seq_printf(m, "#                            %.*s|| / _--=> preempt-depth\n", prec, space);
-	seq_printf(m, "#                            %.*s||| /     delay\n", prec, space);
-	seq_printf(m, "#           TASK-PID  %.*s CPU#  ||||   TIMESTAMP  FUNCTION\n", prec, "     TGID   ");
-	seq_printf(m, "#              | |    %.*s   |   ||||      |         |\n", prec, "       |    ");
+	seq_printf(m, "#                            %.*s  _-------=> irqs-off\n", prec, space);
+	seq_printf(m, "#                            %.*s / _------=> need-resched\n", prec, space);
+	seq_printf(m, "#                            %.*s| / _-----=> need-resched-lazy\n", prec, space);
+	seq_printf(m, "#                            %.*s|| / _----=> hardirq/softirq\n", prec, space);
+	seq_printf(m, "#                            %.*s||| / _---=> preempt-depth\n", prec, space);
+	seq_printf(m, "#                            %.*s|||| / _--=> preempt-lazy-depth\n", prec, space);
+	seq_printf(m, "#                            %.*s||||| / _-=> migrate-disable\n", prec, space);
+	seq_printf(m, "#                            %.*s|||||| /     delay\n", prec, space);
+	seq_printf(m, "#           TASK-PID  %.*s CPU#  |||||||  TIMESTAMP  FUNCTION\n", prec, "     TGID   ");
+	seq_printf(m, "#              | |    %.*s   |   |||||||      |         |\n", prec, "       |    ");
 }
 
 void
@@ -6653,7 +6662,6 @@ tracing_mark_write(struct file *filp, const char __user *ubuf,
 	enum event_trigger_type tt = ETT_NONE;
 	struct trace_buffer *buffer;
 	struct print_entry *entry;
-	unsigned long irq_flags;
 	ssize_t written;
 	int size;
 	int len;
@@ -6673,7 +6681,6 @@ tracing_mark_write(struct file *filp, const char __user *ubuf,
 
 	BUILD_BUG_ON(TRACE_BUF_SIZE >= PAGE_SIZE);
 
-	local_save_flags(irq_flags);
 	size = sizeof(*entry) + cnt + 2; /* add '\0' and possible '\n' */
 
 	/* If less than "<faulted>", then make sure we can still add that */
@@ -6682,7 +6689,7 @@ tracing_mark_write(struct file *filp, const char __user *ubuf,
 
 	buffer = tr->array_buffer.buffer;
 	event = __trace_buffer_lock_reserve(buffer, TRACE_PRINT, size,
-					    irq_flags, preempt_count());
+					    tracing_gen_ctx());
 	if (unlikely(!event))
 		/* Ring buffer disabled, return as if not open for write */
 		return -EBADF;
@@ -6734,7 +6741,6 @@ tracing_mark_raw_write(struct file *filp, const char __user *ubuf,
 	struct ring_buffer_event *event;
 	struct trace_buffer *buffer;
 	struct raw_data_entry *entry;
-	unsigned long irq_flags;
 	ssize_t written;
 	int size;
 	int len;
@@ -6756,14 +6762,13 @@ tracing_mark_raw_write(struct file *filp, const char __user *ubuf,
 
 	BUILD_BUG_ON(TRACE_BUF_SIZE >= PAGE_SIZE);
 
-	local_save_flags(irq_flags);
 	size = sizeof(*entry) + cnt;
 	if (cnt < FAULT_SIZE_ID)
 		size += FAULT_SIZE_ID - cnt;
 
 	buffer = tr->array_buffer.buffer;
 	event = __trace_buffer_lock_reserve(buffer, TRACE_RAW_DATA, size,
-					    irq_flags, preempt_count());
+					    tracing_gen_ctx());
 	if (!event)
 		/* Ring buffer disabled, return as if not open for write */
 		return -EBADF;
@@ -9314,7 +9319,6 @@ void ftrace_dump(enum ftrace_dump_mode oops_dump_mode)
 	tracing_off();
 
 	local_irq_save(flags);
-	printk_nmi_direct_enter();
 
 	/* Simulate the iterator */
 	trace_init_global_iter(&iter);
@@ -9394,7 +9398,6 @@ void ftrace_dump(enum ftrace_dump_mode oops_dump_mode)
 		atomic_dec(&per_cpu_ptr(iter.array_buffer->data, cpu)->disabled);
 	}
 	atomic_dec(&dump_running);
-	printk_nmi_direct_exit();
 	local_irq_restore(flags);
 }
 EXPORT_SYMBOL_GPL(ftrace_dump);
diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 6784b572ce59..2ad9faef718a 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -136,25 +136,6 @@ struct kretprobe_trace_entry_head {
 	unsigned long		ret_ip;
 };
 
-/*
- * trace_flag_type is an enumeration that holds different
- * states when a trace occurs. These are:
- *  IRQS_OFF		- interrupts were disabled
- *  IRQS_NOSUPPORT	- arch does not support irqs_disabled_flags
- *  NEED_RESCHED	- reschedule is requested
- *  HARDIRQ		- inside an interrupt handler
- *  SOFTIRQ		- inside a softirq handler
- */
-enum trace_flag_type {
-	TRACE_FLAG_IRQS_OFF		= 0x01,
-	TRACE_FLAG_IRQS_NOSUPPORT	= 0x02,
-	TRACE_FLAG_NEED_RESCHED		= 0x04,
-	TRACE_FLAG_HARDIRQ		= 0x08,
-	TRACE_FLAG_SOFTIRQ		= 0x10,
-	TRACE_FLAG_PREEMPT_RESCHED	= 0x20,
-	TRACE_FLAG_NMI			= 0x40,
-};
-
 #define TRACE_BUF_SIZE		1024
 
 struct trace_array;
@@ -766,8 +747,7 @@ struct ring_buffer_event *
 trace_buffer_lock_reserve(struct trace_buffer *buffer,
 			  int type,
 			  unsigned long len,
-			  unsigned long flags,
-			  int pc);
+			  unsigned int trace_ctx);
 
 struct trace_entry *tracing_get_trace_entry(struct trace_array *tr,
 						struct trace_array_cpu *data);
@@ -792,11 +772,11 @@ unsigned long trace_total_entries(struct trace_array *tr);
 void trace_function(struct trace_array *tr,
 		    unsigned long ip,
 		    unsigned long parent_ip,
-		    unsigned long flags, int pc);
+		    unsigned int trace_ctx);
 void trace_graph_function(struct trace_array *tr,
 		    unsigned long ip,
 		    unsigned long parent_ip,
-		    unsigned long flags, int pc);
+		    unsigned int trace_ctx);
 void trace_latency_header(struct seq_file *m);
 void trace_default_header(struct seq_file *m);
 void print_trace_header(struct seq_file *m, struct trace_iterator *iter);
@@ -864,11 +844,10 @@ static inline void latency_fsnotify(struct trace_array *tr) { }
 #endif
 
 #ifdef CONFIG_STACKTRACE
-void __trace_stack(struct trace_array *tr, unsigned long flags, int skip,
-		   int pc);
+void __trace_stack(struct trace_array *tr, unsigned int trace_ctx, int skip);
 #else
-static inline void __trace_stack(struct trace_array *tr, unsigned long flags,
-				 int skip, int pc)
+static inline void __trace_stack(struct trace_array *tr, unsigned int trace_ctx,
+				 int skip)
 {
 }
 #endif /* CONFIG_STACKTRACE */
@@ -1008,10 +987,10 @@ extern void graph_trace_open(struct trace_iterator *iter);
 extern void graph_trace_close(struct trace_iterator *iter);
 extern int __trace_graph_entry(struct trace_array *tr,
 			       struct ftrace_graph_ent *trace,
-			       unsigned long flags, int pc);
+			       unsigned int trace_ctx);
 extern void __trace_graph_return(struct trace_array *tr,
 				 struct ftrace_graph_ret *trace,
-				 unsigned long flags, int pc);
+				 unsigned int trace_ctx);
 
 #ifdef CONFIG_DYNAMIC_FTRACE
 extern struct ftrace_hash __rcu *ftrace_graph_hash;
@@ -1474,15 +1453,15 @@ extern int call_filter_check_discard(struct trace_event_call *call, void *rec,
 void trace_buffer_unlock_commit_regs(struct trace_array *tr,
 				     struct trace_buffer *buffer,
 				     struct ring_buffer_event *event,
-				     unsigned long flags, int pc,
+				     unsigned int trcace_ctx,
 				     struct pt_regs *regs);
 
 static inline void trace_buffer_unlock_commit(struct trace_array *tr,
 					      struct trace_buffer *buffer,
 					      struct ring_buffer_event *event,
-					      unsigned long flags, int pc)
+					      unsigned int trace_ctx)
 {
-	trace_buffer_unlock_commit_regs(tr, buffer, event, flags, pc, NULL);
+	trace_buffer_unlock_commit_regs(tr, buffer, event, trace_ctx, NULL);
 }
 
 DECLARE_PER_CPU(struct ring_buffer_event *, trace_buffered_event);
@@ -1543,8 +1522,7 @@ __event_trigger_test_discard(struct trace_event_file *file,
  * @buffer: The ring buffer that the event is being written to
  * @event: The event meta data in the ring buffer
  * @entry: The event itself
- * @irq_flags: The state of the interrupts at the start of the event
- * @pc: The state of the preempt count at the start of the event.
+ * @trace_ctx: The tracing context flags.
  *
  * This is a helper function to handle triggers that require data
  * from the event itself. It also tests the event against filters and
@@ -1554,12 +1532,12 @@ static inline void
 event_trigger_unlock_commit(struct trace_event_file *file,
 			    struct trace_buffer *buffer,
 			    struct ring_buffer_event *event,
-			    void *entry, unsigned long irq_flags, int pc)
+			    void *entry, unsigned int trace_ctx)
 {
 	enum event_trigger_type tt = ETT_NONE;
 
 	if (!__event_trigger_test_discard(file, buffer, event, entry, &tt))
-		trace_buffer_unlock_commit(file->tr, buffer, event, irq_flags, pc);
+		trace_buffer_unlock_commit(file->tr, buffer, event, trace_ctx);
 
 	if (tt)
 		event_triggers_post_call(file, tt);
@@ -1571,8 +1549,7 @@ event_trigger_unlock_commit(struct trace_event_file *file,
  * @buffer: The ring buffer that the event is being written to
  * @event: The event meta data in the ring buffer
  * @entry: The event itself
- * @irq_flags: The state of the interrupts at the start of the event
- * @pc: The state of the preempt count at the start of the event.
+ * @trace_ctx: The tracing context flags.
  *
  * This is a helper function to handle triggers that require data
  * from the event itself. It also tests the event against filters and
@@ -1585,14 +1562,14 @@ static inline void
 event_trigger_unlock_commit_regs(struct trace_event_file *file,
 				 struct trace_buffer *buffer,
 				 struct ring_buffer_event *event,
-				 void *entry, unsigned long irq_flags, int pc,
+				 void *entry, unsigned int trace_ctx,
 				 struct pt_regs *regs)
 {
 	enum event_trigger_type tt = ETT_NONE;
 
 	if (!__event_trigger_test_discard(file, buffer, event, entry, &tt))
 		trace_buffer_unlock_commit_regs(file->tr, buffer, event,
-						irq_flags, pc, regs);
+						trace_ctx, regs);
 
 	if (tt)
 		event_triggers_post_call(file, tt);
diff --git a/kernel/trace/trace_branch.c b/kernel/trace/trace_branch.c
index eff099123aa2..e47fdb4c92fb 100644
--- a/kernel/trace/trace_branch.c
+++ b/kernel/trace/trace_branch.c
@@ -37,7 +37,7 @@ probe_likely_condition(struct ftrace_likely_data *f, int val, int expect)
 	struct ring_buffer_event *event;
 	struct trace_branch *entry;
 	unsigned long flags;
-	int pc;
+	unsigned int trace_ctx;
 	const char *p;
 
 	if (current->trace_recursion & TRACE_BRANCH_BIT)
@@ -59,10 +59,10 @@ probe_likely_condition(struct ftrace_likely_data *f, int val, int expect)
 	if (atomic_read(&data->disabled))
 		goto out;
 
-	pc = preempt_count();
+	trace_ctx = tracing_gen_ctx_flags(flags);
 	buffer = tr->array_buffer.buffer;
 	event = trace_buffer_lock_reserve(buffer, TRACE_BRANCH,
-					  sizeof(*entry), flags, pc);
+					  sizeof(*entry), trace_ctx);
 	if (!event)
 		goto out;
 
diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index 643e0b19920d..0443dd61667b 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -421,11 +421,8 @@ NOKPROBE_SYMBOL(perf_trace_buf_alloc);
 void perf_trace_buf_update(void *record, u16 type)
 {
 	struct trace_entry *entry = record;
-	int pc = preempt_count();
-	unsigned long flags;
 
-	local_save_flags(flags);
-	tracing_generic_entry_update(entry, type, flags, pc);
+	tracing_generic_entry_update(entry, type, tracing_gen_ctx());
 }
 NOKPROBE_SYMBOL(perf_trace_buf_update);
 
diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index ab3cb67b869e..7cfcf301b6e6 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -183,6 +183,8 @@ static int trace_define_common_fields(void)
 	__common_field(unsigned char, flags);
 	__common_field(unsigned char, preempt_count);
 	__common_field(int, pid);
+	__common_field(unsigned char, migrate_disable);
+	__common_field(unsigned char, preempt_lazy_count);
 
 	return ret;
 }
@@ -258,22 +260,19 @@ void *trace_event_buffer_reserve(struct trace_event_buffer *fbuffer,
 	    trace_event_ignore_this_pid(trace_file))
 		return NULL;
 
-	local_save_flags(fbuffer->flags);
-	fbuffer->pc = preempt_count();
 	/*
 	 * If CONFIG_PREEMPTION is enabled, then the tracepoint itself disables
 	 * preemption (adding one to the preempt_count). Since we are
 	 * interested in the preempt_count at the time the tracepoint was
 	 * hit, we need to subtract one to offset the increment.
 	 */
-	if (IS_ENABLED(CONFIG_PREEMPTION))
-		fbuffer->pc--;
+	fbuffer->trace_ctx = tracing_gen_ctx_dec();
 	fbuffer->trace_file = trace_file;
 
 	fbuffer->event =
 		trace_event_buffer_lock_reserve(&fbuffer->buffer, trace_file,
 						event_call->event.type, len,
-						fbuffer->flags, fbuffer->pc);
+						fbuffer->trace_ctx);
 	if (!fbuffer->event)
 		return NULL;
 
@@ -3679,12 +3678,11 @@ function_test_events_call(unsigned long ip, unsigned long parent_ip,
 	struct trace_buffer *buffer;
 	struct ring_buffer_event *event;
 	struct ftrace_entry *entry;
-	unsigned long flags;
+	unsigned int trace_ctx;
 	long disabled;
 	int cpu;
-	int pc;
 
-	pc = preempt_count();
+	trace_ctx = tracing_gen_ctx();
 	preempt_disable_notrace();
 	cpu = raw_smp_processor_id();
 	disabled = atomic_inc_return(&per_cpu(ftrace_test_event_disable, cpu));
@@ -3692,11 +3690,9 @@ function_test_events_call(unsigned long ip, unsigned long parent_ip,
 	if (disabled != 1)
 		goto out;
 
-	local_save_flags(flags);
-
 	event = trace_event_buffer_lock_reserve(&buffer, &event_trace_file,
 						TRACE_FN, sizeof(*entry),
-						flags, pc);
+						trace_ctx);
 	if (!event)
 		goto out;
 	entry	= ring_buffer_event_data(event);
@@ -3704,7 +3700,7 @@ function_test_events_call(unsigned long ip, unsigned long parent_ip,
 	entry->parent_ip		= parent_ip;
 
 	event_trigger_unlock_commit(&event_trace_file, buffer, event,
-				    entry, flags, pc);
+				    entry, trace_ctx);
  out:
 	atomic_dec(&per_cpu(ftrace_test_event_disable, cpu));
 	preempt_enable_notrace();
diff --git a/kernel/trace/trace_events_inject.c b/kernel/trace/trace_events_inject.c
index 22bcf7c51d1e..c188045c5f97 100644
--- a/kernel/trace/trace_events_inject.c
+++ b/kernel/trace/trace_events_inject.c
@@ -192,7 +192,6 @@ static void *trace_alloc_entry(struct trace_event_call *call, int *size)
 static int parse_entry(char *str, struct trace_event_call *call, void **pentry)
 {
 	struct ftrace_event_field *field;
-	unsigned long irq_flags;
 	void *entry = NULL;
 	int entry_size;
 	u64 val = 0;
@@ -203,9 +202,8 @@ static int parse_entry(char *str, struct trace_event_call *call, void **pentry)
 	if (!entry)
 		return -ENOMEM;
 
-	local_save_flags(irq_flags);
-	tracing_generic_entry_update(entry, call->event.type, irq_flags,
-				     preempt_count());
+	tracing_generic_entry_update(entry, call->event.type,
+				     tracing_gen_ctx());
 
 	while ((len = parse_field(str, call, &field, &val)) > 0) {
 		if (is_function_field(field))
diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 2c2126e1871d..9a4362c1e5f0 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -133,15 +133,14 @@ function_trace_call(unsigned long ip, unsigned long parent_ip,
 {
 	struct trace_array *tr = op->private;
 	struct trace_array_cpu *data;
-	unsigned long flags;
+	unsigned int trace_ctx;
 	int bit;
 	int cpu;
-	int pc;
 
 	if (unlikely(!tr->function_enabled))
 		return;
 
-	pc = preempt_count();
+	trace_ctx = tracing_gen_ctx();
 	preempt_disable_notrace();
 
 	bit = trace_test_and_set_recursion(TRACE_FTRACE_START, TRACE_FTRACE_MAX);
@@ -150,10 +149,9 @@ function_trace_call(unsigned long ip, unsigned long parent_ip,
 
 	cpu = smp_processor_id();
 	data = per_cpu_ptr(tr->array_buffer.data, cpu);
-	if (!atomic_read(&data->disabled)) {
-		local_save_flags(flags);
-		trace_function(tr, ip, parent_ip, flags, pc);
-	}
+	if (!atomic_read(&data->disabled))
+		trace_function(tr, ip, parent_ip, trace_ctx);
+
 	trace_clear_recursion(bit);
 
  out:
@@ -187,7 +185,7 @@ function_stack_trace_call(unsigned long ip, unsigned long parent_ip,
 	unsigned long flags;
 	long disabled;
 	int cpu;
-	int pc;
+	unsigned int trace_ctx;
 
 	if (unlikely(!tr->function_enabled))
 		return;
@@ -202,9 +200,9 @@ function_stack_trace_call(unsigned long ip, unsigned long parent_ip,
 	disabled = atomic_inc_return(&data->disabled);
 
 	if (likely(disabled == 1)) {
-		pc = preempt_count();
-		trace_function(tr, ip, parent_ip, flags, pc);
-		__trace_stack(tr, flags, STACK_SKIP, pc);
+		trace_ctx = tracing_gen_ctx_flags(flags);
+		trace_function(tr, ip, parent_ip, trace_ctx);
+		__trace_stack(tr, trace_ctx, STACK_SKIP);
 	}
 
 	atomic_dec(&data->disabled);
@@ -407,13 +405,11 @@ ftrace_traceoff(unsigned long ip, unsigned long parent_ip,
 
 static __always_inline void trace_stack(struct trace_array *tr)
 {
-	unsigned long flags;
-	int pc;
+	unsigned int trace_ctx;
 
-	local_save_flags(flags);
-	pc = preempt_count();
+	trace_ctx = tracing_gen_ctx();
 
-	__trace_stack(tr, flags, FTRACE_STACK_SKIP, pc);
+	__trace_stack(tr, trace_ctx, FTRACE_STACK_SKIP);
 }
 
 static void
diff --git a/kernel/trace/trace_functions_graph.c b/kernel/trace/trace_functions_graph.c
index 60d66278aa0d..b086ba8bb3d6 100644
--- a/kernel/trace/trace_functions_graph.c
+++ b/kernel/trace/trace_functions_graph.c
@@ -96,8 +96,7 @@ print_graph_duration(struct trace_array *tr, unsigned long long duration,
 
 int __trace_graph_entry(struct trace_array *tr,
 				struct ftrace_graph_ent *trace,
-				unsigned long flags,
-				int pc)
+				unsigned int trace_ctx)
 {
 	struct trace_event_call *call = &event_funcgraph_entry;
 	struct ring_buffer_event *event;
@@ -105,7 +104,7 @@ int __trace_graph_entry(struct trace_array *tr,
 	struct ftrace_graph_ent_entry *entry;
 
 	event = trace_buffer_lock_reserve(buffer, TRACE_GRAPH_ENT,
-					  sizeof(*entry), flags, pc);
+					  sizeof(*entry), trace_ctx);
 	if (!event)
 		return 0;
 	entry	= ring_buffer_event_data(event);
@@ -129,10 +128,10 @@ int trace_graph_entry(struct ftrace_graph_ent *trace)
 	struct trace_array *tr = graph_array;
 	struct trace_array_cpu *data;
 	unsigned long flags;
+	unsigned int trace_ctx;
 	long disabled;
 	int ret;
 	int cpu;
-	int pc;
 
 	if (trace_recursion_test(TRACE_GRAPH_NOTRACE_BIT))
 		return 0;
@@ -174,8 +173,8 @@ int trace_graph_entry(struct ftrace_graph_ent *trace)
 	data = per_cpu_ptr(tr->array_buffer.data, cpu);
 	disabled = atomic_inc_return(&data->disabled);
 	if (likely(disabled == 1)) {
-		pc = preempt_count();
-		ret = __trace_graph_entry(tr, trace, flags, pc);
+		trace_ctx = tracing_gen_ctx_flags(flags);
+		ret = __trace_graph_entry(tr, trace, trace_ctx);
 	} else {
 		ret = 0;
 	}
@@ -188,7 +187,7 @@ int trace_graph_entry(struct ftrace_graph_ent *trace)
 
 static void
 __trace_graph_function(struct trace_array *tr,
-		unsigned long ip, unsigned long flags, int pc)
+		unsigned long ip, unsigned int trace_ctx)
 {
 	u64 time = trace_clock_local();
 	struct ftrace_graph_ent ent = {
@@ -202,22 +201,21 @@ __trace_graph_function(struct trace_array *tr,
 		.rettime  = time,
 	};
 
-	__trace_graph_entry(tr, &ent, flags, pc);
-	__trace_graph_return(tr, &ret, flags, pc);
+	__trace_graph_entry(tr, &ent, trace_ctx);
+	__trace_graph_return(tr, &ret, trace_ctx);
 }
 
 void
 trace_graph_function(struct trace_array *tr,
 		unsigned long ip, unsigned long parent_ip,
-		unsigned long flags, int pc)
+		unsigned int trace_ctx)
 {
-	__trace_graph_function(tr, ip, flags, pc);
+	__trace_graph_function(tr, ip, trace_ctx);
 }
 
 void __trace_graph_return(struct trace_array *tr,
 				struct ftrace_graph_ret *trace,
-				unsigned long flags,
-				int pc)
+				unsigned int trace_ctx)
 {
 	struct trace_event_call *call = &event_funcgraph_exit;
 	struct ring_buffer_event *event;
@@ -225,7 +223,7 @@ void __trace_graph_return(struct trace_array *tr,
 	struct ftrace_graph_ret_entry *entry;
 
 	event = trace_buffer_lock_reserve(buffer, TRACE_GRAPH_RET,
-					  sizeof(*entry), flags, pc);
+					  sizeof(*entry), trace_ctx);
 	if (!event)
 		return;
 	entry	= ring_buffer_event_data(event);
@@ -239,9 +237,9 @@ void trace_graph_return(struct ftrace_graph_ret *trace)
 	struct trace_array *tr = graph_array;
 	struct trace_array_cpu *data;
 	unsigned long flags;
+	unsigned int trace_ctx;
 	long disabled;
 	int cpu;
-	int pc;
 
 	ftrace_graph_addr_finish(trace);
 
@@ -255,8 +253,8 @@ void trace_graph_return(struct ftrace_graph_ret *trace)
 	data = per_cpu_ptr(tr->array_buffer.data, cpu);
 	disabled = atomic_inc_return(&data->disabled);
 	if (likely(disabled == 1)) {
-		pc = preempt_count();
-		__trace_graph_return(tr, trace, flags, pc);
+		trace_ctx = tracing_gen_ctx_flags(flags);
+		__trace_graph_return(tr, trace, trace_ctx);
 	}
 	atomic_dec(&data->disabled);
 	local_irq_restore(flags);
diff --git a/kernel/trace/trace_hwlat.c b/kernel/trace/trace_hwlat.c
index d071fc271eef..4c01c5d8b9a7 100644
--- a/kernel/trace/trace_hwlat.c
+++ b/kernel/trace/trace_hwlat.c
@@ -108,14 +108,9 @@ static void trace_hwlat_sample(struct hwlat_sample *sample)
 	struct trace_buffer *buffer = tr->array_buffer.buffer;
 	struct ring_buffer_event *event;
 	struct hwlat_entry *entry;
-	unsigned long flags;
-	int pc;
-
-	pc = preempt_count();
-	local_save_flags(flags);
 
 	event = trace_buffer_lock_reserve(buffer, TRACE_HWLAT, sizeof(*entry),
-					  flags, pc);
+					  tracing_gen_ctx());
 	if (!event)
 		return;
 	entry	= ring_buffer_event_data(event);
diff --git a/kernel/trace/trace_irqsoff.c b/kernel/trace/trace_irqsoff.c
index ee4571b624bc..f11add83c108 100644
--- a/kernel/trace/trace_irqsoff.c
+++ b/kernel/trace/trace_irqsoff.c
@@ -143,11 +143,14 @@ irqsoff_tracer_call(unsigned long ip, unsigned long parent_ip,
 	struct trace_array *tr = irqsoff_trace;
 	struct trace_array_cpu *data;
 	unsigned long flags;
+	unsigned int trace_ctx;
 
 	if (!func_prolog_dec(tr, &data, &flags))
 		return;
 
-	trace_function(tr, ip, parent_ip, flags, preempt_count());
+	trace_ctx = tracing_gen_ctx_flags(flags);
+
+	trace_function(tr, ip, parent_ip, trace_ctx);
 
 	atomic_dec(&data->disabled);
 }
@@ -177,8 +180,8 @@ static int irqsoff_graph_entry(struct ftrace_graph_ent *trace)
 	struct trace_array *tr = irqsoff_trace;
 	struct trace_array_cpu *data;
 	unsigned long flags;
+	unsigned int trace_ctx;
 	int ret;
-	int pc;
 
 	if (ftrace_graph_ignore_func(trace))
 		return 0;
@@ -195,8 +198,8 @@ static int irqsoff_graph_entry(struct ftrace_graph_ent *trace)
 	if (!func_prolog_dec(tr, &data, &flags))
 		return 0;
 
-	pc = preempt_count();
-	ret = __trace_graph_entry(tr, trace, flags, pc);
+	trace_ctx = tracing_gen_ctx_flags(flags);
+	ret = __trace_graph_entry(tr, trace, trace_ctx);
 	atomic_dec(&data->disabled);
 
 	return ret;
@@ -207,15 +210,15 @@ static void irqsoff_graph_return(struct ftrace_graph_ret *trace)
 	struct trace_array *tr = irqsoff_trace;
 	struct trace_array_cpu *data;
 	unsigned long flags;
-	int pc;
+	unsigned int trace_ctx;
 
 	ftrace_graph_addr_finish(trace);
 
 	if (!func_prolog_dec(tr, &data, &flags))
 		return;
 
-	pc = preempt_count();
-	__trace_graph_return(tr, trace, flags, pc);
+	trace_ctx = tracing_gen_ctx_flags(flags);
+	__trace_graph_return(tr, trace, trace_ctx);
 	atomic_dec(&data->disabled);
 }
 
@@ -267,12 +270,12 @@ static void irqsoff_print_header(struct seq_file *s)
 static void
 __trace_function(struct trace_array *tr,
 		 unsigned long ip, unsigned long parent_ip,
-		 unsigned long flags, int pc)
+		 unsigned int trace_ctx)
 {
 	if (is_graph(tr))
-		trace_graph_function(tr, ip, parent_ip, flags, pc);
+		trace_graph_function(tr, ip, parent_ip, trace_ctx);
 	else
-		trace_function(tr, ip, parent_ip, flags, pc);
+		trace_function(tr, ip, parent_ip, trace_ctx);
 }
 
 #else
@@ -322,15 +325,13 @@ check_critical_timing(struct trace_array *tr,
 {
 	u64 T0, T1, delta;
 	unsigned long flags;
-	int pc;
+	unsigned int trace_ctx;
 
 	T0 = data->preempt_timestamp;
 	T1 = ftrace_now(cpu);
 	delta = T1-T0;
 
-	local_save_flags(flags);
-
-	pc = preempt_count();
+	trace_ctx = tracing_gen_ctx();
 
 	if (!report_latency(tr, delta))
 		goto out;
@@ -341,9 +342,9 @@ check_critical_timing(struct trace_array *tr,
 	if (!report_latency(tr, delta))
 		goto out_unlock;
 
-	__trace_function(tr, CALLER_ADDR0, parent_ip, flags, pc);
+	__trace_function(tr, CALLER_ADDR0, parent_ip, trace_ctx);
 	/* Skip 5 functions to get to the irq/preempt enable function */
-	__trace_stack(tr, flags, 5, pc);
+	__trace_stack(tr, trace_ctx, 5);
 
 	if (data->critical_sequence != max_sequence)
 		goto out_unlock;
@@ -363,16 +364,15 @@ check_critical_timing(struct trace_array *tr,
 out:
 	data->critical_sequence = max_sequence;
 	data->preempt_timestamp = ftrace_now(cpu);
-	__trace_function(tr, CALLER_ADDR0, parent_ip, flags, pc);
+	__trace_function(tr, CALLER_ADDR0, parent_ip, trace_ctx);
 }
 
 static nokprobe_inline void
-start_critical_timing(unsigned long ip, unsigned long parent_ip, int pc)
+start_critical_timing(unsigned long ip, unsigned long parent_ip)
 {
 	int cpu;
 	struct trace_array *tr = irqsoff_trace;
 	struct trace_array_cpu *data;
-	unsigned long flags;
 
 	if (!tracer_enabled || !tracing_is_enabled())
 		return;
@@ -393,9 +393,7 @@ start_critical_timing(unsigned long ip, unsigned long parent_ip, int pc)
 	data->preempt_timestamp = ftrace_now(cpu);
 	data->critical_start = parent_ip ? : ip;
 
-	local_save_flags(flags);
-
-	__trace_function(tr, ip, parent_ip, flags, pc);
+	__trace_function(tr, ip, parent_ip, tracing_gen_ctx());
 
 	per_cpu(tracing_cpu, cpu) = 1;
 
@@ -403,12 +401,12 @@ start_critical_timing(unsigned long ip, unsigned long parent_ip, int pc)
 }
 
 static nokprobe_inline void
-stop_critical_timing(unsigned long ip, unsigned long parent_ip, int pc)
+stop_critical_timing(unsigned long ip, unsigned long parent_ip)
 {
 	int cpu;
 	struct trace_array *tr = irqsoff_trace;
 	struct trace_array_cpu *data;
-	unsigned long flags;
+	unsigned int trace_ctx;
 
 	cpu = raw_smp_processor_id();
 	/* Always clear the tracing cpu on stopping the trace */
@@ -428,8 +426,8 @@ stop_critical_timing(unsigned long ip, unsigned long parent_ip, int pc)
 
 	atomic_inc(&data->disabled);
 
-	local_save_flags(flags);
-	__trace_function(tr, ip, parent_ip, flags, pc);
+	trace_ctx = tracing_gen_ctx();
+	__trace_function(tr, ip, parent_ip, trace_ctx);
 	check_critical_timing(tr, data, parent_ip ? : ip, cpu);
 	data->critical_start = 0;
 	atomic_dec(&data->disabled);
@@ -438,20 +436,16 @@ stop_critical_timing(unsigned long ip, unsigned long parent_ip, int pc)
 /* start and stop critical timings used to for stoppage (in idle) */
 void start_critical_timings(void)
 {
-	int pc = preempt_count();
-
-	if (preempt_trace(pc) || irq_trace())
-		start_critical_timing(CALLER_ADDR0, CALLER_ADDR1, pc);
+	if (preempt_trace(preempt_count()) || irq_trace())
+		start_critical_timing(CALLER_ADDR0, CALLER_ADDR1);
 }
 EXPORT_SYMBOL_GPL(start_critical_timings);
 NOKPROBE_SYMBOL(start_critical_timings);
 
 void stop_critical_timings(void)
 {
-	int pc = preempt_count();
-
-	if (preempt_trace(pc) || irq_trace())
-		stop_critical_timing(CALLER_ADDR0, CALLER_ADDR1, pc);
+	if (preempt_trace(preempt_count()) || irq_trace())
+		stop_critical_timing(CALLER_ADDR0, CALLER_ADDR1);
 }
 EXPORT_SYMBOL_GPL(stop_critical_timings);
 NOKPROBE_SYMBOL(stop_critical_timings);
@@ -613,19 +607,15 @@ static void irqsoff_tracer_stop(struct trace_array *tr)
  */
 void tracer_hardirqs_on(unsigned long a0, unsigned long a1)
 {
-	unsigned int pc = preempt_count();
-
-	if (!preempt_trace(pc) && irq_trace())
-		stop_critical_timing(a0, a1, pc);
+	if (!preempt_trace(preempt_count()) && irq_trace())
+		stop_critical_timing(a0, a1);
 }
 NOKPROBE_SYMBOL(tracer_hardirqs_on);
 
 void tracer_hardirqs_off(unsigned long a0, unsigned long a1)
 {
-	unsigned int pc = preempt_count();
-
-	if (!preempt_trace(pc) && irq_trace())
-		start_critical_timing(a0, a1, pc);
+	if (!preempt_trace(preempt_count()) && irq_trace())
+		start_critical_timing(a0, a1);
 }
 NOKPROBE_SYMBOL(tracer_hardirqs_off);
 
@@ -665,18 +655,14 @@ static struct tracer irqsoff_tracer __read_mostly =
 #ifdef CONFIG_PREEMPT_TRACER
 void tracer_preempt_on(unsigned long a0, unsigned long a1)
 {
-	int pc = preempt_count();
-
-	if (preempt_trace(pc) && !irq_trace())
-		stop_critical_timing(a0, a1, pc);
+	if (preempt_trace(preempt_count()) && !irq_trace())
+		stop_critical_timing(a0, a1);
 }
 
 void tracer_preempt_off(unsigned long a0, unsigned long a1)
 {
-	int pc = preempt_count();
-
-	if (preempt_trace(pc) && !irq_trace())
-		start_critical_timing(a0, a1, pc);
+	if (preempt_trace(preempt_count()) && !irq_trace())
+		start_critical_timing(a0, a1);
 }
 
 static int preemptoff_tracer_init(struct trace_array *tr)
diff --git a/kernel/trace/trace_kprobe.c b/kernel/trace/trace_kprobe.c
index 68150b9cbde9..54b8378071d4 100644
--- a/kernel/trace/trace_kprobe.c
+++ b/kernel/trace/trace_kprobe.c
@@ -1386,8 +1386,7 @@ __kprobe_trace_func(struct trace_kprobe *tk, struct pt_regs *regs,
 	if (trace_trigger_soft_disabled(trace_file))
 		return;
 
-	local_save_flags(fbuffer.flags);
-	fbuffer.pc = preempt_count();
+	fbuffer.trace_ctx = tracing_gen_ctx();
 	fbuffer.trace_file = trace_file;
 
 	dsize = __get_data_size(&tk->tp, regs);
@@ -1396,7 +1395,7 @@ __kprobe_trace_func(struct trace_kprobe *tk, struct pt_regs *regs,
 		trace_event_buffer_lock_reserve(&fbuffer.buffer, trace_file,
 					call->event.type,
 					sizeof(*entry) + tk->tp.size + dsize,
-					fbuffer.flags, fbuffer.pc);
+					fbuffer.trace_ctx);
 	if (!fbuffer.event)
 		return;
 
@@ -1434,8 +1433,7 @@ __kretprobe_trace_func(struct trace_kprobe *tk, struct kretprobe_instance *ri,
 	if (trace_trigger_soft_disabled(trace_file))
 		return;
 
-	local_save_flags(fbuffer.flags);
-	fbuffer.pc = preempt_count();
+	fbuffer.trace_ctx = tracing_gen_ctx();
 	fbuffer.trace_file = trace_file;
 
 	dsize = __get_data_size(&tk->tp, regs);
@@ -1443,7 +1441,7 @@ __kretprobe_trace_func(struct trace_kprobe *tk, struct kretprobe_instance *ri,
 		trace_event_buffer_lock_reserve(&fbuffer.buffer, trace_file,
 					call->event.type,
 					sizeof(*entry) + tk->tp.size + dsize,
-					fbuffer.flags, fbuffer.pc);
+					fbuffer.trace_ctx);
 	if (!fbuffer.event)
 		return;
 
diff --git a/kernel/trace/trace_mmiotrace.c b/kernel/trace/trace_mmiotrace.c
index 84582bf1ed5f..7221ae0b4c47 100644
--- a/kernel/trace/trace_mmiotrace.c
+++ b/kernel/trace/trace_mmiotrace.c
@@ -300,10 +300,11 @@ static void __trace_mmiotrace_rw(struct trace_array *tr,
 	struct trace_buffer *buffer = tr->array_buffer.buffer;
 	struct ring_buffer_event *event;
 	struct trace_mmiotrace_rw *entry;
-	int pc = preempt_count();
+	unsigned int trace_ctx;
 
+	trace_ctx = tracing_gen_ctx_flags(0);
 	event = trace_buffer_lock_reserve(buffer, TRACE_MMIO_RW,
-					  sizeof(*entry), 0, pc);
+					  sizeof(*entry), trace_ctx);
 	if (!event) {
 		atomic_inc(&dropped_count);
 		return;
@@ -312,7 +313,7 @@ static void __trace_mmiotrace_rw(struct trace_array *tr,
 	entry->rw			= *rw;
 
 	if (!call_filter_check_discard(call, entry, buffer, event))
-		trace_buffer_unlock_commit(tr, buffer, event, 0, pc);
+		trace_buffer_unlock_commit(tr, buffer, event, trace_ctx);
 }
 
 void mmio_trace_rw(struct mmiotrace_rw *rw)
@@ -330,10 +331,11 @@ static void __trace_mmiotrace_map(struct trace_array *tr,
 	struct trace_buffer *buffer = tr->array_buffer.buffer;
 	struct ring_buffer_event *event;
 	struct trace_mmiotrace_map *entry;
-	int pc = preempt_count();
+	unsigned int trace_ctx;
 
+	trace_ctx = tracing_gen_ctx_flags(0);
 	event = trace_buffer_lock_reserve(buffer, TRACE_MMIO_MAP,
-					  sizeof(*entry), 0, pc);
+					  sizeof(*entry), trace_ctx);
 	if (!event) {
 		atomic_inc(&dropped_count);
 		return;
@@ -342,7 +344,7 @@ static void __trace_mmiotrace_map(struct trace_array *tr,
 	entry->map			= *map;
 
 	if (!call_filter_check_discard(call, entry, buffer, event))
-		trace_buffer_unlock_commit(tr, buffer, event, 0, pc);
+		trace_buffer_unlock_commit(tr, buffer, event, trace_ctx);
 }
 
 void mmio_trace_mapping(struct mmiotrace_map *map)
diff --git a/kernel/trace/trace_output.c b/kernel/trace/trace_output.c
index 000e9dc224c6..bc24ae8e3613 100644
--- a/kernel/trace/trace_output.c
+++ b/kernel/trace/trace_output.c
@@ -441,6 +441,7 @@ int trace_print_lat_fmt(struct trace_seq *s, struct trace_entry *entry)
 {
 	char hardsoft_irq;
 	char need_resched;
+	char need_resched_lazy;
 	char irqs_off;
 	int hardirq;
 	int softirq;
@@ -471,6 +472,9 @@ int trace_print_lat_fmt(struct trace_seq *s, struct trace_entry *entry)
 		break;
 	}
 
+	need_resched_lazy =
+		(entry->flags & TRACE_FLAG_NEED_RESCHED_LAZY) ? 'L' : '.';
+
 	hardsoft_irq =
 		(nmi && hardirq)     ? 'Z' :
 		nmi                  ? 'z' :
@@ -479,14 +483,25 @@ int trace_print_lat_fmt(struct trace_seq *s, struct trace_entry *entry)
 		softirq              ? 's' :
 		                       '.' ;
 
-	trace_seq_printf(s, "%c%c%c",
-			 irqs_off, need_resched, hardsoft_irq);
+	trace_seq_printf(s, "%c%c%c%c",
+			 irqs_off, need_resched, need_resched_lazy,
+			 hardsoft_irq);
 
 	if (entry->preempt_count)
 		trace_seq_printf(s, "%x", entry->preempt_count);
 	else
 		trace_seq_putc(s, '.');
 
+	if (entry->preempt_lazy_count)
+		trace_seq_printf(s, "%x", entry->preempt_lazy_count);
+	else
+		trace_seq_putc(s, '.');
+
+	if (entry->migrate_disable)
+		trace_seq_printf(s, "%x", entry->migrate_disable);
+	else
+		trace_seq_putc(s, '.');
+
 	return !trace_seq_has_overflowed(s);
 }
 
diff --git a/kernel/trace/trace_sched_wakeup.c b/kernel/trace/trace_sched_wakeup.c
index 97b10bb31a1f..f1c603358ff3 100644
--- a/kernel/trace/trace_sched_wakeup.c
+++ b/kernel/trace/trace_sched_wakeup.c
@@ -67,7 +67,7 @@ static bool function_enabled;
 static int
 func_prolog_preempt_disable(struct trace_array *tr,
 			    struct trace_array_cpu **data,
-			    int *pc)
+			    unsigned int *trace_ctx)
 {
 	long disabled;
 	int cpu;
@@ -75,7 +75,7 @@ func_prolog_preempt_disable(struct trace_array *tr,
 	if (likely(!wakeup_task))
 		return 0;
 
-	*pc = preempt_count();
+	*trace_ctx = tracing_gen_ctx();
 	preempt_disable_notrace();
 
 	cpu = raw_smp_processor_id();
@@ -116,8 +116,8 @@ static int wakeup_graph_entry(struct ftrace_graph_ent *trace)
 {
 	struct trace_array *tr = wakeup_trace;
 	struct trace_array_cpu *data;
-	unsigned long flags;
-	int pc, ret = 0;
+	unsigned int trace_ctx;
+	int ret = 0;
 
 	if (ftrace_graph_ignore_func(trace))
 		return 0;
@@ -131,11 +131,10 @@ static int wakeup_graph_entry(struct ftrace_graph_ent *trace)
 	if (ftrace_graph_notrace_addr(trace->func))
 		return 1;
 
-	if (!func_prolog_preempt_disable(tr, &data, &pc))
+	if (!func_prolog_preempt_disable(tr, &data, &trace_ctx))
 		return 0;
 
-	local_save_flags(flags);
-	ret = __trace_graph_entry(tr, trace, flags, pc);
+	ret = __trace_graph_entry(tr, trace, trace_ctx);
 	atomic_dec(&data->disabled);
 	preempt_enable_notrace();
 
@@ -146,16 +145,14 @@ static void wakeup_graph_return(struct ftrace_graph_ret *trace)
 {
 	struct trace_array *tr = wakeup_trace;
 	struct trace_array_cpu *data;
-	unsigned long flags;
-	int pc;
+	unsigned int trace_ctx;
 
 	ftrace_graph_addr_finish(trace);
 
-	if (!func_prolog_preempt_disable(tr, &data, &pc))
+	if (!func_prolog_preempt_disable(tr, &data, &trace_ctx))
 		return;
 
-	local_save_flags(flags);
-	__trace_graph_return(tr, trace, flags, pc);
+	__trace_graph_return(tr, trace, trace_ctx);
 	atomic_dec(&data->disabled);
 
 	preempt_enable_notrace();
@@ -217,13 +214,13 @@ wakeup_tracer_call(unsigned long ip, unsigned long parent_ip,
 	struct trace_array *tr = wakeup_trace;
 	struct trace_array_cpu *data;
 	unsigned long flags;
-	int pc;
+	unsigned int trace_ctx;
 
-	if (!func_prolog_preempt_disable(tr, &data, &pc))
+	if (!func_prolog_preempt_disable(tr, &data, &trace_ctx))
 		return;
 
 	local_irq_save(flags);
-	trace_function(tr, ip, parent_ip, flags, pc);
+	trace_function(tr, ip, parent_ip, trace_ctx);
 	local_irq_restore(flags);
 
 	atomic_dec(&data->disabled);
@@ -303,12 +300,12 @@ static void wakeup_print_header(struct seq_file *s)
 static void
 __trace_function(struct trace_array *tr,
 		 unsigned long ip, unsigned long parent_ip,
-		 unsigned long flags, int pc)
+		 unsigned int trace_ctx)
 {
 	if (is_graph(tr))
-		trace_graph_function(tr, ip, parent_ip, flags, pc);
+		trace_graph_function(tr, ip, parent_ip, trace_ctx);
 	else
-		trace_function(tr, ip, parent_ip, flags, pc);
+		trace_function(tr, ip, parent_ip, trace_ctx);
 }
 
 static int wakeup_flag_changed(struct trace_array *tr, u32 mask, int set)
@@ -375,7 +372,7 @@ static void
 tracing_sched_switch_trace(struct trace_array *tr,
 			   struct task_struct *prev,
 			   struct task_struct *next,
-			   unsigned long flags, int pc)
+			   unsigned int trace_ctx)
 {
 	struct trace_event_call *call = &event_context_switch;
 	struct trace_buffer *buffer = tr->array_buffer.buffer;
@@ -383,7 +380,7 @@ tracing_sched_switch_trace(struct trace_array *tr,
 	struct ctx_switch_entry *entry;
 
 	event = trace_buffer_lock_reserve(buffer, TRACE_CTX,
-					  sizeof(*entry), flags, pc);
+					  sizeof(*entry), trace_ctx);
 	if (!event)
 		return;
 	entry	= ring_buffer_event_data(event);
@@ -396,14 +393,14 @@ tracing_sched_switch_trace(struct trace_array *tr,
 	entry->next_cpu	= task_cpu(next);
 
 	if (!call_filter_check_discard(call, entry, buffer, event))
-		trace_buffer_unlock_commit(tr, buffer, event, flags, pc);
+		trace_buffer_unlock_commit(tr, buffer, event, trace_ctx);
 }
 
 static void
 tracing_sched_wakeup_trace(struct trace_array *tr,
 			   struct task_struct *wakee,
 			   struct task_struct *curr,
-			   unsigned long flags, int pc)
+			   unsigned int trace_ctx)
 {
 	struct trace_event_call *call = &event_wakeup;
 	struct ring_buffer_event *event;
@@ -411,7 +408,7 @@ tracing_sched_wakeup_trace(struct trace_array *tr,
 	struct trace_buffer *buffer = tr->array_buffer.buffer;
 
 	event = trace_buffer_lock_reserve(buffer, TRACE_WAKE,
-					  sizeof(*entry), flags, pc);
+					  sizeof(*entry), trace_ctx);
 	if (!event)
 		return;
 	entry	= ring_buffer_event_data(event);
@@ -424,7 +421,7 @@ tracing_sched_wakeup_trace(struct trace_array *tr,
 	entry->next_cpu			= task_cpu(wakee);
 
 	if (!call_filter_check_discard(call, entry, buffer, event))
-		trace_buffer_unlock_commit(tr, buffer, event, flags, pc);
+		trace_buffer_unlock_commit(tr, buffer, event, trace_ctx);
 }
 
 static void notrace
@@ -436,7 +433,7 @@ probe_wakeup_sched_switch(void *ignore, bool preempt,
 	unsigned long flags;
 	long disabled;
 	int cpu;
-	int pc;
+	unsigned int trace_ctx;
 
 	tracing_record_cmdline(prev);
 
@@ -455,8 +452,6 @@ probe_wakeup_sched_switch(void *ignore, bool preempt,
 	if (next != wakeup_task)
 		return;
 
-	pc = preempt_count();
-
 	/* disable local data, not wakeup_cpu data */
 	cpu = raw_smp_processor_id();
 	disabled = atomic_inc_return(&per_cpu_ptr(wakeup_trace->array_buffer.data, cpu)->disabled);
@@ -464,6 +459,8 @@ probe_wakeup_sched_switch(void *ignore, bool preempt,
 		goto out;
 
 	local_irq_save(flags);
+	trace_ctx = tracing_gen_ctx_flags(flags);
+
 	arch_spin_lock(&wakeup_lock);
 
 	/* We could race with grabbing wakeup_lock */
@@ -473,9 +470,9 @@ probe_wakeup_sched_switch(void *ignore, bool preempt,
 	/* The task we are waiting for is waking up */
 	data = per_cpu_ptr(wakeup_trace->array_buffer.data, wakeup_cpu);
 
-	__trace_function(wakeup_trace, CALLER_ADDR0, CALLER_ADDR1, flags, pc);
-	tracing_sched_switch_trace(wakeup_trace, prev, next, flags, pc);
-	__trace_stack(wakeup_trace, flags, 0, pc);
+	__trace_function(wakeup_trace, CALLER_ADDR0, CALLER_ADDR1, trace_ctx);
+	tracing_sched_switch_trace(wakeup_trace, prev, next, trace_ctx);
+	__trace_stack(wakeup_trace, trace_ctx, 0);
 
 	T0 = data->preempt_timestamp;
 	T1 = ftrace_now(cpu);
@@ -527,9 +524,8 @@ probe_wakeup(void *ignore, struct task_struct *p)
 {
 	struct trace_array_cpu *data;
 	int cpu = smp_processor_id();
-	unsigned long flags;
 	long disabled;
-	int pc;
+	unsigned int trace_ctx;
 
 	if (likely(!tracer_enabled))
 		return;
@@ -550,11 +546,12 @@ probe_wakeup(void *ignore, struct task_struct *p)
 	    (!dl_task(p) && (p->prio >= wakeup_prio || p->prio >= current->prio)))
 		return;
 
-	pc = preempt_count();
 	disabled = atomic_inc_return(&per_cpu_ptr(wakeup_trace->array_buffer.data, cpu)->disabled);
 	if (unlikely(disabled != 1))
 		goto out;
 
+	trace_ctx = tracing_gen_ctx();
+
 	/* interrupts should be off from try_to_wake_up */
 	arch_spin_lock(&wakeup_lock);
 
@@ -581,19 +578,17 @@ probe_wakeup(void *ignore, struct task_struct *p)
 
 	wakeup_task = get_task_struct(p);
 
-	local_save_flags(flags);
-
 	data = per_cpu_ptr(wakeup_trace->array_buffer.data, wakeup_cpu);
 	data->preempt_timestamp = ftrace_now(cpu);
-	tracing_sched_wakeup_trace(wakeup_trace, p, current, flags, pc);
-	__trace_stack(wakeup_trace, flags, 0, pc);
+	tracing_sched_wakeup_trace(wakeup_trace, p, current, trace_ctx);
+	__trace_stack(wakeup_trace, trace_ctx, 0);
 
 	/*
 	 * We must be careful in using CALLER_ADDR2. But since wake_up
 	 * is not called by an assembly function  (where as schedule is)
 	 * it should be safe to use it here.
 	 */
-	__trace_function(wakeup_trace, CALLER_ADDR1, CALLER_ADDR2, flags, pc);
+	__trace_function(wakeup_trace, CALLER_ADDR1, CALLER_ADDR2, trace_ctx);
 
 out_locked:
 	arch_spin_unlock(&wakeup_lock);
diff --git a/kernel/trace/trace_syscalls.c b/kernel/trace/trace_syscalls.c
index d85a2f0f316b..8bfcd3b09422 100644
--- a/kernel/trace/trace_syscalls.c
+++ b/kernel/trace/trace_syscalls.c
@@ -298,9 +298,8 @@ static void ftrace_syscall_enter(void *data, struct pt_regs *regs, long id)
 	struct syscall_metadata *sys_data;
 	struct ring_buffer_event *event;
 	struct trace_buffer *buffer;
-	unsigned long irq_flags;
+	unsigned int trace_ctx;
 	unsigned long args[6];
-	int pc;
 	int syscall_nr;
 	int size;
 
@@ -322,12 +321,11 @@ static void ftrace_syscall_enter(void *data, struct pt_regs *regs, long id)
 
 	size = sizeof(*entry) + sizeof(unsigned long) * sys_data->nb_args;
 
-	local_save_flags(irq_flags);
-	pc = preempt_count();
+	trace_ctx = tracing_gen_ctx();
 
 	buffer = tr->array_buffer.buffer;
 	event = trace_buffer_lock_reserve(buffer,
-			sys_data->enter_event->event.type, size, irq_flags, pc);
+			sys_data->enter_event->event.type, size, trace_ctx);
 	if (!event)
 		return;
 
@@ -337,7 +335,7 @@ static void ftrace_syscall_enter(void *data, struct pt_regs *regs, long id)
 	memcpy(entry->args, args, sizeof(unsigned long) * sys_data->nb_args);
 
 	event_trigger_unlock_commit(trace_file, buffer, event, entry,
-				    irq_flags, pc);
+				    trace_ctx);
 }
 
 static void ftrace_syscall_exit(void *data, struct pt_regs *regs, long ret)
@@ -348,8 +346,7 @@ static void ftrace_syscall_exit(void *data, struct pt_regs *regs, long ret)
 	struct syscall_metadata *sys_data;
 	struct ring_buffer_event *event;
 	struct trace_buffer *buffer;
-	unsigned long irq_flags;
-	int pc;
+	unsigned int trace_ctx;
 	int syscall_nr;
 
 	syscall_nr = trace_get_syscall_nr(current, regs);
@@ -368,13 +365,12 @@ static void ftrace_syscall_exit(void *data, struct pt_regs *regs, long ret)
 	if (!sys_data)
 		return;
 
-	local_save_flags(irq_flags);
-	pc = preempt_count();
+	trace_ctx = tracing_gen_ctx();
 
 	buffer = tr->array_buffer.buffer;
 	event = trace_buffer_lock_reserve(buffer,
 			sys_data->exit_event->event.type, sizeof(*entry),
-			irq_flags, pc);
+			trace_ctx);
 	if (!event)
 		return;
 
@@ -383,7 +379,7 @@ static void ftrace_syscall_exit(void *data, struct pt_regs *regs, long ret)
 	entry->ret = syscall_get_return_value(current, regs);
 
 	event_trigger_unlock_commit(trace_file, buffer, event, entry,
-				    irq_flags, pc);
+				    trace_ctx);
 }
 
 static int reg_event_syscall_enter(struct trace_event_file *file,
diff --git a/kernel/trace/trace_uprobe.c b/kernel/trace/trace_uprobe.c
index 3cf7128e1ad3..a1ed96a7a462 100644
--- a/kernel/trace/trace_uprobe.c
+++ b/kernel/trace/trace_uprobe.c
@@ -961,7 +961,7 @@ static void __uprobe_trace_func(struct trace_uprobe *tu,
 	esize = SIZEOF_TRACE_ENTRY(is_ret_probe(tu));
 	size = esize + tu->tp.size + dsize;
 	event = trace_event_buffer_lock_reserve(&buffer, trace_file,
-						call->event.type, size, 0, 0);
+						call->event.type, size, 0);
 	if (!event)
 		return;
 
@@ -977,7 +977,7 @@ static void __uprobe_trace_func(struct trace_uprobe *tu,
 
 	memcpy(data, ucb->buf, tu->tp.size + dsize);
 
-	event_trigger_unlock_commit(trace_file, buffer, event, entry, 0, 0);
+	event_trigger_unlock_commit(trace_file, buffer, event, entry, 0);
 }
 
 /* uprobe handler */
diff --git a/kernel/workqueue.c b/kernel/workqueue.c
index 1d99c52cc99a..1118c813b04c 100644
--- a/kernel/workqueue.c
+++ b/kernel/workqueue.c
@@ -4912,6 +4912,10 @@ static void unbind_workers(int cpu)
 		pool->flags |= POOL_DISASSOCIATED;
 
 		raw_spin_unlock_irq(&pool->lock);
+
+		for_each_pool_worker(worker, pool)
+			WARN_ON_ONCE(set_cpus_allowed_ptr(worker->task, cpu_active_mask) < 0);
+
 		mutex_unlock(&wq_pool_attach_mutex);
 
 		/*
diff --git a/labgrid-wago b/labgrid-wago
new file mode 160000
index 000000000000..7ee3a714f8ac
--- /dev/null
+++ b/labgrid-wago
@@ -0,0 +1 @@
+Subproject commit 7ee3a714f8ac0f7568712745871628d5e71f6ea0
diff --git a/lib/Kconfig.debug b/lib/Kconfig.debug
index c789b39ed527..e7d04eade8e0 100644
--- a/lib/Kconfig.debug
+++ b/lib/Kconfig.debug
@@ -1330,7 +1330,7 @@ config DEBUG_ATOMIC_SLEEP
 
 config DEBUG_LOCKING_API_SELFTESTS
 	bool "Locking API boot-time self-tests"
-	depends on DEBUG_KERNEL
+	depends on DEBUG_KERNEL && !PREEMPT_RT
 	help
 	  Say Y here if you want the kernel to run a short self-test during
 	  bootup. The self-test checks whether common types of locking bugs
diff --git a/lib/bug.c b/lib/bug.c
index 7103440c0ee1..baf61c307a6a 100644
--- a/lib/bug.c
+++ b/lib/bug.c
@@ -205,6 +205,7 @@ enum bug_trap_type report_bug(unsigned long bugaddr, struct pt_regs *regs)
 	else
 		pr_crit("Kernel BUG at %pB [verbose debug info unavailable]\n",
 			(void *)bugaddr);
+	pr_flush(1000, true);
 
 	return BUG_TRAP_TYPE_BUG;
 }
diff --git a/lib/cpumask.c b/lib/cpumask.c
index fb22fb266f93..c3c76b833384 100644
--- a/lib/cpumask.c
+++ b/lib/cpumask.c
@@ -261,3 +261,21 @@ int cpumask_any_and_distribute(const struct cpumask *src1p,
 	return next;
 }
 EXPORT_SYMBOL(cpumask_any_and_distribute);
+
+int cpumask_any_distribute(const struct cpumask *srcp)
+{
+	int next, prev;
+
+	/* NOTE: our first selection will skip 0. */
+	prev = __this_cpu_read(distribute_cpu_mask_prev);
+
+	next = cpumask_next(prev, srcp);
+	if (next >= nr_cpu_ids)
+		next = cpumask_first(srcp);
+
+	if (next < nr_cpu_ids)
+		__this_cpu_write(distribute_cpu_mask_prev, next);
+
+	return next;
+}
+EXPORT_SYMBOL(cpumask_any_distribute);
diff --git a/lib/debugobjects.c b/lib/debugobjects.c
index 9e14ae02306b..083882a3cf2f 100644
--- a/lib/debugobjects.c
+++ b/lib/debugobjects.c
@@ -557,7 +557,10 @@ __debug_object_init(void *addr, const struct debug_obj_descr *descr, int onstack
 	struct debug_obj *obj;
 	unsigned long flags;
 
-	fill_pool();
+#ifdef CONFIG_PREEMPT_RT
+	if (preempt_count() == 0 && !irqs_disabled())
+#endif
+		fill_pool();
 
 	db = get_bucket((unsigned long) addr);
 
diff --git a/lib/dump_stack.c b/lib/dump_stack.c
index a00ee6eedc7c..f5a33b6f773f 100644
--- a/lib/dump_stack.c
+++ b/lib/dump_stack.c
@@ -12,6 +12,7 @@
 #include <linux/atomic.h>
 #include <linux/kexec.h>
 #include <linux/utsname.h>
+#include <linux/stop_machine.h>
 
 static char dump_stack_arch_desc_str[128];
 
@@ -57,6 +58,7 @@ void dump_stack_print_info(const char *log_lvl)
 		       log_lvl, dump_stack_arch_desc_str);
 
 	print_worker_info(log_lvl, current);
+	print_stop_info(log_lvl, current);
 }
 
 /**
diff --git a/lib/iov_iter.c b/lib/iov_iter.c
index f0b2ccb1bb01..33a88a7e72d9 100644
--- a/lib/iov_iter.c
+++ b/lib/iov_iter.c
@@ -407,6 +407,7 @@ static size_t copy_page_to_iter_pipe(struct page *page, size_t offset, size_t by
 		return 0;
 
 	buf->ops = &page_cache_pipe_buf_ops;
+	buf->flags = 0;
 	get_page(page);
 	buf->page = page;
 	buf->offset = offset;
@@ -543,6 +544,7 @@ static size_t push_pipe(struct iov_iter *i, size_t size,
 			break;
 
 		buf->ops = &default_pipe_buf_ops;
+		buf->flags = 0;
 		buf->page = page;
 		buf->offset = 0;
 		buf->len = min_t(ssize_t, left, PAGE_SIZE);
diff --git a/lib/irq_poll.c b/lib/irq_poll.c
index 2f17b488d58e..7557bf7ecf1f 100644
--- a/lib/irq_poll.c
+++ b/lib/irq_poll.c
@@ -37,6 +37,7 @@ void irq_poll_sched(struct irq_poll *iop)
 	list_add_tail(&iop->list, this_cpu_ptr(&blk_cpu_iopoll));
 	raise_softirq_irqoff(IRQ_POLL_SOFTIRQ);
 	local_irq_restore(flags);
+	preempt_check_resched_rt();
 }
 EXPORT_SYMBOL(irq_poll_sched);
 
@@ -72,6 +73,7 @@ void irq_poll_complete(struct irq_poll *iop)
 	local_irq_save(flags);
 	__irq_poll_complete(iop);
 	local_irq_restore(flags);
+	preempt_check_resched_rt();
 }
 EXPORT_SYMBOL(irq_poll_complete);
 
@@ -96,6 +98,7 @@ static void __latent_entropy irq_poll_softirq(struct softirq_action *h)
 		}
 
 		local_irq_enable();
+		preempt_check_resched_rt();
 
 		/* Even though interrupts have been re-enabled, this
 		 * access is safe because interrupts can only add new
@@ -133,6 +136,7 @@ static void __latent_entropy irq_poll_softirq(struct softirq_action *h)
 		__raise_softirq_irqoff(IRQ_POLL_SOFTIRQ);
 
 	local_irq_enable();
+	preempt_check_resched_rt();
 }
 
 /**
@@ -196,6 +200,7 @@ static int irq_poll_cpu_dead(unsigned int cpu)
 			 this_cpu_ptr(&blk_cpu_iopoll));
 	__raise_softirq_irqoff(IRQ_POLL_SOFTIRQ);
 	local_irq_enable();
+	preempt_check_resched_rt();
 
 	return 0;
 }
diff --git a/lib/locking-selftest.c b/lib/locking-selftest.c
index a899b3f0e2e5..a2baedfff9ee 100644
--- a/lib/locking-selftest.c
+++ b/lib/locking-selftest.c
@@ -786,6 +786,8 @@ GENERATE_TESTCASE(init_held_rtmutex);
 #include "locking-selftest-spin-hardirq.h"
 GENERATE_PERMUTATIONS_2_EVENTS(irqsafe1_hard_spin)
 
+#ifndef CONFIG_PREEMPT_RT
+
 #include "locking-selftest-rlock-hardirq.h"
 GENERATE_PERMUTATIONS_2_EVENTS(irqsafe1_hard_rlock)
 
@@ -801,9 +803,12 @@ GENERATE_PERMUTATIONS_2_EVENTS(irqsafe1_soft_rlock)
 #include "locking-selftest-wlock-softirq.h"
 GENERATE_PERMUTATIONS_2_EVENTS(irqsafe1_soft_wlock)
 
+#endif
+
 #undef E1
 #undef E2
 
+#ifndef CONFIG_PREEMPT_RT
 /*
  * Enabling hardirqs with a softirq-safe lock held:
  */
@@ -836,6 +841,8 @@ GENERATE_PERMUTATIONS_2_EVENTS(irqsafe2A_rlock)
 #undef E1
 #undef E2
 
+#endif
+
 /*
  * Enabling irqs with an irq-safe lock held:
  */
@@ -859,6 +866,8 @@ GENERATE_PERMUTATIONS_2_EVENTS(irqsafe2A_rlock)
 #include "locking-selftest-spin-hardirq.h"
 GENERATE_PERMUTATIONS_2_EVENTS(irqsafe2B_hard_spin)
 
+#ifndef CONFIG_PREEMPT_RT
+
 #include "locking-selftest-rlock-hardirq.h"
 GENERATE_PERMUTATIONS_2_EVENTS(irqsafe2B_hard_rlock)
 
@@ -874,6 +883,8 @@ GENERATE_PERMUTATIONS_2_EVENTS(irqsafe2B_soft_rlock)
 #include "locking-selftest-wlock-softirq.h"
 GENERATE_PERMUTATIONS_2_EVENTS(irqsafe2B_soft_wlock)
 
+#endif
+
 #undef E1
 #undef E2
 
@@ -905,6 +916,8 @@ GENERATE_PERMUTATIONS_2_EVENTS(irqsafe2B_soft_wlock)
 #include "locking-selftest-spin-hardirq.h"
 GENERATE_PERMUTATIONS_3_EVENTS(irqsafe3_hard_spin)
 
+#ifndef CONFIG_PREEMPT_RT
+
 #include "locking-selftest-rlock-hardirq.h"
 GENERATE_PERMUTATIONS_3_EVENTS(irqsafe3_hard_rlock)
 
@@ -920,6 +933,8 @@ GENERATE_PERMUTATIONS_3_EVENTS(irqsafe3_soft_rlock)
 #include "locking-selftest-wlock-softirq.h"
 GENERATE_PERMUTATIONS_3_EVENTS(irqsafe3_soft_wlock)
 
+#endif
+
 #undef E1
 #undef E2
 #undef E3
@@ -953,6 +968,8 @@ GENERATE_PERMUTATIONS_3_EVENTS(irqsafe3_soft_wlock)
 #include "locking-selftest-spin-hardirq.h"
 GENERATE_PERMUTATIONS_3_EVENTS(irqsafe4_hard_spin)
 
+#ifndef CONFIG_PREEMPT_RT
+
 #include "locking-selftest-rlock-hardirq.h"
 GENERATE_PERMUTATIONS_3_EVENTS(irqsafe4_hard_rlock)
 
@@ -968,10 +985,14 @@ GENERATE_PERMUTATIONS_3_EVENTS(irqsafe4_soft_rlock)
 #include "locking-selftest-wlock-softirq.h"
 GENERATE_PERMUTATIONS_3_EVENTS(irqsafe4_soft_wlock)
 
+#endif
+
 #undef E1
 #undef E2
 #undef E3
 
+#ifndef CONFIG_PREEMPT_RT
+
 /*
  * read-lock / write-lock irq inversion.
  *
@@ -1161,6 +1182,11 @@ GENERATE_PERMUTATIONS_3_EVENTS(W1W2_R2R3_R3W1)
 #undef E1
 #undef E2
 #undef E3
+
+#endif
+
+#ifndef CONFIG_PREEMPT_RT
+
 /*
  * read-lock / write-lock recursion that is actually safe.
  */
@@ -1207,6 +1233,8 @@ GENERATE_PERMUTATIONS_3_EVENTS(irq_read_recursion_soft_wlock)
 #undef E2
 #undef E3
 
+#endif
+
 /*
  * read-lock / write-lock recursion that is unsafe.
  */
@@ -2455,6 +2483,7 @@ void locking_selftest(void)
 
 	printk("  --------------------------------------------------------------------------\n");
 
+#ifndef CONFIG_PREEMPT_RT
 	/*
 	 * irq-context testcases:
 	 */
@@ -2469,6 +2498,28 @@ void locking_selftest(void)
 	DO_TESTCASE_6x2x2RW("irq read-recursion #2", irq_read_recursion2);
 	DO_TESTCASE_6x2x2RW("irq read-recursion #3", irq_read_recursion3);
 
+#else
+	/* On -rt, we only do hardirq context test for raw spinlock */
+	DO_TESTCASE_1B("hard-irqs-on + irq-safe-A", irqsafe1_hard_spin, 12);
+	DO_TESTCASE_1B("hard-irqs-on + irq-safe-A", irqsafe1_hard_spin, 21);
+
+	DO_TESTCASE_1B("hard-safe-A + irqs-on", irqsafe2B_hard_spin, 12);
+	DO_TESTCASE_1B("hard-safe-A + irqs-on", irqsafe2B_hard_spin, 21);
+
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #1", irqsafe3_hard_spin, 123);
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #1", irqsafe3_hard_spin, 132);
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #1", irqsafe3_hard_spin, 213);
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #1", irqsafe3_hard_spin, 231);
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #1", irqsafe3_hard_spin, 312);
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #1", irqsafe3_hard_spin, 321);
+
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #2", irqsafe4_hard_spin, 123);
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #2", irqsafe4_hard_spin, 132);
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #2", irqsafe4_hard_spin, 213);
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #2", irqsafe4_hard_spin, 231);
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #2", irqsafe4_hard_spin, 312);
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #2", irqsafe4_hard_spin, 321);
+#endif
 	ww_tests();
 
 	force_read_lock_recursive = 0;
diff --git a/lib/nmi_backtrace.c b/lib/nmi_backtrace.c
index 8abe1870dba4..b09a490f5f70 100644
--- a/lib/nmi_backtrace.c
+++ b/lib/nmi_backtrace.c
@@ -75,12 +75,6 @@ void nmi_trigger_cpumask_backtrace(const cpumask_t *mask,
 		touch_softlockup_watchdog();
 	}
 
-	/*
-	 * Force flush any remote buffers that might be stuck in IRQ context
-	 * and therefore could not run their irq_work.
-	 */
-	printk_safe_flush();
-
 	clear_bit_unlock(0, &backtrace_flag);
 	put_cpu();
 }
diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index a59778946404..907f59045998 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -892,7 +892,7 @@ void sg_miter_stop(struct sg_mapping_iter *miter)
 			flush_kernel_dcache_page(miter->page);
 
 		if (miter->__flags & SG_MITER_ATOMIC) {
-			WARN_ON_ONCE(preemptible());
+			WARN_ON_ONCE(!pagefault_disabled());
 			kunmap_atomic(miter->addr);
 		} else
 			kunmap(miter->page);
diff --git a/lib/smp_processor_id.c b/lib/smp_processor_id.c
index 525222e4f409..1c1dbd300325 100644
--- a/lib/smp_processor_id.c
+++ b/lib/smp_processor_id.c
@@ -26,6 +26,11 @@ unsigned int check_preemption_disabled(const char *what1, const char *what2)
 	if (current->nr_cpus_allowed == 1)
 		goto out;
 
+#ifdef CONFIG_SMP
+	if (current->migration_disabled)
+		goto out;
+#endif
+
 	/*
 	 * It is valid to assume CPU-locality during early bootup:
 	 */
diff --git a/lib/test_lockup.c b/lib/test_lockup.c
index f1a020bcc763..864554e76973 100644
--- a/lib/test_lockup.c
+++ b/lib/test_lockup.c
@@ -480,6 +480,21 @@ static int __init test_lockup_init(void)
 		return -EINVAL;
 
 #ifdef CONFIG_DEBUG_SPINLOCK
+#ifdef CONFIG_PREEMPT_RT
+	if (test_magic(lock_spinlock_ptr,
+		       offsetof(spinlock_t, lock.wait_lock.magic),
+		       SPINLOCK_MAGIC) ||
+	    test_magic(lock_rwlock_ptr,
+		       offsetof(rwlock_t, rtmutex.wait_lock.magic),
+		       SPINLOCK_MAGIC) ||
+	    test_magic(lock_mutex_ptr,
+		       offsetof(struct mutex, lock.wait_lock.magic),
+		       SPINLOCK_MAGIC) ||
+	    test_magic(lock_rwsem_ptr,
+		       offsetof(struct rw_semaphore, rtmutex.wait_lock.magic),
+		       SPINLOCK_MAGIC))
+		return -EINVAL;
+#else
 	if (test_magic(lock_spinlock_ptr,
 		       offsetof(spinlock_t, rlock.magic),
 		       SPINLOCK_MAGIC) ||
@@ -493,6 +508,7 @@ static int __init test_lockup_init(void)
 		       offsetof(struct rw_semaphore, wait_lock.magic),
 		       SPINLOCK_MAGIC))
 		return -EINVAL;
+#endif
 #endif
 
 	if ((wait_state != TASK_RUNNING ||
diff --git a/localversion-rt b/localversion-rt
new file mode 100644
index 000000000000..21988f9ad53f
--- /dev/null
+++ b/localversion-rt
@@ -0,0 +1 @@
+-rt34
diff --git a/localversion-wago b/localversion-wago
new file mode 100644
index 000000000000..ea7f660c413f
--- /dev/null
+++ b/localversion-wago
@@ -0,0 +1 @@
+w03.01.05
diff --git a/mm/Kconfig b/mm/Kconfig
index 390165ffbb0f..c8cbcb5118b0 100644
--- a/mm/Kconfig
+++ b/mm/Kconfig
@@ -387,7 +387,7 @@ config NOMMU_INITIAL_TRIM_EXCESS
 
 config TRANSPARENT_HUGEPAGE
 	bool "Transparent Hugepage Support"
-	depends on HAVE_ARCH_TRANSPARENT_HUGEPAGE
+	depends on HAVE_ARCH_TRANSPARENT_HUGEPAGE && !PREEMPT_RT
 	select COMPACTION
 	select XARRAY_MULTI
 	help
@@ -859,4 +859,7 @@ config ARCH_HAS_HUGEPD
 config MAPPING_DIRTY_HELPERS
         bool
 
+config KMAP_LOCAL
+	bool
+
 endmenu
diff --git a/mm/highmem.c b/mm/highmem.c
index 1352a27951e3..72b9a2d95c72 100644
--- a/mm/highmem.c
+++ b/mm/highmem.c
@@ -31,10 +31,6 @@
 #include <asm/tlbflush.h>
 #include <linux/vmalloc.h>
 
-#if defined(CONFIG_HIGHMEM) || defined(CONFIG_X86_32)
-DEFINE_PER_CPU(int, __kmap_atomic_idx);
-#endif
-
 /*
  * Virtual_count is not a pure "count".
  *  0 means that it is not mapped, and has not been mapped
@@ -108,9 +104,7 @@ static inline wait_queue_head_t *get_pkmap_wait_queue_head(unsigned int color)
 atomic_long_t _totalhigh_pages __read_mostly;
 EXPORT_SYMBOL(_totalhigh_pages);
 
-EXPORT_PER_CPU_SYMBOL(__kmap_atomic_idx);
-
-unsigned int nr_free_highpages (void)
+unsigned int __nr_free_highpages (void)
 {
 	struct zone *zone;
 	unsigned int pages = 0;
@@ -147,7 +141,7 @@ pte_t * pkmap_page_table;
 		do { spin_unlock(&kmap_lock); (void)(flags); } while (0)
 #endif
 
-struct page *kmap_to_page(void *vaddr)
+struct page *__kmap_to_page(void *vaddr)
 {
 	unsigned long addr = (unsigned long)vaddr;
 
@@ -158,7 +152,7 @@ struct page *kmap_to_page(void *vaddr)
 
 	return virt_to_page(addr);
 }
-EXPORT_SYMBOL(kmap_to_page);
+EXPORT_SYMBOL(__kmap_to_page);
 
 static void flush_all_zero_pkmaps(void)
 {
@@ -200,10 +194,7 @@ static void flush_all_zero_pkmaps(void)
 		flush_tlb_kernel_range(PKMAP_ADDR(0), PKMAP_ADDR(LAST_PKMAP));
 }
 
-/**
- * kmap_flush_unused - flush all unused kmap mappings in order to remove stray mappings
- */
-void kmap_flush_unused(void)
+void __kmap_flush_unused(void)
 {
 	lock_kmap();
 	flush_all_zero_pkmaps();
@@ -367,9 +358,250 @@ void kunmap_high(struct page *page)
 	if (need_wakeup)
 		wake_up(pkmap_map_wait);
 }
-
 EXPORT_SYMBOL(kunmap_high);
-#endif	/* CONFIG_HIGHMEM */
+#endif /* CONFIG_HIGHMEM */
+
+#ifdef CONFIG_KMAP_LOCAL
+
+#include <asm/kmap_size.h>
+
+/*
+ * With DEBUG_HIGHMEM the stack depth is doubled and every second
+ * slot is unused which acts as a guard page
+ */
+#ifdef CONFIG_DEBUG_HIGHMEM
+# define KM_INCR	2
+#else
+# define KM_INCR	1
+#endif
+
+static inline int kmap_local_idx_push(void)
+{
+	WARN_ON_ONCE(in_irq() && !irqs_disabled());
+	current->kmap_ctrl.idx += KM_INCR;
+	BUG_ON(current->kmap_ctrl.idx >= KM_MAX_IDX);
+	return current->kmap_ctrl.idx - 1;
+}
+
+static inline int kmap_local_idx(void)
+{
+	return current->kmap_ctrl.idx - 1;
+}
+
+static inline void kmap_local_idx_pop(void)
+{
+	current->kmap_ctrl.idx -= KM_INCR;
+	BUG_ON(current->kmap_ctrl.idx < 0);
+}
+
+#ifndef arch_kmap_local_post_map
+# define arch_kmap_local_post_map(vaddr, pteval)	do { } while (0)
+#endif
+
+#ifndef arch_kmap_local_pre_unmap
+# define arch_kmap_local_pre_unmap(vaddr)		do { } while (0)
+#endif
+
+#ifndef arch_kmap_local_post_unmap
+# define arch_kmap_local_post_unmap(vaddr)		do { } while (0)
+#endif
+
+#ifndef arch_kmap_local_map_idx
+#define arch_kmap_local_map_idx(idx, pfn)	kmap_local_calc_idx(idx)
+#endif
+
+#ifndef arch_kmap_local_unmap_idx
+#define arch_kmap_local_unmap_idx(idx, vaddr)	kmap_local_calc_idx(idx)
+#endif
+
+#ifndef arch_kmap_local_high_get
+static inline void *arch_kmap_local_high_get(struct page *page)
+{
+	return NULL;
+}
+#endif
+
+/* Unmap a local mapping which was obtained by kmap_high_get() */
+static inline bool kmap_high_unmap_local(unsigned long vaddr)
+{
+#ifdef ARCH_NEEDS_KMAP_HIGH_GET
+	if (vaddr >= PKMAP_ADDR(0) && vaddr < PKMAP_ADDR(LAST_PKMAP)) {
+		kunmap_high(pte_page(pkmap_page_table[PKMAP_NR(vaddr)]));
+		return true;
+	}
+#endif
+	return false;
+}
+
+static inline int kmap_local_calc_idx(int idx)
+{
+	return idx + KM_MAX_IDX * smp_processor_id();
+}
+
+static pte_t *__kmap_pte;
+
+static pte_t *kmap_get_pte(void)
+{
+	if (!__kmap_pte)
+		__kmap_pte = virt_to_kpte(__fix_to_virt(FIX_KMAP_BEGIN));
+	return __kmap_pte;
+}
+
+void *__kmap_local_pfn_prot(unsigned long pfn, pgprot_t prot)
+{
+	pte_t pteval, *kmap_pte = kmap_get_pte();
+	unsigned long vaddr;
+	int idx;
+
+	/*
+	 * Disable migration so resulting virtual address is stable
+	 * accross preemption.
+	 */
+	migrate_disable();
+	preempt_disable();
+	idx = arch_kmap_local_map_idx(kmap_local_idx_push(), pfn);
+	vaddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);
+	BUG_ON(!pte_none(*(kmap_pte - idx)));
+	pteval = pfn_pte(pfn, prot);
+	set_pte_at(&init_mm, vaddr, kmap_pte - idx, pteval);
+	arch_kmap_local_post_map(vaddr, pteval);
+	current->kmap_ctrl.pteval[kmap_local_idx()] = pteval;
+	preempt_enable();
+
+	return (void *)vaddr;
+}
+EXPORT_SYMBOL_GPL(__kmap_local_pfn_prot);
+
+void *__kmap_local_page_prot(struct page *page, pgprot_t prot)
+{
+	void *kmap;
+
+	if (!PageHighMem(page))
+		return page_address(page);
+
+	/* Try kmap_high_get() if architecture has it enabled */
+	kmap = arch_kmap_local_high_get(page);
+	if (kmap)
+		return kmap;
+
+	return __kmap_local_pfn_prot(page_to_pfn(page), prot);
+}
+EXPORT_SYMBOL(__kmap_local_page_prot);
+
+void kunmap_local_indexed(void *vaddr)
+{
+	unsigned long addr = (unsigned long) vaddr & PAGE_MASK;
+	pte_t *kmap_pte = kmap_get_pte();
+	int idx;
+
+	if (addr < __fix_to_virt(FIX_KMAP_END) ||
+	    addr > __fix_to_virt(FIX_KMAP_BEGIN)) {
+		/*
+		 * Handle mappings which were obtained by kmap_high_get()
+		 * first as the virtual address of such mappings is below
+		 * PAGE_OFFSET. Warn for all other addresses which are in
+		 * the user space part of the virtual address space.
+		 */
+		if (!kmap_high_unmap_local(addr))
+			WARN_ON_ONCE(addr < PAGE_OFFSET);
+		return;
+	}
+
+	preempt_disable();
+	idx = arch_kmap_local_unmap_idx(kmap_local_idx(), addr);
+	WARN_ON_ONCE(addr != __fix_to_virt(FIX_KMAP_BEGIN + idx));
+
+	arch_kmap_local_pre_unmap(addr);
+	pte_clear(&init_mm, addr, kmap_pte - idx);
+	arch_kmap_local_post_unmap(addr);
+	current->kmap_ctrl.pteval[kmap_local_idx()] = __pte(0);
+	kmap_local_idx_pop();
+	preempt_enable();
+	migrate_enable();
+}
+EXPORT_SYMBOL(kunmap_local_indexed);
+
+/*
+ * Invoked before switch_to(). This is safe even when during or after
+ * clearing the maps an interrupt which needs a kmap_local happens because
+ * the task::kmap_ctrl.idx is not modified by the unmapping code so a
+ * nested kmap_local will use the next unused index and restore the index
+ * on unmap. The already cleared kmaps of the outgoing task are irrelevant
+ * because the interrupt context does not know about them. The same applies
+ * when scheduling back in for an interrupt which happens before the
+ * restore is complete.
+ */
+void __kmap_local_sched_out(void)
+{
+	struct task_struct *tsk = current;
+	pte_t *kmap_pte = kmap_get_pte();
+	int i;
+
+	/* Clear kmaps */
+	for (i = 0; i < tsk->kmap_ctrl.idx; i++) {
+		pte_t pteval = tsk->kmap_ctrl.pteval[i];
+		unsigned long addr;
+		int idx;
+
+		/* With debug all even slots are unmapped and act as guard */
+		if (IS_ENABLED(CONFIG_DEBUG_HIGHMEM) && !(i & 0x01)) {
+			WARN_ON_ONCE(!pte_none(pteval));
+			continue;
+		}
+		if (WARN_ON_ONCE(pte_none(pteval)))
+			continue;
+
+		/*
+		 * This is a horrible hack for XTENSA to calculate the
+		 * coloured PTE index. Uses the PFN encoded into the pteval
+		 * and the map index calculation because the actual mapped
+		 * virtual address is not stored in task::kmap_ctrl.
+		 * For any sane architecture this is optimized out.
+		 */
+		idx = arch_kmap_local_map_idx(i, pte_pfn(pteval));
+
+		addr = __fix_to_virt(FIX_KMAP_BEGIN + idx);
+		arch_kmap_local_pre_unmap(addr);
+		pte_clear(&init_mm, addr, kmap_pte - idx);
+		arch_kmap_local_post_unmap(addr);
+	}
+}
+
+void __kmap_local_sched_in(void)
+{
+	struct task_struct *tsk = current;
+	pte_t *kmap_pte = kmap_get_pte();
+	int i;
+
+	/* Restore kmaps */
+	for (i = 0; i < tsk->kmap_ctrl.idx; i++) {
+		pte_t pteval = tsk->kmap_ctrl.pteval[i];
+		unsigned long addr;
+		int idx;
+
+		/* With debug all even slots are unmapped and act as guard */
+		if (IS_ENABLED(CONFIG_DEBUG_HIGHMEM) && !(i & 0x01)) {
+			WARN_ON_ONCE(!pte_none(pteval));
+			continue;
+		}
+		if (WARN_ON_ONCE(pte_none(pteval)))
+			continue;
+
+		/* See comment in __kmap_local_sched_out() */
+		idx = arch_kmap_local_map_idx(i, pte_pfn(pteval));
+		addr = __fix_to_virt(FIX_KMAP_BEGIN + idx);
+		set_pte_at(&init_mm, addr, kmap_pte - idx, pteval);
+		arch_kmap_local_post_map(addr, pteval);
+	}
+}
+
+void kmap_local_fork(struct task_struct *tsk)
+{
+	if (WARN_ON_ONCE(tsk->kmap_ctrl.idx))
+		memset(&tsk->kmap_ctrl, 0, sizeof(tsk->kmap_ctrl));
+}
+
+#endif
 
 #if defined(HASHED_PAGE_VIRTUAL)
 
diff --git a/mm/memcontrol.c b/mm/memcontrol.c
index d6966f1ebc7a..b5692d2165c8 100644
--- a/mm/memcontrol.c
+++ b/mm/memcontrol.c
@@ -63,6 +63,7 @@
 #include <net/sock.h>
 #include <net/ip.h>
 #include "slab.h"
+#include <linux/local_lock.h>
 
 #include <linux/uaccess.h>
 
@@ -93,6 +94,13 @@ bool cgroup_memory_noswap __read_mostly;
 static DECLARE_WAIT_QUEUE_HEAD(memcg_cgwb_frn_waitq);
 #endif
 
+struct event_lock {
+	local_lock_t l;
+};
+static DEFINE_PER_CPU(struct event_lock, event_lock) = {
+	.l      = INIT_LOCAL_LOCK(l),
+};
+
 /* Whether legacy memory+swap accounting is active */
 static bool do_memsw_account(void)
 {
@@ -816,6 +824,7 @@ void __mod_memcg_lruvec_state(struct lruvec *lruvec, enum node_stat_item idx,
 	pn = container_of(lruvec, struct mem_cgroup_per_node, lruvec);
 	memcg = pn->memcg;
 
+	preempt_disable_rt();
 	/* Update memcg */
 	__mod_memcg_state(memcg, idx, val);
 
@@ -835,6 +844,7 @@ void __mod_memcg_lruvec_state(struct lruvec *lruvec, enum node_stat_item idx,
 		x = 0;
 	}
 	__this_cpu_write(pn->lruvec_stat_cpu->count[idx], x);
+	preempt_enable_rt();
 }
 
 /**
@@ -2200,6 +2210,7 @@ void unlock_page_memcg(struct page *page)
 EXPORT_SYMBOL(unlock_page_memcg);
 
 struct memcg_stock_pcp {
+	local_lock_t lock;
 	struct mem_cgroup *cached; /* this never be root cgroup */
 	unsigned int nr_pages;
 
@@ -2251,7 +2262,7 @@ static bool consume_stock(struct mem_cgroup *memcg, unsigned int nr_pages)
 	if (nr_pages > MEMCG_CHARGE_BATCH)
 		return ret;
 
-	local_irq_save(flags);
+	local_lock_irqsave(&memcg_stock.lock, flags);
 
 	stock = this_cpu_ptr(&memcg_stock);
 	if (memcg == stock->cached && stock->nr_pages >= nr_pages) {
@@ -2259,7 +2270,7 @@ static bool consume_stock(struct mem_cgroup *memcg, unsigned int nr_pages)
 		ret = true;
 	}
 
-	local_irq_restore(flags);
+	local_unlock_irqrestore(&memcg_stock.lock, flags);
 
 	return ret;
 }
@@ -2294,14 +2305,14 @@ static void drain_local_stock(struct work_struct *dummy)
 	 * The only protection from memory hotplug vs. drain_stock races is
 	 * that we always operate on local CPU stock here with IRQ disabled
 	 */
-	local_irq_save(flags);
+	local_lock_irqsave(&memcg_stock.lock, flags);
 
 	stock = this_cpu_ptr(&memcg_stock);
 	drain_obj_stock(stock);
 	drain_stock(stock);
 	clear_bit(FLUSHING_CACHED_CHARGE, &stock->flags);
 
-	local_irq_restore(flags);
+	local_unlock_irqrestore(&memcg_stock.lock, flags);
 }
 
 /*
@@ -2313,7 +2324,7 @@ static void refill_stock(struct mem_cgroup *memcg, unsigned int nr_pages)
 	struct memcg_stock_pcp *stock;
 	unsigned long flags;
 
-	local_irq_save(flags);
+	local_lock_irqsave(&memcg_stock.lock, flags);
 
 	stock = this_cpu_ptr(&memcg_stock);
 	if (stock->cached != memcg) { /* reset if necessary */
@@ -2326,7 +2337,7 @@ static void refill_stock(struct mem_cgroup *memcg, unsigned int nr_pages)
 	if (stock->nr_pages > MEMCG_CHARGE_BATCH)
 		drain_stock(stock);
 
-	local_irq_restore(flags);
+	local_unlock_irqrestore(&memcg_stock.lock, flags);
 }
 
 /*
@@ -2346,7 +2357,7 @@ static void drain_all_stock(struct mem_cgroup *root_memcg)
 	 * as well as workers from this path always operate on the local
 	 * per-cpu data. CPU up doesn't touch memcg_stock at all.
 	 */
-	curcpu = get_cpu();
+	curcpu = get_cpu_light();
 	for_each_online_cpu(cpu) {
 		struct memcg_stock_pcp *stock = &per_cpu(memcg_stock, cpu);
 		struct mem_cgroup *memcg;
@@ -2369,7 +2380,7 @@ static void drain_all_stock(struct mem_cgroup *root_memcg)
 				schedule_work_on(cpu, &stock->work);
 		}
 	}
-	put_cpu();
+	put_cpu_light();
 	mutex_unlock(&percpu_charge_mutex);
 }
 
@@ -3137,7 +3148,7 @@ static bool consume_obj_stock(struct obj_cgroup *objcg, unsigned int nr_bytes)
 	unsigned long flags;
 	bool ret = false;
 
-	local_irq_save(flags);
+	local_lock_irqsave(&memcg_stock.lock, flags);
 
 	stock = this_cpu_ptr(&memcg_stock);
 	if (objcg == stock->cached_objcg && stock->nr_bytes >= nr_bytes) {
@@ -3145,7 +3156,7 @@ static bool consume_obj_stock(struct obj_cgroup *objcg, unsigned int nr_bytes)
 		ret = true;
 	}
 
-	local_irq_restore(flags);
+	local_unlock_irqrestore(&memcg_stock.lock, flags);
 
 	return ret;
 }
@@ -3204,7 +3215,7 @@ static void refill_obj_stock(struct obj_cgroup *objcg, unsigned int nr_bytes)
 	struct memcg_stock_pcp *stock;
 	unsigned long flags;
 
-	local_irq_save(flags);
+	local_lock_irqsave(&memcg_stock.lock, flags);
 
 	stock = this_cpu_ptr(&memcg_stock);
 	if (stock->cached_objcg != objcg) { /* reset if necessary */
@@ -3218,7 +3229,7 @@ static void refill_obj_stock(struct obj_cgroup *objcg, unsigned int nr_bytes)
 	if (stock->nr_bytes > PAGE_SIZE)
 		drain_obj_stock(stock);
 
-	local_irq_restore(flags);
+	local_unlock_irqrestore(&memcg_stock.lock, flags);
 }
 
 int obj_cgroup_charge(struct obj_cgroup *objcg, gfp_t gfp, size_t size)
@@ -5724,12 +5735,12 @@ static int mem_cgroup_move_account(struct page *page,
 
 	ret = 0;
 
-	local_irq_disable();
+	local_lock_irq(&event_lock.l);
 	mem_cgroup_charge_statistics(to, page, nr_pages);
 	memcg_check_events(to, page);
 	mem_cgroup_charge_statistics(from, page, -nr_pages);
 	memcg_check_events(from, page);
-	local_irq_enable();
+	local_unlock_irq(&event_lock.l);
 out_unlock:
 	unlock_page(page);
 out:
@@ -6799,10 +6810,10 @@ int mem_cgroup_charge(struct page *page, struct mm_struct *mm, gfp_t gfp_mask)
 	css_get(&memcg->css);
 	commit_charge(page, memcg);
 
-	local_irq_disable();
+	local_lock_irq(&event_lock.l);
 	mem_cgroup_charge_statistics(memcg, page, nr_pages);
 	memcg_check_events(memcg, page);
-	local_irq_enable();
+	local_unlock_irq(&event_lock.l);
 
 	/*
 	 * Cgroup1's unified memory+swap counter has been charged with the
@@ -6858,11 +6869,11 @@ static void uncharge_batch(const struct uncharge_gather *ug)
 		memcg_oom_recover(ug->memcg);
 	}
 
-	local_irq_save(flags);
+	local_lock_irqsave(&event_lock.l, flags);
 	__count_memcg_events(ug->memcg, PGPGOUT, ug->pgpgout);
 	__this_cpu_add(ug->memcg->vmstats_percpu->nr_page_events, ug->nr_pages);
 	memcg_check_events(ug->memcg, ug->dummy_page);
-	local_irq_restore(flags);
+	local_unlock_irqrestore(&event_lock.l, flags);
 
 	/* drop reference from uncharge_page */
 	css_put(&ug->memcg->css);
@@ -7016,10 +7027,10 @@ void mem_cgroup_migrate(struct page *oldpage, struct page *newpage)
 	css_get(&memcg->css);
 	commit_charge(newpage, memcg);
 
-	local_irq_save(flags);
+	local_lock_irqsave(&event_lock.l, flags);
 	mem_cgroup_charge_statistics(memcg, newpage, nr_pages);
 	memcg_check_events(memcg, newpage);
-	local_irq_restore(flags);
+	local_unlock_irqrestore(&event_lock.l, flags);
 }
 
 DEFINE_STATIC_KEY_FALSE(memcg_sockets_enabled_key);
@@ -7139,9 +7150,13 @@ static int __init mem_cgroup_init(void)
 	cpuhp_setup_state_nocalls(CPUHP_MM_MEMCQ_DEAD, "mm/memctrl:dead", NULL,
 				  memcg_hotplug_cpu_dead);
 
-	for_each_possible_cpu(cpu)
-		INIT_WORK(&per_cpu_ptr(&memcg_stock, cpu)->work,
-			  drain_local_stock);
+	for_each_possible_cpu(cpu) {
+		struct memcg_stock_pcp *stock;
+
+		stock = per_cpu_ptr(&memcg_stock, cpu);
+		INIT_WORK(&stock->work, drain_local_stock);
+		local_lock_init(&stock->lock);
+	}
 
 	for_each_node(node) {
 		struct mem_cgroup_tree_per_node *rtpn;
@@ -7190,6 +7205,7 @@ void mem_cgroup_swapout(struct page *page, swp_entry_t entry)
 	struct mem_cgroup *memcg, *swap_memcg;
 	unsigned int nr_entries;
 	unsigned short oldid;
+	unsigned long flags;
 
 	VM_BUG_ON_PAGE(PageLRU(page), page);
 	VM_BUG_ON_PAGE(page_count(page), page);
@@ -7235,9 +7251,13 @@ void mem_cgroup_swapout(struct page *page, swp_entry_t entry)
 	 * important here to have the interrupts disabled because it is the
 	 * only synchronisation we have for updating the per-CPU variables.
 	 */
+	local_lock_irqsave(&event_lock.l, flags);
+#ifndef CONFIG_PREEMPT_RT
 	VM_BUG_ON(!irqs_disabled());
+#endif
 	mem_cgroup_charge_statistics(memcg, page, -nr_entries);
 	memcg_check_events(memcg, page);
+	local_unlock_irqrestore(&event_lock.l, flags);
 
 	css_put(&memcg->css);
 }
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 88639706ae17..97186fac7a03 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -61,6 +61,7 @@
 #include <linux/hugetlb.h>
 #include <linux/sched/rt.h>
 #include <linux/sched/mm.h>
+#include <linux/local_lock.h>
 #include <linux/page_owner.h>
 #include <linux/kthread.h>
 #include <linux/memcontrol.h>
@@ -386,6 +387,13 @@ EXPORT_SYMBOL(nr_node_ids);
 EXPORT_SYMBOL(nr_online_nodes);
 #endif
 
+struct pa_lock {
+	local_lock_t l;
+};
+static DEFINE_PER_CPU(struct pa_lock, pa_lock) = {
+	.l	= INIT_LOCAL_LOCK(l),
+};
+
 int page_group_by_mobility_disabled __read_mostly;
 
 #ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT
@@ -1331,7 +1339,7 @@ static inline void prefetch_buddy(struct page *page)
 }
 
 /*
- * Frees a number of pages from the PCP lists
+ * Frees a number of pages which have been collected from the pcp lists.
  * Assumes all pages on list are in same zone, and of same order.
  * count is the number of pages to free.
  *
@@ -1341,15 +1349,56 @@ static inline void prefetch_buddy(struct page *page)
  * And clear the zone's pages_scanned counter, to hold off the "all pages are
  * pinned" detection logic.
  */
-static void free_pcppages_bulk(struct zone *zone, int count,
-					struct per_cpu_pages *pcp)
+static void free_pcppages_bulk(struct zone *zone, struct list_head *head,
+			       bool zone_retry)
+{
+	bool isolated_pageblocks;
+	struct page *page, *tmp;
+	unsigned long flags;
+
+	spin_lock_irqsave(&zone->lock, flags);
+	isolated_pageblocks = has_isolate_pageblock(zone);
+
+	/*
+	 * Use safe version since after __free_one_page(),
+	 * page->lru.next will not point to original list.
+	 */
+	list_for_each_entry_safe(page, tmp, head, lru) {
+		int mt = get_pcppage_migratetype(page);
+
+		if (page_zone(page) != zone) {
+			/*
+			 * free_unref_page_list() sorts pages by zone. If we end
+			 * up with pages from a different NUMA nodes belonging
+			 * to the same ZONE index then we need to redo with the
+			 * correct ZONE pointer. Skip the page for now, redo it
+			 * on the next iteration.
+			 */
+			WARN_ON_ONCE(zone_retry == false);
+			if (zone_retry)
+				continue;
+		}
+
+		/* MIGRATE_ISOLATE page should not go to pcplists */
+		VM_BUG_ON_PAGE(is_migrate_isolate(mt), page);
+		/* Pageblock could have been isolated meanwhile */
+		if (unlikely(isolated_pageblocks))
+			mt = get_pageblock_migratetype(page);
+
+		list_del(&page->lru);
+		__free_one_page(page, page_to_pfn(page), zone, 0, mt, FPI_NONE);
+		trace_mm_page_pcpu_drain(page, 0, mt);
+	}
+	spin_unlock_irqrestore(&zone->lock, flags);
+}
+
+static void isolate_pcp_pages(int count, struct per_cpu_pages *pcp,
+			      struct list_head *dst)
 {
 	int migratetype = 0;
 	int batch_free = 0;
 	int prefetch_nr = 0;
-	bool isolated_pageblocks;
-	struct page *page, *tmp;
-	LIST_HEAD(head);
+	struct page *page;
 
 	/*
 	 * Ensure proper count is passed which otherwise would stuck in the
@@ -1386,7 +1435,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,
 			if (bulkfree_pcp_prepare(page))
 				continue;
 
-			list_add_tail(&page->lru, &head);
+			list_add_tail(&page->lru, dst);
 
 			/*
 			 * We are going to put the page back to the global
@@ -1401,26 +1450,6 @@ static void free_pcppages_bulk(struct zone *zone, int count,
 				prefetch_buddy(page);
 		} while (--count && --batch_free && !list_empty(list));
 	}
-
-	spin_lock(&zone->lock);
-	isolated_pageblocks = has_isolate_pageblock(zone);
-
-	/*
-	 * Use safe version since after __free_one_page(),
-	 * page->lru.next will not point to original list.
-	 */
-	list_for_each_entry_safe(page, tmp, &head, lru) {
-		int mt = get_pcppage_migratetype(page);
-		/* MIGRATE_ISOLATE page should not go to pcplists */
-		VM_BUG_ON_PAGE(is_migrate_isolate(mt), page);
-		/* Pageblock could have been isolated meanwhile */
-		if (unlikely(isolated_pageblocks))
-			mt = get_pageblock_migratetype(page);
-
-		__free_one_page(page, page_to_pfn(page), zone, 0, mt, FPI_NONE);
-		trace_mm_page_pcpu_drain(page, 0, mt);
-	}
-	spin_unlock(&zone->lock);
 }
 
 static void free_one_page(struct zone *zone,
@@ -1522,11 +1551,11 @@ static void __free_pages_ok(struct page *page, unsigned int order,
 		return;
 
 	migratetype = get_pfnblock_migratetype(page, pfn);
-	local_irq_save(flags);
+	local_lock_irqsave(&pa_lock.l, flags);
 	__count_vm_events(PGFREE, 1 << order);
 	free_one_page(page_zone(page), page, pfn, order, migratetype,
 		      fpi_flags);
-	local_irq_restore(flags);
+	local_unlock_irqrestore(&pa_lock.l, flags);
 }
 
 void __free_pages_core(struct page *page, unsigned int order)
@@ -2938,13 +2967,18 @@ void drain_zone_pages(struct zone *zone, struct per_cpu_pages *pcp)
 {
 	unsigned long flags;
 	int to_drain, batch;
+	LIST_HEAD(dst);
 
-	local_irq_save(flags);
+	local_lock_irqsave(&pa_lock.l, flags);
 	batch = READ_ONCE(pcp->batch);
 	to_drain = min(pcp->count, batch);
 	if (to_drain > 0)
-		free_pcppages_bulk(zone, to_drain, pcp);
-	local_irq_restore(flags);
+		isolate_pcp_pages(to_drain, pcp, &dst);
+
+	local_unlock_irqrestore(&pa_lock.l, flags);
+
+	if (to_drain > 0)
+		free_pcppages_bulk(zone, &dst, false);
 }
 #endif
 
@@ -2960,14 +2994,21 @@ static void drain_pages_zone(unsigned int cpu, struct zone *zone)
 	unsigned long flags;
 	struct per_cpu_pageset *pset;
 	struct per_cpu_pages *pcp;
+	LIST_HEAD(dst);
+	int count;
 
-	local_irq_save(flags);
+	local_lock_irqsave(&pa_lock.l, flags);
 	pset = per_cpu_ptr(zone->pageset, cpu);
 
 	pcp = &pset->pcp;
-	if (pcp->count)
-		free_pcppages_bulk(zone, pcp->count, pcp);
-	local_irq_restore(flags);
+	count = pcp->count;
+	if (count)
+		isolate_pcp_pages(count, pcp, &dst);
+
+	local_unlock_irqrestore(&pa_lock.l, flags);
+
+	if (count)
+		free_pcppages_bulk(zone, &dst, false);
 }
 
 /*
@@ -3015,9 +3056,9 @@ static void drain_local_pages_wq(struct work_struct *work)
 	 * cpu which is allright but we also have to make sure to not move to
 	 * a different one.
 	 */
-	preempt_disable();
+	migrate_disable();
 	drain_local_pages(drain->zone);
-	preempt_enable();
+	migrate_enable();
 }
 
 /*
@@ -3166,7 +3207,8 @@ static bool free_unref_page_prepare(struct page *page, unsigned long pfn)
 	return true;
 }
 
-static void free_unref_page_commit(struct page *page, unsigned long pfn)
+static void free_unref_page_commit(struct page *page, unsigned long pfn,
+				   struct list_head *dst)
 {
 	struct zone *zone = page_zone(page);
 	struct per_cpu_pages *pcp;
@@ -3196,7 +3238,8 @@ static void free_unref_page_commit(struct page *page, unsigned long pfn)
 	pcp->count++;
 	if (pcp->count >= pcp->high) {
 		unsigned long batch = READ_ONCE(pcp->batch);
-		free_pcppages_bulk(zone, batch, pcp);
+
+		isolate_pcp_pages(batch, pcp, dst);
 	}
 }
 
@@ -3207,13 +3250,17 @@ void free_unref_page(struct page *page)
 {
 	unsigned long flags;
 	unsigned long pfn = page_to_pfn(page);
+	struct zone *zone = page_zone(page);
+	LIST_HEAD(dst);
 
 	if (!free_unref_page_prepare(page, pfn))
 		return;
 
-	local_irq_save(flags);
-	free_unref_page_commit(page, pfn);
-	local_irq_restore(flags);
+	local_lock_irqsave(&pa_lock.l, flags);
+	free_unref_page_commit(page, pfn, &dst);
+	local_unlock_irqrestore(&pa_lock.l, flags);
+	if (!list_empty(&dst))
+		free_pcppages_bulk(zone, &dst, false);
 }
 
 /*
@@ -3224,6 +3271,11 @@ void free_unref_page_list(struct list_head *list)
 	struct page *page, *next;
 	unsigned long flags, pfn;
 	int batch_count = 0;
+	struct list_head dsts[__MAX_NR_ZONES];
+	int i;
+
+	for (i = 0; i < __MAX_NR_ZONES; i++)
+		INIT_LIST_HEAD(&dsts[i]);
 
 	/* Prepare pages for freeing */
 	list_for_each_entry_safe(page, next, list, lru) {
@@ -3233,25 +3285,42 @@ void free_unref_page_list(struct list_head *list)
 		set_page_private(page, pfn);
 	}
 
-	local_irq_save(flags);
+	local_lock_irqsave(&pa_lock.l, flags);
 	list_for_each_entry_safe(page, next, list, lru) {
 		unsigned long pfn = page_private(page);
+		enum zone_type type;
 
 		set_page_private(page, 0);
 		trace_mm_page_free_batched(page);
-		free_unref_page_commit(page, pfn);
+		type = page_zonenum(page);
+		free_unref_page_commit(page, pfn, &dsts[type]);
 
 		/*
 		 * Guard against excessive IRQ disabled times when we get
 		 * a large list of pages to free.
 		 */
 		if (++batch_count == SWAP_CLUSTER_MAX) {
-			local_irq_restore(flags);
+			local_unlock_irqrestore(&pa_lock.l, flags);
 			batch_count = 0;
-			local_irq_save(flags);
+			local_lock_irqsave(&pa_lock.l, flags);
 		}
 	}
-	local_irq_restore(flags);
+	local_unlock_irqrestore(&pa_lock.l, flags);
+
+	for (i = 0; i < __MAX_NR_ZONES; ) {
+		struct page *page;
+		struct zone *zone;
+
+		if (list_empty(&dsts[i])) {
+			i++;
+			continue;
+		}
+
+		page = list_first_entry(&dsts[i], struct page, lru);
+		zone = page_zone(page);
+
+		free_pcppages_bulk(zone, &dsts[i], true);
+	}
 }
 
 /*
@@ -3406,7 +3475,7 @@ static struct page *rmqueue_pcplist(struct zone *preferred_zone,
 	struct page *page;
 	unsigned long flags;
 
-	local_irq_save(flags);
+	local_lock_irqsave(&pa_lock.l, flags);
 	pcp = &this_cpu_ptr(zone->pageset)->pcp;
 	list = &pcp->lists[migratetype];
 	page = __rmqueue_pcplist(zone,  migratetype, alloc_flags, pcp, list);
@@ -3414,7 +3483,7 @@ static struct page *rmqueue_pcplist(struct zone *preferred_zone,
 		__count_zid_vm_events(PGALLOC, page_zonenum(page), 1);
 		zone_statistics(preferred_zone, zone);
 	}
-	local_irq_restore(flags);
+	local_unlock_irqrestore(&pa_lock.l, flags);
 	return page;
 }
 
@@ -3448,7 +3517,8 @@ struct page *rmqueue(struct zone *preferred_zone,
 	 * allocate greater than order-1 page units with __GFP_NOFAIL.
 	 */
 	WARN_ON_ONCE((gfp_flags & __GFP_NOFAIL) && (order > 1));
-	spin_lock_irqsave(&zone->lock, flags);
+	local_lock_irqsave(&pa_lock.l, flags);
+	spin_lock(&zone->lock);
 
 	do {
 		page = NULL;
@@ -3474,7 +3544,7 @@ struct page *rmqueue(struct zone *preferred_zone,
 
 	__count_zid_vm_events(PGALLOC, page_zonenum(page), 1 << order);
 	zone_statistics(preferred_zone, zone);
-	local_irq_restore(flags);
+	local_unlock_irqrestore(&pa_lock.l, flags);
 
 out:
 	/* Separate test+clear to avoid unnecessary atomics */
@@ -3487,7 +3557,7 @@ struct page *rmqueue(struct zone *preferred_zone,
 	return page;
 
 failed:
-	local_irq_restore(flags);
+	local_unlock_irqrestore(&pa_lock.l, flags);
 	return NULL;
 }
 
@@ -8740,7 +8810,7 @@ void zone_pcp_reset(struct zone *zone)
 	struct per_cpu_pageset *pset;
 
 	/* avoid races with drain_pages()  */
-	local_irq_save(flags);
+	local_lock_irqsave(&pa_lock.l, flags);
 	if (zone->pageset != &boot_pageset) {
 		for_each_online_cpu(cpu) {
 			pset = per_cpu_ptr(zone->pageset, cpu);
@@ -8749,7 +8819,7 @@ void zone_pcp_reset(struct zone *zone)
 		free_percpu(zone->pageset);
 		zone->pageset = &boot_pageset;
 	}
-	local_irq_restore(flags);
+	local_unlock_irqrestore(&pa_lock.l, flags);
 }
 
 #ifdef CONFIG_MEMORY_HOTREMOVE
diff --git a/mm/shmem.c b/mm/shmem.c
index 537c137698f8..1c473d6123bc 100644
--- a/mm/shmem.c
+++ b/mm/shmem.c
@@ -278,10 +278,10 @@ static int shmem_reserve_inode(struct super_block *sb, ino_t *inop)
 	ino_t ino;
 
 	if (!(sb->s_flags & SB_KERNMOUNT)) {
-		spin_lock(&sbinfo->stat_lock);
+		raw_spin_lock(&sbinfo->stat_lock);
 		if (sbinfo->max_inodes) {
 			if (!sbinfo->free_inodes) {
-				spin_unlock(&sbinfo->stat_lock);
+				raw_spin_unlock(&sbinfo->stat_lock);
 				return -ENOSPC;
 			}
 			sbinfo->free_inodes--;
@@ -304,7 +304,7 @@ static int shmem_reserve_inode(struct super_block *sb, ino_t *inop)
 			}
 			*inop = ino;
 		}
-		spin_unlock(&sbinfo->stat_lock);
+		raw_spin_unlock(&sbinfo->stat_lock);
 	} else if (inop) {
 		/*
 		 * __shmem_file_setup, one of our callers, is lock-free: it
@@ -319,13 +319,14 @@ static int shmem_reserve_inode(struct super_block *sb, ino_t *inop)
 		 * to worry about things like glibc compatibility.
 		 */
 		ino_t *next_ino;
+
 		next_ino = per_cpu_ptr(sbinfo->ino_batch, get_cpu());
 		ino = *next_ino;
 		if (unlikely(ino % SHMEM_INO_BATCH == 0)) {
-			spin_lock(&sbinfo->stat_lock);
+			raw_spin_lock(&sbinfo->stat_lock);
 			ino = sbinfo->next_ino;
 			sbinfo->next_ino += SHMEM_INO_BATCH;
-			spin_unlock(&sbinfo->stat_lock);
+			raw_spin_unlock(&sbinfo->stat_lock);
 			if (unlikely(is_zero_ino(ino)))
 				ino++;
 		}
@@ -341,9 +342,9 @@ static void shmem_free_inode(struct super_block *sb)
 {
 	struct shmem_sb_info *sbinfo = SHMEM_SB(sb);
 	if (sbinfo->max_inodes) {
-		spin_lock(&sbinfo->stat_lock);
+		raw_spin_lock(&sbinfo->stat_lock);
 		sbinfo->free_inodes++;
-		spin_unlock(&sbinfo->stat_lock);
+		raw_spin_unlock(&sbinfo->stat_lock);
 	}
 }
 
@@ -1479,10 +1480,10 @@ static struct mempolicy *shmem_get_sbmpol(struct shmem_sb_info *sbinfo)
 {
 	struct mempolicy *mpol = NULL;
 	if (sbinfo->mpol) {
-		spin_lock(&sbinfo->stat_lock);	/* prevent replace/use races */
+		raw_spin_lock(&sbinfo->stat_lock);	/* prevent replace/use races */
 		mpol = sbinfo->mpol;
 		mpol_get(mpol);
-		spin_unlock(&sbinfo->stat_lock);
+		raw_spin_unlock(&sbinfo->stat_lock);
 	}
 	return mpol;
 }
@@ -3592,9 +3593,10 @@ static int shmem_reconfigure(struct fs_context *fc)
 	struct shmem_options *ctx = fc->fs_private;
 	struct shmem_sb_info *sbinfo = SHMEM_SB(fc->root->d_sb);
 	unsigned long inodes;
+	struct mempolicy *mpol = NULL;
 	const char *err;
 
-	spin_lock(&sbinfo->stat_lock);
+	raw_spin_lock(&sbinfo->stat_lock);
 	inodes = sbinfo->max_inodes - sbinfo->free_inodes;
 	if ((ctx->seen & SHMEM_SEEN_BLOCKS) && ctx->blocks) {
 		if (!sbinfo->max_blocks) {
@@ -3639,14 +3641,15 @@ static int shmem_reconfigure(struct fs_context *fc)
 	 * Preserve previous mempolicy unless mpol remount option was specified.
 	 */
 	if (ctx->mpol) {
-		mpol_put(sbinfo->mpol);
+		mpol = sbinfo->mpol;
 		sbinfo->mpol = ctx->mpol;	/* transfers initial ref */
 		ctx->mpol = NULL;
 	}
-	spin_unlock(&sbinfo->stat_lock);
+	raw_spin_unlock(&sbinfo->stat_lock);
+	mpol_put(mpol);
 	return 0;
 out:
-	spin_unlock(&sbinfo->stat_lock);
+	raw_spin_unlock(&sbinfo->stat_lock);
 	return invalfc(fc, "%s", err);
 }
 
@@ -3763,7 +3766,7 @@ static int shmem_fill_super(struct super_block *sb, struct fs_context *fc)
 	sbinfo->mpol = ctx->mpol;
 	ctx->mpol = NULL;
 
-	spin_lock_init(&sbinfo->stat_lock);
+	raw_spin_lock_init(&sbinfo->stat_lock);
 	if (percpu_counter_init(&sbinfo->used_blocks, 0, GFP_KERNEL))
 		goto failed;
 	spin_lock_init(&sbinfo->shrinklist_lock);
diff --git a/mm/slab.c b/mm/slab.c
index b1113561b98b..a28b54325d9e 100644
--- a/mm/slab.c
+++ b/mm/slab.c
@@ -233,7 +233,7 @@ static void kmem_cache_node_init(struct kmem_cache_node *parent)
 	parent->shared = NULL;
 	parent->alien = NULL;
 	parent->colour_next = 0;
-	spin_lock_init(&parent->list_lock);
+	raw_spin_lock_init(&parent->list_lock);
 	parent->free_objects = 0;
 	parent->free_touched = 0;
 }
@@ -558,9 +558,9 @@ static noinline void cache_free_pfmemalloc(struct kmem_cache *cachep,
 	page_node = page_to_nid(page);
 	n = get_node(cachep, page_node);
 
-	spin_lock(&n->list_lock);
+	raw_spin_lock(&n->list_lock);
 	free_block(cachep, &objp, 1, page_node, &list);
-	spin_unlock(&n->list_lock);
+	raw_spin_unlock(&n->list_lock);
 
 	slabs_destroy(cachep, &list);
 }
@@ -698,7 +698,7 @@ static void __drain_alien_cache(struct kmem_cache *cachep,
 	struct kmem_cache_node *n = get_node(cachep, node);
 
 	if (ac->avail) {
-		spin_lock(&n->list_lock);
+		raw_spin_lock(&n->list_lock);
 		/*
 		 * Stuff objects into the remote nodes shared array first.
 		 * That way we could avoid the overhead of putting the objects
@@ -709,7 +709,7 @@ static void __drain_alien_cache(struct kmem_cache *cachep,
 
 		free_block(cachep, ac->entry, ac->avail, node, list);
 		ac->avail = 0;
-		spin_unlock(&n->list_lock);
+		raw_spin_unlock(&n->list_lock);
 	}
 }
 
@@ -782,9 +782,9 @@ static int __cache_free_alien(struct kmem_cache *cachep, void *objp,
 		slabs_destroy(cachep, &list);
 	} else {
 		n = get_node(cachep, page_node);
-		spin_lock(&n->list_lock);
+		raw_spin_lock(&n->list_lock);
 		free_block(cachep, &objp, 1, page_node, &list);
-		spin_unlock(&n->list_lock);
+		raw_spin_unlock(&n->list_lock);
 		slabs_destroy(cachep, &list);
 	}
 	return 1;
@@ -825,10 +825,10 @@ static int init_cache_node(struct kmem_cache *cachep, int node, gfp_t gfp)
 	 */
 	n = get_node(cachep, node);
 	if (n) {
-		spin_lock_irq(&n->list_lock);
+		raw_spin_lock_irq(&n->list_lock);
 		n->free_limit = (1 + nr_cpus_node(node)) * cachep->batchcount +
 				cachep->num;
-		spin_unlock_irq(&n->list_lock);
+		raw_spin_unlock_irq(&n->list_lock);
 
 		return 0;
 	}
@@ -907,7 +907,7 @@ static int setup_kmem_cache_node(struct kmem_cache *cachep,
 		goto fail;
 
 	n = get_node(cachep, node);
-	spin_lock_irq(&n->list_lock);
+	raw_spin_lock_irq(&n->list_lock);
 	if (n->shared && force_change) {
 		free_block(cachep, n->shared->entry,
 				n->shared->avail, node, &list);
@@ -925,7 +925,7 @@ static int setup_kmem_cache_node(struct kmem_cache *cachep,
 		new_alien = NULL;
 	}
 
-	spin_unlock_irq(&n->list_lock);
+	raw_spin_unlock_irq(&n->list_lock);
 	slabs_destroy(cachep, &list);
 
 	/*
@@ -964,7 +964,7 @@ static void cpuup_canceled(long cpu)
 		if (!n)
 			continue;
 
-		spin_lock_irq(&n->list_lock);
+		raw_spin_lock_irq(&n->list_lock);
 
 		/* Free limit for this kmem_cache_node */
 		n->free_limit -= cachep->batchcount;
@@ -975,7 +975,7 @@ static void cpuup_canceled(long cpu)
 		nc->avail = 0;
 
 		if (!cpumask_empty(mask)) {
-			spin_unlock_irq(&n->list_lock);
+			raw_spin_unlock_irq(&n->list_lock);
 			goto free_slab;
 		}
 
@@ -989,7 +989,7 @@ static void cpuup_canceled(long cpu)
 		alien = n->alien;
 		n->alien = NULL;
 
-		spin_unlock_irq(&n->list_lock);
+		raw_spin_unlock_irq(&n->list_lock);
 
 		kfree(shared);
 		if (alien) {
@@ -1173,7 +1173,7 @@ static void __init init_list(struct kmem_cache *cachep, struct kmem_cache_node *
 	/*
 	 * Do not assume that spinlocks can be initialized via memcpy:
 	 */
-	spin_lock_init(&ptr->list_lock);
+	raw_spin_lock_init(&ptr->list_lock);
 
 	MAKE_ALL_LISTS(cachep, ptr, nodeid);
 	cachep->node[nodeid] = ptr;
@@ -1344,11 +1344,11 @@ slab_out_of_memory(struct kmem_cache *cachep, gfp_t gfpflags, int nodeid)
 	for_each_kmem_cache_node(cachep, node, n) {
 		unsigned long total_slabs, free_slabs, free_objs;
 
-		spin_lock_irqsave(&n->list_lock, flags);
+		raw_spin_lock_irqsave(&n->list_lock, flags);
 		total_slabs = n->total_slabs;
 		free_slabs = n->free_slabs;
 		free_objs = n->free_objects;
-		spin_unlock_irqrestore(&n->list_lock, flags);
+		raw_spin_unlock_irqrestore(&n->list_lock, flags);
 
 		pr_warn("  node %d: slabs: %ld/%ld, objs: %ld/%ld\n",
 			node, total_slabs - free_slabs, total_slabs,
@@ -2106,7 +2106,7 @@ static void check_spinlock_acquired(struct kmem_cache *cachep)
 {
 #ifdef CONFIG_SMP
 	check_irq_off();
-	assert_spin_locked(&get_node(cachep, numa_mem_id())->list_lock);
+	assert_raw_spin_locked(&get_node(cachep, numa_mem_id())->list_lock);
 #endif
 }
 
@@ -2114,7 +2114,7 @@ static void check_spinlock_acquired_node(struct kmem_cache *cachep, int node)
 {
 #ifdef CONFIG_SMP
 	check_irq_off();
-	assert_spin_locked(&get_node(cachep, node)->list_lock);
+	assert_raw_spin_locked(&get_node(cachep, node)->list_lock);
 #endif
 }
 
@@ -2154,9 +2154,9 @@ static void do_drain(void *arg)
 	check_irq_off();
 	ac = cpu_cache_get(cachep);
 	n = get_node(cachep, node);
-	spin_lock(&n->list_lock);
+	raw_spin_lock(&n->list_lock);
 	free_block(cachep, ac->entry, ac->avail, node, &list);
-	spin_unlock(&n->list_lock);
+	raw_spin_unlock(&n->list_lock);
 	ac->avail = 0;
 	slabs_destroy(cachep, &list);
 }
@@ -2174,9 +2174,9 @@ static void drain_cpu_caches(struct kmem_cache *cachep)
 			drain_alien_cache(cachep, n->alien);
 
 	for_each_kmem_cache_node(cachep, node, n) {
-		spin_lock_irq(&n->list_lock);
+		raw_spin_lock_irq(&n->list_lock);
 		drain_array_locked(cachep, n->shared, node, true, &list);
-		spin_unlock_irq(&n->list_lock);
+		raw_spin_unlock_irq(&n->list_lock);
 
 		slabs_destroy(cachep, &list);
 	}
@@ -2198,10 +2198,10 @@ static int drain_freelist(struct kmem_cache *cache,
 	nr_freed = 0;
 	while (nr_freed < tofree && !list_empty(&n->slabs_free)) {
 
-		spin_lock_irq(&n->list_lock);
+		raw_spin_lock_irq(&n->list_lock);
 		p = n->slabs_free.prev;
 		if (p == &n->slabs_free) {
-			spin_unlock_irq(&n->list_lock);
+			raw_spin_unlock_irq(&n->list_lock);
 			goto out;
 		}
 
@@ -2214,7 +2214,7 @@ static int drain_freelist(struct kmem_cache *cache,
 		 * to the cache.
 		 */
 		n->free_objects -= cache->num;
-		spin_unlock_irq(&n->list_lock);
+		raw_spin_unlock_irq(&n->list_lock);
 		slab_destroy(cache, page);
 		nr_freed++;
 	}
@@ -2650,7 +2650,7 @@ static void cache_grow_end(struct kmem_cache *cachep, struct page *page)
 	INIT_LIST_HEAD(&page->slab_list);
 	n = get_node(cachep, page_to_nid(page));
 
-	spin_lock(&n->list_lock);
+	raw_spin_lock(&n->list_lock);
 	n->total_slabs++;
 	if (!page->active) {
 		list_add_tail(&page->slab_list, &n->slabs_free);
@@ -2660,7 +2660,7 @@ static void cache_grow_end(struct kmem_cache *cachep, struct page *page)
 
 	STATS_INC_GROWN(cachep);
 	n->free_objects += cachep->num - page->active;
-	spin_unlock(&n->list_lock);
+	raw_spin_unlock(&n->list_lock);
 
 	fixup_objfreelist_debug(cachep, &list);
 }
@@ -2826,7 +2826,7 @@ static struct page *get_first_slab(struct kmem_cache_node *n, bool pfmemalloc)
 {
 	struct page *page;
 
-	assert_spin_locked(&n->list_lock);
+	assert_raw_spin_locked(&n->list_lock);
 	page = list_first_entry_or_null(&n->slabs_partial, struct page,
 					slab_list);
 	if (!page) {
@@ -2853,10 +2853,10 @@ static noinline void *cache_alloc_pfmemalloc(struct kmem_cache *cachep,
 	if (!gfp_pfmemalloc_allowed(flags))
 		return NULL;
 
-	spin_lock(&n->list_lock);
+	raw_spin_lock(&n->list_lock);
 	page = get_first_slab(n, true);
 	if (!page) {
-		spin_unlock(&n->list_lock);
+		raw_spin_unlock(&n->list_lock);
 		return NULL;
 	}
 
@@ -2865,7 +2865,7 @@ static noinline void *cache_alloc_pfmemalloc(struct kmem_cache *cachep,
 
 	fixup_slab_list(cachep, n, page, &list);
 
-	spin_unlock(&n->list_lock);
+	raw_spin_unlock(&n->list_lock);
 	fixup_objfreelist_debug(cachep, &list);
 
 	return obj;
@@ -2924,7 +2924,7 @@ static void *cache_alloc_refill(struct kmem_cache *cachep, gfp_t flags)
 	if (!n->free_objects && (!shared || !shared->avail))
 		goto direct_grow;
 
-	spin_lock(&n->list_lock);
+	raw_spin_lock(&n->list_lock);
 	shared = READ_ONCE(n->shared);
 
 	/* See if we can refill from the shared array */
@@ -2948,7 +2948,7 @@ static void *cache_alloc_refill(struct kmem_cache *cachep, gfp_t flags)
 must_grow:
 	n->free_objects -= ac->avail;
 alloc_done:
-	spin_unlock(&n->list_lock);
+	raw_spin_unlock(&n->list_lock);
 	fixup_objfreelist_debug(cachep, &list);
 
 direct_grow:
@@ -3173,7 +3173,7 @@ static void *____cache_alloc_node(struct kmem_cache *cachep, gfp_t flags,
 	BUG_ON(!n);
 
 	check_irq_off();
-	spin_lock(&n->list_lock);
+	raw_spin_lock(&n->list_lock);
 	page = get_first_slab(n, false);
 	if (!page)
 		goto must_grow;
@@ -3191,12 +3191,12 @@ static void *____cache_alloc_node(struct kmem_cache *cachep, gfp_t flags,
 
 	fixup_slab_list(cachep, n, page, &list);
 
-	spin_unlock(&n->list_lock);
+	raw_spin_unlock(&n->list_lock);
 	fixup_objfreelist_debug(cachep, &list);
 	return obj;
 
 must_grow:
-	spin_unlock(&n->list_lock);
+	raw_spin_unlock(&n->list_lock);
 	page = cache_grow_begin(cachep, gfp_exact_node(flags), nodeid);
 	if (page) {
 		/* This slab isn't counted yet so don't update free_objects */
@@ -3374,7 +3374,7 @@ static void cache_flusharray(struct kmem_cache *cachep, struct array_cache *ac)
 
 	check_irq_off();
 	n = get_node(cachep, node);
-	spin_lock(&n->list_lock);
+	raw_spin_lock(&n->list_lock);
 	if (n->shared) {
 		struct array_cache *shared_array = n->shared;
 		int max = shared_array->limit - shared_array->avail;
@@ -3403,7 +3403,7 @@ static void cache_flusharray(struct kmem_cache *cachep, struct array_cache *ac)
 		STATS_SET_FREEABLE(cachep, i);
 	}
 #endif
-	spin_unlock(&n->list_lock);
+	raw_spin_unlock(&n->list_lock);
 	ac->avail -= batchcount;
 	memmove(ac->entry, &(ac->entry[batchcount]), sizeof(void *)*ac->avail);
 	slabs_destroy(cachep, &list);
@@ -3832,9 +3832,9 @@ static int do_tune_cpucache(struct kmem_cache *cachep, int limit,
 
 		node = cpu_to_mem(cpu);
 		n = get_node(cachep, node);
-		spin_lock_irq(&n->list_lock);
+		raw_spin_lock_irq(&n->list_lock);
 		free_block(cachep, ac->entry, ac->avail, node, &list);
-		spin_unlock_irq(&n->list_lock);
+		raw_spin_unlock_irq(&n->list_lock);
 		slabs_destroy(cachep, &list);
 	}
 	free_percpu(prev);
@@ -3929,9 +3929,9 @@ static void drain_array(struct kmem_cache *cachep, struct kmem_cache_node *n,
 		return;
 	}
 
-	spin_lock_irq(&n->list_lock);
+	raw_spin_lock_irq(&n->list_lock);
 	drain_array_locked(cachep, ac, node, false, &list);
-	spin_unlock_irq(&n->list_lock);
+	raw_spin_unlock_irq(&n->list_lock);
 
 	slabs_destroy(cachep, &list);
 }
@@ -4015,7 +4015,7 @@ void get_slabinfo(struct kmem_cache *cachep, struct slabinfo *sinfo)
 
 	for_each_kmem_cache_node(cachep, node, n) {
 		check_irq_on();
-		spin_lock_irq(&n->list_lock);
+		raw_spin_lock_irq(&n->list_lock);
 
 		total_slabs += n->total_slabs;
 		free_slabs += n->free_slabs;
@@ -4024,7 +4024,7 @@ void get_slabinfo(struct kmem_cache *cachep, struct slabinfo *sinfo)
 		if (n->shared)
 			shared_avail += n->shared->avail;
 
-		spin_unlock_irq(&n->list_lock);
+		raw_spin_unlock_irq(&n->list_lock);
 	}
 	num_objs = total_slabs * cachep->num;
 	active_slabs = total_slabs - free_slabs;
diff --git a/mm/slab.h b/mm/slab.h
index f9977d6613d6..c9a43b787609 100644
--- a/mm/slab.h
+++ b/mm/slab.h
@@ -546,7 +546,7 @@ static inline void slab_post_alloc_hook(struct kmem_cache *s,
  * The slab lists for all objects.
  */
 struct kmem_cache_node {
-	spinlock_t list_lock;
+	raw_spinlock_t list_lock;
 
 #ifdef CONFIG_SLAB
 	struct list_head slabs_partial;	/* partial list first, better asm code */
diff --git a/mm/slub.c b/mm/slub.c
index 7b378e2ce270..15690db5223e 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -434,7 +434,7 @@ static inline bool cmpxchg_double_slab(struct kmem_cache *s, struct page *page,
 
 #ifdef CONFIG_SLUB_DEBUG
 static unsigned long object_map[BITS_TO_LONGS(MAX_OBJS_PER_PAGE)];
-static DEFINE_SPINLOCK(object_map_lock);
+static DEFINE_RAW_SPINLOCK(object_map_lock);
 
 /*
  * Determine a map of object in use on a page.
@@ -450,7 +450,7 @@ static unsigned long *get_map(struct kmem_cache *s, struct page *page)
 
 	VM_BUG_ON(!irqs_disabled());
 
-	spin_lock(&object_map_lock);
+	raw_spin_lock(&object_map_lock);
 
 	bitmap_zero(object_map, page->objects);
 
@@ -463,7 +463,7 @@ static unsigned long *get_map(struct kmem_cache *s, struct page *page)
 static void put_map(unsigned long *map) __releases(&object_map_lock)
 {
 	VM_BUG_ON(map != object_map);
-	spin_unlock(&object_map_lock);
+	raw_spin_unlock(&object_map_lock);
 }
 
 static inline unsigned int size_from_object(struct kmem_cache *s)
@@ -1213,7 +1213,7 @@ static noinline int free_debug_processing(
 	unsigned long flags;
 	int ret = 0;
 
-	spin_lock_irqsave(&n->list_lock, flags);
+	raw_spin_lock_irqsave(&n->list_lock, flags);
 	slab_lock(page);
 
 	if (s->flags & SLAB_CONSISTENCY_CHECKS) {
@@ -1248,7 +1248,7 @@ static noinline int free_debug_processing(
 			 bulk_cnt, cnt);
 
 	slab_unlock(page);
-	spin_unlock_irqrestore(&n->list_lock, flags);
+	raw_spin_unlock_irqrestore(&n->list_lock, flags);
 	if (!ret)
 		slab_fix(s, "Object at 0x%p not freed", object);
 	return ret;
@@ -1496,6 +1496,12 @@ static bool freelist_corrupted(struct kmem_cache *s, struct page *page,
 }
 #endif /* CONFIG_SLUB_DEBUG */
 
+struct slub_free_list {
+	raw_spinlock_t		lock;
+	struct list_head	list;
+};
+static DEFINE_PER_CPU(struct slub_free_list, slub_free_list);
+
 /*
  * Hooks for other subsystems that check memory allocations. In a typical
  * production configuration these hooks all should produce no code at all.
@@ -1739,10 +1745,18 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
 	void *start, *p, *next;
 	int idx;
 	bool shuffle;
+	bool enableirqs = false;
 
 	flags &= gfp_allowed_mask;
 
 	if (gfpflags_allow_blocking(flags))
+		enableirqs = true;
+
+#ifdef CONFIG_PREEMPT_RT
+	if (system_state > SYSTEM_BOOTING && system_state < SYSTEM_SUSPEND)
+		enableirqs = true;
+#endif
+	if (enableirqs)
 		local_irq_enable();
 
 	flags |= s->allocflags;
@@ -1801,7 +1815,7 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
 	page->frozen = 1;
 
 out:
-	if (gfpflags_allow_blocking(flags))
+	if (enableirqs)
 		local_irq_disable();
 	if (!page)
 		return NULL;
@@ -1844,6 +1858,16 @@ static void __free_slab(struct kmem_cache *s, struct page *page)
 	__free_pages(page, order);
 }
 
+static void free_delayed(struct list_head *h)
+{
+	while (!list_empty(h)) {
+		struct page *page = list_first_entry(h, struct page, lru);
+
+		list_del(&page->lru);
+		__free_slab(page->slab_cache, page);
+	}
+}
+
 static void rcu_free_slab(struct rcu_head *h)
 {
 	struct page *page = container_of(h, struct page, rcu_head);
@@ -1855,6 +1879,12 @@ static void free_slab(struct kmem_cache *s, struct page *page)
 {
 	if (unlikely(s->flags & SLAB_TYPESAFE_BY_RCU)) {
 		call_rcu(&page->rcu_head, rcu_free_slab);
+	} else if (irqs_disabled()) {
+		struct slub_free_list *f = this_cpu_ptr(&slub_free_list);
+
+		raw_spin_lock(&f->lock);
+		list_add(&page->lru, &f->list);
+		raw_spin_unlock(&f->lock);
 	} else
 		__free_slab(s, page);
 }
@@ -1962,7 +1992,7 @@ static void *get_partial_node(struct kmem_cache *s, struct kmem_cache_node *n,
 	if (!n || !n->nr_partial)
 		return NULL;
 
-	spin_lock(&n->list_lock);
+	raw_spin_lock(&n->list_lock);
 	list_for_each_entry_safe(page, page2, &n->partial, slab_list) {
 		void *t;
 
@@ -1987,7 +2017,7 @@ static void *get_partial_node(struct kmem_cache *s, struct kmem_cache_node *n,
 			break;
 
 	}
-	spin_unlock(&n->list_lock);
+	raw_spin_unlock(&n->list_lock);
 	return object;
 }
 
@@ -2241,7 +2271,7 @@ static void deactivate_slab(struct kmem_cache *s, struct page *page,
 			 * that acquire_slab() will see a slab page that
 			 * is frozen
 			 */
-			spin_lock(&n->list_lock);
+			raw_spin_lock(&n->list_lock);
 		}
 	} else {
 		m = M_FULL;
@@ -2253,7 +2283,7 @@ static void deactivate_slab(struct kmem_cache *s, struct page *page,
 			 * slabs from diagnostic functions will not see
 			 * any frozen slabs.
 			 */
-			spin_lock(&n->list_lock);
+			raw_spin_lock(&n->list_lock);
 		}
 #endif
 	}
@@ -2278,7 +2308,7 @@ static void deactivate_slab(struct kmem_cache *s, struct page *page,
 		goto redo;
 
 	if (lock)
-		spin_unlock(&n->list_lock);
+		raw_spin_unlock(&n->list_lock);
 
 	if (m == M_PARTIAL)
 		stat(s, tail);
@@ -2317,10 +2347,10 @@ static void unfreeze_partials(struct kmem_cache *s,
 		n2 = get_node(s, page_to_nid(page));
 		if (n != n2) {
 			if (n)
-				spin_unlock(&n->list_lock);
+				raw_spin_unlock(&n->list_lock);
 
 			n = n2;
-			spin_lock(&n->list_lock);
+			raw_spin_lock(&n->list_lock);
 		}
 
 		do {
@@ -2349,7 +2379,7 @@ static void unfreeze_partials(struct kmem_cache *s,
 	}
 
 	if (n)
-		spin_unlock(&n->list_lock);
+		raw_spin_unlock(&n->list_lock);
 
 	while (discard_page) {
 		page = discard_page;
@@ -2386,14 +2416,21 @@ static void put_cpu_partial(struct kmem_cache *s, struct page *page, int drain)
 			pobjects = oldpage->pobjects;
 			pages = oldpage->pages;
 			if (drain && pobjects > slub_cpu_partial(s)) {
+				struct slub_free_list *f;
 				unsigned long flags;
+				LIST_HEAD(tofree);
 				/*
 				 * partial array is full. Move the existing
 				 * set to the per node partial list.
 				 */
 				local_irq_save(flags);
 				unfreeze_partials(s, this_cpu_ptr(s->cpu_slab));
+				f = this_cpu_ptr(&slub_free_list);
+				raw_spin_lock(&f->lock);
+				list_splice_init(&f->list, &tofree);
+				raw_spin_unlock(&f->lock);
 				local_irq_restore(flags);
+				free_delayed(&tofree);
 				oldpage = NULL;
 				pobjects = 0;
 				pages = 0;
@@ -2461,7 +2498,19 @@ static bool has_cpu_slab(int cpu, void *info)
 
 static void flush_all(struct kmem_cache *s)
 {
+	LIST_HEAD(tofree);
+	int cpu;
+
 	on_each_cpu_cond(has_cpu_slab, flush_cpu_slab, s, 1);
+	for_each_online_cpu(cpu) {
+		struct slub_free_list *f;
+
+		f = &per_cpu(slub_free_list, cpu);
+		raw_spin_lock_irq(&f->lock);
+		list_splice_init(&f->list, &tofree);
+		raw_spin_unlock_irq(&f->lock);
+		free_delayed(&tofree);
+	}
 }
 
 /*
@@ -2516,10 +2565,10 @@ static unsigned long count_partial(struct kmem_cache_node *n,
 	unsigned long x = 0;
 	struct page *page;
 
-	spin_lock_irqsave(&n->list_lock, flags);
+	raw_spin_lock_irqsave(&n->list_lock, flags);
 	list_for_each_entry(page, &n->partial, slab_list)
 		x += get_count(page);
-	spin_unlock_irqrestore(&n->list_lock, flags);
+	raw_spin_unlock_irqrestore(&n->list_lock, flags);
 	return x;
 }
 #endif /* CONFIG_SLUB_DEBUG || CONFIG_SYSFS */
@@ -2658,8 +2707,10 @@ static inline void *get_freelist(struct kmem_cache *s, struct page *page)
  * already disabled (which is the case for bulk allocation).
  */
 static void *___slab_alloc(struct kmem_cache *s, gfp_t gfpflags, int node,
-			  unsigned long addr, struct kmem_cache_cpu *c)
+			  unsigned long addr, struct kmem_cache_cpu *c,
+			  struct list_head *to_free)
 {
+	struct slub_free_list *f;
 	void *freelist;
 	struct page *page;
 
@@ -2727,6 +2778,13 @@ static void *___slab_alloc(struct kmem_cache *s, gfp_t gfpflags, int node,
 	VM_BUG_ON(!c->page->frozen);
 	c->freelist = get_freepointer(s, freelist);
 	c->tid = next_tid(c->tid);
+
+out:
+	f = this_cpu_ptr(&slub_free_list);
+	raw_spin_lock(&f->lock);
+	list_splice_init(&f->list, to_free);
+	raw_spin_unlock(&f->lock);
+
 	return freelist;
 
 new_slab:
@@ -2742,7 +2800,7 @@ static void *___slab_alloc(struct kmem_cache *s, gfp_t gfpflags, int node,
 
 	if (unlikely(!freelist)) {
 		slab_out_of_memory(s, gfpflags, node);
-		return NULL;
+		goto out;
 	}
 
 	page = c->page;
@@ -2755,7 +2813,7 @@ static void *___slab_alloc(struct kmem_cache *s, gfp_t gfpflags, int node,
 		goto new_slab;	/* Slab failed checks. Next slab needed */
 
 	deactivate_slab(s, page, get_freepointer(s, freelist), c);
-	return freelist;
+	goto out;
 }
 
 /*
@@ -2767,6 +2825,7 @@ static void *__slab_alloc(struct kmem_cache *s, gfp_t gfpflags, int node,
 {
 	void *p;
 	unsigned long flags;
+	LIST_HEAD(tofree);
 
 	local_irq_save(flags);
 #ifdef CONFIG_PREEMPTION
@@ -2778,8 +2837,9 @@ static void *__slab_alloc(struct kmem_cache *s, gfp_t gfpflags, int node,
 	c = this_cpu_ptr(s->cpu_slab);
 #endif
 
-	p = ___slab_alloc(s, gfpflags, node, addr, c);
+	p = ___slab_alloc(s, gfpflags, node, addr, c, &tofree);
 	local_irq_restore(flags);
+	free_delayed(&tofree);
 	return p;
 }
 
@@ -2813,6 +2873,10 @@ static __always_inline void *slab_alloc_node(struct kmem_cache *s,
 	unsigned long tid;
 	struct obj_cgroup *objcg = NULL;
 
+	if (IS_ENABLED(CONFIG_PREEMPT_RT) && IS_ENABLED(CONFIG_DEBUG_ATOMIC_SLEEP))
+		WARN_ON_ONCE(!preemptible() &&
+			     (system_state > SYSTEM_BOOTING && system_state < SYSTEM_SUSPEND));
+
 	s = slab_pre_alloc_hook(s, &objcg, 1, gfpflags);
 	if (!s)
 		return NULL;
@@ -2978,7 +3042,7 @@ static void __slab_free(struct kmem_cache *s, struct page *page,
 
 	do {
 		if (unlikely(n)) {
-			spin_unlock_irqrestore(&n->list_lock, flags);
+			raw_spin_unlock_irqrestore(&n->list_lock, flags);
 			n = NULL;
 		}
 		prior = page->freelist;
@@ -3010,7 +3074,7 @@ static void __slab_free(struct kmem_cache *s, struct page *page,
 				 * Otherwise the list_lock will synchronize with
 				 * other processors updating the list of slabs.
 				 */
-				spin_lock_irqsave(&n->list_lock, flags);
+				raw_spin_lock_irqsave(&n->list_lock, flags);
 
 			}
 		}
@@ -3052,7 +3116,7 @@ static void __slab_free(struct kmem_cache *s, struct page *page,
 		add_partial(n, page, DEACTIVATE_TO_TAIL);
 		stat(s, FREE_ADD_PARTIAL);
 	}
-	spin_unlock_irqrestore(&n->list_lock, flags);
+	raw_spin_unlock_irqrestore(&n->list_lock, flags);
 	return;
 
 slab_empty:
@@ -3067,7 +3131,7 @@ static void __slab_free(struct kmem_cache *s, struct page *page,
 		remove_full(s, n, page);
 	}
 
-	spin_unlock_irqrestore(&n->list_lock, flags);
+	raw_spin_unlock_irqrestore(&n->list_lock, flags);
 	stat(s, FREE_SLAB);
 	discard_slab(s, page);
 }
@@ -3275,9 +3339,14 @@ int kmem_cache_alloc_bulk(struct kmem_cache *s, gfp_t flags, size_t size,
 			  void **p)
 {
 	struct kmem_cache_cpu *c;
+	LIST_HEAD(to_free);
 	int i;
 	struct obj_cgroup *objcg = NULL;
 
+	if (IS_ENABLED(CONFIG_PREEMPT_RT) && IS_ENABLED(CONFIG_DEBUG_ATOMIC_SLEEP))
+		WARN_ON_ONCE(!preemptible() &&
+			     (system_state > SYSTEM_BOOTING && system_state < SYSTEM_SUSPEND));
+
 	/* memcg and kmem_cache debug support */
 	s = slab_pre_alloc_hook(s, &objcg, size, flags);
 	if (unlikely(!s))
@@ -3308,7 +3377,7 @@ int kmem_cache_alloc_bulk(struct kmem_cache *s, gfp_t flags, size_t size,
 			 * of re-populating per CPU c->freelist
 			 */
 			p[i] = ___slab_alloc(s, flags, NUMA_NO_NODE,
-					    _RET_IP_, c);
+					    _RET_IP_, c, &to_free);
 			if (unlikely(!p[i]))
 				goto error;
 
@@ -3323,6 +3392,7 @@ int kmem_cache_alloc_bulk(struct kmem_cache *s, gfp_t flags, size_t size,
 	}
 	c->tid = next_tid(c->tid);
 	local_irq_enable();
+	free_delayed(&to_free);
 
 	/* Clear memory outside IRQ disabled fastpath loop */
 	if (unlikely(slab_want_init_on_alloc(flags, s))) {
@@ -3337,6 +3407,7 @@ int kmem_cache_alloc_bulk(struct kmem_cache *s, gfp_t flags, size_t size,
 	return i;
 error:
 	local_irq_enable();
+	free_delayed(&to_free);
 	slab_post_alloc_hook(s, objcg, flags, i, p);
 	__kmem_cache_free_bulk(s, i, p);
 	return 0;
@@ -3472,7 +3543,7 @@ static void
 init_kmem_cache_node(struct kmem_cache_node *n)
 {
 	n->nr_partial = 0;
-	spin_lock_init(&n->list_lock);
+	raw_spin_lock_init(&n->list_lock);
 	INIT_LIST_HEAD(&n->partial);
 #ifdef CONFIG_SLUB_DEBUG
 	atomic_long_set(&n->nr_slabs, 0);
@@ -3873,7 +3944,7 @@ static void free_partial(struct kmem_cache *s, struct kmem_cache_node *n)
 	struct page *page, *h;
 
 	BUG_ON(irqs_disabled());
-	spin_lock_irq(&n->list_lock);
+	raw_spin_lock_irq(&n->list_lock);
 	list_for_each_entry_safe(page, h, &n->partial, slab_list) {
 		if (!page->inuse) {
 			remove_partial(n, page);
@@ -3883,7 +3954,7 @@ static void free_partial(struct kmem_cache *s, struct kmem_cache_node *n)
 			  "Objects remaining in %s on __kmem_cache_shutdown()");
 		}
 	}
-	spin_unlock_irq(&n->list_lock);
+	raw_spin_unlock_irq(&n->list_lock);
 
 	list_for_each_entry_safe(page, h, &discard, slab_list)
 		discard_slab(s, page);
@@ -4154,7 +4225,7 @@ int __kmem_cache_shrink(struct kmem_cache *s)
 		for (i = 0; i < SHRINK_PROMOTE_MAX; i++)
 			INIT_LIST_HEAD(promote + i);
 
-		spin_lock_irqsave(&n->list_lock, flags);
+		raw_spin_lock_irqsave(&n->list_lock, flags);
 
 		/*
 		 * Build lists of slabs to discard or promote.
@@ -4185,7 +4256,7 @@ int __kmem_cache_shrink(struct kmem_cache *s)
 		for (i = SHRINK_PROMOTE_MAX - 1; i >= 0; i--)
 			list_splice(promote + i, &n->partial);
 
-		spin_unlock_irqrestore(&n->list_lock, flags);
+		raw_spin_unlock_irqrestore(&n->list_lock, flags);
 
 		/* Release empty slabs */
 		list_for_each_entry_safe(page, t, &discard, slab_list)
@@ -4360,6 +4431,12 @@ void __init kmem_cache_init(void)
 {
 	static __initdata struct kmem_cache boot_kmem_cache,
 		boot_kmem_cache_node;
+	int cpu;
+
+	for_each_possible_cpu(cpu) {
+		raw_spin_lock_init(&per_cpu(slub_free_list, cpu).lock);
+		INIT_LIST_HEAD(&per_cpu(slub_free_list, cpu).list);
+	}
 
 	if (debug_guardpage_minorder())
 		slub_max_order = 0;
@@ -4547,7 +4624,7 @@ static int validate_slab_node(struct kmem_cache *s,
 	struct page *page;
 	unsigned long flags;
 
-	spin_lock_irqsave(&n->list_lock, flags);
+	raw_spin_lock_irqsave(&n->list_lock, flags);
 
 	list_for_each_entry(page, &n->partial, slab_list) {
 		validate_slab(s, page);
@@ -4569,7 +4646,7 @@ static int validate_slab_node(struct kmem_cache *s,
 		       s->name, count, atomic_long_read(&n->nr_slabs));
 
 out:
-	spin_unlock_irqrestore(&n->list_lock, flags);
+	raw_spin_unlock_irqrestore(&n->list_lock, flags);
 	return count;
 }
 
@@ -4620,6 +4697,9 @@ static int alloc_loc_track(struct loc_track *t, unsigned long max, gfp_t flags)
 	struct location *l;
 	int order;
 
+	if (IS_ENABLED(CONFIG_PREEMPT_RT) && flags == GFP_ATOMIC)
+		return 0;
+
 	order = get_order(sizeof(struct location) * max);
 
 	l = (void *)__get_free_pages(flags, order);
@@ -4748,12 +4828,12 @@ static int list_locations(struct kmem_cache *s, char *buf,
 		if (!atomic_long_read(&n->nr_slabs))
 			continue;
 
-		spin_lock_irqsave(&n->list_lock, flags);
+		raw_spin_lock_irqsave(&n->list_lock, flags);
 		list_for_each_entry(page, &n->partial, slab_list)
 			process_slab(&t, s, page, alloc);
 		list_for_each_entry(page, &n->full, slab_list)
 			process_slab(&t, s, page, alloc);
-		spin_unlock_irqrestore(&n->list_lock, flags);
+		raw_spin_unlock_irqrestore(&n->list_lock, flags);
 	}
 
 	for (i = 0; i < t.count; i++) {
diff --git a/mm/vmalloc.c b/mm/vmalloc.c
index fff03a331314..2cbbbdb69ec4 100644
--- a/mm/vmalloc.c
+++ b/mm/vmalloc.c
@@ -1542,7 +1542,7 @@ static void *new_vmap_block(unsigned int order, gfp_t gfp_mask)
 	struct vmap_block *vb;
 	struct vmap_area *va;
 	unsigned long vb_idx;
-	int node, err;
+	int node, err, cpu;
 	void *vaddr;
 
 	node = numa_node_id();
@@ -1579,11 +1579,12 @@ static void *new_vmap_block(unsigned int order, gfp_t gfp_mask)
 		return ERR_PTR(err);
 	}
 
-	vbq = &get_cpu_var(vmap_block_queue);
+	cpu = get_cpu_light();
+	vbq = this_cpu_ptr(&vmap_block_queue);
 	spin_lock(&vbq->lock);
 	list_add_tail_rcu(&vb->free_list, &vbq->free);
 	spin_unlock(&vbq->lock);
-	put_cpu_var(vmap_block_queue);
+	put_cpu_light();
 
 	return vaddr;
 }
@@ -1648,6 +1649,7 @@ static void *vb_alloc(unsigned long size, gfp_t gfp_mask)
 	struct vmap_block *vb;
 	void *vaddr = NULL;
 	unsigned int order;
+	int cpu;
 
 	BUG_ON(offset_in_page(size));
 	BUG_ON(size > PAGE_SIZE*VMAP_MAX_ALLOC);
@@ -1662,7 +1664,8 @@ static void *vb_alloc(unsigned long size, gfp_t gfp_mask)
 	order = get_order(size);
 
 	rcu_read_lock();
-	vbq = &get_cpu_var(vmap_block_queue);
+	cpu = get_cpu_light();
+	vbq = this_cpu_ptr(&vmap_block_queue);
 	list_for_each_entry_rcu(vb, &vbq->free, free_list) {
 		unsigned long pages_off;
 
@@ -1685,7 +1688,7 @@ static void *vb_alloc(unsigned long size, gfp_t gfp_mask)
 		break;
 	}
 
-	put_cpu_var(vmap_block_queue);
+	put_cpu_light();
 	rcu_read_unlock();
 
 	/* Allocate new block if nothing was found */
diff --git a/mm/vmstat.c b/mm/vmstat.c
index 698bc0bc18d1..1e7688a43887 100644
--- a/mm/vmstat.c
+++ b/mm/vmstat.c
@@ -321,6 +321,7 @@ void __mod_zone_page_state(struct zone *zone, enum zone_stat_item item,
 	long x;
 	long t;
 
+	preempt_disable_rt();
 	x = delta + __this_cpu_read(*p);
 
 	t = __this_cpu_read(pcp->stat_threshold);
@@ -330,6 +331,7 @@ void __mod_zone_page_state(struct zone *zone, enum zone_stat_item item,
 		x = 0;
 	}
 	__this_cpu_write(*p, x);
+	preempt_enable_rt();
 }
 EXPORT_SYMBOL(__mod_zone_page_state);
 
@@ -346,6 +348,7 @@ void __mod_node_page_state(struct pglist_data *pgdat, enum node_stat_item item,
 		delta >>= PAGE_SHIFT;
 	}
 
+	preempt_disable_rt();
 	x = delta + __this_cpu_read(*p);
 
 	t = __this_cpu_read(pcp->stat_threshold);
@@ -355,6 +358,7 @@ void __mod_node_page_state(struct pglist_data *pgdat, enum node_stat_item item,
 		x = 0;
 	}
 	__this_cpu_write(*p, x);
+	preempt_enable_rt();
 }
 EXPORT_SYMBOL(__mod_node_page_state);
 
@@ -387,6 +391,7 @@ void __inc_zone_state(struct zone *zone, enum zone_stat_item item)
 	s8 __percpu *p = pcp->vm_stat_diff + item;
 	s8 v, t;
 
+	preempt_disable_rt();
 	v = __this_cpu_inc_return(*p);
 	t = __this_cpu_read(pcp->stat_threshold);
 	if (unlikely(v > t)) {
@@ -395,6 +400,7 @@ void __inc_zone_state(struct zone *zone, enum zone_stat_item item)
 		zone_page_state_add(v + overstep, zone, item);
 		__this_cpu_write(*p, -overstep);
 	}
+	preempt_enable_rt();
 }
 
 void __inc_node_state(struct pglist_data *pgdat, enum node_stat_item item)
@@ -405,6 +411,7 @@ void __inc_node_state(struct pglist_data *pgdat, enum node_stat_item item)
 
 	VM_WARN_ON_ONCE(vmstat_item_in_bytes(item));
 
+	preempt_disable_rt();
 	v = __this_cpu_inc_return(*p);
 	t = __this_cpu_read(pcp->stat_threshold);
 	if (unlikely(v > t)) {
@@ -413,6 +420,7 @@ void __inc_node_state(struct pglist_data *pgdat, enum node_stat_item item)
 		node_page_state_add(v + overstep, pgdat, item);
 		__this_cpu_write(*p, -overstep);
 	}
+	preempt_enable_rt();
 }
 
 void __inc_zone_page_state(struct page *page, enum zone_stat_item item)
@@ -433,6 +441,7 @@ void __dec_zone_state(struct zone *zone, enum zone_stat_item item)
 	s8 __percpu *p = pcp->vm_stat_diff + item;
 	s8 v, t;
 
+	preempt_disable_rt();
 	v = __this_cpu_dec_return(*p);
 	t = __this_cpu_read(pcp->stat_threshold);
 	if (unlikely(v < - t)) {
@@ -441,6 +450,7 @@ void __dec_zone_state(struct zone *zone, enum zone_stat_item item)
 		zone_page_state_add(v - overstep, zone, item);
 		__this_cpu_write(*p, overstep);
 	}
+	preempt_enable_rt();
 }
 
 void __dec_node_state(struct pglist_data *pgdat, enum node_stat_item item)
@@ -451,6 +461,7 @@ void __dec_node_state(struct pglist_data *pgdat, enum node_stat_item item)
 
 	VM_WARN_ON_ONCE(vmstat_item_in_bytes(item));
 
+	preempt_disable_rt();
 	v = __this_cpu_dec_return(*p);
 	t = __this_cpu_read(pcp->stat_threshold);
 	if (unlikely(v < - t)) {
@@ -459,6 +470,7 @@ void __dec_node_state(struct pglist_data *pgdat, enum node_stat_item item)
 		node_page_state_add(v - overstep, pgdat, item);
 		__this_cpu_write(*p, overstep);
 	}
+	preempt_enable_rt();
 }
 
 void __dec_zone_page_state(struct page *page, enum zone_stat_item item)
diff --git a/mm/workingset.c b/mm/workingset.c
index 975a4d2dd02e..c3d098c01052 100644
--- a/mm/workingset.c
+++ b/mm/workingset.c
@@ -432,6 +432,8 @@ static struct list_lru shadow_nodes;
 
 void workingset_update_node(struct xa_node *node)
 {
+	struct address_space *mapping;
+
 	/*
 	 * Track non-empty nodes that contain only shadow entries;
 	 * unlink those that contain pages or are being freed.
@@ -440,7 +442,8 @@ void workingset_update_node(struct xa_node *node)
 	 * already where they should be. The list_empty() test is safe
 	 * as node->private_list is protected by the i_pages lock.
 	 */
-	VM_WARN_ON_ONCE(!irqs_disabled());  /* For __inc_lruvec_page_state */
+	mapping = container_of(node->array, struct address_space, i_pages);
+	lockdep_assert_held(&mapping->i_pages.xa_lock);
 
 	if (node->count && node->count == node->nr_values) {
 		if (list_empty(&node->private_list)) {
diff --git a/mm/z3fold.c b/mm/z3fold.c
index 0152ad9931a8..dacb0d70fa61 100644
--- a/mm/z3fold.c
+++ b/mm/z3fold.c
@@ -623,14 +623,16 @@ static inline void add_to_unbuddied(struct z3fold_pool *pool,
 {
 	if (zhdr->first_chunks == 0 || zhdr->last_chunks == 0 ||
 			zhdr->middle_chunks == 0) {
-		struct list_head *unbuddied = get_cpu_ptr(pool->unbuddied);
-
+		struct list_head *unbuddied;
 		int freechunks = num_free_chunks(zhdr);
+
+		migrate_disable();
+		unbuddied = this_cpu_ptr(pool->unbuddied);
 		spin_lock(&pool->lock);
 		list_add(&zhdr->buddy, &unbuddied[freechunks]);
 		spin_unlock(&pool->lock);
 		zhdr->cpu = smp_processor_id();
-		put_cpu_ptr(pool->unbuddied);
+		migrate_enable();
 	}
 }
 
@@ -880,8 +882,9 @@ static inline struct z3fold_header *__z3fold_alloc(struct z3fold_pool *pool,
 	int chunks = size_to_chunks(size), i;
 
 lookup:
+	migrate_disable();
 	/* First, try to find an unbuddied z3fold page. */
-	unbuddied = get_cpu_ptr(pool->unbuddied);
+	unbuddied = this_cpu_ptr(pool->unbuddied);
 	for_each_unbuddied_list(i, chunks) {
 		struct list_head *l = &unbuddied[i];
 
@@ -899,7 +902,7 @@ static inline struct z3fold_header *__z3fold_alloc(struct z3fold_pool *pool,
 		    !z3fold_page_trylock(zhdr)) {
 			spin_unlock(&pool->lock);
 			zhdr = NULL;
-			put_cpu_ptr(pool->unbuddied);
+			migrate_enable();
 			if (can_sleep)
 				cond_resched();
 			goto lookup;
@@ -913,7 +916,7 @@ static inline struct z3fold_header *__z3fold_alloc(struct z3fold_pool *pool,
 		    test_bit(PAGE_CLAIMED, &page->private)) {
 			z3fold_page_unlock(zhdr);
 			zhdr = NULL;
-			put_cpu_ptr(pool->unbuddied);
+			migrate_enable();
 			if (can_sleep)
 				cond_resched();
 			goto lookup;
@@ -928,7 +931,7 @@ static inline struct z3fold_header *__z3fold_alloc(struct z3fold_pool *pool,
 		kref_get(&zhdr->refcount);
 		break;
 	}
-	put_cpu_ptr(pool->unbuddied);
+	migrate_enable();
 
 	if (!zhdr) {
 		int cpu;
diff --git a/mm/zsmalloc.c b/mm/zsmalloc.c
index 7a0b79b0a689..277d426c881f 100644
--- a/mm/zsmalloc.c
+++ b/mm/zsmalloc.c
@@ -57,6 +57,7 @@
 #include <linux/wait.h>
 #include <linux/pagemap.h>
 #include <linux/fs.h>
+#include <linux/local_lock.h>
 
 #define ZSPAGE_MAGIC	0x58
 
@@ -77,6 +78,20 @@
 
 #define ZS_HANDLE_SIZE (sizeof(unsigned long))
 
+#ifdef CONFIG_PREEMPT_RT
+
+struct zsmalloc_handle {
+	unsigned long addr;
+	struct mutex lock;
+};
+
+#define ZS_HANDLE_ALLOC_SIZE (sizeof(struct zsmalloc_handle))
+
+#else
+
+#define ZS_HANDLE_ALLOC_SIZE (sizeof(unsigned long))
+#endif
+
 /*
  * Object location (<PFN>, <obj_idx>) is encoded as
  * a single (unsigned long) handle value.
@@ -293,6 +308,7 @@ struct zspage {
 };
 
 struct mapping_area {
+	local_lock_t lock;
 	char *vm_buf; /* copy buffer for objects that span pages */
 	char *vm_addr; /* address of kmap_atomic()'ed pages */
 	enum zs_mapmode vm_mm; /* mapping mode */
@@ -322,7 +338,7 @@ static void SetZsPageMovable(struct zs_pool *pool, struct zspage *zspage) {}
 
 static int create_cache(struct zs_pool *pool)
 {
-	pool->handle_cachep = kmem_cache_create("zs_handle", ZS_HANDLE_SIZE,
+	pool->handle_cachep = kmem_cache_create("zs_handle", ZS_HANDLE_ALLOC_SIZE,
 					0, 0, NULL);
 	if (!pool->handle_cachep)
 		return 1;
@@ -346,9 +362,26 @@ static void destroy_cache(struct zs_pool *pool)
 
 static unsigned long cache_alloc_handle(struct zs_pool *pool, gfp_t gfp)
 {
-	return (unsigned long)kmem_cache_alloc(pool->handle_cachep,
-			gfp & ~(__GFP_HIGHMEM|__GFP_MOVABLE));
+	void *p;
+
+	p = kmem_cache_alloc(pool->handle_cachep,
+			     gfp & ~(__GFP_HIGHMEM|__GFP_MOVABLE));
+#ifdef CONFIG_PREEMPT_RT
+	if (p) {
+		struct zsmalloc_handle *zh = p;
+
+		mutex_init(&zh->lock);
+	}
+#endif
+	return (unsigned long)p;
+}
+
+#ifdef CONFIG_PREEMPT_RT
+static struct zsmalloc_handle *zs_get_pure_handle(unsigned long handle)
+{
+	return (void *)(handle &~((1 << OBJ_TAG_BITS) - 1));
 }
+#endif
 
 static void cache_free_handle(struct zs_pool *pool, unsigned long handle)
 {
@@ -368,12 +401,18 @@ static void cache_free_zspage(struct zs_pool *pool, struct zspage *zspage)
 
 static void record_obj(unsigned long handle, unsigned long obj)
 {
+#ifdef CONFIG_PREEMPT_RT
+	struct zsmalloc_handle *zh = zs_get_pure_handle(handle);
+
+	WRITE_ONCE(zh->addr, obj);
+#else
 	/*
 	 * lsb of @obj represents handle lock while other bits
 	 * represent object value the handle is pointing so
 	 * updating shouldn't do store tearing.
 	 */
 	WRITE_ONCE(*(unsigned long *)handle, obj);
+#endif
 }
 
 /* zpool driver */
@@ -455,7 +494,10 @@ MODULE_ALIAS("zpool-zsmalloc");
 #endif /* CONFIG_ZPOOL */
 
 /* per-cpu VM mapping areas for zspage accesses that cross page boundaries */
-static DEFINE_PER_CPU(struct mapping_area, zs_map_area);
+static DEFINE_PER_CPU(struct mapping_area, zs_map_area) = {
+	/* XXX remove this and use a spin_lock_t in pin_tag() */
+	.lock	= INIT_LOCAL_LOCK(lock),
+};
 
 static bool is_zspage_isolated(struct zspage *zspage)
 {
@@ -865,7 +907,13 @@ static unsigned long location_to_obj(struct page *page, unsigned int obj_idx)
 
 static unsigned long handle_to_obj(unsigned long handle)
 {
+#ifdef CONFIG_PREEMPT_RT
+	struct zsmalloc_handle *zh = zs_get_pure_handle(handle);
+
+	return zh->addr;
+#else
 	return *(unsigned long *)handle;
+#endif
 }
 
 static unsigned long obj_to_head(struct page *page, void *obj)
@@ -879,22 +927,46 @@ static unsigned long obj_to_head(struct page *page, void *obj)
 
 static inline int testpin_tag(unsigned long handle)
 {
+#ifdef CONFIG_PREEMPT_RT
+	struct zsmalloc_handle *zh = zs_get_pure_handle(handle);
+
+	return mutex_is_locked(&zh->lock);
+#else
 	return bit_spin_is_locked(HANDLE_PIN_BIT, (unsigned long *)handle);
+#endif
 }
 
 static inline int trypin_tag(unsigned long handle)
 {
+#ifdef CONFIG_PREEMPT_RT
+	struct zsmalloc_handle *zh = zs_get_pure_handle(handle);
+
+	return mutex_trylock(&zh->lock);
+#else
 	return bit_spin_trylock(HANDLE_PIN_BIT, (unsigned long *)handle);
+#endif
 }
 
 static void pin_tag(unsigned long handle) __acquires(bitlock)
 {
+#ifdef CONFIG_PREEMPT_RT
+	struct zsmalloc_handle *zh = zs_get_pure_handle(handle);
+
+	return mutex_lock(&zh->lock);
+#else
 	bit_spin_lock(HANDLE_PIN_BIT, (unsigned long *)handle);
+#endif
 }
 
 static void unpin_tag(unsigned long handle) __releases(bitlock)
 {
+#ifdef CONFIG_PREEMPT_RT
+	struct zsmalloc_handle *zh = zs_get_pure_handle(handle);
+
+	return mutex_unlock(&zh->lock);
+#else
 	bit_spin_unlock(HANDLE_PIN_BIT, (unsigned long *)handle);
+#endif
 }
 
 static void reset_page(struct page *page)
@@ -1278,7 +1350,8 @@ void *zs_map_object(struct zs_pool *pool, unsigned long handle,
 	class = pool->size_class[class_idx];
 	off = (class->size * obj_idx) & ~PAGE_MASK;
 
-	area = &get_cpu_var(zs_map_area);
+	local_lock(&zs_map_area.lock);
+	area = this_cpu_ptr(&zs_map_area);
 	area->vm_mm = mm;
 	if (off + class->size <= PAGE_SIZE) {
 		/* this object is contained entirely within a page */
@@ -1332,7 +1405,7 @@ void zs_unmap_object(struct zs_pool *pool, unsigned long handle)
 
 		__zs_unmap_object(area, pages, off, class->size);
 	}
-	put_cpu_var(zs_map_area);
+	local_unlock(&zs_map_area.lock);
 
 	migrate_read_unlock(zspage);
 	unpin_tag(handle);
diff --git a/mm/zswap.c b/mm/zswap.c
index fbb782924ccc..b24f761b9241 100644
--- a/mm/zswap.c
+++ b/mm/zswap.c
@@ -18,6 +18,7 @@
 #include <linux/highmem.h>
 #include <linux/slab.h>
 #include <linux/spinlock.h>
+#include <linux/local_lock.h>
 #include <linux/types.h>
 #include <linux/atomic.h>
 #include <linux/frontswap.h>
@@ -387,27 +388,37 @@ static struct zswap_entry *zswap_entry_find_get(struct rb_root *root,
 /*********************************
 * per-cpu code
 **********************************/
-static DEFINE_PER_CPU(u8 *, zswap_dstmem);
+struct zswap_comp {
+	/* Used for per-CPU dstmem and tfm */
+	local_lock_t lock;
+	u8 *dstmem;
+};
+
+static DEFINE_PER_CPU(struct zswap_comp, zswap_comp) = {
+	.lock = INIT_LOCAL_LOCK(lock),
+};
 
 static int zswap_dstmem_prepare(unsigned int cpu)
 {
+	struct zswap_comp *zcomp;
 	u8 *dst;
 
 	dst = kmalloc_node(PAGE_SIZE * 2, GFP_KERNEL, cpu_to_node(cpu));
 	if (!dst)
 		return -ENOMEM;
 
-	per_cpu(zswap_dstmem, cpu) = dst;
+	zcomp = per_cpu_ptr(&zswap_comp, cpu);
+	zcomp->dstmem = dst;
 	return 0;
 }
 
 static int zswap_dstmem_dead(unsigned int cpu)
 {
-	u8 *dst;
+	struct zswap_comp *zcomp;
 
-	dst = per_cpu(zswap_dstmem, cpu);
-	kfree(dst);
-	per_cpu(zswap_dstmem, cpu) = NULL;
+	zcomp = per_cpu_ptr(&zswap_comp, cpu);
+	kfree(zcomp->dstmem);
+	zcomp->dstmem = NULL;
 
 	return 0;
 }
@@ -919,10 +930,11 @@ static int zswap_writeback_entry(struct zpool *pool, unsigned long handle)
 		dlen = PAGE_SIZE;
 		src = (u8 *)zhdr + sizeof(struct zswap_header);
 		dst = kmap_atomic(page);
-		tfm = *get_cpu_ptr(entry->pool->tfm);
+		local_lock(&zswap_comp.lock);
+		tfm = *this_cpu_ptr(entry->pool->tfm);
 		ret = crypto_comp_decompress(tfm, src, entry->length,
 					     dst, &dlen);
-		put_cpu_ptr(entry->pool->tfm);
+		local_unlock(&zswap_comp.lock);
 		kunmap_atomic(dst);
 		BUG_ON(ret);
 		BUG_ON(dlen != PAGE_SIZE);
@@ -1074,12 +1086,12 @@ static int zswap_frontswap_store(unsigned type, pgoff_t offset,
 	}
 
 	/* compress */
-	dst = get_cpu_var(zswap_dstmem);
-	tfm = *get_cpu_ptr(entry->pool->tfm);
+	local_lock(&zswap_comp.lock);
+	dst = *this_cpu_ptr(&zswap_comp.dstmem);
+	tfm = *this_cpu_ptr(entry->pool->tfm);
 	src = kmap_atomic(page);
 	ret = crypto_comp_compress(tfm, src, PAGE_SIZE, dst, &dlen);
 	kunmap_atomic(src);
-	put_cpu_ptr(entry->pool->tfm);
 	if (ret) {
 		ret = -EINVAL;
 		goto put_dstmem;
@@ -1103,7 +1115,7 @@ static int zswap_frontswap_store(unsigned type, pgoff_t offset,
 	memcpy(buf, &zhdr, hlen);
 	memcpy(buf + hlen, dst, dlen);
 	zpool_unmap_handle(entry->pool->zpool, handle);
-	put_cpu_var(zswap_dstmem);
+	local_unlock(&zswap_comp.lock);
 
 	/* populate entry */
 	entry->offset = offset;
@@ -1131,7 +1143,7 @@ static int zswap_frontswap_store(unsigned type, pgoff_t offset,
 	return 0;
 
 put_dstmem:
-	put_cpu_var(zswap_dstmem);
+	local_unlock(&zswap_comp.lock);
 	zswap_pool_put(entry->pool);
 freepage:
 	zswap_entry_cache_free(entry);
@@ -1176,9 +1188,10 @@ static int zswap_frontswap_load(unsigned type, pgoff_t offset,
 	if (zpool_evictable(entry->pool->zpool))
 		src += sizeof(struct zswap_header);
 	dst = kmap_atomic(page);
-	tfm = *get_cpu_ptr(entry->pool->tfm);
+	local_lock(&zswap_comp.lock);
+	tfm = *this_cpu_ptr(entry->pool->tfm);
 	ret = crypto_comp_decompress(tfm, src, entry->length, dst, &dlen);
-	put_cpu_ptr(entry->pool->tfm);
+	local_unlock(&zswap_comp.lock);
 	kunmap_atomic(dst);
 	zpool_unmap_handle(entry->pool->zpool, entry->handle);
 	BUG_ON(ret);
diff --git a/net/Kconfig b/net/Kconfig
index d6567162c1cf..05b0f041f039 100644
--- a/net/Kconfig
+++ b/net/Kconfig
@@ -282,7 +282,7 @@ config CGROUP_NET_CLASSID
 
 config NET_RX_BUSY_POLL
 	bool
-	default y
+	default y if !PREEMPT_RT
 
 config BQL
 	bool
diff --git a/net/core/dev.c b/net/core/dev.c
index 75ca6c6d01d6..47876d415eed 100644
--- a/net/core/dev.c
+++ b/net/core/dev.c
@@ -154,6 +154,32 @@
 /* This should be increased if a protocol with a bigger head is added. */
 #define GRO_MAX_HEAD (MAX_HEADER + 128)
 
+static char *__macaddr = NULL;
+core_param(macaddr, __macaddr, charp, 0);
+EXPORT_SYMBOL(__macaddr);
+
+static int pxc_parse_mac(char *mac, unsigned char *macarray)
+{
+	int i, j;
+	short byte1, byte0;
+
+	if (mac == NULL || macarray == NULL)
+		return -1;
+	j = 0;
+	for (i = 0; i < 6; i++) {
+		if ((byte1 = hex_to_bin(mac[j++])) < 0)
+			return -1;
+		if ((byte0 = hex_to_bin(mac[j++])) < 0)
+			return -1;
+		macarray[i] = (unsigned char)(byte1 * 16 + byte0);
+		if (i < 5) {
+			if (mac[j++] != ':')
+				return -1;
+		}
+	}
+	return 0;
+}
+
 static DEFINE_SPINLOCK(ptype_lock);
 static DEFINE_SPINLOCK(offload_lock);
 struct list_head ptype_base[PTYPE_HASH_SIZE] __read_mostly;
@@ -221,14 +247,14 @@ static inline struct hlist_head *dev_index_hash(struct net *net, int ifindex)
 static inline void rps_lock(struct softnet_data *sd)
 {
 #ifdef CONFIG_RPS
-	spin_lock(&sd->input_pkt_queue.lock);
+	raw_spin_lock(&sd->input_pkt_queue.raw_lock);
 #endif
 }
 
 static inline void rps_unlock(struct softnet_data *sd)
 {
 #ifdef CONFIG_RPS
-	spin_unlock(&sd->input_pkt_queue.lock);
+	raw_spin_unlock(&sd->input_pkt_queue.raw_lock);
 #endif
 }
 
@@ -3036,6 +3062,7 @@ static void __netif_reschedule(struct Qdisc *q)
 	sd->output_queue_tailp = &q->next_sched;
 	raise_softirq_irqoff(NET_TX_SOFTIRQ);
 	local_irq_restore(flags);
+	preempt_check_resched_rt();
 }
 
 void __netif_schedule(struct Qdisc *q)
@@ -3098,6 +3125,7 @@ void __dev_kfree_skb_irq(struct sk_buff *skb, enum skb_free_reason reason)
 	__this_cpu_write(softnet_data.completion_queue, skb);
 	raise_softirq_irqoff(NET_TX_SOFTIRQ);
 	local_irq_restore(flags);
+	preempt_check_resched_rt();
 }
 EXPORT_SYMBOL(__dev_kfree_skb_irq);
 
@@ -3765,7 +3793,11 @@ static inline int __dev_xmit_skb(struct sk_buff *skb, struct Qdisc *q,
 	 * This permits qdisc->running owner to get the lock more
 	 * often and dequeue packets faster.
 	 */
+#ifdef CONFIG_PREEMPT_RT
+	contended = true;
+#else
 	contended = qdisc_is_running(q);
+#endif
 	if (unlikely(contended))
 		spin_lock(&q->busylock);
 
@@ -4560,6 +4592,7 @@ static int enqueue_to_backlog(struct sk_buff *skb, int cpu,
 	rps_unlock(sd);
 
 	local_irq_restore(flags);
+	preempt_check_resched_rt();
 
 	atomic_long_inc(&skb->dev->rx_dropped);
 	kfree_skb(skb);
@@ -4775,7 +4808,7 @@ static int netif_rx_internal(struct sk_buff *skb)
 		struct rps_dev_flow voidflow, *rflow = &voidflow;
 		int cpu;
 
-		preempt_disable();
+		migrate_disable();
 		rcu_read_lock();
 
 		cpu = get_rps_cpu(skb->dev, skb, &rflow);
@@ -4785,14 +4818,14 @@ static int netif_rx_internal(struct sk_buff *skb)
 		ret = enqueue_to_backlog(skb, cpu, &rflow->last_qtail);
 
 		rcu_read_unlock();
-		preempt_enable();
+		migrate_enable();
 	} else
 #endif
 	{
 		unsigned int qtail;
 
-		ret = enqueue_to_backlog(skb, get_cpu(), &qtail);
-		put_cpu();
+		ret = enqueue_to_backlog(skb, get_cpu_light(), &qtail);
+		put_cpu_light();
 	}
 	return ret;
 }
@@ -4831,11 +4864,9 @@ int netif_rx_ni(struct sk_buff *skb)
 
 	trace_netif_rx_ni_entry(skb);
 
-	preempt_disable();
+	local_bh_disable();
 	err = netif_rx_internal(skb);
-	if (local_softirq_pending())
-		do_softirq();
-	preempt_enable();
+	local_bh_enable();
 	trace_netif_rx_ni_exit(err);
 
 	return err;
@@ -6278,12 +6309,14 @@ static void net_rps_action_and_irq_enable(struct softnet_data *sd)
 		sd->rps_ipi_list = NULL;
 
 		local_irq_enable();
+		preempt_check_resched_rt();
 
 		/* Send pending IPI's to kick RPS processing on remote cpus. */
 		net_rps_send_ipi(remsd);
 	} else
 #endif
 		local_irq_enable();
+	preempt_check_resched_rt();
 }
 
 static bool sd_has_rps_ipi_waiting(struct softnet_data *sd)
@@ -6361,6 +6394,7 @@ void __napi_schedule(struct napi_struct *n)
 	local_irq_save(flags);
 	____napi_schedule(this_cpu_ptr(&softnet_data), n);
 	local_irq_restore(flags);
+	preempt_check_resched_rt();
 }
 EXPORT_SYMBOL(__napi_schedule);
 
@@ -8320,7 +8354,7 @@ void __dev_set_rx_mode(struct net_device *dev)
 	const struct net_device_ops *ops = dev->netdev_ops;
 
 	/* dev_open will call this function so the list will stay sane. */
-	if (!(dev->flags&IFF_UP))
+	if (!(dev->flags & IFF_UP))
 		return;
 
 	if (!netif_device_present(dev))
@@ -9929,6 +9963,7 @@ int register_netdevice(struct net_device *dev)
 {
 	int ret;
 	struct net *net = dev_net(dev);
+	struct sockaddr *sockaddr;
 
 	BUILD_BUG_ON(sizeof(netdev_features_t) * BITS_PER_BYTE <
 		     NETDEV_FEATURE_COUNT);
@@ -10059,6 +10094,21 @@ int register_netdevice(struct net_device *dev)
 	if (dev->addr_assign_type == NET_ADDR_PERM)
 		memcpy(dev->perm_addr, dev->dev_addr, dev->addr_len);
 
+	/* if we get an mac-addr from the bootloader (in our case barebox),
+	 * we will set this mac-addr overwrite other mac-addr source like
+	 * device-tree.
+	 */
+	if(__macaddr) {
+		sockaddr = kzalloc(sizeof(*sockaddr), GFP_KERNEL);
+		pxc_parse_mac(__macaddr, sockaddr->sa_data);
+
+		ret = eth_mac_addr(dev, sockaddr);
+		if(ret)
+			pr_warn("error: cant set given mac: %s, using random-mac\n", __macaddr);
+
+		kfree(sockaddr);
+	}
+
 	/* Notify protocols, that a new device appeared. */
 	ret = call_netdevice_notifiers(NETDEV_REGISTER, dev);
 	ret = notifier_to_errno(ret);
@@ -10892,6 +10942,7 @@ static int dev_cpu_dead(unsigned int oldcpu)
 
 	raise_softirq_irqoff(NET_TX_SOFTIRQ);
 	local_irq_enable();
+	preempt_check_resched_rt();
 
 #ifdef CONFIG_RPS
 	remsd = oldsd->rps_ipi_list;
@@ -10905,7 +10956,7 @@ static int dev_cpu_dead(unsigned int oldcpu)
 		netif_rx_ni(skb);
 		input_queue_head_incr(oldsd);
 	}
-	while ((skb = skb_dequeue(&oldsd->input_pkt_queue))) {
+	while ((skb = __skb_dequeue(&oldsd->input_pkt_queue))) {
 		netif_rx_ni(skb);
 		input_queue_head_incr(oldsd);
 	}
@@ -11221,7 +11272,7 @@ static int __init net_dev_init(void)
 
 		INIT_WORK(flush, flush_backlog);
 
-		skb_queue_head_init(&sd->input_pkt_queue);
+		skb_queue_head_init_raw(&sd->input_pkt_queue);
 		skb_queue_head_init(&sd->process_queue);
 #ifdef CONFIG_XFRM_OFFLOAD
 		skb_queue_head_init(&sd->xfrm_backlog);
diff --git a/net/core/gen_estimator.c b/net/core/gen_estimator.c
index 8e582e29a41e..e51f4854d8b2 100644
--- a/net/core/gen_estimator.c
+++ b/net/core/gen_estimator.c
@@ -42,7 +42,7 @@
 struct net_rate_estimator {
 	struct gnet_stats_basic_packed	*bstats;
 	spinlock_t		*stats_lock;
-	seqcount_t		*running;
+	net_seqlock_t		*running;
 	struct gnet_stats_basic_cpu __percpu *cpu_bstats;
 	u8			ewma_log;
 	u8			intvl_log; /* period : (250ms << intvl_log) */
@@ -125,7 +125,7 @@ int gen_new_estimator(struct gnet_stats_basic_packed *bstats,
 		      struct gnet_stats_basic_cpu __percpu *cpu_bstats,
 		      struct net_rate_estimator __rcu **rate_est,
 		      spinlock_t *lock,
-		      seqcount_t *running,
+		      net_seqlock_t *running,
 		      struct nlattr *opt)
 {
 	struct gnet_estimator *parm = nla_data(opt);
@@ -226,7 +226,7 @@ int gen_replace_estimator(struct gnet_stats_basic_packed *bstats,
 			  struct gnet_stats_basic_cpu __percpu *cpu_bstats,
 			  struct net_rate_estimator __rcu **rate_est,
 			  spinlock_t *lock,
-			  seqcount_t *running, struct nlattr *opt)
+			  net_seqlock_t *running, struct nlattr *opt)
 {
 	return gen_new_estimator(bstats, cpu_bstats, rate_est,
 				 lock, running, opt);
diff --git a/net/core/gen_stats.c b/net/core/gen_stats.c
index e491b083b348..ef432cea2e10 100644
--- a/net/core/gen_stats.c
+++ b/net/core/gen_stats.c
@@ -137,7 +137,7 @@ __gnet_stats_copy_basic_cpu(struct gnet_stats_basic_packed *bstats,
 }
 
 void
-__gnet_stats_copy_basic(const seqcount_t *running,
+__gnet_stats_copy_basic(net_seqlock_t *running,
 			struct gnet_stats_basic_packed *bstats,
 			struct gnet_stats_basic_cpu __percpu *cpu,
 			struct gnet_stats_basic_packed *b)
@@ -150,15 +150,15 @@ __gnet_stats_copy_basic(const seqcount_t *running,
 	}
 	do {
 		if (running)
-			seq = read_seqcount_begin(running);
+			seq = net_seq_begin(running);
 		bstats->bytes = b->bytes;
 		bstats->packets = b->packets;
-	} while (running && read_seqcount_retry(running, seq));
+	} while (running && net_seq_retry(running, seq));
 }
 EXPORT_SYMBOL(__gnet_stats_copy_basic);
 
 static int
-___gnet_stats_copy_basic(const seqcount_t *running,
+___gnet_stats_copy_basic(net_seqlock_t *running,
 			 struct gnet_dump *d,
 			 struct gnet_stats_basic_cpu __percpu *cpu,
 			 struct gnet_stats_basic_packed *b,
@@ -204,7 +204,7 @@ ___gnet_stats_copy_basic(const seqcount_t *running,
  * if the room in the socket buffer was not sufficient.
  */
 int
-gnet_stats_copy_basic(const seqcount_t *running,
+gnet_stats_copy_basic(net_seqlock_t *running,
 		      struct gnet_dump *d,
 		      struct gnet_stats_basic_cpu __percpu *cpu,
 		      struct gnet_stats_basic_packed *b)
@@ -228,7 +228,7 @@ EXPORT_SYMBOL(gnet_stats_copy_basic);
  * if the room in the socket buffer was not sufficient.
  */
 int
-gnet_stats_copy_basic_hw(const seqcount_t *running,
+gnet_stats_copy_basic_hw(net_seqlock_t *running,
 			 struct gnet_dump *d,
 			 struct gnet_stats_basic_cpu __percpu *cpu,
 			 struct gnet_stats_basic_packed *b)
diff --git a/net/core/sock.c b/net/core/sock.c
index 727ea1cc633c..8179c8072e6f 100644
--- a/net/core/sock.c
+++ b/net/core/sock.c
@@ -3037,12 +3037,11 @@ void lock_sock_nested(struct sock *sk, int subclass)
 	if (sk->sk_lock.owned)
 		__lock_sock(sk);
 	sk->sk_lock.owned = 1;
-	spin_unlock(&sk->sk_lock.slock);
+	spin_unlock_bh(&sk->sk_lock.slock);
 	/*
 	 * The sk_lock has mutex_lock() semantics here:
 	 */
 	mutex_acquire(&sk->sk_lock.dep_map, subclass, 0, _RET_IP_);
-	local_bh_enable();
 }
 EXPORT_SYMBOL(lock_sock_nested);
 
@@ -3091,12 +3090,11 @@ bool lock_sock_fast(struct sock *sk)
 
 	__lock_sock(sk);
 	sk->sk_lock.owned = 1;
-	spin_unlock(&sk->sk_lock.slock);
+	spin_unlock_bh(&sk->sk_lock.slock);
 	/*
 	 * The sk_lock has mutex_lock() semantics here:
 	 */
 	mutex_acquire(&sk->sk_lock.dep_map, 0, 0, _RET_IP_);
-	local_bh_enable();
 	return true;
 }
 EXPORT_SYMBOL(lock_sock_fast);
diff --git a/net/dsa/Kconfig b/net/dsa/Kconfig
index 1f9b9b11008c..5292135ebeb3 100644
--- a/net/dsa/Kconfig
+++ b/net/dsa/Kconfig
@@ -128,4 +128,10 @@ config NET_DSA_TAG_TRAILER
 	  Say Y or M if you want to enable support for tagging frames at
 	  with a trailed. e.g. Marvell 88E6060.
 
+config NET_DSA_TAG_TAIL
+	tristate "Tag driver for switches using a tail tag"
+	help
+	  Say Y or M if you want to enable support for tagging frames at
+	  with a trailed. e.g. Micrel KSZ8863.
+
 endif
diff --git a/net/dsa/Makefile b/net/dsa/Makefile
index 4f47b2025ff5..bdbce0233b64 100644
--- a/net/dsa/Makefile
+++ b/net/dsa/Makefile
@@ -18,3 +18,4 @@ obj-$(CONFIG_NET_DSA_TAG_OCELOT) += tag_ocelot.o
 obj-$(CONFIG_NET_DSA_TAG_QCA) += tag_qca.o
 obj-$(CONFIG_NET_DSA_TAG_SJA1105) += tag_sja1105.o
 obj-$(CONFIG_NET_DSA_TAG_TRAILER) += tag_trailer.o
+obj-$(CONFIG_NET_DSA_TAG_TAIL) += tag_tail.o
diff --git a/net/dsa/slave.c b/net/dsa/slave.c
index 3bc5ca40c9fb..d6e4844ed369 100644
--- a/net/dsa/slave.c
+++ b/net/dsa/slave.c
@@ -1639,6 +1639,7 @@ static int dsa_slave_phy_connect(struct net_device *slave_dev, int addr)
 {
 	struct dsa_port *dp = dsa_slave_to_port(slave_dev);
 	struct dsa_switch *ds = dp->ds;
+	int res;
 
 	slave_dev->phydev = mdiobus_get_phy(ds->slave_mii_bus, addr);
 	if (!slave_dev->phydev) {
@@ -1646,7 +1647,9 @@ static int dsa_slave_phy_connect(struct net_device *slave_dev, int addr)
 		return -ENODEV;
 	}
 
-	return phylink_connect_phy(dp->pl, slave_dev->phydev);
+	res = phylink_connect_phy(dp->pl, slave_dev->phydev);
+	phy_suspend(slave_dev->phydev);
+	return res;
 }
 
 static int dsa_slave_phy_setup(struct net_device *slave_dev)
diff --git a/net/dsa/tag_tail.c b/net/dsa/tag_tail.c
new file mode 100644
index 000000000000..b2ffa0d2063d
--- /dev/null
+++ b/net/dsa/tag_tail.c
@@ -0,0 +1,107 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/* net/dsa/tag_tail.c - tail tag format handling
+ * from net/dsa/tag_tail.c
+ *
+ * Copyright (c) 2008 Marvell Semiconductor
+ * Copyright (c) 2010 SAGEMCOM
+ *
+ * Karl Beldan <karl.beldan@sagemcom.com>
+ *
+ * Dixit Micrel:
+ *  Tail tag priority settings override DSCP and 8021P settings
+ *  unless tag is null ?
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ */
+
+#include <linux/etherdevice.h>
+#include <linux/list.h>
+#include <linux/netdevice.h>
+#include <linux/slab.h>
+
+#include "dsa_priv.h"
+
+#define TAIL_TAG_LEN 1
+
+#define PORT_TX_PRIO 3
+#define DSA_PHY_OFFSET 1
+
+static struct sk_buff *tail_xmit(struct sk_buff *skb, struct net_device *dev)
+{
+	struct dsa_port *dp = dsa_slave_to_port(dev);
+	int padlen;
+	u8 *tail;
+
+	/* expand skb to min length
+	 * ensure that the dsa tail tag is always at the end, even when
+	 * padding is needed
+	 */
+	padlen = (skb->len < ETH_ZLEN) ? ETH_ZLEN - skb->len : 0;
+
+	if (unlikely(skb_pad(skb, padlen + TAIL_TAG_LEN)))
+		return NULL;
+
+	skb_put(skb, padlen);
+
+	/* tail tag */
+	tail = skb_put(skb, 1);
+
+	tail[0] = (1 << (dp->index - DSA_PHY_OFFSET))
+		  | ((PORT_TX_PRIO & 3) << 2);
+
+	skb->protocol = htons(ETH_P_TAIL);
+
+	return skb;
+}
+
+static struct sk_buff *tail_rcv(struct sk_buff *skb, struct net_device *dev,
+				struct packet_type *pt)
+{
+	u8 *tail;
+	int source_port;
+
+	skb = skb_unshare(skb, GFP_ATOMIC);
+	if (!skb)
+		goto out;
+
+	if (skb_linearize(skb))
+		goto out;
+
+	tail = skb_tail_pointer(skb) - TAIL_TAG_LEN;
+
+	source_port = (tail[0] & 3) + DSA_PHY_OFFSET;
+	if (source_port >= DSA_MAX_PORTS)
+		goto out;
+
+	skb->dev = dsa_master_find_slave(dev, 0, source_port);
+	if (!skb->dev)
+		return NULL;
+
+	pskb_trim_rcsum(skb, skb->len - TAIL_TAG_LEN);
+
+	skb_push(skb, ETH_HLEN);
+	skb->pkt_type = PACKET_HOST;
+	skb->offload_fwd_mark = 1;
+	skb->protocol = eth_type_trans(skb, skb->dev);
+
+	return skb;
+
+out:
+	return NULL;
+}
+
+const struct dsa_device_ops tail_netdev_ops = {
+	.name = "tail",
+	.proto = DSA_TAG_PROTO_KSZ8863,
+	.xmit = tail_xmit,
+	.rcv = tail_rcv,
+	.overhead = TAIL_TAG_LEN,
+};
+
+MODULE_LICENSE("GPL");
+MODULE_ALIAS_DSA_TAG_DRIVER(DSA_TAG_PROTO_KSZ8863);
+
+module_dsa_tag_driver(tail_netdev_ops);
\ No newline at end of file
diff --git a/net/ipv4/inet_hashtables.c b/net/ipv4/inet_hashtables.c
index 45fb450b4522..5fb95030e7c0 100644
--- a/net/ipv4/inet_hashtables.c
+++ b/net/ipv4/inet_hashtables.c
@@ -635,7 +635,9 @@ int __inet_hash(struct sock *sk, struct sock *osk)
 	int err = 0;
 
 	if (sk->sk_state != TCP_LISTEN) {
+		local_bh_disable();
 		inet_ehash_nolisten(sk, osk, NULL);
+		local_bh_enable();
 		return 0;
 	}
 	WARN_ON(!sk_unhashed(sk));
@@ -667,11 +669,8 @@ int inet_hash(struct sock *sk)
 {
 	int err = 0;
 
-	if (sk->sk_state != TCP_CLOSE) {
-		local_bh_disable();
+	if (sk->sk_state != TCP_CLOSE)
 		err = __inet_hash(sk, NULL);
-		local_bh_enable();
-	}
 
 	return err;
 }
@@ -682,17 +681,20 @@ void inet_unhash(struct sock *sk)
 	struct inet_hashinfo *hashinfo = sk->sk_prot->h.hashinfo;
 	struct inet_listen_hashbucket *ilb = NULL;
 	spinlock_t *lock;
+	bool state_listen;
 
 	if (sk_unhashed(sk))
 		return;
 
 	if (sk->sk_state == TCP_LISTEN) {
+		state_listen = true;
 		ilb = &hashinfo->listening_hash[inet_sk_listen_hashfn(sk)];
-		lock = &ilb->lock;
+		spin_lock(&ilb->lock);
 	} else {
+		state_listen = false;
 		lock = inet_ehash_lockp(hashinfo, sk->sk_hash);
+		spin_lock_bh(lock);
 	}
-	spin_lock_bh(lock);
 	if (sk_unhashed(sk))
 		goto unlock;
 
@@ -705,7 +707,10 @@ void inet_unhash(struct sock *sk)
 	__sk_nulls_del_node_init_rcu(sk);
 	sock_prot_inuse_add(sock_net(sk), sk->sk_prot, -1);
 unlock:
-	spin_unlock_bh(lock);
+	if (state_listen)
+		spin_unlock(&ilb->lock);
+	else
+		spin_unlock_bh(lock);
 }
 EXPORT_SYMBOL_GPL(inet_unhash);
 
diff --git a/net/ipv6/inet6_hashtables.c b/net/ipv6/inet6_hashtables.c
index 55c290d55605..9bad345cba9a 100644
--- a/net/ipv6/inet6_hashtables.c
+++ b/net/ipv6/inet6_hashtables.c
@@ -333,11 +333,8 @@ int inet6_hash(struct sock *sk)
 {
 	int err = 0;
 
-	if (sk->sk_state != TCP_CLOSE) {
-		local_bh_disable();
+	if (sk->sk_state != TCP_CLOSE)
 		err = __inet_hash(sk, NULL);
-		local_bh_enable();
-	}
 
 	return err;
 }
diff --git a/net/sched/sch_api.c b/net/sched/sch_api.c
index 5e8e49c4ab5c..c3163d6b919a 100644
--- a/net/sched/sch_api.c
+++ b/net/sched/sch_api.c
@@ -1258,7 +1258,7 @@ static struct Qdisc *qdisc_create(struct net_device *dev,
 		rcu_assign_pointer(sch->stab, stab);
 	}
 	if (tca[TCA_RATE]) {
-		seqcount_t *running;
+		net_seqlock_t *running;
 
 		err = -EOPNOTSUPP;
 		if (sch->flags & TCQ_F_MQROOT) {
diff --git a/net/sched/sch_generic.c b/net/sched/sch_generic.c
index 49eae93d1489..512a39d6edec 100644
--- a/net/sched/sch_generic.c
+++ b/net/sched/sch_generic.c
@@ -553,7 +553,11 @@ struct Qdisc noop_qdisc = {
 	.ops		=	&noop_qdisc_ops,
 	.q.lock		=	__SPIN_LOCK_UNLOCKED(noop_qdisc.q.lock),
 	.dev_queue	=	&noop_netdev_queue,
+#ifdef CONFIG_PREEMPT_RT
+	.running	=	__SEQLOCK_UNLOCKED(noop_qdisc.running),
+#else
 	.running	=	SEQCNT_ZERO(noop_qdisc.running),
+#endif
 	.busylock	=	__SPIN_LOCK_UNLOCKED(noop_qdisc.busylock),
 	.gso_skb = {
 		.next = (struct sk_buff *)&noop_qdisc.gso_skb,
@@ -845,9 +849,15 @@ struct Qdisc *qdisc_alloc(struct netdev_queue *dev_queue,
 	lockdep_set_class(&sch->busylock,
 			  dev->qdisc_tx_busylock ?: &qdisc_tx_busylock);
 
+#ifdef CONFIG_PREEMPT_RT
+	seqlock_init(&sch->running);
+	lockdep_set_class(&sch->running.lock,
+			  dev->qdisc_running_key ?: &qdisc_running_key);
+#else
 	seqcount_init(&sch->running);
 	lockdep_set_class(&sch->running,
 			  dev->qdisc_running_key ?: &qdisc_running_key);
+#endif
 
 	sch->ops = ops;
 	sch->flags = ops->static_flags;
diff --git a/net/sunrpc/svc_xprt.c b/net/sunrpc/svc_xprt.c
index 43cf8dbde898..d5516102491b 100644
--- a/net/sunrpc/svc_xprt.c
+++ b/net/sunrpc/svc_xprt.c
@@ -422,7 +422,7 @@ void svc_xprt_do_enqueue(struct svc_xprt *xprt)
 	if (test_and_set_bit(XPT_BUSY, &xprt->xpt_flags))
 		return;
 
-	cpu = get_cpu();
+	cpu = get_cpu_light();
 	pool = svc_pool_for_cpu(xprt->xpt_server, cpu);
 
 	atomic_long_inc(&pool->sp_stats.packets);
@@ -446,7 +446,7 @@ void svc_xprt_do_enqueue(struct svc_xprt *xprt)
 	rqstp = NULL;
 out_unlock:
 	rcu_read_unlock();
-	put_cpu();
+	put_cpu_light();
 	trace_svc_xprt_do_enqueue(xprt, rqstp);
 }
 EXPORT_SYMBOL_GPL(svc_xprt_do_enqueue);
diff --git a/net/xfrm/xfrm_state.c b/net/xfrm/xfrm_state.c
index 2f1517827995..d8b3385bcf34 100644
--- a/net/xfrm/xfrm_state.c
+++ b/net/xfrm/xfrm_state.c
@@ -44,7 +44,7 @@ static void xfrm_state_gc_task(struct work_struct *work);
  */
 
 static unsigned int xfrm_state_hashmax __read_mostly = 1 * 1024 * 1024;
-static __read_mostly seqcount_t xfrm_state_hash_generation = SEQCNT_ZERO(xfrm_state_hash_generation);
+static __read_mostly seqcount_spinlock_t xfrm_state_hash_generation;
 static struct kmem_cache *xfrm_state_cache __ro_after_init;
 
 static DECLARE_WORK(xfrm_state_gc_work, xfrm_state_gc_task);
@@ -139,6 +139,11 @@ static void xfrm_hash_resize(struct work_struct *work)
 		return;
 	}
 
+	/* XXX - the locking which protects the sequence counter appears
+	 * to be broken here. The sequence counter is global, but the
+	 * spinlock used for the sequence counter write serialization is
+	 * per network namespace...
+	 */
 	spin_lock_bh(&net->xfrm.xfrm_state_lock);
 	write_seqcount_begin(&xfrm_state_hash_generation);
 
@@ -2664,6 +2669,8 @@ int __net_init xfrm_state_init(struct net *net)
 	net->xfrm.state_num = 0;
 	INIT_WORK(&net->xfrm.state_hash_work, xfrm_hash_resize);
 	spin_lock_init(&net->xfrm.xfrm_state_lock);
+	seqcount_spinlock_init(&xfrm_state_hash_generation,
+			       &net->xfrm.xfrm_state_lock);
 	return 0;
 
 out_byspi:
